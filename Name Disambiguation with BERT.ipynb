{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87a13d2a-ad0b-4cf8-99e7-526f83971f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 15:12:55.808409: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 15:12:56.282888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-02 15:12:57.786135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-02 15:12:57.810276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-02 15:12:57.810497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import json\n",
    "import lmdb\n",
    "import logging\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import chain\n",
    "from numpy.random import shuffle\n",
    "from os.path import join\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # 0 = All logs are displayed, including debug information\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-12.4/nvvm/libdevice'\n",
    "tf.config.optimizer.set_jit(False) \n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "# Verify that the configuration is correct\n",
    "print(\"Visible devices:\", tf.config.get_visible_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, model_from_json, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Layer, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c939856-4f3a-4354-8f42-bb652cd11808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory： /home/netdb/project/Name_Disambiguation_BERT\n"
     ]
    }
   ],
   "source": [
    "# Change the working directoryQ to \"/path/to/your/directory\"\n",
    "os.chdir('/home/netdb/project/Name_Disambiguation_BERT')\n",
    "print(\"Current directory：\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc410f3b-a2d4-49dd-b08c-ed2f212958b6",
   "metadata": {},
   "source": [
    "# 1. Preprocessing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef039b6-2c0e-4161-ab01-f532a0a4aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class settings:\n",
    "    # Define settings using class attributes\n",
    "    PROJ_DIR = os.getcwd()  # Get the current working directory as the project directory\n",
    "    DATA_DIR = join(PROJ_DIR, 'data')\n",
    "    OUT_DIR = join(PROJ_DIR, 'out')\n",
    "    EMB_DATA_DIR = join(DATA_DIR, 'emb')\n",
    "    GLOBAL_DATA_DIR = join(DATA_DIR, 'global')\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(settings.OUT_DIR, exist_ok=True)\n",
    "os.makedirs(settings.EMB_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c4f5c23-214a-4e71-a41c-c8c746f9939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_utils:\n",
    "    @staticmethod\n",
    "    def load_json(rfdir, rfname):\n",
    "        with codecs.open(join(rfdir, rfname), 'r', encoding='utf-8') as rf:\n",
    "            return json.load(rf)\n",
    "\n",
    "    @staticmethod\n",
    "    def dump_json(obj, wfpath, wfname, indent=None):\n",
    "        with codecs.open(join(wfpath, wfname), 'w', encoding='utf-8') as wf:\n",
    "            json.dump(obj, wf, ensure_ascii=False, indent=indent)\n",
    "\n",
    "    @staticmethod\n",
    "    def dump_data(obj, wfpath, wfname):\n",
    "        with open(join(wfpath, wfname), 'wb') as wf:\n",
    "            pickle.dump(obj, wf)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(rfpath, rfname):\n",
    "        with open(join(rfpath, rfname), 'rb') as rf:\n",
    "            return pickle.load(rf)\n",
    "\n",
    "    @staticmethod\n",
    "    def serialize_embedding(embedding):\n",
    "        return pickle.dumps(embedding)\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize_embedding(s):\n",
    "        return pickle.loads(s)\n",
    "\n",
    "# Define Singleton decorator\n",
    "class Singleton:\n",
    "    def __init__(self, decorated):\n",
    "        self._decorated = decorated\n",
    "\n",
    "    def Instance(self):\n",
    "        try:\n",
    "            return self._instance\n",
    "        except AttributeError:\n",
    "            self._instance = self._decorated()\n",
    "            return self._instance\n",
    "\n",
    "    def __call__(self):\n",
    "        raise TypeError('Singletons must be accessed through `Instance()`.')\n",
    "\n",
    "    def __instancecheck__(self, inst):\n",
    "        return isinstance(inst, self._decorated)\n",
    "\n",
    "data_utils = data_utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f903bf-f706-4682-ac8f-dcef9406c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_size = 1099511627776\n",
    "\n",
    "class LMDBClient(object):\n",
    "    def __init__(self, name, readonly=False):\n",
    "        try:\n",
    "            # Add the database name to the path here\n",
    "            lmdb_dir = os.path.join(settings.DATA_DIR, 'lmdb', name)\n",
    "            print(f\"LMDB directory: {lmdb_dir}\")  # Confirm that the full path is correct\n",
    "            os.makedirs(lmdb_dir, exist_ok=True)\n",
    "            self.db = lmdb.open(lmdb_dir, map_size=map_size, readonly=readonly)\n",
    "        except Exception as e:\n",
    "            print(f\"LMDB initialization error: {e}\")\n",
    "            raise e\n",
    "            \n",
    "    def get(self, key):\n",
    "        try:\n",
    "            with self.db.begin() as txn:\n",
    "                value = txn.get(key.encode())\n",
    "            if value:\n",
    "                return data_utils.deserialize_embedding(value)\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:  \n",
    "            print(f\"Error retrieving key {key}: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def get_batch(self, keys):\n",
    "        values = []\n",
    "        with self.db.begin() as txn:\n",
    "            for key in keys:\n",
    "                value = txn.get(key.encode())\n",
    "                if value:\n",
    "                    values.append(data_utils.deserialize_embedding(value))\n",
    "        return values\n",
    "\n",
    "    def set(self, key, vector):\n",
    "        with self.db.begin(write=True) as txn:\n",
    "            txn.put(key.encode(\"utf-8\"), data_utils.serialize_embedding(vector))\n",
    "\n",
    "    def set_batch(self, generator):\n",
    "        with self.db.begin(write=True) as txn:\n",
    "            for key, vector in generator:\n",
    "                txn.put(key.encode(\"utf-8\"), data_utils.serialize_embedding(vector))\n",
    "                print(key, self.get(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8788cb38-24a8-430b-b899-c53be3e4d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class string_utils:\n",
    "    punct = set(u''':!),.:;?.]}¢'\"、。〉》」』〕〗〞︰︱︳﹐､﹒\n",
    "    ﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､￠\n",
    "    々‖•·ˇˉ―′’”([{£¥'\"‵〈《「『〔〖（［｛￡￥〝︵︷︹︻\n",
    "    ︽︿﹁﹃﹙﹛﹝（｛“‘_…/''')\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "    @staticmethod\n",
    "    def stem(word):\n",
    "        return string_utils.stemmer.stem(word)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_sentence(text, stemming=False):\n",
    "        for token in string_utils.punct:\n",
    "            text = text.replace(token, \"\")\n",
    "        words = text.split()\n",
    "        if stemming:\n",
    "            stemmed_words = [string_utils.stem(w) for w in words]\n",
    "            words = stemmed_words\n",
    "        return \" \".join(words)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_name(name):\n",
    "        if name is None:\n",
    "            return \"\"\n",
    "        x = [k.strip() for k in name.lower().strip().replace(\".\", \" \").replace(\"-\", \" \").split()]\n",
    "        return \"_\".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7d3fb8-498a-4690-bb38-f91b4c3e1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_utils:\n",
    "    @staticmethod\n",
    "    def transform_feature(data, f_name, k=1):\n",
    "        if isinstance(data, str):\n",
    "            data = data.split()\n",
    "        assert isinstance(data, list), \"Data must be a list\"\n",
    "        features = []\n",
    "        for d in data:\n",
    "            features.append(\"__%s__%s\" % (f_name.upper(), d))\n",
    "        return features\n",
    "\n",
    "    def extract_common_features(self, item):\n",
    "        title_features = self.transform_feature(string_utils.clean_sentence(item[\"title\"], stemming=True).lower(), \"title\")\n",
    "        keywords_features = []\n",
    "        keywords = item.get(\"keywords\")\n",
    "        if keywords:\n",
    "            keywords_features = self.transform_feature([string_utils.clean_name(k) for k in keywords], 'keyword')\n",
    "        venue_features = []\n",
    "        venue_name = item.get('venue', '')\n",
    "        if len(venue_name) > 2:\n",
    "            venue_features = self.transform_feature(string_utils.clean_sentence(venue_name.lower()), \"venue\")\n",
    "        return title_features, keywords_features, venue_features\n",
    "\n",
    "    def extract_author_features(self, item, order=None):\n",
    "        title_features, keywords_features, venue_features = self.extract_common_features(item)\n",
    "        author_features = []\n",
    "        for i, author in enumerate(item[\"authors\"]):\n",
    "            if order is not None and i != order:\n",
    "                continue\n",
    "            name_feature = []\n",
    "            org_features = []\n",
    "            org_name = string_utils.clean_name(author.get(\"org\", \"\"))\n",
    "            if len(org_name) > 2:\n",
    "                org_features.extend(self.transform_feature(org_name, \"org\"))\n",
    "            for j, coauthor in enumerate(item[\"authors\"]):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                coauthor_name = coauthor.get(\"name\", \"\")\n",
    "                coauthor_org = string_utils.clean_name(coauthor.get(\"org\", \"\"))\n",
    "                if len(coauthor_name) > 2:\n",
    "                    name_feature.extend(\n",
    "                        self.transform_feature([string_utils.clean_name(coauthor_name)], \"name\")\n",
    "                    )\n",
    "                if len(coauthor_org) > 2:\n",
    "                    org_features.extend(\n",
    "                        self.transform_feature(string_utils.clean_sentence(coauthor_org.lower()), \"org\")\n",
    "                    )\n",
    "            author_features.append(\n",
    "                name_feature + org_features + title_features + keywords_features + venue_features\n",
    "            )\n",
    "        author_features = list(chain.from_iterable(author_features))\n",
    "        return author_features\n",
    "\n",
    "feature_utils = feature_utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c524d5b3-a947-47e0-a213-2a134ea39d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration log\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "            \n",
    "# Dimensions of embedding vectors, BERT's specific model has fixed dimensions, the 'bert-base-uncased' used is 768\n",
    "EMB_DIM = 768\n",
    "\n",
    "@Singleton\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, name=\"bert-base-uncased\"):\n",
    "        self.model_name = name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(name)\n",
    "        self.model = BertModel.from_pretrained(name)\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    @lru_cache()\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        # Take the first token (CLS token) of the last layer’s hidden state as the representation of the sentence\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "\n",
    "    def process_data_from_lmdb(self):\n",
    "        LMDB_NAME = 'pub_authors_feature'\n",
    "        lc = LMDBClient(LMDB_NAME)\n",
    "        author_cnt = 0\n",
    "        data = []\n",
    "        with lc.db.begin() as txn:\n",
    "            for k in txn.cursor():\n",
    "                author_feature = data_utils.deserialize_embedding(k[1])\n",
    "                if author_cnt % 10000 == 0:\n",
    "                    print(author_cnt, author_feature[0])\n",
    "                author_cnt += 1\n",
    "                random.shuffle(author_feature)\n",
    "                # BERT embeddings can be obtained using each author feature here\n",
    "                embedding = self.get_embedding(\" \".join(author_feature))\n",
    "                data.append(embedding)\n",
    "        return data\n",
    "\n",
    "    def save(self, wf_name):\n",
    "        # Save model and tokenizer\n",
    "        model_path = join(settings.EMB_DATA_DIR, '{}_model'.format(wf_name))\n",
    "        tokenizer_path = join(settings.EMB_DATA_DIR, '{}_tokenizer'.format(wf_name))\n",
    "        self.model.save_pretrained(model_path)\n",
    "        self.tokenizer.save_pretrained(tokenizer_path)\n",
    "\n",
    "    def load(self, name):\n",
    "        # Load model and tokenizer\n",
    "        model_path = join(settings.EMB_DATA_DIR, '{}_model'.format(name))\n",
    "        tokenizer_path = join(settings.EMB_DATA_DIR, '{}_tokenizer'.format(name))\n",
    "        self.model = BertModel.from_pretrained(model_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.model.eval()\n",
    "\n",
    "    def project_embedding(self, tokens, idf=None):\n",
    "        \"\"\"\n",
    "        Weighted average token embedding\n",
    "        :param tokens: input word list\n",
    "        :param idf: IDF dictionary\n",
    "        :return: Obtained weighted average embedding\n",
    "        \"\"\"\n",
    "        vectors = []\n",
    "        sum_weight = 0\n",
    "        for token in tokens:\n",
    "            embedding = self.get_embedding(token)\n",
    "            weight = idf[token] if idf and token in idf else 1\n",
    "            v = embedding * weight\n",
    "            vectors.append(v)\n",
    "            sum_weight += weight\n",
    "        if len(vectors) == 0:\n",
    "            print('all tokens not in BERT models')\n",
    "            return None\n",
    "        emb = torch.sum(torch.stack(vectors), dim=0)\n",
    "        emb /= sum_weight\n",
    "        return emb.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb563b9-916f-4a30-b6eb-c4ce936bde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "def dump_author_features_to_file():\n",
    "    \"\"\"\n",
    "    generate author features by raw publication data and dump to files\n",
    "    author features are defined by his/her paper attributes excluding the author's name\n",
    "    \"\"\"\n",
    "    pubs_dict = data_utils.load_json(settings.GLOBAL_DATA_DIR, 'pubs_raw.json')\n",
    "    print('n_papers', len(pubs_dict))\n",
    "    wf = codecs.open(join(settings.GLOBAL_DATA_DIR, 'author_features.txt'), 'w', encoding='utf-8')\n",
    "    for i, pid in enumerate(pubs_dict):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, datetime.now()-start_time)\n",
    "        paper = pubs_dict[pid]\n",
    "        if \"title\" not in paper or \"authors\" not in paper:\n",
    "            continue\n",
    "        if len(paper[\"authors\"]) > 30:\n",
    "            print(i, pid, len(paper[\"authors\"]))\n",
    "        if len(paper[\"authors\"]) > 100:\n",
    "            continue\n",
    "        n_authors = len(paper.get('authors', []))\n",
    "        for j in range(n_authors):\n",
    "            author_feature = feature_utils.extract_author_features(paper, j)\n",
    "            aid = '{}-{}'.format(pid, j)\n",
    "            wf.write(aid + '\\t' + ' '.join(author_feature) + '\\n')\n",
    "    wf.close()\n",
    "\n",
    "def dump_author_features_to_cache():\n",
    "    \"\"\"\n",
    "    dump author features to lmdb\n",
    "    \"\"\"\n",
    "    LMDB_NAME = 'pub_authors_feature'\n",
    "    lc = LMDBClient(LMDB_NAME)\n",
    "    with codecs.open(join(settings.GLOBAL_DATA_DIR, 'author_features.txt'), 'r', encoding='utf-8') as rf:\n",
    "        for i, line in enumerate(rf):\n",
    "            if i % 1000 == 0:\n",
    "                print('line', i)\n",
    "            items = line.rstrip().split('\\t')\n",
    "            pid_order = items[0]\n",
    "            author_features = items[1].split()\n",
    "            lc.set(pid_order, author_features)\n",
    "\n",
    "def cal_feature_idf():\n",
    "    \"\"\"\n",
    "    calculate word IDF (Inverse document frequency) using publication data\n",
    "    \"\"\"\n",
    "    feature_dir = join(settings.DATA_DIR, 'global')\n",
    "    counter = dd(int)\n",
    "    cnt = 0\n",
    "    LMDB_NAME = 'pub_authors_feature'\n",
    "    lc = LMDBClient(LMDB_NAME)\n",
    "    author_cnt = 0\n",
    "    with lc.db.begin() as txn:\n",
    "        for k in txn.cursor():\n",
    "            features = data_utils.deserialize_embedding(k[1])\n",
    "            if author_cnt % 10000 == 0:\n",
    "                print(author_cnt, features[0], counter.get(features[0]))\n",
    "            author_cnt += 1\n",
    "            for f in features:\n",
    "                cnt += 1\n",
    "                counter[f] += 1\n",
    "    idf = {}\n",
    "    for k in counter:\n",
    "        idf[k] = math.log(cnt / counter[k])\n",
    "    data_utils.dump_data(dict(idf), feature_dir, \"feature_idf.pkl\")\n",
    "\n",
    "def dump_author_embs():\n",
    "    \"\"\"\n",
    "    dump author embedding to lmdb\n",
    "    author embedding is calculated by weighted-average of word vectors with IDF\n",
    "    \"\"\"\n",
    "    emb_model = EmbeddingModel.Instance()\n",
    "    idf = data_utils.load_data(settings.GLOBAL_DATA_DIR, 'feature_idf.pkl')\n",
    "    print('idf loaded')\n",
    "    LMDB_NAME_FEATURE = 'pub_authors_feature'\n",
    "    lc_feature = LMDBClient(LMDB_NAME_FEATURE)\n",
    "    LMDB_NAME_EMB = \"author_100_emb_weighted\"\n",
    "    lc_emb = LMDBClient(LMDB_NAME_EMB)\n",
    "    cnt = 0\n",
    "    with lc_feature.db.begin() as txn:\n",
    "        for k in txn.cursor():\n",
    "            if cnt % 1000 == 0:\n",
    "                print('cnt', cnt, datetime.now()-start_time)\n",
    "            cnt += 1\n",
    "            pid_order = k[0].decode('utf-8')\n",
    "            features = data_utils.deserialize_embedding(k[1])\n",
    "            cur_emb = emb_model.project_embedding(features, idf)\n",
    "            if cur_emb is not None:\n",
    "                lc_emb.set(pid_order, cur_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02fbadd7-3de2-41d3-a80d-9738b87f4e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_papers 203078\n",
      "0 0:00:47.192593\n",
      "252 5b5433e5e1cd8e4e15f76e38 42\n",
      "358 5b5433e8e1cd8e4e15fd6be3 49\n",
      "394 5b5433e9e1cd8e4e15ff4e38 56\n",
      "748 5b5433e6e1cd8e4e15fa9f20 41\n",
      "750 5b5433f7e1cd8e4e151f569d 44\n",
      "978 5b5433f0e1cd8e4e15104ca3 43\n",
      "1000 0:00:47.611963\n",
      "1351 5b5433ebe1cd8e4e150513f2 47\n",
      "2000 0:00:47.887945\n",
      "3000 0:00:48.141939\n",
      "4000 0:00:48.374982\n",
      "4720 5b5433e3e1cd8e4e15f469df 46\n",
      "5000 0:00:48.685206\n",
      "5721 5b5433ede1cd8e4e1509e1c0 36\n",
      "5994 5b5433e6e1cd8e4e15fadcf2 32\n",
      "5997 5b5433f5e1cd8e4e151b4445 33\n",
      "6000 0:00:49.009975\n",
      "7000 0:00:49.290679\n",
      "7034 5b5433e8e1cd8e4e15fd693f 31\n",
      "7881 5b5433eae1cd8e4e1502ca53 34\n",
      "7882 5b5433ede1cd8e4e1508f522 33\n",
      "8000 0:00:49.626557\n",
      "9000 0:00:49.926262\n",
      "9867 5b5433e9e1cd8e4e1500318e 32\n",
      "10000 0:00:50.247870\n",
      "10051 5b5433eae1cd8e4e1501f81a 50\n",
      "10062 5b5433ebe1cd8e4e15053cfd 43\n",
      "10936 5b5433f1e1cd8e4e151129cc 92\n",
      "10966 5b5433eae1cd8e4e1502a624 84\n",
      "11000 0:00:50.760103\n",
      "11580 5b5433e4e1cd8e4e15f6718e 100\n",
      "11670 5b5433e9e1cd8e4e15007eb9 59\n",
      "12000 0:00:51.072238\n",
      "12375 5b5433e4e1cd8e4e15f4ba95 32\n",
      "12428 5b5433f1e1cd8e4e15125275 43\n",
      "12472 5b5433f6e1cd8e4e151ca9ce 38\n",
      "12608 5b5433eee1cd8e4e150c1ca2 33\n",
      "13000 0:00:51.452106\n",
      "13453 5b5433f1e1cd8e4e1511c20d 61\n",
      "13695 5b5433eae1cd8e4e150215cd 99\n",
      "13696 5b5433f7e1cd8e4e151e4d6c 99\n",
      "13697 5b5433f6e1cd8e4e151da499 99\n",
      "13698 5b5433f4e1cd8e4e15187e88 99\n",
      "13699 5b5433eee1cd8e4e150ab179 99\n",
      "13700 5b5433f4e1cd8e4e1517b2eb 99\n",
      "13701 5b5433f1e1cd8e4e1510e434 99\n",
      "13702 5b5433f5e1cd8e4e151ab274 99\n",
      "13703 5b5433f3e1cd8e4e1515296a 99\n",
      "13705 5b5433e6e1cd8e4e15faa944 99\n",
      "13706 5b5433e8e1cd8e4e15fed8a8 99\n",
      "14000 0:00:52.257854\n",
      "14742 5b5433ede1cd8e4e15088b6e 40\n",
      "15000 0:00:52.609890\n",
      "15845 5b5433ebe1cd8e4e1503b66e 37\n",
      "15846 5b5433f4e1cd8e4e151945f3 37\n",
      "15869 5b5433eae1cd8e4e1502555f 35\n",
      "16000 0:00:52.943220\n",
      "16608 5b5433f0e1cd8e4e1510a8b6 31\n",
      "16858 5b5433ece1cd8e4e1506f016 34\n",
      "17000 0:00:53.265654\n",
      "18000 0:00:53.579308\n",
      "18256 5b5433f5e1cd8e4e151a6e22 78\n",
      "18297 5b5433e9e1cd8e4e15004fde 95\n",
      "18457 5b5433e5e1cd8e4e15f899ff 86\n",
      "18458 5b5433e8e1cd8e4e15fddd14 84\n",
      "18459 5b5433f8e1cd8e4e1520e70f 82\n",
      "18460 5b5433f3e1cd8e4e1516fe01 91\n",
      "18521 5b5433f1e1cd8e4e15129ff2 46\n",
      "19000 0:00:54.068187\n",
      "19291 5b5433f6e1cd8e4e151d1a9b 37\n",
      "20000 0:00:54.391686\n",
      "20977 5b5433e8e1cd8e4e15fe1774 37\n",
      "21000 0:00:54.671230\n",
      "21115 5b5433ebe1cd8e4e15041a09 90\n",
      "22000 0:00:55.073664\n",
      "23000 0:00:55.335304\n",
      "23231 5b5433f0e1cd8e4e151098c8 47\n",
      "23970 5b5433f2e1cd8e4e15133c66 55\n",
      "24000 0:00:55.664334\n",
      "24075 5b5433e8e1cd8e4e15fd18e2 36\n",
      "24833 5b5433f4e1cd8e4e1519267a 31\n",
      "25000 0:00:56.015168\n",
      "25092 5b5433f4e1cd8e4e1517956a 48\n",
      "25178 5b5433e8e1cd8e4e15ff0ee3 33\n",
      "25538 5b5433e4e1cd8e4e15f54e2b 61\n",
      "26000 0:00:56.378332\n",
      "27000 0:00:56.638757\n",
      "28000 0:00:56.982045\n",
      "28034 5b5433ebe1cd8e4e15057281 59\n",
      "28615 5b5433eee1cd8e4e150a0c61 51\n",
      "29000 0:00:57.354459\n",
      "29252 5b5433e9e1cd8e4e15ff65d1 85\n",
      "29719 5b5433eee1cd8e4e150a6947 37\n",
      "29720 5b5433efe1cd8e4e150dfd4b 38\n",
      "30000 0:00:57.726462\n",
      "30442 5b5433ebe1cd8e4e15056704 31\n",
      "30636 5b5433f7e1cd8e4e151ea3c4 43\n",
      "31000 0:00:58.038196\n",
      "32000 0:00:58.374044\n",
      "32425 5b5433f1e1cd8e4e1512ce4a 67\n",
      "33000 0:00:58.687692\n",
      "33696 5b5433e9e1cd8e4e1500a1da 48\n",
      "33701 5b5433f8e1cd8e4e15207ae9 74\n",
      "33939 5b5433e9e1cd8e4e15ff8592 57\n",
      "34000 0:00:59.027645\n",
      "35000 0:00:59.339958\n",
      "36000 0:00:59.625432\n",
      "36354 5b5433eae1cd8e4e150303b2 44\n",
      "37000 0:00:59.931292\n",
      "38000 0:01:00.161622\n",
      "38625 5b5433efe1cd8e4e150de3f4 54\n",
      "39000 0:01:00.419717\n",
      "39898 5b5433f2e1cd8e4e151393db 67\n",
      "40000 0:01:00.776540\n",
      "40840 5b5433f2e1cd8e4e15139404 77\n",
      "41000 0:01:01.059630\n",
      "41255 5b5433e9e1cd8e4e1500f33b 32\n",
      "41257 5b5433f5e1cd8e4e151b2a6b 47\n",
      "41707 5b5433e5e1cd8e4e15f71593 93\n",
      "41716 5b5433efe1cd8e4e150c6a95 77\n",
      "41720 5b5433f4e1cd8e4e1518bcf4 96\n",
      "41735 5b5433e5e1cd8e4e15f896e7 53\n",
      "42000 0:01:01.468197\n",
      "42071 5b5433e8e1cd8e4e15fd8cae 34\n",
      "42286 5b5433e7e1cd8e4e15fcdf52 42\n",
      "42305 5b5433f1e1cd8e4e1512f772 34\n",
      "42620 5b5433f4e1cd8e4e15181bc4 36\n",
      "43000 0:01:01.814339\n",
      "43233 5b5433e4e1cd8e4e15f618f5 37\n",
      "44000 0:01:02.164367\n",
      "44686 5b5433eee1cd8e4e150a0b47 32\n",
      "44879 5b5433ece1cd8e4e1506f8bc 31\n",
      "44880 5b5433eee1cd8e4e150b6e60 31\n",
      "45000 0:01:02.544645\n",
      "45615 5b5433ece1cd8e4e15077d6e 38\n",
      "45616 5b5433f3e1cd8e4e15152698 35\n",
      "45617 5b5433f6e1cd8e4e151c7fc4 61\n",
      "45618 5b5433e8e1cd8e4e15fd698e 61\n",
      "45779 5b5433efe1cd8e4e150d0eeb 80\n",
      "46000 0:01:02.911606\n",
      "47000 0:01:03.171656\n",
      "48000 0:01:03.501616\n",
      "49000 0:01:03.811844\n",
      "50000 0:01:04.159169\n",
      "50676 5b5433efe1cd8e4e150e2acd 44\n",
      "51000 0:01:04.494383\n",
      "52000 0:01:04.823089\n",
      "53000 0:01:05.129322\n",
      "53388 5b5433ece1cd8e4e1506066b 36\n",
      "53408 5b5433e6e1cd8e4e15fa3be8 45\n",
      "53614 5b5433ebe1cd8e4e15059d4c 35\n",
      "53718 5b5433f5e1cd8e4e151b21cc 33\n",
      "54000 0:01:05.502862\n",
      "54519 5b5433e9e1cd8e4e15006f37 79\n",
      "54571 5b5433e6e1cd8e4e15fadcb5 76\n",
      "54572 5b5433efe1cd8e4e150e63a7 74\n",
      "55000 0:01:05.950618\n",
      "55543 5b5433f8e1cd8e4e15200b91 33\n",
      "55549 5b5433f1e1cd8e4e1510d330 70\n",
      "55550 5b5433efe1cd8e4e150dc125 42\n",
      "55557 5b5433efe1cd8e4e150c57d2 33\n",
      "55788 5b5433e8e1cd8e4e15fe5046 85\n",
      "56000 0:01:06.316615\n",
      "56993 5b5433ede1cd8e4e1508605f 46\n",
      "57000 0:01:06.636828\n",
      "57010 5b5433e4e1cd8e4e15f5b0ff 86\n",
      "57011 5b5433eae1cd8e4e1502a568 85\n",
      "57413 5b5433e6e1cd8e4e15f9c5d9 54\n",
      "57427 5b5433ebe1cd8e4e15041e60 47\n",
      "57590 5b5433f2e1cd8e4e151447b3 67\n",
      "58000 0:01:07.108997\n",
      "58412 5b5433ede1cd8e4e1509db85 62\n",
      "58457 5b5433e8e1cd8e4e15fd10f7 100\n",
      "59000 0:01:07.507755\n",
      "59550 5b5433efe1cd8e4e150ce018 34\n",
      "59708 5b5433eee1cd8e4e150c2207 75\n",
      "59709 5b5433ece1cd8e4e1506d187 44\n",
      "59712 5b5433ebe1cd8e4e150438a1 55\n",
      "59714 5b5433ebe1cd8e4e1505c425 61\n",
      "59732 5b5433f8e1cd8e4e151fb29c 34\n",
      "59733 5b5433f4e1cd8e4e151801a3 38\n",
      "59734 5b5433ebe1cd8e4e1503da48 38\n",
      "59735 5b5433e5e1cd8e4e15f7b8f3 38\n",
      "60000 0:01:07.979027\n",
      "60352 5b5433efe1cd8e4e150db029 36\n",
      "61000 0:01:08.306459\n",
      "61748 5b5433f1e1cd8e4e15118b7d 40\n",
      "61959 5b5433efe1cd8e4e150cd760 48\n",
      "62000 0:01:08.658229\n",
      "62563 5b5433f3e1cd8e4e15166814 81\n",
      "63000 0:01:08.990058\n",
      "64000 0:01:09.368428\n",
      "65000 0:01:09.697406\n",
      "66000 0:01:10.055379\n",
      "67000 0:01:10.378328\n",
      "67192 5b5433f2e1cd8e4e15140a8c 51\n",
      "67832 5b5433f4e1cd8e4e15181536 32\n",
      "67863 5b5433f7e1cd8e4e151e626d 34\n",
      "68000 0:01:10.720479\n",
      "68076 5b5433e4e1cd8e4e15f5b253 71\n",
      "68116 5b5433e7e1cd8e4e15fc8b46 38\n",
      "68361 5b5433e9e1cd8e4e15007b83 34\n",
      "69000 0:01:11.086510\n",
      "69285 5b5433f2e1cd8e4e151358e8 47\n",
      "69338 5b5433f5e1cd8e4e1519a349 73\n",
      "69486 5b5433f6e1cd8e4e151c176e 39\n",
      "70000 0:01:11.378460\n",
      "70420 5b5433efe1cd8e4e150d591d 35\n",
      "70421 5b5433ebe1cd8e4e1503dd33 32\n",
      "70454 5b5433eee1cd8e4e150b37ba 50\n",
      "70992 5b5433e4e1cd8e4e15f64318 35\n",
      "71000 0:01:11.700957\n",
      "71292 5b5433e9e1cd8e4e150032cf 35\n",
      "71294 5b5433f6e1cd8e4e151d75b5 50\n",
      "72000 0:01:12.037627\n",
      "72364 5b5433eee1cd8e4e150af100 48\n",
      "73000 0:01:12.393875\n",
      "74000 0:01:12.613082\n",
      "75000 0:01:12.900243\n",
      "75005 5b5433f5e1cd8e4e151a2ebe 35\n",
      "75499 5b5433f5e1cd8e4e151a9e4d 33\n",
      "76000 0:01:13.217252\n",
      "76351 5b5433ebe1cd8e4e1503e8e2 65\n",
      "77000 0:01:13.518900\n",
      "77404 5b5433ece1cd8e4e15077d7a 64\n",
      "77580 5b5433f7e1cd8e4e151f257a 41\n",
      "77933 5b5433f8e1cd8e4e152109b5 31\n",
      "78000 0:01:13.876727\n",
      "79000 0:01:14.209697\n",
      "79612 5b5433f1e1cd8e4e15121499 43\n",
      "80000 0:01:14.550288\n",
      "80040 5b5433ece1cd8e4e15069646 46\n",
      "80223 5b5433f1e1cd8e4e1510fcdf 73\n",
      "80410 5b5433f4e1cd8e4e1517ccf2 78\n",
      "80411 5b5433f3e1cd8e4e1516de87 95\n",
      "80412 5b5433f5e1cd8e4e151a4118 93\n",
      "80413 5b5433e4e1cd8e4e15f6792b 93\n",
      "80637 5b5433eae1cd8e4e1503731a 99\n",
      "81000 0:01:15.132168\n",
      "81047 5b5433ebe1cd8e4e1503d018 98\n",
      "82000 0:01:15.503781\n",
      "82912 5b5433f2e1cd8e4e1513a968 67\n",
      "82990 5b5433ebe1cd8e4e1503f9d1 54\n",
      "83000 0:01:15.832691\n",
      "84000 0:01:16.153992\n",
      "85000 0:01:16.338113\n",
      "85556 5b5433e9e1cd8e4e15007ca9 87\n",
      "85557 5b5433f1e1cd8e4e15122d3d 89\n",
      "85558 5b5433f3e1cd8e4e1516ca2a 90\n",
      "85559 5b5433eee1cd8e4e1509f47a 93\n",
      "85633 5b5433ede1cd8e4e15093520 65\n",
      "85674 5b5433f4e1cd8e4e15189015 61\n",
      "85675 5b5433eee1cd8e4e150a27fc 61\n",
      "86000 0:01:16.812009\n",
      "87000 0:01:17.087013\n",
      "88000 0:01:17.310360\n",
      "89000 0:01:17.608774\n",
      "89712 5b5433f6e1cd8e4e151d4087 32\n",
      "90000 0:01:17.913376\n",
      "91000 0:01:18.190828\n",
      "91056 5b5433f6e1cd8e4e151bcbbc 32\n",
      "91459 5b5433efe1cd8e4e150e3a9a 36\n",
      "92000 0:01:18.472422\n",
      "92527 5b5433e7e1cd8e4e15fc8aad 43\n",
      "92533 5b5433f3e1cd8e4e15160837 38\n",
      "93000 0:01:18.842723\n",
      "94000 0:01:19.096488\n",
      "94054 5b5433e9e1cd8e4e15ff60e2 32\n",
      "94055 5b5433eee1cd8e4e150aac9a 32\n",
      "94385 5b5433f1e1cd8e4e15118b5e 86\n",
      "94387 5b5433efe1cd8e4e150e6c18 36\n",
      "94413 5b5433e3e1cd8e4e15f46058 82\n",
      "95000 0:01:19.549008\n",
      "95123 5b5433e7e1cd8e4e15fd0499 76\n",
      "95193 5b5433f8e1cd8e4e151fdf4d 51\n",
      "95389 5b5433f7e1cd8e4e151ea69d 46\n",
      "95810 5b5433e7e1cd8e4e15faf679 76\n",
      "95811 5b5433e7e1cd8e4e15fc7f17 96\n",
      "95812 5b5433f4e1cd8e4e1518f71b 99\n",
      "95813 5b5433f2e1cd8e4e1513edf9 99\n",
      "96000 0:01:20.111855\n",
      "97000 0:01:20.417098\n",
      "98000 0:01:20.719219\n",
      "99000 0:01:21.050485\n",
      "99044 5b5433ede1cd8e4e15090e5f 32\n",
      "99045 5b5433ece1cd8e4e15067e7e 32\n",
      "99094 5b5433f0e1cd8e4e150fc22c 32\n",
      "99291 5b5433f2e1cd8e4e1513d02f 74\n",
      "99628 5b5433f4e1cd8e4e151775b1 63\n",
      "100000 0:01:21.463063\n",
      "101000 0:01:21.703966\n",
      "102000 0:01:22.016558\n",
      "102196 5b5433f4e1cd8e4e15183990 72\n",
      "102384 5b5433e8e1cd8e4e15fdf3f9 50\n",
      "103000 0:01:22.422761\n",
      "104000 0:01:22.684602\n",
      "104462 5b5433f0e1cd8e4e15109aa8 58\n",
      "105000 0:01:23.039673\n",
      "106000 0:01:23.306362\n",
      "107000 0:01:23.575426\n",
      "107781 5b5433e9e1cd8e4e15012dcb 32\n",
      "108000 0:01:23.854373\n",
      "108079 5b5433f6e1cd8e4e151dd19d 58\n",
      "108102 5b5433ece1cd8e4e1507938b 72\n",
      "108111 5b5433f0e1cd8e4e150fd2bc 57\n",
      "109000 0:01:24.251986\n",
      "110000 0:01:24.523637\n",
      "111000 0:01:24.805004\n",
      "111158 5b5433efe1cd8e4e150c90bb 36\n",
      "112000 0:01:25.113072\n",
      "112851 5b5433f4e1cd8e4e15193946 66\n",
      "113000 0:01:25.446988\n",
      "114000 0:01:25.779976\n",
      "115000 0:01:26.052193\n",
      "115244 5b5433efe1cd8e4e150de6e8 93\n",
      "115245 5b5433e8e1cd8e4e15fefa69 86\n",
      "115283 5b5433e6e1cd8e4e15f9305d 58\n",
      "115391 5b5433ebe1cd8e4e15048ce9 64\n",
      "115392 5b5433e8e1cd8e4e15fde01d 62\n",
      "115583 5b5433f5e1cd8e4e151ad500 35\n",
      "115712 5b5433ede1cd8e4e1508fb4c 34\n",
      "116000 0:01:26.506386\n",
      "116425 5b5433f5e1cd8e4e151af906 35\n",
      "116426 5b5433f1e1cd8e4e1512ee35 44\n",
      "116427 5b5433f2e1cd8e4e1513dcb7 42\n",
      "117000 0:01:26.830904\n",
      "117732 5b5433eee1cd8e4e150a0623 32\n",
      "117733 5b5433ede1cd8e4e15097d57 34\n",
      "117736 5b5433eee1cd8e4e150b7c64 73\n",
      "117737 5b5433efe1cd8e4e150d94b9 47\n",
      "117765 5b5433f4e1cd8e4e15189e6d 49\n",
      "117766 5b5433e5e1cd8e4e15f784b7 38\n",
      "117804 5b5433e8e1cd8e4e15fe21e4 74\n",
      "118000 0:01:27.196408\n",
      "119000 0:01:27.506689\n",
      "120000 0:01:27.819314\n",
      "121000 0:01:28.108446\n",
      "121180 5b5433f5e1cd8e4e151a726b 49\n",
      "121181 5b5433e4e1cd8e4e15f61ce9 49\n",
      "121232 5b5433e6e1cd8e4e15f90b13 37\n",
      "122000 0:01:28.440948\n",
      "122890 5b5433e6e1cd8e4e15f99e0d 59\n",
      "123000 0:01:28.756479\n",
      "124000 0:01:29.048849\n",
      "124313 5b5433e5e1cd8e4e15f87d62 95\n",
      "124434 5b5433ebe1cd8e4e15058c06 32\n",
      "124441 5b5433e7e1cd8e4e15fb4fcf 31\n",
      "125000 0:01:29.474949\n",
      "125524 5b5433eae1cd8e4e15033eba 44\n",
      "126000 0:01:29.791754\n",
      "127000 0:01:30.110400\n",
      "128000 0:01:30.367265\n",
      "129000 0:01:30.687332\n",
      "129837 5b5433e5e1cd8e4e15f718ae 38\n",
      "130000 0:01:31.010670\n",
      "130433 5b5433e8e1cd8e4e15fe94bb 40\n",
      "130469 5b5433f6e1cd8e4e151cf408 67\n",
      "130505 5b5433f1e1cd8e4e15123c7f 67\n",
      "130524 5b5433ebe1cd8e4e15043fb2 67\n",
      "130574 5b5433f8e1cd8e4e1520a12a 47\n",
      "130577 5b5433e7e1cd8e4e15fcc238 56\n",
      "130586 5b5433f2e1cd8e4e1513bedb 37\n",
      "130597 5b5433f7e1cd8e4e151f04db 51\n",
      "130650 5b5433efe1cd8e4e150d3dfe 41\n",
      "130689 5b5433f6e1cd8e4e151d2ed8 44\n",
      "130691 5b5433e5e1cd8e4e15f89872 44\n",
      "130699 5b5433ece1cd8e4e1507a774 46\n",
      "131000 0:01:31.548679\n",
      "131254 5b5433f8e1cd8e4e15207317 32\n",
      "131857 5b5433eee1cd8e4e150c0f9a 41\n",
      "132000 0:01:31.899463\n",
      "133000 0:01:32.193213\n",
      "134000 0:01:32.507868\n",
      "134296 5b5433f1e1cd8e4e1511ceb5 38\n",
      "134814 5b5433f0e1cd8e4e150f58c5 32\n",
      "135000 0:01:32.804820\n",
      "136000 0:01:33.058950\n",
      "136694 5b5433e7e1cd8e4e15fc95da 45\n",
      "136962 5b5433f6e1cd8e4e151bd330 35\n",
      "136970 5b5433f7e1cd8e4e151f6977 40\n",
      "136976 5b5433f3e1cd8e4e1515d9ae 39\n",
      "137000 0:01:33.443640\n",
      "137016 5b5433efe1cd8e4e150d661a 46\n",
      "137475 5b5433e9e1cd8e4e15ff6a4d 32\n",
      "137795 5b5433f3e1cd8e4e15153d82 35\n",
      "137831 5b5433e9e1cd8e4e15ff64d7 42\n",
      "138000 0:01:33.794126\n",
      "139000 0:01:34.159753\n",
      "139060 5b5433f3e1cd8e4e15170918 51\n",
      "139061 5b5433f2e1cd8e4e15145ab1 53\n",
      "139062 5b5433eae1cd8e4e1502543c 47\n",
      "139063 5b5433efe1cd8e4e150e731d 49\n",
      "139066 5b5433f7e1cd8e4e151ebe3e 51\n",
      "139067 5b5433eae1cd8e4e150163e1 51\n",
      "139068 5b5433ebe1cd8e4e1504fedb 53\n",
      "139069 5b5433f2e1cd8e4e1513676b 50\n",
      "139070 5b5433f5e1cd8e4e151a75bc 50\n",
      "139258 5b5433e8e1cd8e4e15fda782 34\n",
      "139802 5b5433eee1cd8e4e150a6c5f 40\n",
      "139803 5b5433e6e1cd8e4e15f8df13 52\n",
      "139809 5b5433f5e1cd8e4e151b0f8a 31\n",
      "139813 5b5433f6e1cd8e4e151d193f 56\n",
      "140000 0:01:34.630752\n",
      "140112 5b5433f6e1cd8e4e151d686d 33\n",
      "140124 5b5433e6e1cd8e4e15fa0151 37\n",
      "141000 0:01:35.016342\n",
      "142000 0:01:35.283582\n",
      "142452 5b5433f6e1cd8e4e151d5e28 44\n",
      "142498 5b5433eee1cd8e4e150b99da 47\n",
      "143000 0:01:35.677141\n",
      "143907 5b5433e8e1cd8e4e15fe2916 56\n",
      "144000 0:01:35.967742\n",
      "145000 0:01:36.268172\n",
      "145082 5b5433f8e1cd8e4e151f9f17 41\n",
      "145772 5b5433ece1cd8e4e1506cad5 36\n",
      "145775 5b5433efe1cd8e4e150dfab8 77\n",
      "145777 5b5433f8e1cd8e4e151ff47c 67\n",
      "145779 5b5433e7e1cd8e4e15fcc20d 67\n",
      "145782 5b5433e3e1cd8e4e15f4424e 34\n",
      "145786 5b5433ede1cd8e4e1508b059 49\n",
      "145802 5b5433e8e1cd8e4e15fd69f5 47\n",
      "145868 5b5433efe1cd8e4e150cf651 33\n",
      "146000 0:01:36.711033\n",
      "147000 0:01:36.949147\n",
      "148000 0:01:37.211864\n",
      "148458 5b5433f2e1cd8e4e1513d2f0 77\n",
      "148910 5b5433ebe1cd8e4e15059281 31\n",
      "148911 5b5433eee1cd8e4e150b3749 42\n",
      "149000 0:01:37.592658\n",
      "150000 0:01:37.840077\n",
      "150069 5b5433efe1cd8e4e150e3034 48\n",
      "150874 5b5433f5e1cd8e4e151b868f 31\n",
      "151000 0:01:38.210069\n",
      "151152 5b5433f5e1cd8e4e151a2598 32\n",
      "151154 5b5433eae1cd8e4e1502f02b 31\n",
      "151155 5b5433eee1cd8e4e150c377c 31\n",
      "151649 5b5433e8e1cd8e4e15fd8786 38\n",
      "152000 0:01:38.623952\n",
      "152524 5b5433f5e1cd8e4e1519f71e 41\n",
      "153000 0:01:38.938104\n",
      "154000 0:01:39.261794\n",
      "155000 0:01:39.558110\n",
      "155629 5b5433f0e1cd8e4e150ee4d8 40\n",
      "155630 5b5433f6e1cd8e4e151c73b8 34\n",
      "155963 5b5433e5e1cd8e4e15f75619 33\n",
      "155967 5b5433ebe1cd8e4e15058e26 46\n",
      "156000 0:01:39.899051\n",
      "157000 0:01:40.183736\n",
      "158000 0:01:40.451824\n",
      "159000 0:01:40.760831\n",
      "160000 0:01:41.041398\n",
      "160094 5b5433e6e1cd8e4e15f91a75 54\n",
      "161000 0:01:41.396074\n",
      "161533 5b5433e4e1cd8e4e15f56fd3 35\n",
      "162000 0:01:41.690566\n",
      "163000 0:01:42.000870\n",
      "163319 5b5433eee1cd8e4e150a6c37 45\n",
      "163530 5b5433ece1cd8e4e15060729 33\n",
      "163531 5b5433f3e1cd8e4e15158b99 48\n",
      "163532 5b5433e5e1cd8e4e15f89f3c 37\n",
      "163533 5b5433e8e1cd8e4e15fdc86d 32\n",
      "163535 5b5433f6e1cd8e4e151db061 41\n",
      "163536 5b5433f8e1cd8e4e15200113 32\n",
      "163537 5b5433f2e1cd8e4e151328e9 32\n",
      "163538 5b5433eee1cd8e4e150c21ca 37\n",
      "163539 5b5433e8e1cd8e4e15fe0293 59\n",
      "163540 5b5433efe1cd8e4e150c572b 32\n",
      "163541 5b5433f3e1cd8e4e1516f5e4 36\n",
      "163542 5b5433ede1cd8e4e150862ad 34\n",
      "163543 5b5433f5e1cd8e4e151b8fde 34\n",
      "163544 5b5433f1e1cd8e4e1510e1da 34\n",
      "163545 5b5433ebe1cd8e4e15054a2b 33\n",
      "163546 5b5433eae1cd8e4e15036e4b 36\n",
      "163552 5b5433f5e1cd8e4e151af9c2 31\n",
      "163567 5b5433eae1cd8e4e15016ff9 92\n",
      "163606 5b5433f3e1cd8e4e15171837 67\n",
      "164000 0:01:42.525708\n",
      "165000 0:01:42.874047\n",
      "165178 5b5433e9e1cd8e4e15ffdbb5 34\n",
      "165227 5b5433eee1cd8e4e150ba218 59\n",
      "165659 5b5433e4e1cd8e4e15f4c771 90\n",
      "165667 5b5433efe1cd8e4e150c8ab4 31\n",
      "166000 0:01:43.254200\n",
      "166276 5b5433f8e1cd8e4e151f89f3 34\n",
      "166296 5b5433eee1cd8e4e150b5aac 33\n",
      "167000 0:01:43.608998\n",
      "168000 0:01:43.911187\n",
      "168321 5b5433e3e1cd8e4e15f41d49 53\n",
      "168389 5b5433e7e1cd8e4e15fb513a 38\n",
      "169000 0:01:44.208949\n",
      "169024 5b5433e9e1cd8e4e15ffffbb 33\n",
      "170000 0:01:44.514446\n",
      "170403 5b5433e4e1cd8e4e15f4b70b 36\n",
      "170404 5b5433efe1cd8e4e150e582b 36\n",
      "170405 5b5433f7e1cd8e4e151e1bc0 42\n",
      "171000 0:01:44.906007\n",
      "172000 0:01:45.174703\n",
      "173000 0:01:45.491864\n",
      "174000 0:01:45.800279\n",
      "174902 5b5433f7e1cd8e4e151e108b 46\n",
      "175000 0:01:46.129665\n",
      "176000 0:01:46.363358\n",
      "177000 0:01:46.640115\n",
      "177273 5b5433eae1cd8e4e15038743 38\n",
      "178000 0:01:46.937732\n",
      "178342 5b5433e9e1cd8e4e15007c73 31\n",
      "178770 5b5433f6e1cd8e4e151bfcaf 39\n",
      "178794 5b5433f1e1cd8e4e1510cf99 45\n",
      "179000 0:01:47.232612\n",
      "180000 0:01:47.459695\n",
      "180070 5b5433e3e1cd8e4e15f4569b 79\n",
      "181000 0:01:47.782887\n",
      "182000 0:01:48.097186\n",
      "183000 0:01:48.381952\n",
      "183696 5b5433e5e1cd8e4e15f88f92 42\n",
      "184000 0:01:48.671943\n",
      "184171 5b5433efe1cd8e4e150dfe6f 41\n",
      "184181 5b5433e5e1cd8e4e15f8acf1 41\n",
      "185000 0:01:48.946718\n",
      "186000 0:01:49.255115\n",
      "187000 0:01:49.569264\n",
      "187395 5b5433ede1cd8e4e1509edc3 54\n",
      "188000 0:01:49.920455\n",
      "188404 5b5433efe1cd8e4e150caca4 38\n",
      "189000 0:01:50.405776\n",
      "189553 5b5433f0e1cd8e4e150f46f1 33\n",
      "190000 0:01:50.741263\n",
      "190520 5b5433e7e1cd8e4e15fbe754 99\n",
      "190582 5b5433e7e1cd8e4e15fcb74f 33\n",
      "191000 0:01:51.125333\n",
      "191148 5b5433f8e1cd8e4e151faa7a 84\n",
      "192000 0:01:51.449025\n",
      "192619 5b5433f2e1cd8e4e1514456e 35\n",
      "193000 0:01:51.814935\n",
      "193411 5b5433eee1cd8e4e1509f9de 38\n",
      "194000 0:01:52.151069\n",
      "194775 5b5433ebe1cd8e4e1504f81b 36\n",
      "194933 5b5433e9e1cd8e4e15006a3c 38\n",
      "194961 5b5433ece1cd8e4e150671af 44\n",
      "194977 5b5433f7e1cd8e4e151e6868 39\n",
      "194979 5b5433f3e1cd8e4e1515574d 42\n",
      "195000 0:01:52.629353\n",
      "195011 5b5433f0e1cd8e4e15103603 32\n",
      "196000 0:01:52.966269\n",
      "196079 5b5433e9e1cd8e4e1501029d 36\n",
      "196093 5b5433e9e1cd8e4e1500c38e 93\n",
      "196210 5b5433e8e1cd8e4e15fe79ea 31\n",
      "196214 5b5433f8e1cd8e4e151fccce 42\n",
      "197000 0:01:53.399258\n",
      "198000 0:01:53.718694\n",
      "199000 0:01:54.066416\n",
      "200000 0:01:54.393201\n",
      "200992 5b5433f6e1cd8e4e151d2209 37\n",
      "201000 0:01:54.653963\n",
      "202000 0:01:54.978507\n",
      "203000 0:01:55.320477\n",
      "LMDB directory: /home/netdb/project/Name_Disambiguation_BERT/data/lmdb/pub_authors_feature\n",
      "line 0\n",
      "line 1000\n",
      "line 2000\n",
      "line 3000\n",
      "line 4000\n",
      "line 5000\n",
      "line 6000\n",
      "line 7000\n",
      "line 8000\n",
      "line 9000\n",
      "line 10000\n",
      "line 11000\n",
      "line 12000\n",
      "line 13000\n",
      "line 14000\n",
      "line 15000\n",
      "line 16000\n",
      "line 17000\n",
      "line 18000\n",
      "line 19000\n",
      "line 20000\n",
      "line 21000\n",
      "line 22000\n",
      "line 23000\n",
      "line 24000\n",
      "line 25000\n",
      "line 26000\n",
      "line 27000\n",
      "line 28000\n",
      "line 29000\n",
      "line 30000\n",
      "line 31000\n",
      "line 32000\n",
      "line 33000\n",
      "line 34000\n",
      "line 35000\n",
      "line 36000\n",
      "line 37000\n",
      "line 38000\n",
      "line 39000\n",
      "line 40000\n",
      "line 41000\n",
      "line 42000\n",
      "line 43000\n",
      "line 44000\n",
      "line 45000\n",
      "line 46000\n",
      "line 47000\n",
      "line 48000\n",
      "line 49000\n",
      "line 50000\n",
      "line 51000\n",
      "line 52000\n",
      "line 53000\n",
      "line 54000\n",
      "line 55000\n",
      "line 56000\n",
      "line 57000\n",
      "line 58000\n",
      "line 59000\n",
      "line 60000\n",
      "line 61000\n",
      "line 62000\n",
      "line 63000\n",
      "line 64000\n",
      "line 65000\n",
      "line 66000\n",
      "line 67000\n",
      "line 68000\n",
      "line 69000\n",
      "line 70000\n",
      "line 71000\n",
      "line 72000\n",
      "line 73000\n",
      "line 74000\n",
      "line 75000\n",
      "line 76000\n",
      "line 77000\n",
      "line 78000\n",
      "line 79000\n",
      "line 80000\n",
      "line 81000\n",
      "line 82000\n",
      "line 83000\n",
      "line 84000\n",
      "line 85000\n",
      "line 86000\n",
      "line 87000\n",
      "line 88000\n",
      "line 89000\n",
      "line 90000\n",
      "line 91000\n",
      "line 92000\n",
      "line 93000\n",
      "line 94000\n",
      "line 95000\n",
      "line 96000\n",
      "line 97000\n",
      "line 98000\n",
      "line 99000\n",
      "line 100000\n",
      "line 101000\n",
      "line 102000\n",
      "line 103000\n",
      "line 104000\n",
      "line 105000\n",
      "line 106000\n",
      "line 107000\n",
      "line 108000\n",
      "line 109000\n",
      "line 110000\n",
      "line 111000\n",
      "line 112000\n",
      "line 113000\n",
      "line 114000\n",
      "line 115000\n",
      "line 116000\n",
      "line 117000\n",
      "line 118000\n",
      "line 119000\n",
      "line 120000\n",
      "line 121000\n",
      "line 122000\n",
      "line 123000\n",
      "line 124000\n",
      "line 125000\n",
      "line 126000\n",
      "line 127000\n",
      "line 128000\n",
      "line 129000\n",
      "line 130000\n",
      "line 131000\n",
      "line 132000\n",
      "line 133000\n",
      "line 134000\n",
      "line 135000\n",
      "line 136000\n",
      "line 137000\n",
      "line 138000\n",
      "line 139000\n",
      "line 140000\n",
      "line 141000\n",
      "line 142000\n",
      "line 143000\n",
      "line 144000\n",
      "line 145000\n",
      "line 146000\n",
      "line 147000\n",
      "line 148000\n",
      "line 149000\n",
      "line 150000\n",
      "line 151000\n",
      "line 152000\n",
      "line 153000\n",
      "line 154000\n",
      "line 155000\n",
      "line 156000\n",
      "line 157000\n",
      "line 158000\n",
      "line 159000\n",
      "line 160000\n",
      "line 161000\n",
      "line 162000\n",
      "line 163000\n",
      "line 164000\n",
      "line 165000\n",
      "line 166000\n",
      "line 167000\n",
      "line 168000\n",
      "line 169000\n",
      "line 170000\n",
      "line 171000\n",
      "line 172000\n",
      "line 173000\n",
      "line 174000\n",
      "line 175000\n",
      "line 176000\n",
      "line 177000\n",
      "line 178000\n",
      "line 179000\n",
      "line 180000\n",
      "line 181000\n",
      "line 182000\n",
      "line 183000\n",
      "line 184000\n",
      "line 185000\n",
      "line 186000\n",
      "line 187000\n",
      "line 188000\n",
      "line 189000\n",
      "line 190000\n",
      "line 191000\n",
      "line 192000\n",
      "line 193000\n",
      "line 194000\n",
      "line 195000\n",
      "line 196000\n",
      "line 197000\n",
      "line 198000\n",
      "line 199000\n",
      "line 200000\n",
      "line 201000\n",
      "line 202000\n",
      "line 203000\n",
      "line 204000\n",
      "line 205000\n",
      "line 206000\n",
      "line 207000\n",
      "line 208000\n",
      "line 209000\n",
      "line 210000\n",
      "line 211000\n",
      "line 212000\n",
      "line 213000\n",
      "line 214000\n",
      "line 215000\n",
      "line 216000\n",
      "line 217000\n",
      "line 218000\n",
      "line 219000\n",
      "line 220000\n",
      "line 221000\n",
      "line 222000\n",
      "line 223000\n",
      "line 224000\n",
      "line 225000\n",
      "line 226000\n",
      "line 227000\n",
      "line 228000\n",
      "line 229000\n",
      "line 230000\n",
      "line 231000\n",
      "line 232000\n",
      "line 233000\n",
      "line 234000\n",
      "line 235000\n",
      "line 236000\n",
      "line 237000\n",
      "line 238000\n",
      "line 239000\n",
      "line 240000\n",
      "line 241000\n",
      "line 242000\n",
      "line 243000\n",
      "line 244000\n",
      "line 245000\n",
      "line 246000\n",
      "line 247000\n",
      "line 248000\n",
      "line 249000\n",
      "line 250000\n",
      "line 251000\n",
      "line 252000\n",
      "line 253000\n",
      "line 254000\n",
      "line 255000\n",
      "line 256000\n",
      "line 257000\n",
      "line 258000\n",
      "line 259000\n",
      "line 260000\n",
      "line 261000\n",
      "line 262000\n",
      "line 263000\n",
      "line 264000\n",
      "line 265000\n",
      "line 266000\n",
      "line 267000\n",
      "line 268000\n",
      "line 269000\n",
      "line 270000\n",
      "line 271000\n",
      "line 272000\n",
      "line 273000\n",
      "line 274000\n",
      "line 275000\n",
      "line 276000\n",
      "line 277000\n",
      "line 278000\n",
      "line 279000\n",
      "line 280000\n",
      "line 281000\n",
      "line 282000\n",
      "line 283000\n",
      "line 284000\n",
      "line 285000\n",
      "line 286000\n",
      "line 287000\n",
      "line 288000\n",
      "line 289000\n",
      "line 290000\n",
      "line 291000\n",
      "line 292000\n",
      "line 293000\n",
      "line 294000\n",
      "line 295000\n",
      "line 296000\n",
      "line 297000\n",
      "line 298000\n",
      "line 299000\n",
      "line 300000\n",
      "line 301000\n",
      "line 302000\n",
      "line 303000\n",
      "line 304000\n",
      "line 305000\n",
      "line 306000\n",
      "line 307000\n",
      "line 308000\n",
      "line 309000\n",
      "line 310000\n",
      "line 311000\n",
      "line 312000\n",
      "line 313000\n",
      "line 314000\n",
      "line 315000\n",
      "line 316000\n",
      "line 317000\n",
      "line 318000\n",
      "line 319000\n",
      "line 320000\n",
      "line 321000\n",
      "line 322000\n",
      "line 323000\n",
      "line 324000\n",
      "line 325000\n",
      "line 326000\n",
      "line 327000\n",
      "line 328000\n",
      "line 329000\n",
      "line 330000\n",
      "line 331000\n",
      "line 332000\n",
      "line 333000\n",
      "line 334000\n",
      "line 335000\n",
      "line 336000\n",
      "line 337000\n",
      "line 338000\n",
      "line 339000\n",
      "line 340000\n",
      "line 341000\n",
      "line 342000\n",
      "line 343000\n",
      "line 344000\n",
      "line 345000\n",
      "line 346000\n",
      "line 347000\n",
      "line 348000\n",
      "line 349000\n",
      "line 350000\n",
      "line 351000\n",
      "line 352000\n",
      "line 353000\n",
      "line 354000\n",
      "line 355000\n",
      "line 356000\n",
      "line 357000\n",
      "line 358000\n",
      "line 359000\n",
      "line 360000\n",
      "line 361000\n",
      "line 362000\n",
      "line 363000\n",
      "line 364000\n",
      "line 365000\n",
      "line 366000\n",
      "line 367000\n",
      "line 368000\n",
      "line 369000\n",
      "line 370000\n",
      "line 371000\n",
      "line 372000\n",
      "line 373000\n",
      "line 374000\n",
      "line 375000\n",
      "line 376000\n",
      "line 377000\n",
      "line 378000\n",
      "line 379000\n",
      "line 380000\n",
      "line 381000\n",
      "line 382000\n",
      "line 383000\n",
      "line 384000\n",
      "line 385000\n",
      "line 386000\n",
      "line 387000\n",
      "line 388000\n",
      "line 389000\n",
      "line 390000\n",
      "line 391000\n",
      "line 392000\n",
      "line 393000\n",
      "line 394000\n",
      "line 395000\n",
      "line 396000\n",
      "line 397000\n",
      "line 398000\n",
      "line 399000\n",
      "line 400000\n",
      "line 401000\n",
      "line 402000\n",
      "line 403000\n",
      "line 404000\n",
      "line 405000\n",
      "line 406000\n",
      "line 407000\n",
      "line 408000\n",
      "line 409000\n",
      "line 410000\n",
      "line 411000\n",
      "line 412000\n",
      "line 413000\n",
      "line 414000\n",
      "line 415000\n",
      "line 416000\n",
      "line 417000\n",
      "line 418000\n",
      "line 419000\n",
      "line 420000\n",
      "line 421000\n",
      "line 422000\n",
      "line 423000\n",
      "line 424000\n",
      "line 425000\n",
      "line 426000\n",
      "line 427000\n",
      "line 428000\n",
      "line 429000\n",
      "line 430000\n",
      "line 431000\n",
      "line 432000\n",
      "line 433000\n",
      "line 434000\n",
      "line 435000\n",
      "line 436000\n",
      "line 437000\n",
      "line 438000\n",
      "line 439000\n",
      "line 440000\n",
      "line 441000\n",
      "line 442000\n",
      "line 443000\n",
      "line 444000\n",
      "line 445000\n",
      "line 446000\n",
      "line 447000\n",
      "line 448000\n",
      "line 449000\n",
      "line 450000\n",
      "line 451000\n",
      "line 452000\n",
      "line 453000\n",
      "line 454000\n",
      "line 455000\n",
      "line 456000\n",
      "line 457000\n",
      "line 458000\n",
      "line 459000\n",
      "line 460000\n",
      "line 461000\n",
      "line 462000\n",
      "line 463000\n",
      "line 464000\n",
      "line 465000\n",
      "line 466000\n",
      "line 467000\n",
      "line 468000\n",
      "line 469000\n",
      "line 470000\n",
      "line 471000\n",
      "line 472000\n",
      "line 473000\n",
      "line 474000\n",
      "line 475000\n",
      "line 476000\n",
      "line 477000\n",
      "line 478000\n",
      "line 479000\n",
      "line 480000\n",
      "line 481000\n",
      "line 482000\n",
      "line 483000\n",
      "line 484000\n",
      "line 485000\n",
      "line 486000\n",
      "line 487000\n",
      "line 488000\n",
      "line 489000\n",
      "line 490000\n",
      "line 491000\n",
      "line 492000\n",
      "line 493000\n",
      "line 494000\n",
      "line 495000\n",
      "line 496000\n",
      "line 497000\n",
      "line 498000\n",
      "line 499000\n",
      "line 500000\n",
      "line 501000\n",
      "line 502000\n",
      "line 503000\n",
      "line 504000\n",
      "line 505000\n",
      "line 506000\n",
      "line 507000\n",
      "line 508000\n",
      "line 509000\n",
      "line 510000\n",
      "line 511000\n",
      "line 512000\n",
      "line 513000\n",
      "line 514000\n",
      "line 515000\n",
      "line 516000\n",
      "line 517000\n",
      "line 518000\n",
      "line 519000\n",
      "line 520000\n",
      "line 521000\n",
      "line 522000\n",
      "line 523000\n",
      "line 524000\n",
      "line 525000\n",
      "line 526000\n",
      "line 527000\n",
      "line 528000\n",
      "line 529000\n",
      "line 530000\n",
      "line 531000\n",
      "line 532000\n",
      "line 533000\n",
      "line 534000\n",
      "line 535000\n",
      "line 536000\n",
      "line 537000\n",
      "line 538000\n",
      "line 539000\n",
      "line 540000\n",
      "line 541000\n",
      "line 542000\n",
      "line 543000\n",
      "line 544000\n",
      "line 545000\n",
      "line 546000\n",
      "line 547000\n",
      "line 548000\n",
      "line 549000\n",
      "line 550000\n",
      "line 551000\n",
      "line 552000\n",
      "line 553000\n",
      "line 554000\n",
      "line 555000\n",
      "line 556000\n",
      "line 557000\n",
      "line 558000\n",
      "line 559000\n",
      "line 560000\n",
      "line 561000\n",
      "line 562000\n",
      "line 563000\n",
      "line 564000\n",
      "line 565000\n",
      "line 566000\n",
      "line 567000\n",
      "line 568000\n",
      "line 569000\n",
      "line 570000\n",
      "line 571000\n",
      "line 572000\n",
      "line 573000\n",
      "line 574000\n",
      "line 575000\n",
      "line 576000\n",
      "line 577000\n",
      "line 578000\n",
      "line 579000\n",
      "line 580000\n",
      "line 581000\n",
      "line 582000\n",
      "line 583000\n",
      "line 584000\n",
      "line 585000\n",
      "line 586000\n",
      "line 587000\n",
      "line 588000\n",
      "line 589000\n",
      "line 590000\n",
      "line 591000\n",
      "line 592000\n",
      "line 593000\n",
      "line 594000\n",
      "line 595000\n",
      "line 596000\n",
      "line 597000\n",
      "line 598000\n",
      "line 599000\n",
      "line 600000\n",
      "line 601000\n",
      "line 602000\n",
      "line 603000\n",
      "line 604000\n",
      "line 605000\n",
      "line 606000\n",
      "line 607000\n",
      "line 608000\n",
      "line 609000\n",
      "line 610000\n",
      "line 611000\n",
      "line 612000\n",
      "line 613000\n",
      "line 614000\n",
      "line 615000\n",
      "line 616000\n",
      "line 617000\n",
      "line 618000\n",
      "line 619000\n",
      "line 620000\n",
      "line 621000\n",
      "line 622000\n",
      "line 623000\n",
      "line 624000\n",
      "line 625000\n",
      "line 626000\n",
      "line 627000\n",
      "line 628000\n",
      "line 629000\n",
      "line 630000\n",
      "line 631000\n",
      "line 632000\n",
      "line 633000\n",
      "line 634000\n",
      "line 635000\n",
      "line 636000\n",
      "line 637000\n",
      "line 638000\n",
      "line 639000\n",
      "line 640000\n",
      "line 641000\n",
      "line 642000\n",
      "line 643000\n",
      "line 644000\n",
      "line 645000\n",
      "line 646000\n",
      "line 647000\n",
      "line 648000\n",
      "line 649000\n",
      "line 650000\n",
      "line 651000\n",
      "line 652000\n",
      "line 653000\n",
      "line 654000\n",
      "line 655000\n",
      "line 656000\n",
      "line 657000\n",
      "line 658000\n",
      "line 659000\n",
      "line 660000\n",
      "line 661000\n",
      "line 662000\n",
      "line 663000\n",
      "line 664000\n",
      "line 665000\n",
      "line 666000\n",
      "line 667000\n",
      "line 668000\n",
      "line 669000\n",
      "line 670000\n",
      "line 671000\n",
      "line 672000\n",
      "line 673000\n",
      "line 674000\n",
      "line 675000\n",
      "line 676000\n",
      "line 677000\n",
      "line 678000\n",
      "line 679000\n",
      "line 680000\n",
      "line 681000\n",
      "line 682000\n",
      "line 683000\n",
      "line 684000\n",
      "line 685000\n",
      "line 686000\n",
      "line 687000\n",
      "line 688000\n",
      "line 689000\n",
      "line 690000\n",
      "line 691000\n",
      "line 692000\n",
      "line 693000\n",
      "line 694000\n",
      "line 695000\n",
      "line 696000\n",
      "line 697000\n",
      "line 698000\n",
      "line 699000\n",
      "line 700000\n",
      "line 701000\n",
      "line 702000\n",
      "line 703000\n",
      "line 704000\n",
      "line 705000\n",
      "line 706000\n",
      "line 707000\n",
      "line 708000\n",
      "line 709000\n",
      "line 710000\n",
      "line 711000\n",
      "line 712000\n",
      "line 713000\n",
      "line 714000\n",
      "line 715000\n",
      "line 716000\n",
      "line 717000\n",
      "line 718000\n",
      "line 719000\n",
      "line 720000\n",
      "line 721000\n",
      "line 722000\n",
      "line 723000\n",
      "line 724000\n",
      "line 725000\n",
      "line 726000\n",
      "line 727000\n",
      "line 728000\n",
      "line 729000\n",
      "line 730000\n",
      "line 731000\n",
      "line 732000\n",
      "line 733000\n",
      "line 734000\n",
      "line 735000\n",
      "line 736000\n",
      "line 737000\n",
      "line 738000\n",
      "line 739000\n",
      "line 740000\n",
      "line 741000\n",
      "line 742000\n",
      "line 743000\n",
      "line 744000\n",
      "line 745000\n",
      "line 746000\n",
      "line 747000\n",
      "line 748000\n",
      "line 749000\n",
      "line 750000\n",
      "line 751000\n",
      "line 752000\n",
      "line 753000\n",
      "line 754000\n",
      "line 755000\n",
      "line 756000\n",
      "line 757000\n",
      "line 758000\n",
      "line 759000\n",
      "line 760000\n",
      "line 761000\n",
      "line 762000\n",
      "line 763000\n",
      "line 764000\n",
      "line 765000\n",
      "line 766000\n",
      "line 767000\n",
      "line 768000\n",
      "line 769000\n",
      "line 770000\n",
      "line 771000\n",
      "line 772000\n",
      "line 773000\n",
      "line 774000\n",
      "line 775000\n",
      "line 776000\n",
      "line 777000\n",
      "line 778000\n",
      "line 779000\n",
      "line 780000\n",
      "line 781000\n",
      "line 782000\n",
      "line 783000\n",
      "line 784000\n",
      "line 785000\n",
      "line 786000\n",
      "line 787000\n",
      "line 788000\n",
      "line 789000\n",
      "line 790000\n",
      "line 791000\n",
      "line 792000\n",
      "line 793000\n",
      "line 794000\n",
      "line 795000\n",
      "line 796000\n",
      "line 797000\n",
      "line 798000\n",
      "line 799000\n",
      "line 800000\n",
      "line 801000\n",
      "line 802000\n",
      "line 803000\n",
      "line 804000\n",
      "line 805000\n",
      "line 806000\n",
      "line 807000\n",
      "line 808000\n",
      "line 809000\n",
      "line 810000\n",
      "line 811000\n",
      "line 812000\n",
      "line 813000\n",
      "line 814000\n",
      "line 815000\n",
      "line 816000\n",
      "line 817000\n",
      "line 818000\n",
      "line 819000\n",
      "line 820000\n",
      "line 821000\n",
      "line 822000\n",
      "line 823000\n",
      "line 824000\n",
      "line 825000\n",
      "line 826000\n",
      "line 827000\n",
      "line 828000\n",
      "line 829000\n",
      "line 830000\n",
      "line 831000\n",
      "line 832000\n",
      "line 833000\n",
      "line 834000\n",
      "line 835000\n",
      "line 836000\n",
      "line 837000\n",
      "line 838000\n",
      "line 839000\n",
      "line 840000\n",
      "line 841000\n",
      "line 842000\n",
      "line 843000\n",
      "line 844000\n",
      "line 845000\n",
      "line 846000\n",
      "line 847000\n",
      "line 848000\n",
      "line 849000\n",
      "line 850000\n",
      "line 851000\n",
      "line 852000\n",
      "line 853000\n",
      "line 854000\n",
      "line 855000\n",
      "line 856000\n",
      "line 857000\n",
      "line 858000\n",
      "line 859000\n",
      "line 860000\n",
      "line 861000\n",
      "line 862000\n",
      "line 863000\n",
      "line 864000\n",
      "line 865000\n",
      "line 866000\n",
      "line 867000\n",
      "line 868000\n",
      "line 869000\n",
      "line 870000\n",
      "line 871000\n",
      "line 872000\n",
      "line 873000\n",
      "line 874000\n",
      "line 875000\n",
      "line 876000\n",
      "line 877000\n",
      "line 878000\n",
      "line 879000\n",
      "line 880000\n",
      "line 881000\n",
      "line 882000\n",
      "line 883000\n",
      "line 884000\n",
      "line 885000\n",
      "line 886000\n",
      "line 887000\n",
      "line 888000\n",
      "line 889000\n",
      "line 890000\n",
      "line 891000\n",
      "line 892000\n",
      "line 893000\n",
      "line 894000\n",
      "line 895000\n",
      "line 896000\n",
      "line 897000\n",
      "line 898000\n",
      "line 899000\n",
      "line 900000\n",
      "line 901000\n",
      "line 902000\n",
      "line 903000\n",
      "line 904000\n",
      "line 905000\n",
      "line 906000\n",
      "line 907000\n",
      "line 908000\n",
      "line 909000\n",
      "line 910000\n",
      "line 911000\n",
      "line 912000\n",
      "line 913000\n",
      "line 914000\n",
      "line 915000\n",
      "line 916000\n",
      "line 917000\n",
      "line 918000\n",
      "line 919000\n",
      "line 920000\n",
      "line 921000\n",
      "line 922000\n",
      "line 923000\n",
      "line 924000\n",
      "line 925000\n",
      "line 926000\n",
      "line 927000\n",
      "line 928000\n",
      "line 929000\n",
      "line 930000\n",
      "line 931000\n",
      "line 932000\n",
      "line 933000\n",
      "line 934000\n",
      "line 935000\n",
      "line 936000\n",
      "line 937000\n",
      "line 938000\n",
      "line 939000\n",
      "line 940000\n",
      "line 941000\n",
      "line 942000\n",
      "line 943000\n",
      "line 944000\n",
      "line 945000\n",
      "line 946000\n",
      "line 947000\n",
      "line 948000\n",
      "line 949000\n",
      "line 950000\n",
      "line 951000\n",
      "line 952000\n",
      "line 953000\n",
      "line 954000\n",
      "line 955000\n",
      "line 956000\n",
      "line 957000\n",
      "line 958000\n",
      "line 959000\n",
      "line 960000\n",
      "line 961000\n",
      "line 962000\n",
      "line 963000\n",
      "line 964000\n",
      "line 965000\n",
      "line 966000\n",
      "line 967000\n",
      "line 968000\n",
      "line 969000\n",
      "line 970000\n",
      "line 971000\n",
      "line 972000\n",
      "line 973000\n",
      "line 974000\n",
      "line 975000\n",
      "line 976000\n",
      "line 977000\n",
      "line 978000\n",
      "line 979000\n",
      "line 980000\n",
      "line 981000\n",
      "line 982000\n",
      "line 983000\n",
      "line 984000\n",
      "line 985000\n",
      "line 986000\n",
      "line 987000\n",
      "line 988000\n",
      "line 989000\n",
      "line 990000\n",
      "line 991000\n",
      "line 992000\n",
      "line 993000\n",
      "line 994000\n",
      "line 995000\n",
      "line 996000\n",
      "line 997000\n",
      "line 998000\n",
      "line 999000\n",
      "line 1000000\n",
      "line 1001000\n",
      "line 1002000\n",
      "line 1003000\n",
      "line 1004000\n",
      "line 1005000\n",
      "line 1006000\n",
      "line 1007000\n",
      "line 1008000\n",
      "line 1009000\n",
      "line 1010000\n",
      "line 1011000\n",
      "line 1012000\n",
      "line 1013000\n",
      "line 1014000\n",
      "line 1015000\n",
      "line 1016000\n",
      "line 1017000\n",
      "line 1018000\n",
      "line 1019000\n",
      "line 1020000\n",
      "line 1021000\n",
      "line 1022000\n",
      "line 1023000\n",
      "line 1024000\n",
      "line 1025000\n",
      "line 1026000\n",
      "line 1027000\n",
      "line 1028000\n",
      "line 1029000\n",
      "line 1030000\n",
      "line 1031000\n",
      "line 1032000\n",
      "line 1033000\n",
      "line 1034000\n",
      "line 1035000\n",
      "line 1036000\n",
      "line 1037000\n",
      "line 1038000\n",
      "line 1039000\n",
      "line 1040000\n",
      "line 1041000\n",
      "line 1042000\n",
      "line 1043000\n",
      "line 1044000\n",
      "line 1045000\n",
      "line 1046000\n",
      "line 1047000\n",
      "line 1048000\n",
      "line 1049000\n",
      "line 1050000\n",
      "line 1051000\n",
      "line 1052000\n",
      "line 1053000\n",
      "line 1054000\n",
      "line 1055000\n",
      "line 1056000\n",
      "line 1057000\n",
      "line 1058000\n",
      "line 1059000\n",
      "line 1060000\n",
      "line 1061000\n",
      "line 1062000\n",
      "line 1063000\n",
      "line 1064000\n",
      "line 1065000\n",
      "line 1066000\n",
      "line 1067000\n",
      "line 1068000\n",
      "line 1069000\n",
      "line 1070000\n",
      "line 1071000\n",
      "line 1072000\n",
      "line 1073000\n",
      "line 1074000\n",
      "line 1075000\n",
      "line 1076000\n",
      "line 1077000\n",
      "line 1078000\n",
      "line 1079000\n",
      "line 1080000\n",
      "line 1081000\n",
      "line 1082000\n",
      "line 1083000\n",
      "line 1084000\n",
      "line 1085000\n",
      "line 1086000\n",
      "line 1087000\n",
      "line 1088000\n",
      "line 1089000\n",
      "line 1090000\n",
      "line 1091000\n",
      "line 1092000\n",
      "line 1093000\n",
      "line 1094000\n",
      "line 1095000\n",
      "line 1096000\n",
      "line 1097000\n",
      "line 1098000\n",
      "line 1099000\n",
      "line 1100000\n",
      "line 1101000\n",
      "line 1102000\n",
      "line 1103000\n",
      "line 1104000\n",
      "line 1105000\n",
      "line 1106000\n",
      "line 1107000\n",
      "line 1108000\n",
      "line 1109000\n",
      "line 1110000\n",
      "line 1111000\n",
      "line 1112000\n",
      "line 1113000\n",
      "line 1114000\n",
      "line 1115000\n",
      "line 1116000\n",
      "line 1117000\n",
      "line 1118000\n",
      "line 1119000\n",
      "line 1120000\n",
      "line 1121000\n"
     ]
    }
   ],
   "source": [
    "dump_author_features_to_file()  # Generate author characteristics and save to file\n",
    "dump_author_features_to_cache()  # Export author characteristics to LMDB cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b879190-f120-44bd-acb7-0b5f96b50c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB directory: /home/netdb/project/Name_Disambiguation_BERT/data/lmdb/pub_authors_feature\n",
      "0 __NAME__guoliang_li None\n",
      "10000 __NAME__xiu_ping_zhang 4\n",
      "20000 __NAME__ning_hu 1\n",
      "30000 __NAME__lin_huang 78\n",
      "40000 __NAME__ming_fu_zhao None\n",
      "50000 __NAME__gentao_cao 6\n",
      "60000 __NAME__m_wu 104\n",
      "70000 __NAME__fenghuang_zhan 5\n",
      "80000 __NAME__dong_wang 125\n",
      "90000 __NAME__guangpin_chen None\n",
      "100000 __NAME__xiangrui_jiang 12\n",
      "110000 __NAME__xiao_huang 1\n",
      "120000 __NAME__liqiang_cui None\n",
      "130000 __NAME__jianjun_yang 240\n",
      "140000 __NAME__zhuyuan_wang 60\n",
      "150000 __NAME__baocheng_hu 8\n",
      "160000 __NAME__lin_wang 283\n",
      "170000 __NAME__kui_jiao 10\n",
      "180000 __NAME__rong_yu 275\n",
      "190000 __NAME__yehui_yang 2\n",
      "200000 __NAME__yu_guo 730\n",
      "210000 __NAME__li_dai 17\n",
      "220000 __NAME__jianhua_zhang 50\n",
      "230000 __NAME__xian_du 2\n",
      "240000 __NAME__yi_feng 70\n",
      "250000 __NAME__xian_jin_tang 2\n",
      "260000 __NAME__rui_zhou 83\n",
      "270000 __NAME__xiqing_chen 2\n",
      "280000 __NAME__w_chen 196\n",
      "290000 __NAME__linghong_liang 12\n",
      "300000 __NAME__zhen_yu_li 383\n",
      "310000 __NAME__yulei_hou 63\n",
      "320000 __NAME__chi_ming_chung 16\n",
      "330000 __NAME__qi_wang 656\n",
      "340000 __NAME__jinling_zhao 9\n",
      "350000 __NAME__ying_zhang 725\n",
      "360000 __NAME__yan_yao 69\n",
      "370000 __NAME__zhongju_chen 112\n",
      "380000 __NAME__mingxiang_kong 1\n",
      "390000 __NAME__yanjun_zhang 936\n",
      "400000 __NAME__hao_wu 462\n",
      "410000 __NAME__d_rumble 85\n",
      "420000 __NAME__l_z_ouyang 97\n",
      "430000 __NAME__chih_ming_lin 303\n",
      "440000 __NAME__s_h_liang 59\n",
      "450000 __NAME__s_h_wang 62\n",
      "460000 __NAME__ting_guo 43\n",
      "470000 __NAME__ruyi_zheng 2\n",
      "480000 __NAME__chunguang_wei 30\n",
      "490000 __NAME__xiao_jing_li 87\n",
      "500000 __NAME__tao_hai_li 8\n",
      "510000 __NAME__zhi_qiang_wu 30\n",
      "520000 __NAME__jie_gan None\n",
      "530000 __NAME__yu_lung_lo 253\n",
      "540000 __NAME__shehzad_afzal 13\n",
      "550000 __NAME__ming_liang_ji None\n",
      "560000 __NAME__a_c_m_lin 3\n",
      "570000 __NAME__lixiao_zhang 5\n",
      "580000 __NAME__shijie_liu 508\n",
      "590000 __NAME__c_h_hsieh 68\n",
      "600000 __NAME__jing_zhang 1751\n",
      "610000 __NAME__minshen_wang None\n",
      "620000 __NAME__s_qiu 25\n",
      "630000 __NAME__teng_shao 7\n",
      "640000 __NAME__zhi_gang_yang 54\n",
      "650000 __NAME__jiansheng_wu 98\n",
      "660000 __NAME__dongyu_xu 57\n",
      "670000 __NAME__ming_yang 388\n",
      "680000 __NAME__minoru_isobe 9\n",
      "690000 __NAME__tao_peng 426\n",
      "700000 __NAME__zhibin_wang 551\n",
      "710000 __NAME__s_lin 1034\n",
      "720000 __NAME__zixing_lu 18\n",
      "730000 __NAME__wei_peng 141\n",
      "740000 __NAME__chunjing_hu None\n",
      "750000 __NAME__xiaojuan_bai 4\n",
      "760000 __NAME__wa_gao 39\n",
      "770000 __NAME__chenwen_lin 8\n",
      "780000 __NAME__lige_song 6\n",
      "790000 __NAME__andrew_h_paterson 375\n",
      "800000 __NAME__bojiang_yang 3\n",
      "810000 __NAME__fengnian_wei 1\n",
      "820000 __NAME__tian_xia 1556\n",
      "830000 __NAME__lunyong_zhang 3\n",
      "840000 __NAME__c_huang 142\n",
      "850000 __NAME__junhui_xing 3\n",
      "860000 __NAME__dan_liu 821\n",
      "870000 __NAME__fawu_wang None\n",
      "880000 __NAME__liang_cong 4\n",
      "890000 __NAME__yanwu_yang 4\n",
      "900000 __NAME__h_t_quan 18\n",
      "910000 __NAME__zhicheng_zhang 36\n",
      "920000 __NAME__minjie_lu 190\n",
      "930000 __NAME__b_gao 48\n",
      "940000 __NAME__yan_qun_yi 3\n",
      "950000 __NAME__di_chen 2621\n",
      "960000 __NAME__dong_xie 214\n",
      "970000 __NAME__tobia_zordan 9\n",
      "980000 __NAME__kun_yang 460\n",
      "990000 __NAME__yu_sheng_wang 1090\n",
      "1000000 __NAME__dan_chen 3713\n",
      "1010000 __NAME__chuanwen_gu None\n",
      "1020000 __NAME__wen_you_li 6\n",
      "1030000 __NAME__huan_li 3347\n",
      "1040000 __NAME__chunyun_wan 2\n",
      "1050000 __NAME__li_hua_yu 73\n",
      "1060000 __NAME__ding_ding 72\n",
      "1070000 __NAME__chuanzhi_liu 8\n",
      "1080000 __NAME__xiaoyan_dai 20\n",
      "1090000 __NAME__yujian_zhang 20\n",
      "1100000 __NAME__kun_zhang 510\n",
      "1110000 __NAME__tingyong_jiang 30\n",
      "1120000 __NAME__fanliao_wang 12\n"
     ]
    }
   ],
   "source": [
    "# Calculate IDF and export\n",
    "cal_feature_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e9657ab-0667-44e9-a884-2cff61962fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf loaded\n",
      "LMDB directory: /home/netdb/project/Name_Disambiguation_BERT/data/lmdb/pub_authors_feature\n",
      "LMDB directory: /home/netdb/project/Name_Disambiguation_BERT/data/lmdb/author_100_emb_weighted\n",
      "cnt 0 3:27:15.822908\n",
      "cnt 1000 3:29:32.345855\n",
      "cnt 2000 3:31:46.393450\n",
      "cnt 3000 3:33:59.199200\n",
      "cnt 4000 3:36:17.853232\n",
      "cnt 5000 3:38:27.516465\n",
      "cnt 6000 3:40:39.877542\n",
      "cnt 7000 3:42:52.218178\n",
      "cnt 8000 3:44:53.406388\n",
      "cnt 9000 3:46:44.224302\n",
      "cnt 10000 3:55:20.407536\n",
      "cnt 11000 3:57:16.712885\n",
      "cnt 12000 3:59:18.276801\n",
      "cnt 13000 4:01:19.076367\n",
      "cnt 14000 4:03:15.867377\n",
      "cnt 15000 4:05:15.824586\n",
      "cnt 16000 4:07:13.426399\n",
      "cnt 17000 4:09:14.988720\n",
      "cnt 18000 4:11:23.659170\n",
      "cnt 19000 4:13:27.046744\n",
      "cnt 20000 4:15:41.153685\n",
      "cnt 21000 4:25:55.195940\n",
      "cnt 22000 4:28:24.135301\n",
      "cnt 23000 4:30:55.680782\n",
      "cnt 24000 4:33:26.945444\n",
      "cnt 25000 4:35:58.939260\n",
      "cnt 26000 4:38:24.044127\n",
      "cnt 27000 4:40:55.806190\n",
      "cnt 28000 4:43:28.221740\n",
      "cnt 29000 4:45:52.438826\n",
      "cnt 30000 4:48:23.388849\n",
      "cnt 31000 4:50:54.277282\n",
      "cnt 32000 4:53:23.393779\n",
      "cnt 33000 4:55:53.873264\n",
      "cnt 34000 4:58:13.964117\n",
      "cnt 35000 5:00:41.179038\n",
      "cnt 36000 5:03:06.735181\n",
      "cnt 37000 5:05:31.542181\n",
      "cnt 38000 5:07:56.620890\n",
      "cnt 39000 5:10:29.117156\n",
      "cnt 40000 5:12:54.382734\n",
      "cnt 41000 5:15:24.023279\n",
      "cnt 42000 5:17:58.760615\n",
      "cnt 43000 5:22:32.303998\n",
      "cnt 44000 5:30:09.522390\n",
      "cnt 45000 5:32:42.072580\n",
      "cnt 46000 5:35:05.620421\n",
      "cnt 47000 5:37:29.351873\n",
      "cnt 48000 5:39:53.295772\n",
      "cnt 49000 5:42:10.359460\n",
      "cnt 50000 5:44:28.927369\n",
      "cnt 51000 5:46:50.076061\n",
      "cnt 52000 5:49:12.024898\n",
      "cnt 53000 5:51:35.857632\n",
      "cnt 54000 5:53:51.388437\n",
      "cnt 55000 5:56:10.201733\n",
      "cnt 56000 5:58:25.433419\n",
      "cnt 57000 6:00:43.072424\n",
      "cnt 58000 6:02:56.314417\n",
      "cnt 59000 6:04:59.181022\n",
      "cnt 60000 6:07:06.869917\n",
      "cnt 61000 6:09:12.503910\n",
      "cnt 62000 6:11:28.760800\n",
      "cnt 63000 6:13:44.885348\n",
      "cnt 64000 6:16:10.539042\n",
      "cnt 65000 6:18:28.788299\n",
      "cnt 66000 6:20:54.746110\n",
      "cnt 67000 6:23:12.473589\n",
      "cnt 68000 6:25:27.901733\n",
      "cnt 69000 6:27:31.753558\n",
      "cnt 70000 6:29:35.269247\n",
      "cnt 71000 6:31:43.439782\n",
      "cnt 72000 6:33:51.482846\n",
      "cnt 73000 6:36:00.667943\n",
      "cnt 74000 6:38:12.466265\n",
      "cnt 75000 6:40:17.184690\n",
      "cnt 76000 6:42:16.976930\n",
      "cnt 77000 6:44:27.839573\n",
      "cnt 78000 6:46:23.759367\n",
      "cnt 79000 6:48:29.208588\n",
      "cnt 80000 6:50:34.369550\n",
      "cnt 81000 6:52:43.203195\n",
      "cnt 82000 6:54:48.594615\n",
      "cnt 83000 6:57:01.208917\n",
      "cnt 84000 6:59:04.031879\n",
      "cnt 85000 7:01:13.356026\n",
      "cnt 86000 7:03:21.575925\n",
      "cnt 87000 7:05:21.493886\n",
      "cnt 88000 7:07:38.806300\n",
      "cnt 89000 7:09:47.685147\n",
      "cnt 90000 7:11:58.210491\n",
      "cnt 91000 7:14:09.771626\n",
      "cnt 92000 7:16:21.845742\n",
      "cnt 93000 7:18:42.109810\n",
      "cnt 94000 7:21:04.192359\n",
      "cnt 95000 7:23:29.222222\n",
      "cnt 96000 7:25:51.602099\n",
      "cnt 97000 7:28:08.591123\n",
      "cnt 98000 7:30:35.098579\n",
      "cnt 99000 7:32:58.536198\n",
      "cnt 100000 7:35:20.457118\n",
      "cnt 101000 7:37:44.390823\n",
      "cnt 102000 7:40:05.537498\n",
      "cnt 103000 7:42:25.855998\n",
      "cnt 104000 7:44:50.041606\n",
      "cnt 105000 7:47:05.845280\n",
      "cnt 106000 7:49:26.291569\n",
      "cnt 107000 7:51:40.455523\n",
      "cnt 108000 7:53:58.946322\n",
      "cnt 109000 7:56:15.884461\n",
      "cnt 110000 7:58:36.155344\n",
      "cnt 111000 8:00:50.164057\n",
      "cnt 112000 8:12:12.285586\n",
      "cnt 113000 8:14:23.440038\n",
      "cnt 114000 8:16:36.993408\n",
      "cnt 115000 8:18:43.446404\n",
      "cnt 116000 8:21:03.848562\n",
      "cnt 117000 8:23:16.413260\n",
      "cnt 118000 8:25:32.808393\n",
      "cnt 119000 8:27:46.924546\n",
      "cnt 120000 8:29:55.759587\n",
      "cnt 121000 8:32:12.623168\n",
      "cnt 122000 8:34:21.617249\n",
      "cnt 123000 8:36:25.760800\n",
      "cnt 124000 8:38:38.100632\n",
      "cnt 125000 8:40:51.316064\n",
      "cnt 126000 8:43:02.810784\n",
      "cnt 127000 8:45:12.220880\n",
      "cnt 128000 8:47:24.722254\n",
      "cnt 129000 8:49:37.769206\n",
      "cnt 130000 8:51:45.852223\n",
      "cnt 131000 8:53:58.088804\n",
      "cnt 132000 8:56:08.664786\n",
      "cnt 133000 8:58:17.839919\n",
      "cnt 134000 9:00:26.359734\n",
      "cnt 135000 9:02:39.647168\n",
      "cnt 136000 9:04:49.452460\n",
      "cnt 137000 9:07:01.243568\n",
      "cnt 138000 9:09:09.568737\n",
      "cnt 139000 9:11:09.691091\n",
      "cnt 140000 9:13:13.298675\n",
      "cnt 141000 9:15:19.358671\n",
      "cnt 142000 9:17:29.845281\n",
      "cnt 143000 9:19:39.786889\n",
      "cnt 144000 9:21:55.408310\n",
      "cnt 145000 9:24:05.492591\n",
      "cnt 146000 9:26:13.738041\n",
      "cnt 147000 9:28:24.102937\n",
      "cnt 148000 9:30:37.942413\n",
      "cnt 149000 9:32:45.055946\n",
      "cnt 150000 9:34:53.550393\n",
      "cnt 151000 9:37:04.616364\n",
      "cnt 152000 9:39:17.087195\n",
      "cnt 153000 9:41:27.377285\n",
      "cnt 154000 9:43:37.456711\n",
      "cnt 155000 9:45:41.370893\n",
      "cnt 156000 9:47:50.876018\n",
      "cnt 157000 9:49:58.287444\n",
      "cnt 158000 9:52:02.289705\n",
      "cnt 159000 9:54:15.232165\n",
      "cnt 160000 9:56:23.898404\n",
      "cnt 161000 9:58:32.977661\n",
      "cnt 162000 10:00:35.457877\n",
      "cnt 163000 10:02:45.093084\n",
      "cnt 164000 10:04:57.070373\n",
      "cnt 165000 10:07:07.169571\n",
      "cnt 166000 10:20:31.024874\n",
      "cnt 167000 10:22:56.007563\n",
      "cnt 168000 10:25:29.196012\n",
      "cnt 169000 10:28:01.845048\n",
      "cnt 170000 10:30:29.861407\n",
      "cnt 171000 10:32:52.956945\n",
      "cnt 172000 10:35:21.672783\n",
      "cnt 173000 10:37:48.875000\n",
      "cnt 174000 10:40:11.057473\n",
      "cnt 175000 10:42:39.890085\n",
      "cnt 176000 10:45:09.834791\n",
      "cnt 177000 10:47:33.846371\n",
      "cnt 178000 10:49:47.058805\n",
      "cnt 179000 10:52:02.447343\n",
      "cnt 180000 10:54:07.965943\n",
      "cnt 181000 10:56:12.870648\n",
      "cnt 182000 10:58:11.576170\n",
      "cnt 183000 11:00:10.817981\n",
      "cnt 184000 11:02:10.390034\n",
      "cnt 185000 11:04:11.939346\n",
      "cnt 186000 11:06:25.369444\n",
      "cnt 187000 11:08:55.107498\n",
      "cnt 188000 11:11:23.803353\n",
      "cnt 189000 11:13:59.268168\n",
      "cnt 190000 11:16:30.511553\n",
      "cnt 191000 11:19:02.760143\n",
      "cnt 192000 11:21:32.989565\n",
      "cnt 193000 11:24:10.658645\n",
      "cnt 194000 11:26:43.511709\n",
      "cnt 195000 11:29:21.151378\n",
      "cnt 196000 11:31:48.788992\n",
      "cnt 197000 11:34:00.266008\n",
      "cnt 198000 11:36:11.460725\n",
      "cnt 199000 11:38:28.091917\n",
      "cnt 200000 11:40:39.809462\n",
      "cnt 201000 11:42:56.109129\n",
      "cnt 202000 11:45:00.875481\n",
      "cnt 203000 11:47:05.147368\n",
      "cnt 204000 11:49:06.754716\n",
      "cnt 205000 11:51:08.648519\n",
      "cnt 206000 11:53:06.178857\n",
      "cnt 207000 11:55:04.317151\n",
      "cnt 208000 11:57:08.118971\n",
      "cnt 209000 11:59:08.681842\n",
      "cnt 210000 12:01:11.233345\n",
      "cnt 211000 12:03:07.564804\n",
      "cnt 212000 12:05:10.576676\n",
      "cnt 213000 12:07:11.403879\n",
      "cnt 214000 12:09:13.183451\n",
      "cnt 215000 12:11:13.396287\n",
      "cnt 216000 12:13:18.130239\n",
      "cnt 217000 12:15:33.333359\n",
      "cnt 218000 12:17:41.020917\n",
      "cnt 219000 12:19:56.066873\n",
      "cnt 220000 12:22:05.640951\n",
      "cnt 221000 12:24:10.300525\n",
      "cnt 222000 12:26:18.548039\n",
      "cnt 223000 12:28:34.460330\n",
      "cnt 224000 12:30:39.497799\n",
      "cnt 225000 12:32:39.388502\n",
      "cnt 226000 12:45:20.462985\n",
      "cnt 227000 12:47:23.102730\n",
      "cnt 228000 12:49:32.285330\n",
      "cnt 229000 12:51:43.829101\n",
      "cnt 230000 12:53:38.217027\n",
      "cnt 231000 12:55:34.941462\n",
      "cnt 232000 12:57:36.972647\n",
      "cnt 233000 12:59:31.526168\n",
      "cnt 234000 13:01:31.127183\n",
      "cnt 235000 13:03:26.088172\n",
      "cnt 236000 13:05:16.314710\n",
      "cnt 237000 13:07:23.525531\n",
      "cnt 238000 13:09:37.206995\n",
      "cnt 239000 13:11:56.615542\n",
      "cnt 240000 13:13:53.933320\n",
      "cnt 241000 13:15:48.008396\n",
      "cnt 242000 13:17:48.116000\n",
      "cnt 243000 13:20:00.214070\n",
      "cnt 244000 13:21:58.470956\n",
      "cnt 245000 13:23:52.309001\n",
      "cnt 246000 13:25:45.756660\n",
      "cnt 247000 13:27:44.963735\n",
      "cnt 248000 13:29:38.115656\n",
      "cnt 249000 13:31:33.701720\n",
      "cnt 250000 13:33:33.107822\n",
      "cnt 251000 13:35:28.948470\n",
      "cnt 252000 13:44:33.898534\n",
      "cnt 253000 13:46:37.741038\n",
      "cnt 254000 13:48:43.312412\n",
      "cnt 255000 13:50:51.932902\n",
      "cnt 256000 13:52:58.606922\n",
      "cnt 257000 13:55:06.433028\n",
      "cnt 258000 13:57:13.006555\n",
      "cnt 259000 13:59:23.459262\n",
      "cnt 260000 14:01:28.775428\n",
      "cnt 261000 14:03:53.338098\n",
      "cnt 262000 14:06:25.239180\n",
      "cnt 263000 14:08:51.322819\n",
      "cnt 264000 14:11:20.248952\n",
      "cnt 265000 14:13:36.144877\n",
      "cnt 266000 14:16:03.067105\n",
      "cnt 267000 14:18:35.251161\n",
      "cnt 268000 14:21:01.956710\n",
      "cnt 269000 14:23:31.796061\n",
      "cnt 270000 14:26:01.496386\n",
      "cnt 271000 14:28:26.213970\n",
      "cnt 272000 14:31:00.254446\n",
      "cnt 273000 14:33:28.371592\n",
      "cnt 274000 14:36:04.141751\n",
      "cnt 275000 14:38:33.389999\n",
      "cnt 276000 14:40:56.041757\n",
      "cnt 277000 14:43:06.805406\n",
      "cnt 278000 14:45:02.995953\n",
      "cnt 279000 14:46:52.914624\n",
      "cnt 280000 14:48:44.186559\n",
      "cnt 281000 14:50:34.060725\n",
      "cnt 282000 14:52:34.407581\n",
      "cnt 283000 14:54:31.453128\n",
      "cnt 284000 15:04:30.151409\n",
      "cnt 285000 15:06:34.012674\n",
      "cnt 286000 15:08:36.028187\n",
      "cnt 287000 15:10:37.166244\n",
      "cnt 288000 15:12:36.426336\n",
      "cnt 289000 15:14:44.756419\n",
      "cnt 290000 15:16:52.255341\n",
      "cnt 291000 15:18:59.629894\n",
      "cnt 292000 15:20:49.231405\n",
      "cnt 293000 15:22:47.507093\n",
      "cnt 294000 15:24:43.857607\n",
      "cnt 295000 15:26:33.555555\n",
      "cnt 296000 15:28:28.625206\n",
      "cnt 297000 15:30:25.896342\n",
      "cnt 298000 15:32:51.858283\n",
      "cnt 299000 15:35:13.735565\n",
      "cnt 300000 15:37:15.719757\n",
      "cnt 301000 15:39:28.926819\n",
      "cnt 302000 15:41:37.748718\n",
      "cnt 303000 15:43:57.999983\n",
      "cnt 304000 15:46:21.342287\n",
      "cnt 305000 15:48:44.282194\n",
      "cnt 306000 15:51:08.218215\n",
      "cnt 307000 15:53:31.692426\n",
      "cnt 308000 15:55:41.717391\n",
      "cnt 309000 15:57:43.083168\n",
      "cnt 310000 15:59:51.041653\n",
      "cnt 311000 16:01:57.760447\n",
      "cnt 312000 16:04:01.462756\n",
      "cnt 313000 16:06:04.121381\n",
      "cnt 314000 16:08:05.773314\n",
      "cnt 315000 16:10:17.340312\n",
      "cnt 316000 16:12:22.263737\n",
      "cnt 317000 16:23:18.165562\n",
      "cnt 318000 16:25:41.013920\n",
      "cnt 319000 16:28:00.436461\n",
      "cnt 320000 16:30:19.740574\n",
      "cnt 321000 16:32:36.283445\n",
      "cnt 322000 16:34:51.423998\n",
      "cnt 323000 16:37:06.517886\n",
      "cnt 324000 16:39:22.726129\n",
      "cnt 325000 16:41:42.472898\n",
      "cnt 326000 16:43:58.032617\n",
      "cnt 327000 16:46:11.371734\n",
      "cnt 328000 16:48:25.405305\n",
      "cnt 329000 16:50:37.507151\n",
      "cnt 330000 16:52:57.508633\n",
      "cnt 331000 16:55:17.191005\n",
      "cnt 332000 16:57:33.478402\n",
      "cnt 333000 16:59:54.567078\n",
      "cnt 334000 17:14:15.333986\n",
      "cnt 335000 17:16:36.429413\n",
      "cnt 336000 17:18:57.273826\n",
      "cnt 337000 17:21:12.046745\n",
      "cnt 338000 17:23:31.334246\n",
      "cnt 339000 17:25:53.553903\n",
      "cnt 340000 17:28:17.515942\n",
      "cnt 341000 17:30:37.798321\n",
      "cnt 342000 17:32:58.226321\n",
      "cnt 343000 17:35:17.402672\n",
      "cnt 344000 17:37:32.748925\n",
      "cnt 345000 17:39:55.217261\n",
      "cnt 346000 17:42:25.421942\n",
      "cnt 347000 17:44:45.277046\n",
      "cnt 348000 17:47:06.395159\n",
      "cnt 349000 17:49:21.979305\n",
      "cnt 350000 17:51:39.344002\n",
      "cnt 351000 17:53:57.192137\n",
      "cnt 352000 17:56:10.248313\n",
      "cnt 353000 17:58:36.130622\n",
      "cnt 354000 18:00:50.738962\n",
      "cnt 355000 18:03:03.391488\n",
      "cnt 356000 18:05:22.480754\n",
      "cnt 357000 18:07:41.706020\n",
      "cnt 358000 18:10:05.791353\n",
      "cnt 359000 18:12:09.990880\n",
      "cnt 360000 18:14:11.874362\n",
      "cnt 361000 18:16:15.449901\n",
      "cnt 362000 18:18:22.392695\n",
      "cnt 363000 18:24:48.873148\n",
      "cnt 364000 18:26:54.841022\n",
      "cnt 365000 18:29:05.123872\n",
      "cnt 366000 18:31:12.335437\n",
      "cnt 367000 18:33:18.977333\n",
      "cnt 368000 18:35:36.942165\n",
      "cnt 369000 18:38:00.954683\n",
      "cnt 370000 18:40:22.616554\n",
      "cnt 371000 18:42:52.608599\n",
      "cnt 372000 18:45:22.489877\n",
      "cnt 373000 18:47:49.149410\n",
      "cnt 374000 18:50:14.286491\n",
      "cnt 375000 18:52:45.386486\n",
      "cnt 376000 18:55:10.473316\n",
      "cnt 377000 18:57:38.087498\n",
      "cnt 378000 19:00:01.698537\n",
      "cnt 379000 19:02:24.939424\n",
      "cnt 380000 19:04:54.706706\n",
      "cnt 381000 19:07:27.627836\n",
      "cnt 382000 19:09:58.762257\n",
      "cnt 383000 19:23:42.035017\n",
      "cnt 384000 19:26:00.567037\n",
      "cnt 385000 19:28:18.708422\n",
      "cnt 386000 19:30:47.175017\n",
      "cnt 387000 19:33:11.839560\n",
      "cnt 388000 19:35:40.155202\n",
      "cnt 389000 19:38:02.392188\n",
      "cnt 390000 19:40:28.735719\n",
      "cnt 391000 19:43:05.408466\n",
      "cnt 392000 19:45:30.530818\n",
      "cnt 393000 19:48:00.265397\n",
      "cnt 394000 19:50:34.738334\n",
      "cnt 395000 19:53:03.497474\n",
      "cnt 396000 19:55:28.004918\n",
      "cnt 397000 19:57:56.860247\n",
      "cnt 398000 20:00:34.459198\n",
      "cnt 399000 20:03:05.630953\n",
      "cnt 400000 20:05:35.415050\n",
      "cnt 401000 20:08:00.506746\n",
      "cnt 402000 20:10:24.514534\n",
      "cnt 403000 20:12:32.521338\n",
      "cnt 404000 20:14:45.053188\n",
      "cnt 405000 20:16:54.541379\n",
      "cnt 406000 20:19:13.544997\n",
      "cnt 407000 20:21:21.819802\n",
      "cnt 408000 20:23:26.323202\n",
      "cnt 409000 20:25:22.908231\n",
      "cnt 410000 20:27:26.305493\n",
      "cnt 411000 20:29:27.603238\n",
      "cnt 412000 20:31:33.286006\n",
      "cnt 413000 20:33:38.433424\n",
      "cnt 414000 20:35:40.158593\n",
      "cnt 415000 20:37:42.613000\n",
      "cnt 416000 20:39:47.279052\n",
      "cnt 417000 20:41:58.122969\n",
      "cnt 418000 20:44:01.616725\n",
      "cnt 419000 20:46:02.618446\n",
      "cnt 420000 20:48:08.690927\n",
      "cnt 421000 20:50:15.641341\n",
      "cnt 422000 20:52:14.383920\n",
      "cnt 423000 20:54:17.180765\n",
      "cnt 424000 20:56:12.661505\n",
      "cnt 425000 20:58:10.131338\n",
      "cnt 426000 21:00:13.844395\n",
      "cnt 427000 21:02:15.152519\n",
      "cnt 428000 21:04:17.149644\n",
      "cnt 429000 21:06:22.319745\n",
      "cnt 430000 21:08:23.160665\n",
      "cnt 431000 21:10:25.947846\n",
      "cnt 432000 21:12:27.709831\n",
      "cnt 433000 21:14:26.557989\n",
      "cnt 434000 21:16:28.353172\n",
      "cnt 435000 21:18:21.982887\n",
      "cnt 436000 21:20:16.688225\n",
      "cnt 437000 21:22:08.677808\n",
      "cnt 438000 21:24:04.754964\n",
      "cnt 439000 21:26:02.537866\n",
      "cnt 440000 21:28:02.409138\n",
      "cnt 441000 21:29:55.100126\n",
      "cnt 442000 21:31:52.055358\n",
      "cnt 443000 21:33:48.660620\n",
      "cnt 444000 21:35:48.696488\n",
      "cnt 445000 21:37:44.990143\n",
      "cnt 446000 21:39:43.738489\n",
      "cnt 447000 21:41:46.676551\n",
      "cnt 448000 21:43:42.537963\n",
      "cnt 449000 21:45:38.587512\n",
      "cnt 450000 21:47:12.899345\n",
      "cnt 451000 21:49:01.956539\n",
      "cnt 452000 21:50:59.848363\n",
      "cnt 453000 21:52:58.319239\n",
      "cnt 454000 21:54:59.893068\n",
      "cnt 455000 21:56:54.699371\n",
      "cnt 456000 21:58:52.590573\n",
      "cnt 457000 22:00:51.301684\n",
      "cnt 458000 22:02:47.016497\n",
      "cnt 459000 22:04:45.262389\n",
      "cnt 460000 22:06:47.732680\n",
      "cnt 461000 22:08:51.216626\n",
      "cnt 462000 22:10:53.710081\n",
      "cnt 463000 22:12:50.237700\n",
      "cnt 464000 22:14:47.364410\n",
      "cnt 465000 22:16:44.031607\n",
      "cnt 466000 22:18:41.761946\n",
      "cnt 467000 22:20:31.490596\n",
      "cnt 468000 22:22:26.155601\n",
      "cnt 469000 22:24:19.253036\n",
      "cnt 470000 22:26:13.112660\n",
      "cnt 471000 22:28:05.182358\n",
      "cnt 472000 22:29:57.015043\n",
      "cnt 473000 22:31:50.395715\n",
      "cnt 474000 22:33:44.963429\n",
      "cnt 475000 22:35:50.975128\n",
      "cnt 476000 22:37:57.328580\n",
      "cnt 477000 22:40:00.658057\n",
      "cnt 478000 22:42:00.691337\n",
      "cnt 479000 22:44:12.186616\n",
      "cnt 480000 22:46:10.389296\n",
      "cnt 481000 22:48:07.520325\n",
      "cnt 482000 22:50:00.628744\n",
      "cnt 483000 22:51:58.100560\n",
      "cnt 484000 22:53:48.295793\n",
      "cnt 485000 22:55:42.120336\n",
      "cnt 486000 22:57:39.292604\n",
      "cnt 487000 22:59:31.641548\n",
      "cnt 488000 23:01:24.312823\n",
      "cnt 489000 23:03:19.152652\n",
      "cnt 490000 23:05:12.020370\n",
      "cnt 491000 23:07:12.316619\n",
      "cnt 492000 23:09:05.925269\n",
      "cnt 493000 23:11:02.251394\n",
      "cnt 494000 23:12:53.801156\n",
      "cnt 495000 23:14:44.797362\n",
      "cnt 496000 23:16:44.101393\n",
      "cnt 497000 23:18:41.806428\n",
      "cnt 498000 23:20:37.076429\n",
      "cnt 499000 23:22:35.862762\n",
      "cnt 500000 23:24:32.041948\n",
      "cnt 501000 23:26:28.321417\n",
      "cnt 502000 23:28:29.267925\n",
      "cnt 503000 23:30:19.366017\n",
      "cnt 504000 23:32:15.192714\n",
      "cnt 505000 23:34:17.245170\n",
      "cnt 506000 23:36:25.922174\n",
      "cnt 507000 23:38:31.579413\n",
      "cnt 508000 23:40:40.830202\n",
      "cnt 509000 23:42:54.437027\n",
      "cnt 510000 23:44:56.511738\n",
      "cnt 511000 23:46:44.645479\n",
      "cnt 512000 23:48:31.908316\n",
      "cnt 513000 23:50:13.958220\n",
      "cnt 514000 23:51:56.204749\n",
      "cnt 515000 23:53:44.967717\n",
      "cnt 516000 23:55:30.572411\n",
      "cnt 517000 23:57:18.089014\n",
      "cnt 518000 23:59:04.990790\n",
      "cnt 519000 1 day, 0:00:52.601705\n",
      "cnt 520000 1 day, 0:02:36.997420\n",
      "cnt 521000 1 day, 0:04:22.502287\n",
      "cnt 522000 1 day, 0:06:12.663213\n",
      "cnt 523000 1 day, 0:07:57.849594\n",
      "cnt 524000 1 day, 0:09:44.005825\n",
      "cnt 525000 1 day, 0:11:20.222519\n",
      "cnt 526000 1 day, 0:12:57.922851\n",
      "cnt 527000 1 day, 0:14:31.556877\n",
      "cnt 528000 1 day, 0:16:06.038609\n",
      "cnt 529000 1 day, 0:17:41.369624\n",
      "cnt 530000 1 day, 0:19:20.287604\n",
      "cnt 531000 1 day, 0:20:57.749623\n",
      "cnt 532000 1 day, 0:22:35.540534\n",
      "cnt 533000 1 day, 0:24:09.692862\n",
      "cnt 534000 1 day, 0:25:45.853764\n",
      "cnt 535000 1 day, 0:27:25.742726\n",
      "cnt 536000 1 day, 0:29:28.587587\n",
      "cnt 537000 1 day, 0:31:28.691222\n",
      "cnt 538000 1 day, 0:33:35.109179\n",
      "cnt 539000 1 day, 0:35:32.360015\n",
      "cnt 540000 1 day, 0:37:31.369318\n",
      "cnt 541000 1 day, 0:39:31.597144\n",
      "cnt 542000 1 day, 0:41:27.262191\n",
      "cnt 543000 1 day, 0:43:23.663227\n",
      "cnt 544000 1 day, 0:44:51.135559\n",
      "cnt 545000 1 day, 0:46:27.077789\n",
      "cnt 546000 1 day, 0:48:07.385368\n",
      "cnt 547000 1 day, 0:49:43.280086\n",
      "cnt 548000 1 day, 0:51:19.406191\n",
      "cnt 549000 1 day, 0:52:57.833606\n",
      "cnt 550000 1 day, 0:54:50.330672\n",
      "cnt 551000 1 day, 0:56:44.177607\n",
      "cnt 552000 1 day, 0:58:40.547672\n",
      "cnt 553000 1 day, 1:00:43.046514\n",
      "cnt 554000 1 day, 1:02:57.286400\n",
      "cnt 555000 1 day, 1:04:59.016920\n",
      "cnt 556000 1 day, 1:07:04.153510\n",
      "cnt 557000 1 day, 1:09:03.373565\n",
      "cnt 558000 1 day, 1:11:02.752811\n",
      "cnt 559000 1 day, 1:12:54.833625\n",
      "cnt 560000 1 day, 1:14:52.021202\n",
      "cnt 561000 1 day, 1:16:48.484225\n",
      "cnt 562000 1 day, 1:18:46.345711\n",
      "cnt 563000 1 day, 1:20:37.902219\n",
      "cnt 564000 1 day, 1:22:39.823420\n",
      "cnt 565000 1 day, 1:25:02.113204\n",
      "cnt 566000 1 day, 1:27:33.640690\n",
      "cnt 567000 1 day, 1:29:58.039552\n",
      "cnt 568000 1 day, 1:32:21.556550\n",
      "cnt 569000 1 day, 1:34:45.106565\n",
      "cnt 570000 1 day, 1:37:11.658997\n",
      "cnt 571000 1 day, 1:39:36.254354\n",
      "cnt 572000 1 day, 1:42:05.432497\n",
      "cnt 573000 1 day, 1:44:31.537914\n",
      "cnt 574000 1 day, 1:46:54.267276\n",
      "cnt 575000 1 day, 1:49:22.088723\n",
      "cnt 576000 1 day, 1:51:18.348664\n",
      "cnt 577000 1 day, 1:53:18.224892\n",
      "cnt 578000 1 day, 1:55:14.276380\n",
      "cnt 579000 1 day, 1:57:16.343269\n",
      "cnt 580000 1 day, 1:59:28.043618\n",
      "cnt 581000 1 day, 2:01:42.411694\n",
      "cnt 582000 1 day, 2:04:01.210635\n",
      "cnt 583000 1 day, 2:12:37.310624\n",
      "cnt 584000 1 day, 2:14:47.184276\n",
      "cnt 585000 1 day, 2:16:56.877287\n",
      "cnt 586000 1 day, 2:19:11.350428\n",
      "cnt 587000 1 day, 2:21:38.146354\n",
      "cnt 588000 1 day, 2:24:00.198627\n",
      "cnt 589000 1 day, 2:26:23.401226\n",
      "cnt 590000 1 day, 2:28:38.637726\n",
      "cnt 591000 1 day, 2:30:59.628146\n",
      "cnt 592000 1 day, 2:33:23.471382\n",
      "cnt 593000 1 day, 2:35:47.486417\n",
      "cnt 594000 1 day, 2:37:56.344318\n",
      "cnt 595000 1 day, 2:40:05.323010\n",
      "cnt 596000 1 day, 2:42:30.751999\n",
      "cnt 597000 1 day, 2:44:54.992781\n",
      "cnt 598000 1 day, 2:47:11.854154\n",
      "cnt 599000 1 day, 2:49:31.083163\n",
      "cnt 600000 1 day, 2:51:45.184769\n",
      "cnt 601000 1 day, 2:54:06.986007\n",
      "cnt 602000 1 day, 2:56:28.168284\n",
      "cnt 603000 1 day, 2:58:49.031014\n",
      "cnt 604000 1 day, 3:01:06.697218\n",
      "cnt 605000 1 day, 3:03:23.846515\n",
      "cnt 606000 1 day, 3:05:42.155351\n",
      "cnt 607000 1 day, 3:08:12.529615\n",
      "cnt 608000 1 day, 3:10:37.826485\n",
      "cnt 609000 1 day, 3:12:51.300178\n",
      "cnt 610000 1 day, 3:15:08.902199\n",
      "cnt 611000 1 day, 3:17:28.652181\n",
      "cnt 612000 1 day, 3:19:50.580387\n",
      "cnt 613000 1 day, 3:22:06.914534\n",
      "cnt 614000 1 day, 3:24:20.506929\n",
      "cnt 615000 1 day, 3:26:28.490958\n",
      "cnt 616000 1 day, 3:28:36.748476\n",
      "cnt 617000 1 day, 3:30:41.368237\n",
      "cnt 618000 1 day, 3:32:52.991729\n",
      "cnt 619000 1 day, 3:35:04.323238\n",
      "cnt 620000 1 day, 3:37:12.449660\n",
      "cnt 621000 1 day, 3:39:14.614692\n",
      "cnt 622000 1 day, 3:41:29.486216\n",
      "cnt 623000 1 day, 3:43:38.788872\n",
      "cnt 624000 1 day, 3:45:45.792403\n",
      "cnt 625000 1 day, 3:47:57.495081\n",
      "cnt 626000 1 day, 3:50:10.386588\n",
      "cnt 627000 1 day, 3:52:15.565217\n",
      "cnt 628000 1 day, 3:54:17.424455\n",
      "cnt 629000 1 day, 3:56:25.164770\n",
      "cnt 630000 1 day, 3:58:28.647681\n",
      "cnt 631000 1 day, 4:00:36.518993\n",
      "cnt 632000 1 day, 4:02:40.569427\n",
      "cnt 633000 1 day, 4:04:43.470760\n",
      "cnt 634000 1 day, 4:07:01.139834\n",
      "cnt 635000 1 day, 4:09:22.463976\n",
      "cnt 636000 1 day, 4:11:43.951155\n",
      "cnt 637000 1 day, 4:14:01.773440\n",
      "cnt 638000 1 day, 4:16:19.721666\n",
      "cnt 639000 1 day, 4:18:34.359364\n",
      "cnt 640000 1 day, 4:20:51.743312\n",
      "cnt 641000 1 day, 4:23:09.028184\n",
      "cnt 642000 1 day, 4:25:28.961426\n",
      "cnt 643000 1 day, 4:27:35.282307\n",
      "cnt 644000 1 day, 4:29:51.719477\n",
      "cnt 645000 1 day, 4:32:04.531094\n",
      "cnt 646000 1 day, 4:34:21.618259\n",
      "cnt 647000 1 day, 4:36:39.653850\n",
      "cnt 648000 1 day, 4:38:54.754473\n",
      "cnt 649000 1 day, 4:41:13.179509\n",
      "cnt 650000 1 day, 4:43:31.205704\n",
      "cnt 651000 1 day, 4:45:48.181930\n",
      "cnt 652000 1 day, 4:48:07.148563\n",
      "cnt 653000 1 day, 4:50:23.462945\n",
      "cnt 654000 1 day, 4:52:38.080313\n",
      "cnt 655000 1 day, 4:54:57.738008\n",
      "cnt 656000 1 day, 4:57:14.560410\n",
      "cnt 657000 1 day, 4:59:28.819011\n",
      "cnt 658000 1 day, 5:01:47.106083\n",
      "cnt 659000 1 day, 5:04:13.688242\n",
      "cnt 660000 1 day, 5:06:28.775204\n",
      "cnt 661000 1 day, 5:08:49.313653\n",
      "cnt 662000 1 day, 5:11:09.189525\n",
      "cnt 663000 1 day, 5:13:31.775349\n",
      "cnt 664000 1 day, 5:15:49.718611\n",
      "cnt 665000 1 day, 5:18:05.561597\n",
      "cnt 666000 1 day, 5:20:21.706546\n",
      "cnt 667000 1 day, 5:22:42.007352\n",
      "cnt 668000 1 day, 5:24:56.950709\n",
      "cnt 669000 1 day, 5:27:19.199080\n",
      "cnt 670000 1 day, 5:29:39.111882\n",
      "cnt 671000 1 day, 5:31:56.531926\n",
      "cnt 672000 1 day, 5:34:21.824008\n",
      "cnt 673000 1 day, 5:36:42.258856\n",
      "cnt 674000 1 day, 5:39:08.735480\n",
      "cnt 675000 1 day, 5:41:28.038613\n",
      "cnt 676000 1 day, 5:43:55.103298\n",
      "cnt 677000 1 day, 5:46:21.996693\n",
      "cnt 678000 1 day, 5:48:40.314909\n",
      "cnt 679000 1 day, 5:51:01.848685\n",
      "cnt 680000 1 day, 5:53:22.621457\n",
      "cnt 681000 1 day, 5:55:48.180528\n",
      "cnt 682000 1 day, 5:58:10.930128\n",
      "cnt 683000 1 day, 6:00:32.843196\n",
      "cnt 684000 1 day, 6:02:52.437321\n",
      "cnt 685000 1 day, 6:05:11.888333\n",
      "cnt 686000 1 day, 6:07:32.930934\n",
      "cnt 687000 1 day, 6:09:52.573584\n",
      "cnt 688000 1 day, 6:12:21.045496\n",
      "cnt 689000 1 day, 6:14:38.731045\n",
      "cnt 690000 1 day, 6:17:02.752594\n",
      "cnt 691000 1 day, 6:19:21.908117\n",
      "cnt 692000 1 day, 6:21:51.398928\n",
      "cnt 693000 1 day, 6:24:12.719790\n",
      "cnt 694000 1 day, 6:26:29.382315\n",
      "cnt 695000 1 day, 6:28:50.374331\n",
      "cnt 696000 1 day, 6:31:13.883453\n",
      "cnt 697000 1 day, 6:33:39.003742\n",
      "cnt 698000 1 day, 6:36:02.139330\n",
      "cnt 699000 1 day, 6:38:22.285365\n",
      "cnt 700000 1 day, 6:40:42.656653\n",
      "cnt 701000 1 day, 6:43:06.577546\n",
      "cnt 702000 1 day, 6:45:28.142740\n",
      "cnt 703000 1 day, 6:47:52.952719\n",
      "cnt 704000 1 day, 6:50:12.638130\n",
      "cnt 705000 1 day, 6:52:07.204104\n",
      "cnt 706000 1 day, 6:53:56.018375\n",
      "cnt 707000 1 day, 6:55:53.399587\n",
      "cnt 708000 1 day, 6:57:48.859511\n",
      "cnt 709000 1 day, 6:59:42.304844\n",
      "cnt 710000 1 day, 7:01:34.366043\n",
      "cnt 711000 1 day, 7:03:25.541314\n",
      "cnt 712000 1 day, 7:05:18.315140\n",
      "cnt 713000 1 day, 7:07:10.874251\n",
      "cnt 714000 1 day, 7:09:03.414695\n",
      "cnt 715000 1 day, 7:10:56.542994\n",
      "cnt 716000 1 day, 7:12:48.026449\n",
      "cnt 717000 1 day, 7:14:35.548391\n",
      "cnt 718000 1 day, 7:16:19.389832\n",
      "cnt 719000 1 day, 7:18:09.341652\n",
      "cnt 720000 1 day, 7:20:06.444123\n",
      "cnt 721000 1 day, 7:21:55.179664\n",
      "cnt 722000 1 day, 7:23:46.403022\n",
      "cnt 723000 1 day, 7:25:40.914401\n",
      "cnt 724000 1 day, 7:27:48.347985\n",
      "cnt 725000 1 day, 7:29:57.944624\n",
      "cnt 726000 1 day, 7:32:13.269349\n",
      "cnt 727000 1 day, 7:34:27.512051\n",
      "cnt 728000 1 day, 7:36:33.186868\n",
      "cnt 729000 1 day, 7:38:50.201203\n",
      "cnt 730000 1 day, 7:41:14.432873\n",
      "cnt 731000 1 day, 7:43:29.885284\n",
      "cnt 732000 1 day, 7:45:48.970490\n",
      "cnt 733000 1 day, 7:48:06.077855\n",
      "cnt 734000 1 day, 7:50:28.395459\n",
      "cnt 735000 1 day, 7:52:44.983355\n",
      "cnt 736000 1 day, 7:55:07.392625\n",
      "cnt 737000 1 day, 7:57:33.727676\n",
      "cnt 738000 1 day, 7:59:57.275120\n",
      "cnt 739000 1 day, 8:02:22.366971\n",
      "cnt 740000 1 day, 8:04:40.479661\n",
      "cnt 741000 1 day, 8:07:00.504984\n",
      "cnt 742000 1 day, 8:09:18.138121\n",
      "cnt 743000 1 day, 8:11:33.905033\n",
      "cnt 744000 1 day, 8:13:49.179550\n",
      "cnt 745000 1 day, 8:16:11.187203\n",
      "cnt 746000 1 day, 8:18:31.705034\n",
      "cnt 747000 1 day, 8:20:59.940704\n",
      "cnt 748000 1 day, 8:23:22.321575\n",
      "cnt 749000 1 day, 8:25:47.248945\n",
      "cnt 750000 1 day, 8:28:20.733410\n",
      "cnt 751000 1 day, 8:30:44.509386\n",
      "cnt 752000 1 day, 8:33:07.322351\n",
      "cnt 753000 1 day, 8:35:36.275091\n",
      "cnt 754000 1 day, 8:38:04.532053\n",
      "cnt 755000 1 day, 8:40:35.699519\n",
      "cnt 756000 1 day, 8:42:57.692259\n",
      "cnt 757000 1 day, 8:45:25.692182\n",
      "cnt 758000 1 day, 8:47:24.923271\n",
      "cnt 759000 1 day, 8:49:23.329399\n",
      "cnt 760000 1 day, 8:51:24.051912\n",
      "cnt 761000 1 day, 8:53:21.188399\n",
      "cnt 762000 1 day, 8:55:13.697902\n",
      "cnt 763000 1 day, 8:57:08.200045\n",
      "cnt 764000 1 day, 8:59:02.577909\n",
      "cnt 765000 1 day, 9:01:06.301747\n",
      "cnt 766000 1 day, 9:03:01.348504\n",
      "cnt 767000 1 day, 9:04:59.971384\n",
      "cnt 768000 1 day, 9:06:57.160399\n",
      "cnt 769000 1 day, 9:08:58.869847\n",
      "cnt 770000 1 day, 9:10:56.061448\n",
      "cnt 771000 1 day, 9:12:54.511829\n",
      "cnt 772000 1 day, 9:14:52.894572\n",
      "cnt 773000 1 day, 9:16:50.277030\n",
      "cnt 774000 1 day, 9:19:07.198551\n",
      "cnt 775000 1 day, 9:21:29.335048\n",
      "cnt 776000 1 day, 9:23:49.325479\n",
      "cnt 777000 1 day, 9:26:14.520142\n",
      "cnt 778000 1 day, 9:28:36.303475\n",
      "cnt 779000 1 day, 9:30:59.912955\n",
      "cnt 780000 1 day, 9:33:15.603431\n",
      "cnt 781000 1 day, 9:35:43.700831\n",
      "cnt 782000 1 day, 9:38:16.770794\n",
      "cnt 783000 1 day, 9:40:41.033907\n",
      "cnt 784000 1 day, 9:43:05.305153\n",
      "cnt 785000 1 day, 9:45:19.869625\n",
      "cnt 786000 1 day, 9:47:47.663243\n",
      "cnt 787000 1 day, 9:50:04.565965\n",
      "cnt 788000 1 day, 9:52:29.775306\n",
      "cnt 789000 1 day, 9:54:50.818482\n",
      "cnt 790000 1 day, 9:57:15.282762\n",
      "cnt 791000 1 day, 9:59:35.769083\n",
      "cnt 792000 1 day, 10:01:54.513836\n",
      "cnt 793000 1 day, 10:04:19.065510\n",
      "cnt 794000 1 day, 10:06:35.067965\n",
      "cnt 795000 1 day, 10:09:00.987338\n",
      "cnt 796000 1 day, 10:11:31.718806\n",
      "cnt 797000 1 day, 10:13:52.087647\n",
      "cnt 798000 1 day, 10:16:12.750624\n",
      "cnt 799000 1 day, 10:18:31.575623\n",
      "cnt 800000 1 day, 10:20:56.304336\n",
      "cnt 801000 1 day, 10:23:23.618242\n",
      "cnt 802000 1 day, 10:25:40.455526\n",
      "cnt 803000 1 day, 10:27:54.913379\n",
      "cnt 804000 1 day, 10:30:18.552581\n",
      "cnt 805000 1 day, 10:32:43.191028\n",
      "cnt 806000 1 day, 10:35:01.995394\n",
      "cnt 807000 1 day, 10:37:29.971852\n",
      "cnt 808000 1 day, 10:39:56.256946\n",
      "cnt 809000 1 day, 10:42:17.099331\n",
      "cnt 810000 1 day, 10:44:44.196666\n",
      "cnt 811000 1 day, 10:47:08.598458\n",
      "cnt 812000 1 day, 10:49:34.253061\n",
      "cnt 813000 1 day, 10:52:01.726622\n",
      "cnt 814000 1 day, 10:54:30.360559\n",
      "cnt 815000 1 day, 10:56:50.965557\n",
      "cnt 816000 1 day, 10:59:13.008469\n",
      "cnt 817000 1 day, 11:01:37.935301\n",
      "cnt 818000 1 day, 11:03:57.521053\n",
      "cnt 819000 1 day, 11:06:16.254186\n",
      "cnt 820000 1 day, 11:08:25.652171\n",
      "cnt 821000 1 day, 11:10:34.342009\n",
      "cnt 822000 1 day, 11:12:43.879965\n",
      "cnt 823000 1 day, 11:14:53.886144\n",
      "cnt 824000 1 day, 11:16:57.019266\n",
      "cnt 825000 1 day, 11:19:09.367132\n",
      "cnt 826000 1 day, 11:21:18.342909\n",
      "cnt 827000 1 day, 11:23:29.277880\n",
      "cnt 828000 1 day, 11:25:36.387400\n",
      "cnt 829000 1 day, 11:27:46.607706\n",
      "cnt 830000 1 day, 11:29:55.972893\n",
      "cnt 831000 1 day, 11:32:06.575860\n",
      "cnt 832000 1 day, 11:34:19.061983\n",
      "cnt 833000 1 day, 11:36:28.898450\n",
      "cnt 834000 1 day, 11:38:32.766806\n",
      "cnt 835000 1 day, 11:40:45.062561\n",
      "cnt 836000 1 day, 11:42:52.151896\n",
      "cnt 837000 1 day, 11:45:01.547875\n",
      "cnt 838000 1 day, 11:47:16.464288\n",
      "cnt 839000 1 day, 11:49:24.211639\n",
      "cnt 840000 1 day, 11:51:35.826021\n",
      "cnt 841000 1 day, 11:53:45.529048\n",
      "cnt 842000 1 day, 11:55:55.265726\n",
      "cnt 843000 1 day, 11:57:59.844541\n",
      "cnt 844000 1 day, 11:59:56.729563\n",
      "cnt 845000 1 day, 12:01:54.649190\n",
      "cnt 846000 1 day, 12:03:44.677309\n",
      "cnt 847000 1 day, 12:05:37.713062\n",
      "cnt 848000 1 day, 12:07:36.000751\n",
      "cnt 849000 1 day, 12:09:26.698937\n",
      "cnt 850000 1 day, 12:11:20.490071\n",
      "cnt 851000 1 day, 12:13:16.588961\n",
      "cnt 852000 1 day, 12:15:15.491204\n",
      "cnt 853000 1 day, 12:17:07.746752\n",
      "cnt 854000 1 day, 12:18:58.984375\n",
      "cnt 855000 1 day, 12:20:53.300446\n",
      "cnt 856000 1 day, 12:22:45.776236\n",
      "cnt 857000 1 day, 12:24:37.193960\n",
      "cnt 858000 1 day, 12:26:34.698257\n",
      "cnt 859000 1 day, 12:28:30.687647\n",
      "cnt 860000 1 day, 12:30:27.631482\n",
      "cnt 861000 1 day, 12:32:25.520477\n",
      "cnt 862000 1 day, 12:34:26.002009\n",
      "cnt 863000 1 day, 12:36:22.065491\n",
      "cnt 864000 1 day, 12:38:20.430836\n",
      "cnt 865000 1 day, 12:40:13.912719\n",
      "cnt 866000 1 day, 12:42:11.865968\n",
      "cnt 867000 1 day, 12:44:03.489959\n",
      "cnt 868000 1 day, 12:45:59.359761\n",
      "cnt 869000 1 day, 12:47:54.284662\n",
      "cnt 870000 1 day, 12:49:44.448201\n",
      "cnt 871000 1 day, 12:51:39.750135\n",
      "cnt 872000 1 day, 12:53:35.742079\n",
      "cnt 873000 1 day, 12:55:30.504423\n",
      "cnt 874000 1 day, 12:57:26.301443\n",
      "cnt 875000 1 day, 12:59:24.413605\n",
      "cnt 876000 1 day, 13:01:21.803208\n",
      "cnt 877000 1 day, 13:03:14.941503\n",
      "cnt 878000 1 day, 13:05:13.563868\n",
      "cnt 879000 1 day, 13:07:13.916393\n",
      "cnt 880000 1 day, 13:09:07.006209\n",
      "cnt 881000 1 day, 13:11:01.355271\n",
      "cnt 882000 1 day, 13:12:52.864988\n",
      "cnt 883000 1 day, 13:14:50.321867\n",
      "cnt 884000 1 day, 13:16:43.544391\n",
      "cnt 885000 1 day, 13:18:40.518279\n",
      "cnt 886000 1 day, 13:20:31.411774\n",
      "cnt 887000 1 day, 13:22:18.885131\n",
      "cnt 888000 1 day, 13:24:17.174863\n",
      "cnt 889000 1 day, 13:26:15.362733\n",
      "cnt 890000 1 day, 13:28:07.767018\n",
      "cnt 891000 1 day, 13:30:03.887875\n",
      "cnt 892000 1 day, 13:31:58.509788\n",
      "cnt 893000 1 day, 13:33:53.048424\n",
      "cnt 894000 1 day, 13:35:48.087218\n",
      "cnt 895000 1 day, 13:37:43.575380\n",
      "cnt 896000 1 day, 13:39:40.920718\n",
      "cnt 897000 1 day, 13:41:34.136740\n",
      "cnt 898000 1 day, 13:43:30.806374\n",
      "cnt 899000 1 day, 13:45:30.114748\n",
      "cnt 900000 1 day, 13:47:22.136601\n",
      "cnt 901000 1 day, 13:49:19.208772\n",
      "cnt 902000 1 day, 13:51:16.938246\n",
      "cnt 903000 1 day, 13:53:16.690102\n",
      "cnt 904000 1 day, 13:55:17.514975\n",
      "cnt 905000 1 day, 13:57:17.024966\n",
      "cnt 906000 1 day, 13:59:16.743723\n",
      "cnt 907000 1 day, 14:01:07.773573\n",
      "cnt 908000 1 day, 14:03:11.405179\n",
      "cnt 909000 1 day, 14:05:08.825855\n",
      "cnt 910000 1 day, 14:07:08.051225\n",
      "cnt 911000 1 day, 14:09:09.879268\n",
      "cnt 912000 1 day, 14:11:14.533452\n",
      "cnt 913000 1 day, 14:13:10.605951\n",
      "cnt 914000 1 day, 14:15:16.709817\n",
      "cnt 915000 1 day, 14:17:21.761544\n",
      "cnt 916000 1 day, 14:19:22.155359\n",
      "cnt 917000 1 day, 14:21:22.883155\n",
      "cnt 918000 1 day, 14:23:24.865142\n",
      "cnt 919000 1 day, 14:25:20.189450\n",
      "cnt 920000 1 day, 14:27:13.362513\n",
      "cnt 921000 1 day, 14:29:15.909178\n",
      "cnt 922000 1 day, 14:31:18.825477\n",
      "cnt 923000 1 day, 14:33:14.944793\n",
      "cnt 924000 1 day, 14:35:15.699361\n",
      "cnt 925000 1 day, 14:37:11.587478\n",
      "cnt 926000 1 day, 14:39:15.594236\n",
      "cnt 927000 1 day, 14:41:14.361784\n",
      "cnt 928000 1 day, 14:43:17.056167\n",
      "cnt 929000 1 day, 14:45:20.261920\n",
      "cnt 930000 1 day, 14:47:23.360767\n",
      "cnt 931000 1 day, 14:49:22.513074\n",
      "cnt 932000 1 day, 14:51:26.199746\n",
      "cnt 933000 1 day, 14:53:26.990123\n",
      "cnt 934000 1 day, 14:55:32.935963\n",
      "cnt 935000 1 day, 15:04:20.831931\n",
      "cnt 936000 1 day, 15:06:39.479669\n",
      "cnt 937000 1 day, 15:09:01.344096\n",
      "cnt 938000 1 day, 15:11:21.490279\n",
      "cnt 939000 1 day, 15:13:44.940096\n",
      "cnt 940000 1 day, 15:16:07.184181\n",
      "cnt 941000 1 day, 15:18:28.136457\n",
      "cnt 942000 1 day, 15:20:48.152202\n",
      "cnt 943000 1 day, 15:23:02.791638\n",
      "cnt 944000 1 day, 15:25:26.167969\n",
      "cnt 945000 1 day, 15:27:41.426141\n",
      "cnt 946000 1 day, 15:30:08.202250\n",
      "cnt 947000 1 day, 15:32:28.791279\n",
      "cnt 948000 1 day, 15:34:47.184193\n",
      "cnt 949000 1 day, 15:37:12.201559\n",
      "cnt 950000 1 day, 15:39:23.362256\n",
      "cnt 951000 1 day, 15:41:42.487864\n",
      "cnt 952000 1 day, 15:43:58.571032\n",
      "cnt 953000 1 day, 15:46:20.025145\n",
      "cnt 954000 1 day, 15:48:38.764270\n",
      "cnt 955000 1 day, 15:50:49.356991\n",
      "cnt 956000 1 day, 15:53:06.750952\n",
      "cnt 957000 1 day, 15:55:18.621577\n",
      "cnt 958000 1 day, 15:57:28.635300\n",
      "cnt 959000 1 day, 15:59:38.420412\n",
      "cnt 960000 1 day, 16:01:44.442911\n",
      "cnt 961000 1 day, 16:03:41.568475\n",
      "cnt 962000 1 day, 16:05:48.261661\n",
      "cnt 963000 1 day, 16:07:58.291195\n",
      "cnt 964000 1 day, 16:10:03.829872\n",
      "cnt 965000 1 day, 16:12:05.362230\n",
      "cnt 966000 1 day, 16:14:16.342575\n",
      "cnt 967000 1 day, 16:16:27.281301\n",
      "cnt 968000 1 day, 16:18:36.296664\n",
      "cnt 969000 1 day, 16:20:44.566941\n",
      "cnt 970000 1 day, 16:22:52.187530\n",
      "cnt 971000 1 day, 16:25:03.701944\n",
      "cnt 972000 1 day, 16:27:12.813814\n",
      "cnt 973000 1 day, 16:29:20.955395\n",
      "cnt 974000 1 day, 16:31:26.773078\n",
      "cnt 975000 1 day, 16:33:35.558047\n",
      "cnt 976000 1 day, 16:35:41.391163\n",
      "cnt 977000 1 day, 16:37:51.089283\n",
      "cnt 978000 1 day, 16:40:04.526467\n",
      "cnt 979000 1 day, 16:42:31.620158\n",
      "cnt 980000 1 day, 16:44:55.144297\n",
      "cnt 981000 1 day, 16:47:24.113053\n",
      "cnt 982000 1 day, 16:49:49.453305\n",
      "cnt 983000 1 day, 16:52:13.167996\n",
      "cnt 984000 1 day, 16:54:36.415312\n",
      "cnt 985000 1 day, 16:56:45.879360\n",
      "cnt 986000 1 day, 16:58:57.799565\n",
      "cnt 987000 1 day, 17:01:07.262144\n",
      "cnt 988000 1 day, 17:03:17.413924\n",
      "cnt 989000 1 day, 17:05:33.418904\n",
      "cnt 990000 1 day, 17:07:48.839065\n",
      "cnt 991000 1 day, 17:09:58.804953\n",
      "cnt 992000 1 day, 17:12:15.964624\n",
      "cnt 993000 1 day, 17:14:25.396266\n",
      "cnt 994000 1 day, 17:16:36.196911\n",
      "cnt 995000 1 day, 17:18:46.861696\n",
      "cnt 996000 1 day, 17:20:57.651990\n",
      "cnt 997000 1 day, 17:23:13.998207\n",
      "cnt 998000 1 day, 17:25:25.705820\n",
      "cnt 999000 1 day, 17:27:41.624334\n",
      "cnt 1000000 1 day, 17:29:58.158577\n",
      "cnt 1001000 1 day, 17:32:08.531847\n",
      "cnt 1002000 1 day, 17:34:19.180579\n",
      "cnt 1003000 1 day, 17:36:27.643051\n",
      "cnt 1004000 1 day, 17:38:46.427456\n",
      "cnt 1005000 1 day, 17:40:58.830227\n",
      "cnt 1006000 1 day, 17:43:57.407153\n",
      "cnt 1007000 1 day, 17:46:20.303894\n",
      "cnt 1008000 1 day, 17:48:46.734821\n",
      "cnt 1009000 1 day, 17:51:07.369965\n",
      "cnt 1010000 1 day, 17:53:29.084457\n",
      "cnt 1011000 1 day, 17:55:55.298815\n",
      "cnt 1012000 1 day, 17:58:20.673694\n",
      "cnt 1013000 1 day, 18:00:38.907269\n",
      "cnt 1014000 1 day, 18:03:10.558882\n",
      "cnt 1015000 1 day, 18:05:41.446327\n",
      "cnt 1016000 1 day, 18:08:07.424502\n",
      "cnt 1017000 1 day, 18:10:27.184016\n",
      "cnt 1018000 1 day, 18:12:58.716542\n",
      "cnt 1019000 1 day, 18:15:23.289025\n",
      "cnt 1020000 1 day, 18:17:50.115841\n",
      "cnt 1021000 1 day, 18:20:16.825681\n",
      "cnt 1022000 1 day, 18:22:37.281826\n",
      "cnt 1023000 1 day, 18:25:01.375258\n",
      "cnt 1024000 1 day, 18:27:33.119470\n",
      "cnt 1025000 1 day, 18:29:54.104911\n",
      "cnt 1026000 1 day, 18:32:22.430732\n",
      "cnt 1027000 1 day, 18:34:52.018497\n",
      "cnt 1028000 1 day, 18:37:19.448350\n",
      "cnt 1029000 1 day, 18:39:46.770583\n",
      "cnt 1030000 1 day, 18:42:09.765353\n",
      "cnt 1031000 1 day, 18:44:36.618916\n",
      "cnt 1032000 1 day, 18:47:01.550180\n",
      "cnt 1033000 1 day, 18:49:15.399277\n",
      "cnt 1034000 1 day, 18:51:26.814678\n",
      "cnt 1035000 1 day, 18:53:31.883610\n",
      "cnt 1036000 1 day, 18:55:37.330026\n",
      "cnt 1037000 1 day, 18:57:47.637050\n",
      "cnt 1038000 1 day, 18:59:58.827433\n",
      "cnt 1039000 1 day, 19:02:09.893529\n",
      "cnt 1040000 1 day, 19:04:21.021414\n",
      "cnt 1041000 1 day, 19:06:32.650900\n",
      "cnt 1042000 1 day, 19:08:41.402251\n",
      "cnt 1043000 1 day, 19:10:48.289179\n",
      "cnt 1044000 1 day, 19:12:58.712362\n",
      "cnt 1045000 1 day, 19:15:05.493102\n",
      "cnt 1046000 1 day, 19:17:15.322157\n",
      "cnt 1047000 1 day, 19:19:26.211569\n",
      "cnt 1048000 1 day, 19:21:39.622075\n",
      "cnt 1049000 1 day, 19:23:51.992048\n",
      "cnt 1050000 1 day, 19:25:56.110395\n",
      "cnt 1051000 1 day, 19:28:05.355155\n",
      "cnt 1052000 1 day, 19:30:17.024739\n",
      "cnt 1053000 1 day, 19:32:19.478901\n",
      "cnt 1054000 1 day, 19:34:35.546493\n",
      "cnt 1055000 1 day, 19:36:55.523818\n",
      "cnt 1056000 1 day, 19:39:22.144445\n",
      "cnt 1057000 1 day, 19:41:45.505429\n",
      "cnt 1058000 1 day, 19:44:08.159075\n",
      "cnt 1059000 1 day, 19:46:28.372121\n",
      "cnt 1060000 1 day, 19:48:55.402302\n",
      "cnt 1061000 1 day, 19:51:12.352488\n",
      "cnt 1062000 1 day, 19:53:29.812721\n",
      "cnt 1063000 1 day, 19:55:50.376006\n",
      "cnt 1064000 1 day, 19:58:11.284578\n",
      "cnt 1065000 1 day, 20:00:32.142502\n",
      "cnt 1066000 1 day, 20:03:01.705225\n",
      "cnt 1067000 1 day, 20:05:25.479883\n",
      "cnt 1068000 1 day, 20:07:46.843303\n",
      "cnt 1069000 1 day, 20:10:11.814350\n",
      "cnt 1070000 1 day, 20:12:36.424209\n",
      "cnt 1071000 1 day, 20:15:03.954840\n",
      "cnt 1072000 1 day, 20:17:29.927927\n",
      "cnt 1073000 1 day, 20:19:51.192222\n",
      "cnt 1074000 1 day, 20:22:14.530509\n",
      "cnt 1075000 1 day, 20:24:38.079359\n",
      "cnt 1076000 1 day, 20:26:59.711075\n",
      "cnt 1077000 1 day, 20:29:21.906893\n",
      "cnt 1078000 1 day, 20:31:40.756000\n",
      "cnt 1079000 1 day, 20:34:06.753966\n",
      "cnt 1080000 1 day, 20:36:32.973739\n",
      "cnt 1081000 1 day, 20:38:40.756604\n",
      "cnt 1082000 1 day, 20:40:47.725236\n",
      "cnt 1083000 1 day, 20:42:57.502586\n",
      "cnt 1084000 1 day, 20:45:04.678439\n",
      "cnt 1085000 1 day, 20:47:02.010523\n",
      "cnt 1086000 1 day, 20:49:10.169522\n",
      "cnt 1087000 1 day, 20:51:23.411030\n",
      "cnt 1088000 1 day, 20:53:28.194800\n",
      "cnt 1089000 1 day, 20:55:39.292737\n",
      "cnt 1090000 1 day, 20:57:46.819215\n",
      "cnt 1091000 1 day, 20:59:48.993099\n",
      "cnt 1092000 1 day, 21:01:52.826831\n",
      "cnt 1093000 1 day, 21:03:54.339148\n",
      "cnt 1094000 1 day, 21:06:00.500131\n",
      "cnt 1095000 1 day, 21:08:09.329125\n",
      "cnt 1096000 1 day, 21:10:24.627151\n",
      "cnt 1097000 1 day, 21:12:34.747811\n",
      "cnt 1098000 1 day, 21:14:40.734661\n",
      "cnt 1099000 1 day, 21:16:49.976732\n",
      "cnt 1100000 1 day, 21:19:00.420674\n",
      "cnt 1101000 1 day, 21:21:08.471306\n",
      "cnt 1102000 1 day, 21:23:14.408981\n",
      "cnt 1103000 1 day, 21:25:22.970647\n",
      "cnt 1104000 1 day, 21:27:26.395345\n",
      "cnt 1105000 1 day, 21:29:29.105904\n",
      "cnt 1106000 1 day, 21:31:31.861365\n",
      "cnt 1107000 1 day, 21:33:40.299115\n",
      "cnt 1108000 1 day, 21:35:48.604352\n",
      "cnt 1109000 1 day, 21:37:54.168573\n",
      "cnt 1110000 1 day, 21:40:02.194440\n",
      "cnt 1111000 1 day, 21:42:07.413177\n",
      "cnt 1112000 1 day, 21:44:13.813415\n",
      "cnt 1113000 1 day, 21:46:18.333894\n",
      "cnt 1114000 1 day, 21:48:24.497745\n",
      "cnt 1115000 1 day, 21:50:34.481970\n",
      "cnt 1116000 1 day, 21:52:39.741581\n",
      "cnt 1117000 1 day, 21:54:48.047892\n",
      "cnt 1118000 1 day, 21:56:58.608791\n",
      "cnt 1119000 1 day, 21:59:06.560158\n",
      "cnt 1120000 1 day, 22:01:14.820661\n",
      "cnt 1121000 1 day, 22:03:41.179554\n"
     ]
    }
   ],
   "source": [
    "# Export author embed\n",
    "dump_author_embs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15cbfb1d-e0e9-4138-941e-803897c50ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 1 day, 23:11:40.401152\n"
     ]
    }
   ],
   "source": [
    "# Print completion information\n",
    "print('done', datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1df2d8-9fb6-4db8-9a15-5b299a4fa7a6",
   "metadata": {},
   "source": [
    "# 2. Global model_Gen_Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "818987e2-f7e2-4cd2-8894-236d8187f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletsGenerator:\n",
    "    name2pubs_train = {}\n",
    "    name2pubs_test = {}\n",
    "    names_train = None\n",
    "    names_test = None\n",
    "    n_pubs_train = None\n",
    "    n_pubs_test = None\n",
    "    pids_train = []\n",
    "    pids_test = []\n",
    "    n_triplets = 0\n",
    "    batch_size = 100000\n",
    "\n",
    "    def __init__(self, train_scale=10000):\n",
    "        self.prepare_data()\n",
    "        self.save_size = train_scale\n",
    "        self.idf = data_utils.load_data(settings.GLOBAL_DATA_DIR, 'feature_idf.pkl')\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.name2pubs_train = data_utils.load_json(settings.GLOBAL_DATA_DIR, 'name_to_pubs_train_500.json')  # for test\n",
    "        self.name2pubs_test = data_utils.load_json(settings.GLOBAL_DATA_DIR, 'name_to_pubs_test_100.json')\n",
    "        self.names_train = self.name2pubs_train.keys()\n",
    "        print('names train', len(self.names_train))\n",
    "        self.names_test = self.name2pubs_test.keys()\n",
    "        print('names test', len(self.names_test))\n",
    "        assert not set(self.names_train).intersection(set(self.names_test))\n",
    "        for name in self.names_train:\n",
    "            name_pubs_dict = self.name2pubs_train[name]\n",
    "            for aid in name_pubs_dict:\n",
    "                self.pids_train += name_pubs_dict[aid]\n",
    "        random.shuffle(self.pids_train)\n",
    "        self.n_pubs_train = len(self.pids_train)\n",
    "        print('pubs2train', self.n_pubs_train)\n",
    "\n",
    "        for name in self.names_test:\n",
    "            name_pubs_dict = self.name2pubs_test[name]\n",
    "            for aid in name_pubs_dict:\n",
    "                self.pids_test += name_pubs_dict[aid]\n",
    "        random.shuffle(self.pids_test)\n",
    "        self.n_pubs_test = len(self.pids_test)\n",
    "        print('pubs2test', self.n_pubs_test)\n",
    "\n",
    "    def gen_neg_pid(self, not_in_pids, role='train'):\n",
    "        if role == 'train':\n",
    "            sample_from_pids = self.pids_train\n",
    "        else:\n",
    "            sample_from_pids = self.pids_test\n",
    "        while True:\n",
    "            idx = random.randint(0, len(sample_from_pids)-1)\n",
    "            pid = sample_from_pids[idx]\n",
    "            if pid not in not_in_pids:\n",
    "                return pid\n",
    "\n",
    "    def sample_triplet_ids(self, task_q, role='train', N_PROC=8):\n",
    "        n_sample_triplets = 0\n",
    "        if role == 'train':\n",
    "            names = self.names_train\n",
    "            name2pubs = self.name2pubs_train\n",
    "        else:  # test\n",
    "            names = self.names_test\n",
    "            name2pubs = self.name2pubs_test\n",
    "            self.save_size = 200000  # test save size\n",
    "        for name in names:\n",
    "            name_pubs_dict = name2pubs[name]\n",
    "            for aid in name_pubs_dict:\n",
    "                pub_items = name_pubs_dict[aid]\n",
    "                if len(pub_items) == 1:\n",
    "                    continue\n",
    "                pids = pub_items\n",
    "                cur_n_pubs = len(pids)\n",
    "                random.shuffle(pids)\n",
    "                for i in range(cur_n_pubs):\n",
    "                    pid1 = pids[i]  # pid\n",
    "\n",
    "                    # batch samples\n",
    "                    n_samples_anchor = min(6, cur_n_pubs)\n",
    "                    idx_pos = random.sample(range(cur_n_pubs), n_samples_anchor)\n",
    "                    for ii, i_pos in enumerate(idx_pos):\n",
    "                        if i_pos != i:\n",
    "                            if n_sample_triplets % 100 == 0:\n",
    "                                # print('sampled triplet ids', n_sample_triplets)\n",
    "                                pass\n",
    "                            pid_pos = pids[i_pos]\n",
    "                            pid_neg = self.gen_neg_pid(pids, role)\n",
    "                            n_sample_triplets += 1\n",
    "                            task_q.put((pid1, pid_pos, pid_neg))\n",
    "\n",
    "                            if n_sample_triplets >= self.save_size:\n",
    "                                for j in range(N_PROC):\n",
    "                                    task_q.put((None, None, None))\n",
    "                                return\n",
    "        for j in range(N_PROC):\n",
    "            task_q.put((None, None, None))\n",
    "\n",
    "    def gen_emb_mp(self, task_q, emb_q):\n",
    "        while True:\n",
    "            pid1, pid_pos, pid_neg = task_q.get()\n",
    "            if pid1 is None:\n",
    "                break\n",
    "            emb1 = lc.get(pid1)\n",
    "            emb_pos = lc.get(pid_pos)\n",
    "            emb_neg = lc.get(pid_neg)\n",
    "            if emb1 is not None and emb_pos is not None and emb_neg is not None:\n",
    "                emb_q.put((emb1, emb_pos, emb_neg))\n",
    "        emb_q.put((False, False, False))\n",
    "\n",
    "    def gen_triplets_mp(self, role='train'):\n",
    "        N_PROC = 8\n",
    "\n",
    "        task_q = mp.Queue(N_PROC * 6)\n",
    "        emb_q = mp.Queue(1000)\n",
    "\n",
    "        producer_p = mp.Process(target=self.sample_triplet_ids, args=(task_q, role, N_PROC))\n",
    "        consumer_ps = [mp.Process(target=self.gen_emb_mp, args=(task_q, emb_q)) for _ in range(N_PROC)]\n",
    "        producer_p.start()\n",
    "        [p.start() for p in consumer_ps]\n",
    "\n",
    "        cnt = 0\n",
    "\n",
    "        while True:\n",
    "            if cnt % 1000 == 0:\n",
    "                print('get', cnt, datetime.now()-start_time)\n",
    "            emb1, emb_pos, emb_neg = emb_q.get()\n",
    "            if emb1 is False:\n",
    "                producer_p.terminate()\n",
    "                producer_p.join()\n",
    "                [p.terminate() for p in consumer_ps]\n",
    "                [p.join() for p in consumer_ps]\n",
    "                break\n",
    "            cnt += 1\n",
    "            yield (emb1, emb_pos, emb_neg)\n",
    "\n",
    "    def dump_triplets(self, role='train'):\n",
    "        triplets = self.gen_triplets_mp(role)\n",
    "        if role == 'train':\n",
    "            out_dir = join(settings.OUT_DIR, 'triplets-{}'.format(self.save_size))\n",
    "        else:\n",
    "            out_dir = join(settings.OUT_DIR, 'test-triplets')\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        anchor_embs = []\n",
    "        pos_embs = []\n",
    "        neg_embs = []\n",
    "        f_idx = 0\n",
    "        for i, t in enumerate(triplets):\n",
    "            if i % 100 == 0:\n",
    "                print(i, datetime.now()-start_time)\n",
    "            emb_anc, emb_pos, emb_neg = t[0], t[1], t[2]\n",
    "            anchor_embs.append(emb_anc)\n",
    "            pos_embs.append(emb_pos)\n",
    "            neg_embs.append(emb_neg)\n",
    "            if len(anchor_embs) == self.batch_size:\n",
    "                data_utils.dump_data(anchor_embs, out_dir, 'anchor_embs_{}_{}.pkl'.format(role, f_idx))\n",
    "                data_utils.dump_data(pos_embs, out_dir, 'pos_embs_{}_{}.pkl'.format(role, f_idx))\n",
    "                data_utils.dump_data(neg_embs, out_dir, 'neg_embs_{}_{}.pkl'.format(role, f_idx))\n",
    "                f_idx += 1\n",
    "                anchor_embs = []\n",
    "                pos_embs = []\n",
    "                neg_embs = []\n",
    "        if anchor_embs:\n",
    "            data_utils.dump_data(anchor_embs, out_dir, 'anchor_embs_{}_{}.pkl'.format(role, f_idx))\n",
    "            data_utils.dump_data(pos_embs, out_dir, 'pos_embs_{}_{}.pkl'.format(role, f_idx))\n",
    "            data_utils.dump_data(neg_embs, out_dir, 'neg_embs_{}_{}.pkl'.format(role, f_idx))\n",
    "        print('dumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d866ebf-e1b6-4f5d-970f-bf4b25cd5585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB directory: /home/netdb/project/Name_Disambiguation_BERT/data/lmdb/author_100_emb_weighted\n",
      "names train 500\n",
      "names test 100\n",
      "pubs2train 173698\n",
      "pubs2test 35129\n",
      "get 0 0:00:00.556977\n",
      "0 0:00:00.557749\n",
      "100 0:00:00.561587\n",
      "200 0:00:00.563693\n",
      "300 0:00:00.565761\n",
      "400 0:00:00.567806\n",
      "500 0:00:00.569664\n",
      "600 0:00:00.571510\n",
      "700 0:00:00.573472\n",
      "800 0:00:00.575406\n",
      "900 0:00:00.577342\n",
      "get 1000 0:00:00.578962\n",
      "1000 0:00:00.578991\n",
      "1100 0:00:00.580678\n",
      "1200 0:00:00.582281\n",
      "1300 0:00:00.584125\n",
      "1400 0:00:00.585713\n",
      "1500 0:00:00.587355\n",
      "1600 0:00:00.589083\n",
      "1700 0:00:00.590737\n",
      "1800 0:00:00.592531\n",
      "1900 0:00:00.594376\n",
      "get 2000 0:00:00.595908\n",
      "2000 0:00:00.595934\n",
      "2100 0:00:00.597589\n",
      "2200 0:00:00.599234\n",
      "2300 0:00:00.600842\n",
      "2400 0:00:00.602513\n",
      "2500 0:00:00.604096\n",
      "2600 0:00:00.605690\n",
      "2700 0:00:00.607277\n",
      "2800 0:00:00.608876\n",
      "2900 0:00:00.610608\n",
      "get 3000 0:00:00.612125\n",
      "3000 0:00:00.612148\n",
      "3100 0:00:00.613761\n",
      "3200 0:00:00.615347\n",
      "3300 0:00:00.616928\n",
      "3400 0:00:00.618698\n",
      "3500 0:00:00.620229\n",
      "3600 0:00:00.621989\n",
      "3700 0:00:00.624000\n",
      "3800 0:00:00.625534\n",
      "3900 0:00:00.627097\n",
      "get 4000 0:00:00.628622\n",
      "4000 0:00:00.628649\n",
      "4100 0:00:00.630225\n",
      "4200 0:00:00.631805\n",
      "4300 0:00:00.633562\n",
      "4400 0:00:00.635237\n",
      "4500 0:00:00.636899\n",
      "4600 0:00:00.638605\n",
      "4700 0:00:00.640237\n",
      "4800 0:00:00.641872\n",
      "4900 0:00:00.643589\n",
      "get 5000 0:00:00.645164\n",
      "5000 0:00:00.645193\n",
      "5100 0:00:00.646745\n",
      "5200 0:00:00.648435\n",
      "5300 0:00:00.650077\n",
      "5400 0:00:00.651687\n",
      "5500 0:00:00.653255\n",
      "5600 0:00:00.654883\n",
      "5700 0:00:00.656407\n",
      "5800 0:00:00.657987\n",
      "5900 0:00:00.659711\n",
      "get 6000 0:00:00.661247\n",
      "6000 0:00:00.661295\n",
      "6100 0:00:00.662739\n",
      "6200 0:00:00.664294\n",
      "6300 0:00:00.665858\n",
      "6400 0:00:00.667352\n",
      "6500 0:00:00.669152\n",
      "6600 0:00:00.671167\n",
      "6700 0:00:00.672747\n",
      "6800 0:00:00.674419\n",
      "6900 0:00:00.675986\n",
      "get 7000 0:00:00.677454\n",
      "7000 0:00:00.677476\n",
      "7100 0:00:00.679096\n",
      "7200 0:00:00.680713\n",
      "7300 0:00:00.682252\n",
      "7400 0:00:00.683970\n",
      "7500 0:00:00.685872\n",
      "7600 0:00:00.687388\n",
      "7700 0:00:00.688900\n",
      "7800 0:00:00.690675\n",
      "7900 0:00:00.692104\n",
      "get 8000 0:00:00.693659\n",
      "8000 0:00:00.693684\n",
      "8100 0:00:00.695284\n",
      "8200 0:00:00.696875\n",
      "8300 0:00:00.698507\n",
      "8400 0:00:00.700185\n",
      "8500 0:00:00.701767\n",
      "8600 0:00:00.703307\n",
      "8700 0:00:00.704822\n",
      "8800 0:00:00.706312\n",
      "8900 0:00:00.708136\n",
      "get 9000 0:00:00.709654\n",
      "9000 0:00:00.709683\n",
      "9100 0:00:00.711403\n",
      "9200 0:00:00.713012\n",
      "9300 0:00:00.714800\n",
      "9400 0:00:00.716437\n",
      "9500 0:00:00.718174\n",
      "9600 0:00:00.719809\n",
      "9700 0:00:00.721625\n",
      "9800 0:00:00.723416\n",
      "9900 0:00:00.725124\n",
      "get 10000 0:00:00.727024\n",
      "10000 0:00:00.727047\n",
      "10100 0:00:00.728979\n",
      "10200 0:00:00.730627\n",
      "10300 0:00:00.732501\n",
      "10400 0:00:00.734066\n",
      "10500 0:00:00.735942\n",
      "10600 0:00:00.737509\n",
      "10700 0:00:00.739141\n",
      "10800 0:00:00.740693\n",
      "10900 0:00:00.742246\n",
      "get 11000 0:00:00.743814\n",
      "11000 0:00:00.743842\n",
      "11100 0:00:00.745553\n",
      "11200 0:00:00.747226\n",
      "11300 0:00:00.749106\n",
      "11400 0:00:00.750765\n",
      "11500 0:00:00.752496\n",
      "11600 0:00:00.754060\n",
      "11700 0:00:00.755615\n",
      "11800 0:00:00.757198\n",
      "11900 0:00:00.759286\n",
      "get 12000 0:00:00.761276\n",
      "12000 0:00:00.761327\n",
      "12100 0:00:00.763033\n",
      "12200 0:00:00.764923\n",
      "12300 0:00:00.766364\n",
      "12400 0:00:00.767768\n",
      "12500 0:00:00.769224\n",
      "12600 0:00:00.770702\n",
      "12700 0:00:00.772135\n",
      "12800 0:00:00.773870\n",
      "12900 0:00:00.775604\n",
      "get 13000 0:00:00.777293\n",
      "13000 0:00:00.777321\n",
      "13100 0:00:00.778991\n",
      "13200 0:00:00.780832\n",
      "13300 0:00:00.782554\n",
      "13400 0:00:00.784221\n",
      "13500 0:00:00.786152\n",
      "13600 0:00:00.787980\n",
      "13700 0:00:00.789725\n",
      "13800 0:00:00.791433\n",
      "13900 0:00:00.793285\n",
      "get 14000 0:00:00.795014\n",
      "14000 0:00:00.795037\n",
      "14100 0:00:00.796814\n",
      "14200 0:00:00.798491\n",
      "14300 0:00:00.800166\n",
      "14400 0:00:00.802060\n",
      "14500 0:00:00.803780\n",
      "14600 0:00:00.805457\n",
      "14700 0:00:00.807304\n",
      "14800 0:00:00.809021\n",
      "14900 0:00:00.810738\n",
      "get 15000 0:00:00.812689\n",
      "15000 0:00:00.812713\n",
      "15100 0:00:00.814455\n",
      "15200 0:00:00.816299\n",
      "15300 0:00:00.818084\n",
      "15400 0:00:00.819979\n",
      "15500 0:00:00.822017\n",
      "15600 0:00:00.823894\n",
      "15700 0:00:00.826172\n",
      "15800 0:00:00.827867\n",
      "15900 0:00:00.829555\n",
      "get 16000 0:00:00.831232\n",
      "16000 0:00:00.831254\n",
      "16100 0:00:00.833343\n",
      "16200 0:00:00.835061\n",
      "16300 0:00:00.837049\n",
      "16400 0:00:00.838874\n",
      "16500 0:00:00.840673\n",
      "16600 0:00:00.842410\n",
      "16700 0:00:00.844103\n",
      "16800 0:00:00.846004\n",
      "16900 0:00:00.847800\n",
      "get 17000 0:00:00.849467\n",
      "17000 0:00:00.849493\n",
      "17100 0:00:00.851605\n",
      "17200 0:00:00.853726\n",
      "17300 0:00:00.855396\n",
      "17400 0:00:00.857545\n",
      "17500 0:00:00.859495\n",
      "17600 0:00:00.861190\n",
      "17700 0:00:00.863317\n",
      "17800 0:00:00.865041\n",
      "17900 0:00:00.867043\n",
      "get 18000 0:00:00.868748\n",
      "18000 0:00:00.868772\n",
      "18100 0:00:00.870548\n",
      "18200 0:00:00.872142\n",
      "18300 0:00:00.873956\n",
      "18400 0:00:00.875508\n",
      "18500 0:00:00.876942\n",
      "18600 0:00:00.878616\n",
      "18700 0:00:00.880139\n",
      "18800 0:00:00.881572\n",
      "18900 0:00:00.883383\n",
      "get 19000 0:00:00.884979\n",
      "19000 0:00:00.885007\n",
      "19100 0:00:00.886480\n",
      "19200 0:00:00.888103\n",
      "19300 0:00:00.890010\n",
      "19400 0:00:00.891377\n",
      "19500 0:00:00.892772\n",
      "19600 0:00:00.894328\n",
      "19700 0:00:00.895927\n",
      "19800 0:00:00.897590\n",
      "19900 0:00:00.899318\n",
      "get 20000 0:00:00.900983\n",
      "20000 0:00:00.901010\n",
      "20100 0:00:00.902917\n",
      "20200 0:00:00.904983\n",
      "20300 0:00:00.906474\n",
      "20400 0:00:00.908135\n",
      "20500 0:00:00.910232\n",
      "20600 0:00:00.912199\n",
      "20700 0:00:00.914255\n",
      "20800 0:00:00.916088\n",
      "20900 0:00:00.917675\n",
      "get 21000 0:00:00.919493\n",
      "21000 0:00:00.919518\n",
      "21100 0:00:00.921193\n",
      "21200 0:00:00.923051\n",
      "21300 0:00:00.924793\n",
      "21400 0:00:00.926739\n",
      "21500 0:00:00.928382\n",
      "21600 0:00:00.929827\n",
      "21700 0:00:00.932079\n",
      "21800 0:00:00.933781\n",
      "21900 0:00:00.935627\n",
      "get 22000 0:00:00.937314\n",
      "22000 0:00:00.937341\n",
      "22100 0:00:00.938940\n",
      "22200 0:00:00.940764\n",
      "22300 0:00:00.942669\n",
      "22400 0:00:00.944539\n",
      "22500 0:00:00.946448\n",
      "22600 0:00:00.948255\n",
      "22700 0:00:00.950113\n",
      "22800 0:00:00.951962\n",
      "22900 0:00:00.953893\n",
      "get 23000 0:00:00.955639\n",
      "23000 0:00:00.955666\n",
      "23100 0:00:00.957552\n",
      "23200 0:00:00.959377\n",
      "23300 0:00:00.961660\n",
      "23400 0:00:00.963665\n",
      "23500 0:00:00.965390\n",
      "23600 0:00:00.967172\n",
      "23700 0:00:00.968920\n",
      "23800 0:00:00.970718\n",
      "23900 0:00:00.972515\n",
      "get 24000 0:00:00.974389\n",
      "24000 0:00:00.974415\n",
      "24100 0:00:00.976115\n",
      "24200 0:00:00.977943\n",
      "24300 0:00:00.979707\n",
      "24400 0:00:00.981580\n",
      "24500 0:00:00.983439\n",
      "24600 0:00:00.985203\n",
      "24700 0:00:00.986796\n",
      "24800 0:00:00.988148\n",
      "24900 0:00:00.989494\n",
      "get 25000 0:00:00.990958\n",
      "25000 0:00:00.990989\n",
      "25100 0:00:00.992515\n",
      "25200 0:00:00.994001\n",
      "25300 0:00:00.995591\n",
      "25400 0:00:00.997541\n",
      "25500 0:00:00.999311\n",
      "25600 0:00:01.000714\n",
      "25700 0:00:01.002255\n",
      "25800 0:00:01.004004\n",
      "25900 0:00:01.005753\n",
      "get 26000 0:00:01.007365\n",
      "26000 0:00:01.007387\n",
      "26100 0:00:01.009092\n",
      "26200 0:00:01.010889\n",
      "26300 0:00:01.012549\n",
      "26400 0:00:01.014401\n",
      "26500 0:00:01.016148\n",
      "26600 0:00:01.017908\n",
      "26700 0:00:01.019448\n",
      "26800 0:00:01.021121\n",
      "26900 0:00:01.022821\n",
      "get 27000 0:00:01.024520\n",
      "27000 0:00:01.024554\n",
      "27100 0:00:01.026236\n",
      "27200 0:00:01.027864\n",
      "27300 0:00:01.029623\n",
      "27400 0:00:01.031281\n",
      "27500 0:00:01.032927\n",
      "27600 0:00:01.034652\n",
      "27700 0:00:01.036345\n",
      "27800 0:00:01.038127\n",
      "27900 0:00:01.039974\n",
      "get 28000 0:00:01.041734\n",
      "28000 0:00:01.041759\n",
      "28100 0:00:01.043535\n",
      "28200 0:00:01.045284\n",
      "28300 0:00:01.047061\n",
      "28400 0:00:01.048696\n",
      "28500 0:00:01.050545\n",
      "28600 0:00:01.052267\n",
      "28700 0:00:01.054085\n",
      "28800 0:00:01.055904\n",
      "28900 0:00:01.057436\n",
      "get 29000 0:00:01.059151\n",
      "29000 0:00:01.059173\n",
      "29100 0:00:01.060937\n",
      "29200 0:00:01.062718\n",
      "29300 0:00:01.064621\n",
      "29400 0:00:01.066360\n",
      "29500 0:00:01.068045\n",
      "29600 0:00:01.069804\n",
      "29700 0:00:01.071685\n",
      "29800 0:00:01.073427\n",
      "29900 0:00:01.075344\n",
      "get 30000 0:00:01.077185\n",
      "30000 0:00:01.077226\n",
      "30100 0:00:01.078971\n",
      "30200 0:00:01.080680\n",
      "30300 0:00:01.082474\n",
      "30400 0:00:01.084409\n",
      "30500 0:00:01.086079\n",
      "30600 0:00:01.087937\n",
      "30700 0:00:01.089685\n",
      "30800 0:00:01.091632\n",
      "30900 0:00:01.093292\n",
      "get 31000 0:00:01.095185\n",
      "31000 0:00:01.095210\n",
      "31100 0:00:01.097050\n",
      "31200 0:00:01.098774\n",
      "31300 0:00:01.100654\n",
      "31400 0:00:01.102529\n",
      "31500 0:00:01.104221\n",
      "31600 0:00:01.105892\n",
      "31700 0:00:01.107618\n",
      "31800 0:00:01.109391\n",
      "31900 0:00:01.111068\n",
      "get 32000 0:00:01.112830\n",
      "32000 0:00:01.112855\n",
      "32100 0:00:01.114613\n",
      "32200 0:00:01.116330\n",
      "32300 0:00:01.118115\n",
      "32400 0:00:01.120149\n",
      "32500 0:00:01.121860\n",
      "32600 0:00:01.123552\n",
      "32700 0:00:01.125313\n",
      "32800 0:00:01.127045\n",
      "32900 0:00:01.128697\n",
      "get 33000 0:00:01.130384\n",
      "33000 0:00:01.130409\n",
      "33100 0:00:01.132276\n",
      "33200 0:00:01.134067\n",
      "33300 0:00:01.135682\n",
      "33400 0:00:01.137477\n",
      "33500 0:00:01.139279\n",
      "33600 0:00:01.141014\n",
      "33700 0:00:01.142735\n",
      "33800 0:00:01.144403\n",
      "33900 0:00:01.146247\n",
      "get 34000 0:00:01.147889\n",
      "34000 0:00:01.147911\n",
      "34100 0:00:01.149597\n",
      "34200 0:00:01.151295\n",
      "34300 0:00:01.153038\n",
      "34400 0:00:01.154764\n",
      "34500 0:00:01.156541\n",
      "34600 0:00:01.158256\n",
      "34700 0:00:01.159997\n",
      "34800 0:00:01.161778\n",
      "34900 0:00:01.163934\n",
      "get 35000 0:00:01.165456\n",
      "35000 0:00:01.165481\n",
      "35100 0:00:01.167113\n",
      "35200 0:00:01.168869\n",
      "35300 0:00:01.170408\n",
      "35400 0:00:01.172279\n",
      "35500 0:00:01.173798\n",
      "35600 0:00:01.175902\n",
      "35700 0:00:01.177715\n",
      "35800 0:00:01.179599\n",
      "35900 0:00:01.181450\n",
      "get 36000 0:00:01.183322\n",
      "36000 0:00:01.183357\n",
      "36100 0:00:01.185173\n",
      "36200 0:00:01.186998\n",
      "36300 0:00:01.188839\n",
      "36400 0:00:01.190717\n",
      "36500 0:00:01.192507\n",
      "36600 0:00:01.194466\n",
      "36700 0:00:01.196302\n",
      "36800 0:00:01.198284\n",
      "36900 0:00:01.200649\n",
      "get 37000 0:00:01.202797\n",
      "37000 0:00:01.202833\n",
      "37100 0:00:01.204670\n",
      "37200 0:00:01.206547\n",
      "37300 0:00:01.208367\n",
      "37400 0:00:01.210317\n",
      "37500 0:00:01.212233\n",
      "37600 0:00:01.214038\n",
      "37700 0:00:01.215922\n",
      "37800 0:00:01.217834\n",
      "37900 0:00:01.219825\n",
      "get 38000 0:00:01.221861\n",
      "38000 0:00:01.221928\n",
      "38100 0:00:01.223816\n",
      "38200 0:00:01.225654\n",
      "38300 0:00:01.227652\n",
      "38400 0:00:01.229475\n",
      "38500 0:00:01.231416\n",
      "38600 0:00:01.233426\n",
      "38700 0:00:01.235305\n",
      "38800 0:00:01.237134\n",
      "38900 0:00:01.239080\n",
      "get 39000 0:00:01.241297\n",
      "39000 0:00:01.241325\n",
      "39100 0:00:01.242795\n",
      "39200 0:00:01.244689\n",
      "39300 0:00:01.246676\n",
      "39400 0:00:01.248577\n",
      "39500 0:00:01.250432\n",
      "39600 0:00:01.252270\n",
      "39700 0:00:01.254382\n",
      "39800 0:00:01.256279\n",
      "39900 0:00:01.258249\n",
      "get 40000 0:00:01.260210\n",
      "40000 0:00:01.260249\n",
      "40100 0:00:01.262161\n",
      "40200 0:00:01.263696\n",
      "40300 0:00:01.266039\n",
      "40400 0:00:01.268046\n",
      "40500 0:00:01.269667\n",
      "40600 0:00:01.271367\n",
      "40700 0:00:01.273312\n",
      "40800 0:00:01.275078\n",
      "40900 0:00:01.276860\n",
      "get 41000 0:00:01.278748\n",
      "41000 0:00:01.278771\n",
      "41100 0:00:01.280413\n",
      "41200 0:00:01.282299\n",
      "41300 0:00:01.283958\n",
      "41400 0:00:01.285485\n",
      "41500 0:00:01.287029\n",
      "41600 0:00:01.288551\n",
      "41700 0:00:01.290039\n",
      "41800 0:00:01.291900\n",
      "41900 0:00:01.293841\n",
      "get 42000 0:00:01.295531\n",
      "42000 0:00:01.295556\n",
      "42100 0:00:01.297290\n",
      "42200 0:00:01.298942\n",
      "42300 0:00:01.300742\n",
      "42400 0:00:01.302438\n",
      "42500 0:00:01.303975\n",
      "42600 0:00:01.305828\n",
      "42700 0:00:01.307404\n",
      "42800 0:00:01.309007\n",
      "42900 0:00:01.310740\n",
      "get 43000 0:00:01.312775\n",
      "43000 0:00:01.312800\n",
      "43100 0:00:01.314454\n",
      "43200 0:00:01.316465\n",
      "43300 0:00:01.318600\n",
      "43400 0:00:01.320678\n",
      "43500 0:00:01.322756\n",
      "43600 0:00:01.325085\n",
      "43700 0:00:01.327223\n",
      "43800 0:00:01.329406\n",
      "43900 0:00:01.332068\n",
      "get 44000 0:00:01.333872\n",
      "44000 0:00:01.333895\n",
      "44100 0:00:01.336150\n",
      "44200 0:00:01.338360\n",
      "44300 0:00:01.340570\n",
      "44400 0:00:01.342864\n",
      "44500 0:00:01.345132\n",
      "44600 0:00:01.347424\n",
      "44700 0:00:01.349594\n",
      "44800 0:00:01.351859\n",
      "44900 0:00:01.353912\n",
      "get 45000 0:00:01.355984\n",
      "45000 0:00:01.356026\n",
      "45100 0:00:01.358227\n",
      "45200 0:00:01.360347\n",
      "45300 0:00:01.362490\n",
      "45400 0:00:01.364951\n",
      "45500 0:00:01.367031\n",
      "45600 0:00:01.369101\n",
      "45700 0:00:01.371251\n",
      "45800 0:00:01.373321\n",
      "45900 0:00:01.375514\n",
      "get 46000 0:00:01.377594\n",
      "46000 0:00:01.377628\n",
      "46100 0:00:01.379676\n",
      "46200 0:00:01.382004\n",
      "46300 0:00:01.383963\n",
      "46400 0:00:01.386178\n",
      "46500 0:00:01.388532\n",
      "46600 0:00:01.390628\n",
      "46700 0:00:01.392759\n",
      "46800 0:00:01.394937\n",
      "46900 0:00:01.397320\n",
      "get 47000 0:00:01.399487\n",
      "47000 0:00:01.399512\n",
      "47100 0:00:01.401977\n",
      "47200 0:00:01.404058\n",
      "47300 0:00:01.406251\n",
      "47400 0:00:01.408466\n",
      "47500 0:00:01.410713\n",
      "47600 0:00:01.412804\n",
      "47700 0:00:01.414859\n",
      "47800 0:00:01.416851\n",
      "47900 0:00:01.418822\n",
      "get 48000 0:00:01.420897\n",
      "48000 0:00:01.420921\n",
      "48100 0:00:01.423130\n",
      "48200 0:00:01.425216\n",
      "48300 0:00:01.427244\n",
      "48400 0:00:01.429198\n",
      "48500 0:00:01.431234\n",
      "48600 0:00:01.433318\n",
      "48700 0:00:01.435393\n",
      "48800 0:00:01.437264\n",
      "48900 0:00:01.439464\n",
      "get 49000 0:00:01.441401\n",
      "49000 0:00:01.441431\n",
      "49100 0:00:01.443416\n",
      "49200 0:00:01.445507\n",
      "49300 0:00:01.447632\n",
      "49400 0:00:01.449987\n",
      "49500 0:00:01.451764\n",
      "49600 0:00:01.453921\n",
      "49700 0:00:01.456072\n",
      "49800 0:00:01.458469\n",
      "49900 0:00:01.460693\n",
      "get 50000 0:00:01.462800\n",
      "50000 0:00:01.462843\n",
      "50100 0:00:01.464894\n",
      "50200 0:00:01.467026\n",
      "50300 0:00:01.469254\n",
      "50400 0:00:01.471188\n",
      "50500 0:00:01.473264\n",
      "50600 0:00:01.475169\n",
      "50700 0:00:01.477297\n",
      "50800 0:00:01.479555\n",
      "50900 0:00:01.481769\n",
      "get 51000 0:00:01.483844\n",
      "51000 0:00:01.483867\n",
      "51100 0:00:01.486104\n",
      "51200 0:00:01.488617\n",
      "51300 0:00:01.490816\n",
      "51400 0:00:01.493458\n",
      "51500 0:00:01.496146\n",
      "51600 0:00:01.498481\n",
      "51700 0:00:01.500760\n",
      "51800 0:00:01.502996\n",
      "51900 0:00:01.505333\n",
      "get 52000 0:00:01.507498\n",
      "52000 0:00:01.507533\n",
      "52100 0:00:01.509795\n",
      "52200 0:00:01.511930\n",
      "52300 0:00:01.514337\n",
      "52400 0:00:01.516524\n",
      "52500 0:00:01.518803\n",
      "52600 0:00:01.520732\n",
      "52700 0:00:01.522604\n",
      "52800 0:00:01.524712\n",
      "52900 0:00:01.526976\n",
      "get 53000 0:00:01.529353\n",
      "53000 0:00:01.529380\n",
      "53100 0:00:01.531521\n",
      "53200 0:00:01.533719\n",
      "53300 0:00:01.536000\n",
      "53400 0:00:01.538392\n",
      "53500 0:00:01.540428\n",
      "53600 0:00:01.542603\n",
      "53700 0:00:01.544886\n",
      "53800 0:00:01.547170\n",
      "53900 0:00:01.549503\n",
      "get 54000 0:00:01.551654\n",
      "54000 0:00:01.551683\n",
      "54100 0:00:01.554077\n",
      "54200 0:00:01.556290\n",
      "54300 0:00:01.558669\n",
      "54400 0:00:01.561024\n",
      "54500 0:00:01.563283\n",
      "54600 0:00:01.565797\n",
      "54700 0:00:01.567877\n",
      "54800 0:00:01.570046\n",
      "54900 0:00:01.572323\n",
      "get 55000 0:00:01.574739\n",
      "55000 0:00:01.574771\n",
      "55100 0:00:01.576867\n",
      "55200 0:00:01.578993\n",
      "55300 0:00:01.581290\n",
      "55400 0:00:01.583522\n",
      "55500 0:00:01.585541\n",
      "55600 0:00:01.587783\n",
      "55700 0:00:01.589968\n",
      "55800 0:00:01.592205\n",
      "55900 0:00:01.594210\n",
      "get 56000 0:00:01.596503\n",
      "56000 0:00:01.596541\n",
      "56100 0:00:01.598806\n",
      "56200 0:00:01.601062\n",
      "56300 0:00:01.603230\n",
      "56400 0:00:01.605564\n",
      "56500 0:00:01.607690\n",
      "56600 0:00:01.609984\n",
      "56700 0:00:01.612310\n",
      "56800 0:00:01.614671\n",
      "56900 0:00:01.616677\n",
      "get 57000 0:00:01.618491\n",
      "57000 0:00:01.618518\n",
      "57100 0:00:01.620437\n",
      "57200 0:00:01.622291\n",
      "57300 0:00:01.624099\n",
      "57400 0:00:01.626036\n",
      "57500 0:00:01.628082\n",
      "57600 0:00:01.629865\n",
      "57700 0:00:01.632207\n",
      "57800 0:00:01.634081\n",
      "57900 0:00:01.636088\n",
      "get 58000 0:00:01.638360\n",
      "58000 0:00:01.638398\n",
      "58100 0:00:01.640584\n",
      "58200 0:00:01.642910\n",
      "58300 0:00:01.644985\n",
      "58400 0:00:01.647119\n",
      "58500 0:00:01.649034\n",
      "58600 0:00:01.651167\n",
      "58700 0:00:01.653441\n",
      "58800 0:00:01.655550\n",
      "58900 0:00:01.657779\n",
      "get 59000 0:00:01.659724\n",
      "59000 0:00:01.659763\n",
      "59100 0:00:01.661877\n",
      "59200 0:00:01.664104\n",
      "59300 0:00:01.666172\n",
      "59400 0:00:01.668323\n",
      "59500 0:00:01.670612\n",
      "59600 0:00:01.672742\n",
      "59700 0:00:01.674899\n",
      "59800 0:00:01.677058\n",
      "59900 0:00:01.678920\n",
      "get 60000 0:00:01.680900\n",
      "60000 0:00:01.680943\n",
      "60100 0:00:01.683281\n",
      "60200 0:00:01.685363\n",
      "60300 0:00:01.687468\n",
      "60400 0:00:01.689710\n",
      "60500 0:00:01.691901\n",
      "60600 0:00:01.693974\n",
      "60700 0:00:01.695978\n",
      "60800 0:00:01.698250\n",
      "60900 0:00:01.700409\n",
      "get 61000 0:00:01.702682\n",
      "61000 0:00:01.702735\n",
      "61100 0:00:01.704854\n",
      "61200 0:00:01.706911\n",
      "61300 0:00:01.709318\n",
      "61400 0:00:01.711391\n",
      "61500 0:00:01.713670\n",
      "61600 0:00:01.715628\n",
      "61700 0:00:01.717685\n",
      "61800 0:00:01.719794\n",
      "61900 0:00:01.721990\n",
      "get 62000 0:00:01.724191\n",
      "62000 0:00:01.724220\n",
      "62100 0:00:01.726414\n",
      "62200 0:00:01.728477\n",
      "62300 0:00:01.730738\n",
      "62400 0:00:01.732792\n",
      "62500 0:00:01.735453\n",
      "62600 0:00:01.737093\n",
      "62700 0:00:01.739497\n",
      "62800 0:00:01.741664\n",
      "62900 0:00:01.743789\n",
      "get 63000 0:00:01.746071\n",
      "63000 0:00:01.746097\n",
      "63100 0:00:01.748315\n",
      "63200 0:00:01.750568\n",
      "63300 0:00:01.753013\n",
      "63400 0:00:01.755322\n",
      "63500 0:00:01.757543\n",
      "63600 0:00:01.759742\n",
      "63700 0:00:01.762046\n",
      "63800 0:00:01.764222\n",
      "63900 0:00:01.766781\n",
      "get 64000 0:00:01.768863\n",
      "64000 0:00:01.768897\n",
      "64100 0:00:01.771144\n",
      "64200 0:00:01.773249\n",
      "64300 0:00:01.775507\n",
      "64400 0:00:01.777590\n",
      "64500 0:00:01.779844\n",
      "64600 0:00:01.782043\n",
      "64700 0:00:01.784369\n",
      "64800 0:00:01.786140\n",
      "64900 0:00:01.788060\n",
      "get 65000 0:00:01.790013\n",
      "65000 0:00:01.790045\n",
      "65100 0:00:01.791940\n",
      "65200 0:00:01.793914\n",
      "65300 0:00:01.795782\n",
      "65400 0:00:01.797579\n",
      "65500 0:00:01.799406\n",
      "65600 0:00:01.801172\n",
      "65700 0:00:01.803012\n",
      "65800 0:00:01.804838\n",
      "65900 0:00:01.806714\n",
      "get 66000 0:00:01.808627\n",
      "66000 0:00:01.808662\n",
      "66100 0:00:01.810418\n",
      "66200 0:00:01.812222\n",
      "66300 0:00:01.814039\n",
      "66400 0:00:01.815923\n",
      "66500 0:00:01.817773\n",
      "66600 0:00:01.819825\n",
      "66700 0:00:01.821612\n",
      "66800 0:00:01.823505\n",
      "66900 0:00:01.825463\n",
      "get 67000 0:00:01.827365\n",
      "67000 0:00:01.827407\n",
      "67100 0:00:01.829266\n",
      "67200 0:00:01.831077\n",
      "67300 0:00:01.832987\n",
      "67400 0:00:01.834939\n",
      "67500 0:00:01.836880\n",
      "67600 0:00:01.838886\n",
      "67700 0:00:01.840870\n",
      "67800 0:00:01.842894\n",
      "67900 0:00:01.844871\n",
      "get 68000 0:00:01.846929\n",
      "68000 0:00:01.846958\n",
      "68100 0:00:01.848992\n",
      "68200 0:00:01.851066\n",
      "68300 0:00:01.853117\n",
      "68400 0:00:01.855165\n",
      "68500 0:00:01.857212\n",
      "68600 0:00:01.859220\n",
      "68700 0:00:01.861208\n",
      "68800 0:00:01.863260\n",
      "68900 0:00:01.865316\n",
      "get 69000 0:00:01.867468\n",
      "69000 0:00:01.867501\n",
      "69100 0:00:01.869500\n",
      "69200 0:00:01.871549\n",
      "69300 0:00:01.873504\n",
      "69400 0:00:01.875589\n",
      "69500 0:00:01.877609\n",
      "69600 0:00:01.879636\n",
      "69700 0:00:01.881607\n",
      "69800 0:00:01.883650\n",
      "69900 0:00:01.885665\n",
      "get 70000 0:00:01.887695\n",
      "70000 0:00:01.887737\n",
      "70100 0:00:01.889726\n",
      "70200 0:00:01.891761\n",
      "70300 0:00:01.893748\n",
      "70400 0:00:01.895837\n",
      "70500 0:00:01.897813\n",
      "70600 0:00:01.899982\n",
      "70700 0:00:01.902104\n",
      "70800 0:00:01.904120\n",
      "70900 0:00:01.906200\n",
      "get 71000 0:00:01.908219\n",
      "71000 0:00:01.908251\n",
      "71100 0:00:01.910264\n",
      "71200 0:00:01.912298\n",
      "71300 0:00:01.914412\n",
      "71400 0:00:01.916485\n",
      "71500 0:00:01.918547\n",
      "71600 0:00:01.920587\n",
      "71700 0:00:01.922780\n",
      "71800 0:00:01.924843\n",
      "71900 0:00:01.926860\n",
      "get 72000 0:00:01.928803\n",
      "72000 0:00:01.928837\n",
      "72100 0:00:01.930873\n",
      "72200 0:00:01.932860\n",
      "72300 0:00:01.934877\n",
      "72400 0:00:01.936894\n",
      "72500 0:00:01.939002\n",
      "72600 0:00:01.940947\n",
      "72700 0:00:01.943019\n",
      "72800 0:00:01.945131\n",
      "72900 0:00:01.947189\n",
      "get 73000 0:00:01.949206\n",
      "73000 0:00:01.949237\n",
      "73100 0:00:01.951181\n",
      "73200 0:00:01.953171\n",
      "73300 0:00:01.955173\n",
      "73400 0:00:01.957221\n",
      "73500 0:00:01.959216\n",
      "73600 0:00:01.961206\n",
      "73700 0:00:01.963128\n",
      "73800 0:00:01.965130\n",
      "73900 0:00:01.967214\n",
      "get 74000 0:00:01.969547\n",
      "74000 0:00:01.969636\n",
      "74100 0:00:01.971697\n",
      "74200 0:00:01.973696\n",
      "74300 0:00:01.975793\n",
      "74400 0:00:01.977848\n",
      "74500 0:00:01.979854\n",
      "74600 0:00:01.981849\n",
      "74700 0:00:01.983864\n",
      "74800 0:00:01.985982\n",
      "74900 0:00:01.987952\n",
      "get 75000 0:00:01.989981\n",
      "75000 0:00:01.990011\n",
      "75100 0:00:01.991940\n",
      "75200 0:00:01.993957\n",
      "75300 0:00:01.996004\n",
      "75400 0:00:01.998013\n",
      "75500 0:00:01.999594\n",
      "75600 0:00:02.001230\n",
      "75700 0:00:02.002908\n",
      "75800 0:00:02.004585\n",
      "75900 0:00:02.006264\n",
      "get 76000 0:00:02.007894\n",
      "76000 0:00:02.007921\n",
      "76100 0:00:02.009589\n",
      "76200 0:00:02.011263\n",
      "76300 0:00:02.012914\n",
      "76400 0:00:02.014585\n",
      "76500 0:00:02.016280\n",
      "76600 0:00:02.017961\n",
      "76700 0:00:02.019634\n",
      "76800 0:00:02.021323\n",
      "76900 0:00:02.023267\n",
      "get 77000 0:00:02.025165\n",
      "77000 0:00:02.025194\n",
      "77100 0:00:02.027048\n",
      "77200 0:00:02.028960\n",
      "77300 0:00:02.030865\n",
      "77400 0:00:02.032843\n",
      "77500 0:00:02.034787\n",
      "77600 0:00:02.036662\n",
      "77700 0:00:02.038591\n",
      "77800 0:00:02.040470\n",
      "77900 0:00:02.042419\n",
      "get 78000 0:00:02.044339\n",
      "78000 0:00:02.044377\n",
      "78100 0:00:02.046201\n",
      "78200 0:00:02.048079\n",
      "78300 0:00:02.049995\n",
      "78400 0:00:02.051770\n",
      "78500 0:00:02.053637\n",
      "78600 0:00:02.055454\n",
      "78700 0:00:02.057287\n",
      "78800 0:00:02.058977\n",
      "78900 0:00:02.060354\n",
      "get 79000 0:00:02.062100\n",
      "79000 0:00:02.062130\n",
      "79100 0:00:02.064352\n",
      "79200 0:00:02.065970\n",
      "79300 0:00:02.067810\n",
      "79400 0:00:02.069659\n",
      "79500 0:00:02.071586\n",
      "79600 0:00:02.073478\n",
      "79700 0:00:02.075321\n",
      "79800 0:00:02.077067\n",
      "79900 0:00:02.078969\n",
      "get 80000 0:00:02.080787\n",
      "80000 0:00:02.080819\n",
      "80100 0:00:02.082699\n",
      "80200 0:00:02.084460\n",
      "80300 0:00:02.086201\n",
      "80400 0:00:02.088039\n",
      "80500 0:00:02.089890\n",
      "80600 0:00:02.091839\n",
      "80700 0:00:02.093554\n",
      "80800 0:00:02.095376\n",
      "80900 0:00:02.097239\n",
      "get 81000 0:00:02.099116\n",
      "81000 0:00:02.099147\n",
      "81100 0:00:02.101004\n",
      "81200 0:00:02.102840\n",
      "81300 0:00:02.104759\n",
      "81400 0:00:02.106702\n",
      "81500 0:00:02.108577\n",
      "81600 0:00:02.110549\n",
      "81700 0:00:02.112360\n",
      "81800 0:00:02.114245\n",
      "81900 0:00:02.115875\n",
      "get 82000 0:00:02.117699\n",
      "82000 0:00:02.117740\n",
      "82100 0:00:02.119494\n",
      "82200 0:00:02.121220\n",
      "82300 0:00:02.123005\n",
      "82400 0:00:02.124845\n",
      "82500 0:00:02.126745\n",
      "82600 0:00:02.128587\n",
      "82700 0:00:02.130469\n",
      "82800 0:00:02.132322\n",
      "82900 0:00:02.134305\n",
      "get 83000 0:00:02.136112\n",
      "83000 0:00:02.136139\n",
      "83100 0:00:02.137955\n",
      "83200 0:00:02.139849\n",
      "83300 0:00:02.141719\n",
      "83400 0:00:02.143683\n",
      "83500 0:00:02.145554\n",
      "83600 0:00:02.147369\n",
      "83700 0:00:02.149061\n",
      "83800 0:00:02.150842\n",
      "83900 0:00:02.152715\n",
      "get 84000 0:00:02.154406\n",
      "84000 0:00:02.154431\n",
      "84100 0:00:02.156252\n",
      "84200 0:00:02.158194\n",
      "84300 0:00:02.159918\n",
      "84400 0:00:02.161657\n",
      "84500 0:00:02.163397\n",
      "84600 0:00:02.165209\n",
      "84700 0:00:02.167042\n",
      "84800 0:00:02.168746\n",
      "84900 0:00:02.170857\n",
      "get 85000 0:00:02.172819\n",
      "85000 0:00:02.172850\n",
      "85100 0:00:02.174647\n",
      "85200 0:00:02.176318\n",
      "85300 0:00:02.178034\n",
      "85400 0:00:02.179729\n",
      "85500 0:00:02.181548\n",
      "85600 0:00:02.183227\n",
      "85700 0:00:02.184929\n",
      "85800 0:00:02.186663\n",
      "85900 0:00:02.188192\n",
      "get 86000 0:00:02.189549\n",
      "86000 0:00:02.189574\n",
      "86100 0:00:02.191170\n",
      "86200 0:00:02.192587\n",
      "86300 0:00:02.194078\n",
      "86400 0:00:02.195644\n",
      "86500 0:00:02.197237\n",
      "86600 0:00:02.198712\n",
      "86700 0:00:02.200075\n",
      "86800 0:00:02.201493\n",
      "86900 0:00:02.202874\n",
      "get 87000 0:00:02.204257\n",
      "87000 0:00:02.204282\n",
      "87100 0:00:02.205719\n",
      "87200 0:00:02.207302\n",
      "87300 0:00:02.208815\n",
      "87400 0:00:02.210175\n",
      "87500 0:00:02.211618\n",
      "87600 0:00:02.213115\n",
      "87700 0:00:02.215006\n",
      "87800 0:00:02.216781\n",
      "87900 0:00:02.218376\n",
      "get 88000 0:00:02.219911\n",
      "88000 0:00:02.219933\n",
      "88100 0:00:02.221566\n",
      "88200 0:00:02.223252\n",
      "88300 0:00:02.224864\n",
      "88400 0:00:02.226381\n",
      "88500 0:00:02.228161\n",
      "88600 0:00:02.229701\n",
      "88700 0:00:02.231312\n",
      "88800 0:00:02.232793\n",
      "88900 0:00:02.234474\n",
      "get 89000 0:00:02.236759\n",
      "89000 0:00:02.236778\n",
      "89100 0:00:02.238387\n",
      "89200 0:00:02.239816\n",
      "89300 0:00:02.241396\n",
      "89400 0:00:02.242805\n",
      "89500 0:00:02.244492\n",
      "89600 0:00:02.245852\n",
      "89700 0:00:02.247459\n",
      "89800 0:00:02.248975\n",
      "89900 0:00:02.250390\n",
      "get 90000 0:00:02.251803\n",
      "90000 0:00:02.251826\n",
      "90100 0:00:02.253244\n",
      "90200 0:00:02.254791\n",
      "90300 0:00:02.256281\n",
      "90400 0:00:02.258145\n",
      "90500 0:00:02.259752\n",
      "90600 0:00:02.261192\n",
      "90700 0:00:02.262698\n",
      "90800 0:00:02.264272\n",
      "90900 0:00:02.265913\n",
      "get 91000 0:00:02.267848\n",
      "91000 0:00:02.267871\n",
      "91100 0:00:02.269325\n",
      "91200 0:00:02.270846\n",
      "91300 0:00:02.272479\n",
      "91400 0:00:02.274356\n",
      "91500 0:00:02.276446\n",
      "91600 0:00:02.277993\n",
      "91700 0:00:02.279698\n",
      "91800 0:00:02.281252\n",
      "91900 0:00:02.283088\n",
      "get 92000 0:00:02.284554\n",
      "92000 0:00:02.284583\n",
      "92100 0:00:02.286028\n",
      "92200 0:00:02.287408\n",
      "92300 0:00:02.288963\n",
      "92400 0:00:02.290597\n",
      "92500 0:00:02.292185\n",
      "92600 0:00:02.293845\n",
      "92700 0:00:02.295586\n",
      "92800 0:00:02.297198\n",
      "92900 0:00:02.298701\n",
      "get 93000 0:00:02.300173\n",
      "93000 0:00:02.300195\n",
      "93100 0:00:02.301809\n",
      "93200 0:00:02.303269\n",
      "93300 0:00:02.304927\n",
      "93400 0:00:02.306515\n",
      "93500 0:00:02.307965\n",
      "93600 0:00:02.309503\n",
      "93700 0:00:02.310862\n",
      "93800 0:00:02.312309\n",
      "93900 0:00:02.314061\n",
      "get 94000 0:00:02.315425\n",
      "94000 0:00:02.315448\n",
      "94100 0:00:02.317194\n",
      "94200 0:00:02.319049\n",
      "94300 0:00:02.320934\n",
      "94400 0:00:02.322626\n",
      "94500 0:00:02.324516\n",
      "94600 0:00:02.326876\n",
      "94700 0:00:02.328370\n",
      "94800 0:00:02.330136\n",
      "94900 0:00:02.331999\n",
      "get 95000 0:00:02.333723\n",
      "95000 0:00:02.333750\n",
      "95100 0:00:02.335597\n",
      "95200 0:00:02.337737\n",
      "95300 0:00:02.339746\n",
      "95400 0:00:02.341569\n",
      "95500 0:00:02.343314\n",
      "95600 0:00:02.345094\n",
      "95700 0:00:02.346847\n",
      "95800 0:00:02.348662\n",
      "95900 0:00:02.350480\n",
      "get 96000 0:00:02.352389\n",
      "96000 0:00:02.352418\n",
      "96100 0:00:02.354238\n",
      "96200 0:00:02.356091\n",
      "96300 0:00:02.357761\n",
      "96400 0:00:02.359498\n",
      "96500 0:00:02.361222\n",
      "96600 0:00:02.363035\n",
      "96700 0:00:02.364840\n",
      "96800 0:00:02.366595\n",
      "96900 0:00:02.368157\n",
      "get 97000 0:00:02.369869\n",
      "97000 0:00:02.369893\n",
      "97100 0:00:02.372031\n",
      "97200 0:00:02.373688\n",
      "97300 0:00:02.375229\n",
      "97400 0:00:02.376914\n",
      "97500 0:00:02.378799\n",
      "97600 0:00:02.380820\n",
      "97700 0:00:02.382550\n",
      "97800 0:00:02.384019\n",
      "97900 0:00:02.385692\n",
      "get 98000 0:00:02.387355\n",
      "98000 0:00:02.387379\n",
      "98100 0:00:02.389344\n",
      "98200 0:00:02.390912\n",
      "98300 0:00:02.392646\n",
      "98400 0:00:02.394345\n",
      "98500 0:00:02.395909\n",
      "98600 0:00:02.397728\n",
      "98700 0:00:02.399521\n",
      "98800 0:00:02.401416\n",
      "98900 0:00:02.403248\n",
      "get 99000 0:00:02.405162\n",
      "99000 0:00:02.405186\n",
      "99100 0:00:02.406783\n",
      "99200 0:00:02.408537\n",
      "99300 0:00:02.410578\n",
      "99400 0:00:02.412188\n",
      "99500 0:00:02.413664\n",
      "99600 0:00:02.415383\n",
      "99700 0:00:02.417127\n",
      "99800 0:00:02.418877\n",
      "99900 0:00:02.420789\n",
      "get 100000 0:00:05.108099\n",
      "100000 0:00:05.108343\n",
      "100100 0:00:05.110847\n",
      "100200 0:00:05.112404\n",
      "100300 0:00:05.114221\n",
      "100400 0:00:05.115484\n",
      "100500 0:00:05.116749\n",
      "100600 0:00:05.118531\n",
      "100700 0:00:05.120080\n",
      "100800 0:00:05.121792\n",
      "100900 0:00:05.123146\n",
      "get 101000 0:00:05.124750\n",
      "101000 0:00:05.124817\n",
      "101100 0:00:05.126301\n",
      "101200 0:00:05.128129\n",
      "101300 0:00:05.129767\n",
      "101400 0:00:05.131375\n",
      "101500 0:00:05.132942\n",
      "101600 0:00:05.134497\n",
      "101700 0:00:05.135777\n",
      "101800 0:00:05.136831\n",
      "101900 0:00:05.138398\n",
      "get 102000 0:00:05.139790\n",
      "102000 0:00:05.139810\n",
      "102100 0:00:05.141413\n",
      "102200 0:00:05.142889\n",
      "102300 0:00:05.144449\n",
      "102400 0:00:05.146333\n",
      "102500 0:00:05.147857\n",
      "102600 0:00:05.149308\n",
      "102700 0:00:05.150772\n",
      "102800 0:00:05.152160\n",
      "102900 0:00:05.153622\n",
      "get 103000 0:00:05.155386\n",
      "103000 0:00:05.155407\n",
      "103100 0:00:05.157231\n",
      "103200 0:00:05.159063\n",
      "103300 0:00:05.161148\n",
      "103400 0:00:05.162832\n",
      "103500 0:00:05.164150\n",
      "103600 0:00:05.166112\n",
      "103700 0:00:05.168231\n",
      "103800 0:00:05.170316\n",
      "103900 0:00:05.172390\n",
      "get 104000 0:00:05.174511\n",
      "104000 0:00:05.174531\n",
      "104100 0:00:05.176310\n",
      "104200 0:00:05.178390\n",
      "104300 0:00:05.180505\n",
      "104400 0:00:05.182395\n",
      "104500 0:00:05.184432\n",
      "104600 0:00:05.186634\n",
      "104700 0:00:05.188558\n",
      "104800 0:00:05.190594\n",
      "104900 0:00:05.192806\n",
      "get 105000 0:00:05.195065\n",
      "105000 0:00:05.195084\n",
      "105100 0:00:05.196755\n",
      "105200 0:00:05.198636\n",
      "105300 0:00:05.200376\n",
      "105400 0:00:05.202229\n",
      "105500 0:00:05.203785\n",
      "105600 0:00:05.205537\n",
      "105700 0:00:05.207349\n",
      "105800 0:00:05.209129\n",
      "105900 0:00:05.210757\n",
      "get 106000 0:00:05.212405\n",
      "106000 0:00:05.212423\n",
      "106100 0:00:05.214207\n",
      "106200 0:00:05.215923\n",
      "106300 0:00:05.217847\n",
      "106400 0:00:05.219410\n",
      "106500 0:00:05.221097\n",
      "106600 0:00:05.222877\n",
      "106700 0:00:05.224149\n",
      "106800 0:00:05.226032\n",
      "106900 0:00:05.227816\n",
      "get 107000 0:00:05.229567\n",
      "107000 0:00:05.229585\n",
      "107100 0:00:05.231281\n",
      "107200 0:00:05.233224\n",
      "107300 0:00:05.234962\n",
      "107400 0:00:05.236796\n",
      "107500 0:00:05.238658\n",
      "107600 0:00:05.240174\n",
      "107700 0:00:05.242061\n",
      "107800 0:00:05.243647\n",
      "107900 0:00:05.245265\n",
      "get 108000 0:00:05.246811\n",
      "108000 0:00:05.246829\n",
      "108100 0:00:05.248341\n",
      "108200 0:00:05.250136\n",
      "108300 0:00:05.251772\n",
      "108400 0:00:05.253579\n",
      "108500 0:00:05.255130\n",
      "108600 0:00:05.256993\n",
      "108700 0:00:05.258791\n",
      "108800 0:00:05.260237\n",
      "108900 0:00:05.262042\n",
      "get 109000 0:00:05.263717\n",
      "109000 0:00:05.263738\n",
      "109100 0:00:05.265151\n",
      "109200 0:00:05.266722\n",
      "109300 0:00:05.268380\n",
      "109400 0:00:05.270075\n",
      "109500 0:00:05.271819\n",
      "109600 0:00:05.273546\n",
      "109700 0:00:05.275289\n",
      "109800 0:00:05.277101\n",
      "109900 0:00:05.278814\n",
      "get 110000 0:00:05.280745\n",
      "110000 0:00:05.280764\n",
      "110100 0:00:05.282636\n",
      "110200 0:00:05.284475\n",
      "110300 0:00:05.286028\n",
      "110400 0:00:05.287747\n",
      "110500 0:00:05.289502\n",
      "110600 0:00:05.291285\n",
      "110700 0:00:05.292978\n",
      "110800 0:00:05.294811\n",
      "110900 0:00:05.296550\n",
      "get 111000 0:00:05.298361\n",
      "111000 0:00:05.298380\n",
      "111100 0:00:05.300217\n",
      "111200 0:00:05.301906\n",
      "111300 0:00:05.303780\n",
      "111400 0:00:05.305465\n",
      "111500 0:00:05.307265\n",
      "111600 0:00:05.308928\n",
      "111700 0:00:05.310887\n",
      "111800 0:00:05.312380\n",
      "111900 0:00:05.314309\n",
      "get 112000 0:00:05.316096\n",
      "112000 0:00:05.316116\n",
      "112100 0:00:05.317689\n",
      "112200 0:00:05.319562\n",
      "112300 0:00:05.321313\n",
      "112400 0:00:05.323003\n",
      "112500 0:00:05.324937\n",
      "112600 0:00:05.326796\n",
      "112700 0:00:05.328578\n",
      "112800 0:00:05.330460\n",
      "112900 0:00:05.332113\n",
      "get 113000 0:00:05.333877\n",
      "113000 0:00:05.333978\n",
      "113100 0:00:05.335725\n",
      "113200 0:00:05.337344\n",
      "113300 0:00:05.339103\n",
      "113400 0:00:05.340890\n",
      "113500 0:00:05.342507\n",
      "113600 0:00:05.344054\n",
      "113700 0:00:05.345531\n",
      "113800 0:00:05.347582\n",
      "113900 0:00:05.349513\n",
      "get 114000 0:00:05.351109\n",
      "114000 0:00:05.351128\n",
      "114100 0:00:05.353008\n",
      "114200 0:00:05.354937\n",
      "114300 0:00:05.356416\n",
      "114400 0:00:05.358276\n",
      "114500 0:00:05.360042\n",
      "114600 0:00:05.361738\n",
      "114700 0:00:05.363576\n",
      "114800 0:00:05.365716\n",
      "114900 0:00:05.367257\n",
      "get 115000 0:00:05.368914\n",
      "115000 0:00:05.368933\n",
      "115100 0:00:05.370877\n",
      "115200 0:00:05.372520\n",
      "115300 0:00:05.374604\n",
      "115400 0:00:05.376421\n",
      "115500 0:00:05.378009\n",
      "115600 0:00:05.379727\n",
      "115700 0:00:05.381579\n",
      "115800 0:00:05.383336\n",
      "115900 0:00:05.385059\n",
      "get 116000 0:00:05.386965\n",
      "116000 0:00:05.386985\n",
      "116100 0:00:05.388694\n",
      "116200 0:00:05.390552\n",
      "116300 0:00:05.392317\n",
      "116400 0:00:05.394215\n",
      "116500 0:00:05.395977\n",
      "116600 0:00:05.397871\n",
      "116700 0:00:05.399559\n",
      "116800 0:00:05.401192\n",
      "116900 0:00:05.402949\n",
      "get 117000 0:00:05.404867\n",
      "117000 0:00:05.404885\n",
      "117100 0:00:05.406661\n",
      "117200 0:00:05.408582\n",
      "117300 0:00:05.410542\n",
      "117400 0:00:05.411827\n",
      "117500 0:00:05.413706\n",
      "117600 0:00:05.415078\n",
      "117700 0:00:05.416690\n",
      "117800 0:00:05.418459\n",
      "117900 0:00:05.420360\n",
      "get 118000 0:00:05.422294\n",
      "118000 0:00:05.422313\n",
      "118100 0:00:05.423930\n",
      "118200 0:00:05.425800\n",
      "118300 0:00:05.427380\n",
      "118400 0:00:05.429182\n",
      "118500 0:00:05.431074\n",
      "118600 0:00:05.432888\n",
      "118700 0:00:05.434598\n",
      "118800 0:00:05.436568\n",
      "118900 0:00:05.438727\n",
      "get 119000 0:00:05.440656\n",
      "119000 0:00:05.440681\n",
      "119100 0:00:05.442333\n",
      "119200 0:00:05.444253\n",
      "119300 0:00:05.445876\n",
      "119400 0:00:05.447679\n",
      "119500 0:00:05.449554\n",
      "119600 0:00:05.451705\n",
      "119700 0:00:05.453370\n",
      "119800 0:00:05.455315\n",
      "119900 0:00:05.457381\n",
      "get 120000 0:00:05.458887\n",
      "120000 0:00:05.458910\n",
      "120100 0:00:05.460834\n",
      "120200 0:00:05.462641\n",
      "120300 0:00:05.464536\n",
      "120400 0:00:05.466384\n",
      "120500 0:00:05.468236\n",
      "120600 0:00:05.470132\n",
      "120700 0:00:05.471731\n",
      "120800 0:00:05.473648\n",
      "120900 0:00:05.475912\n",
      "get 121000 0:00:05.477671\n",
      "121000 0:00:05.477692\n",
      "121100 0:00:05.479830\n",
      "121200 0:00:05.481526\n",
      "121300 0:00:05.483497\n",
      "121400 0:00:05.485348\n",
      "121500 0:00:05.487245\n",
      "121600 0:00:05.489026\n",
      "121700 0:00:05.490860\n",
      "121800 0:00:05.492496\n",
      "121900 0:00:05.494416\n",
      "get 122000 0:00:05.496392\n",
      "122000 0:00:05.496425\n",
      "122100 0:00:05.498093\n",
      "122200 0:00:05.499582\n",
      "122300 0:00:05.501162\n",
      "122400 0:00:05.502561\n",
      "122500 0:00:05.504069\n",
      "122600 0:00:05.505532\n",
      "122700 0:00:05.506937\n",
      "122800 0:00:05.508227\n",
      "122900 0:00:05.509829\n",
      "get 123000 0:00:05.511449\n",
      "123000 0:00:05.511474\n",
      "123100 0:00:05.513640\n",
      "123200 0:00:05.515251\n",
      "123300 0:00:05.516472\n",
      "123400 0:00:05.517690\n",
      "123500 0:00:05.519171\n",
      "123600 0:00:05.520601\n",
      "123700 0:00:05.522089\n",
      "123800 0:00:05.523506\n",
      "123900 0:00:05.524935\n",
      "get 124000 0:00:05.526519\n",
      "124000 0:00:05.526541\n",
      "124100 0:00:05.528110\n",
      "124200 0:00:05.529802\n",
      "124300 0:00:05.531339\n",
      "124400 0:00:05.532745\n",
      "124500 0:00:05.534719\n",
      "124600 0:00:05.536364\n",
      "124700 0:00:05.538020\n",
      "124800 0:00:05.539353\n",
      "124900 0:00:05.541029\n",
      "get 125000 0:00:05.542652\n",
      "125000 0:00:05.542674\n",
      "125100 0:00:05.544404\n",
      "125200 0:00:05.546130\n",
      "125300 0:00:05.547923\n",
      "125400 0:00:05.549332\n",
      "125500 0:00:05.550893\n",
      "125600 0:00:05.552563\n",
      "125700 0:00:05.553945\n",
      "125800 0:00:05.555340\n",
      "125900 0:00:05.556923\n",
      "get 126000 0:00:05.558947\n",
      "126000 0:00:05.558970\n",
      "126100 0:00:05.560636\n",
      "126200 0:00:05.562581\n",
      "126300 0:00:05.563947\n",
      "126400 0:00:05.565702\n",
      "126500 0:00:05.567392\n",
      "126600 0:00:05.569199\n",
      "126700 0:00:05.571137\n",
      "126800 0:00:05.572961\n",
      "126900 0:00:05.574688\n",
      "get 127000 0:00:05.576678\n",
      "127000 0:00:05.576702\n",
      "127100 0:00:05.578922\n",
      "127200 0:00:05.581199\n",
      "127300 0:00:05.583114\n",
      "127400 0:00:05.584957\n",
      "127500 0:00:05.586864\n",
      "127600 0:00:05.588862\n",
      "127700 0:00:05.590740\n",
      "127800 0:00:05.592477\n",
      "127900 0:00:05.594029\n",
      "get 128000 0:00:05.595711\n",
      "128000 0:00:05.595746\n",
      "128100 0:00:05.597836\n",
      "128200 0:00:05.599491\n",
      "128300 0:00:05.601195\n",
      "128400 0:00:05.602768\n",
      "128500 0:00:05.604628\n",
      "128600 0:00:05.606410\n",
      "128700 0:00:05.608330\n",
      "128800 0:00:05.610027\n",
      "128900 0:00:05.611730\n",
      "get 129000 0:00:05.613864\n",
      "129000 0:00:05.613933\n",
      "129100 0:00:05.615843\n",
      "129200 0:00:05.617568\n",
      "129300 0:00:05.619359\n",
      "129400 0:00:05.621168\n",
      "129500 0:00:05.622982\n",
      "129600 0:00:05.625526\n",
      "129700 0:00:05.627784\n",
      "129800 0:00:05.629470\n",
      "129900 0:00:05.631868\n",
      "get 130000 0:00:05.633659\n",
      "130000 0:00:05.633684\n",
      "130100 0:00:05.635481\n",
      "130200 0:00:05.637439\n",
      "130300 0:00:05.639522\n",
      "130400 0:00:05.641427\n",
      "130500 0:00:05.643672\n",
      "130600 0:00:05.645820\n",
      "130700 0:00:05.647809\n",
      "130800 0:00:05.650032\n",
      "130900 0:00:05.651982\n",
      "get 131000 0:00:05.653718\n",
      "131000 0:00:05.653746\n",
      "131100 0:00:05.655714\n",
      "131200 0:00:05.657446\n",
      "131300 0:00:05.659490\n",
      "131400 0:00:05.661394\n",
      "131500 0:00:05.663440\n",
      "131600 0:00:05.665381\n",
      "131700 0:00:05.666823\n",
      "131800 0:00:05.668421\n",
      "131900 0:00:05.670773\n",
      "get 132000 0:00:05.672681\n",
      "132000 0:00:05.672718\n",
      "132100 0:00:05.674672\n",
      "132200 0:00:05.676473\n",
      "132300 0:00:05.677992\n",
      "132400 0:00:05.679984\n",
      "132500 0:00:05.681993\n",
      "132600 0:00:05.684215\n",
      "132700 0:00:05.686141\n",
      "132800 0:00:05.688147\n",
      "132900 0:00:05.689890\n",
      "get 133000 0:00:05.691807\n",
      "133000 0:00:05.691836\n",
      "133100 0:00:05.693684\n",
      "133200 0:00:05.695518\n",
      "133300 0:00:05.696969\n",
      "133400 0:00:05.699261\n",
      "133500 0:00:05.701037\n",
      "133600 0:00:05.703060\n",
      "133700 0:00:05.704918\n",
      "133800 0:00:05.706762\n",
      "133900 0:00:05.708785\n",
      "get 134000 0:00:05.710692\n",
      "134000 0:00:05.710723\n",
      "134100 0:00:05.712140\n",
      "134200 0:00:05.713923\n",
      "134300 0:00:05.715906\n",
      "134400 0:00:05.717510\n",
      "134500 0:00:05.719015\n",
      "134600 0:00:05.720295\n",
      "134700 0:00:05.721754\n",
      "134800 0:00:05.723254\n",
      "134900 0:00:05.724809\n",
      "get 135000 0:00:05.726552\n",
      "135000 0:00:05.726584\n",
      "135100 0:00:05.727823\n",
      "135200 0:00:05.729275\n",
      "135300 0:00:05.730923\n",
      "135400 0:00:05.732550\n",
      "135500 0:00:05.734117\n",
      "135600 0:00:05.735374\n",
      "135700 0:00:05.736992\n",
      "135800 0:00:05.738353\n",
      "135900 0:00:05.740025\n",
      "get 136000 0:00:05.741436\n",
      "136000 0:00:05.741458\n",
      "136100 0:00:05.742799\n",
      "136200 0:00:05.744204\n",
      "136300 0:00:05.745567\n",
      "136400 0:00:05.747002\n",
      "136500 0:00:05.748587\n",
      "136600 0:00:05.750335\n",
      "136700 0:00:05.752205\n",
      "136800 0:00:05.753574\n",
      "136900 0:00:05.755127\n",
      "get 137000 0:00:05.756948\n",
      "137000 0:00:05.756969\n",
      "137100 0:00:05.758492\n",
      "137200 0:00:05.759906\n",
      "137300 0:00:05.761754\n",
      "137400 0:00:05.763294\n",
      "137500 0:00:05.765376\n",
      "137600 0:00:05.766809\n",
      "137700 0:00:05.768122\n",
      "137800 0:00:05.769791\n",
      "137900 0:00:05.771578\n",
      "get 138000 0:00:05.772713\n",
      "138000 0:00:05.772746\n",
      "138100 0:00:05.774442\n",
      "138200 0:00:05.775997\n",
      "138300 0:00:05.777684\n",
      "138400 0:00:05.779201\n",
      "138500 0:00:05.780834\n",
      "138600 0:00:05.782491\n",
      "138700 0:00:05.783982\n",
      "138800 0:00:05.785864\n",
      "138900 0:00:05.787382\n",
      "get 139000 0:00:05.789093\n",
      "139000 0:00:05.789115\n",
      "139100 0:00:05.790883\n",
      "139200 0:00:05.792312\n",
      "139300 0:00:05.793698\n",
      "139400 0:00:05.795502\n",
      "139500 0:00:05.796970\n",
      "139600 0:00:05.798708\n",
      "139700 0:00:05.800249\n",
      "139800 0:00:05.801632\n",
      "139900 0:00:05.803437\n",
      "get 140000 0:00:05.805027\n",
      "140000 0:00:05.805049\n",
      "140100 0:00:05.806724\n",
      "140200 0:00:05.808520\n",
      "140300 0:00:05.810437\n",
      "140400 0:00:05.812091\n",
      "140500 0:00:05.813868\n",
      "140600 0:00:05.815728\n",
      "140700 0:00:05.817178\n",
      "140800 0:00:05.818718\n",
      "140900 0:00:05.820414\n",
      "get 141000 0:00:05.822179\n",
      "141000 0:00:05.822207\n",
      "141100 0:00:05.824099\n",
      "141200 0:00:05.825836\n",
      "141300 0:00:05.827563\n",
      "141400 0:00:05.829278\n",
      "141500 0:00:05.830980\n",
      "141600 0:00:05.832647\n",
      "141700 0:00:05.834263\n",
      "141800 0:00:05.835960\n",
      "141900 0:00:05.837711\n",
      "get 142000 0:00:05.839380\n",
      "142000 0:00:05.839404\n",
      "142100 0:00:05.841074\n",
      "142200 0:00:05.842760\n",
      "142300 0:00:05.844358\n",
      "142400 0:00:05.846012\n",
      "142500 0:00:05.847657\n",
      "142600 0:00:05.849264\n",
      "142700 0:00:05.851167\n",
      "142800 0:00:05.852844\n",
      "142900 0:00:05.854622\n",
      "get 143000 0:00:05.856329\n",
      "143000 0:00:05.856355\n",
      "143100 0:00:05.857952\n",
      "143200 0:00:05.859642\n",
      "143300 0:00:05.861136\n",
      "143400 0:00:05.863050\n",
      "143500 0:00:05.864679\n",
      "143600 0:00:05.866493\n",
      "143700 0:00:05.868377\n",
      "143800 0:00:05.869922\n",
      "143900 0:00:05.871577\n",
      "get 144000 0:00:05.873410\n",
      "144000 0:00:05.873435\n",
      "144100 0:00:05.875302\n",
      "144200 0:00:05.876900\n",
      "144300 0:00:05.878378\n",
      "144400 0:00:05.880088\n",
      "144500 0:00:05.881736\n",
      "144600 0:00:05.883411\n",
      "144700 0:00:05.884840\n",
      "144800 0:00:05.886605\n",
      "144900 0:00:05.888102\n",
      "get 145000 0:00:05.889704\n",
      "145000 0:00:05.889726\n",
      "145100 0:00:05.891420\n",
      "145200 0:00:05.893180\n",
      "145300 0:00:05.894869\n",
      "145400 0:00:05.896556\n",
      "145500 0:00:05.898523\n",
      "145600 0:00:05.899960\n",
      "145700 0:00:05.901466\n",
      "145800 0:00:05.903356\n",
      "145900 0:00:05.904940\n",
      "get 146000 0:00:05.906472\n",
      "146000 0:00:05.906496\n",
      "146100 0:00:05.908337\n",
      "146200 0:00:05.909984\n",
      "146300 0:00:05.911827\n",
      "146400 0:00:05.913338\n",
      "146500 0:00:05.914810\n",
      "146600 0:00:05.916326\n",
      "146700 0:00:05.918247\n",
      "146800 0:00:05.919905\n",
      "146900 0:00:05.921333\n",
      "get 147000 0:00:05.923090\n",
      "147000 0:00:05.923113\n",
      "147100 0:00:05.924569\n",
      "147200 0:00:05.926306\n",
      "147300 0:00:05.928102\n",
      "147400 0:00:05.929587\n",
      "147500 0:00:05.931257\n",
      "147600 0:00:05.933103\n",
      "147700 0:00:05.935186\n",
      "147800 0:00:05.936739\n",
      "147900 0:00:05.938457\n",
      "get 148000 0:00:05.939891\n",
      "148000 0:00:05.939912\n",
      "148100 0:00:05.941507\n",
      "148200 0:00:05.943275\n",
      "148300 0:00:05.945028\n",
      "148400 0:00:05.946925\n",
      "148500 0:00:05.948491\n",
      "148600 0:00:05.950275\n",
      "148700 0:00:05.952017\n",
      "148800 0:00:05.953752\n",
      "148900 0:00:05.955717\n",
      "get 149000 0:00:05.957266\n",
      "149000 0:00:05.957287\n",
      "149100 0:00:05.958805\n",
      "149200 0:00:05.960679\n",
      "149300 0:00:05.962208\n",
      "149400 0:00:05.963932\n",
      "149500 0:00:05.965922\n",
      "149600 0:00:05.967486\n",
      "149700 0:00:05.969211\n",
      "149800 0:00:05.970979\n",
      "149900 0:00:05.972744\n",
      "get 150000 0:00:05.974718\n",
      "150000 0:00:05.974750\n",
      "150100 0:00:05.976004\n",
      "150200 0:00:05.977721\n",
      "150300 0:00:05.979482\n",
      "150400 0:00:05.981161\n",
      "150500 0:00:05.982871\n",
      "150600 0:00:05.984861\n",
      "150700 0:00:05.986274\n",
      "150800 0:00:05.987847\n",
      "150900 0:00:05.989584\n",
      "get 151000 0:00:05.991318\n",
      "151000 0:00:05.991339\n",
      "151100 0:00:05.993344\n",
      "151200 0:00:05.995114\n",
      "151300 0:00:05.996606\n",
      "151400 0:00:05.998378\n",
      "151500 0:00:05.999945\n",
      "151600 0:00:06.001824\n",
      "151700 0:00:06.003523\n",
      "151800 0:00:06.005444\n",
      "151900 0:00:06.006877\n",
      "get 152000 0:00:06.008633\n",
      "152000 0:00:06.008651\n",
      "152100 0:00:06.010373\n",
      "152200 0:00:06.012275\n",
      "152300 0:00:06.013931\n",
      "152400 0:00:06.016019\n",
      "152500 0:00:06.017930\n",
      "152600 0:00:06.019714\n",
      "152700 0:00:06.021515\n",
      "152800 0:00:06.023673\n",
      "152900 0:00:06.026036\n",
      "get 153000 0:00:06.028005\n",
      "153000 0:00:06.028034\n",
      "153100 0:00:06.029884\n",
      "153200 0:00:06.031944\n",
      "153300 0:00:06.034276\n",
      "153400 0:00:06.036081\n",
      "153500 0:00:06.038047\n",
      "153600 0:00:06.039902\n",
      "153700 0:00:06.042103\n",
      "153800 0:00:06.044205\n",
      "153900 0:00:06.046155\n",
      "get 154000 0:00:06.047759\n",
      "154000 0:00:06.047795\n",
      "154100 0:00:06.050350\n",
      "154200 0:00:06.052421\n",
      "154300 0:00:06.054201\n",
      "154400 0:00:06.055827\n",
      "154500 0:00:06.057773\n",
      "154600 0:00:06.059555\n",
      "154700 0:00:06.061470\n",
      "154800 0:00:06.063375\n",
      "154900 0:00:06.065016\n",
      "get 155000 0:00:06.067091\n",
      "155000 0:00:06.067114\n",
      "155100 0:00:06.069355\n",
      "155200 0:00:06.071273\n",
      "155300 0:00:06.073140\n",
      "155400 0:00:06.075186\n",
      "155500 0:00:06.077074\n",
      "155600 0:00:06.078768\n",
      "155700 0:00:06.080513\n",
      "155800 0:00:06.082526\n",
      "155900 0:00:06.084267\n",
      "get 156000 0:00:06.085966\n",
      "156000 0:00:06.085997\n",
      "156100 0:00:06.087713\n",
      "156200 0:00:06.089280\n",
      "156300 0:00:06.090959\n",
      "156400 0:00:06.092602\n",
      "156500 0:00:06.094291\n",
      "156600 0:00:06.096080\n",
      "156700 0:00:06.097895\n",
      "156800 0:00:06.099666\n",
      "156900 0:00:06.101235\n",
      "get 157000 0:00:06.102892\n",
      "157000 0:00:06.102923\n",
      "157100 0:00:06.104518\n",
      "157200 0:00:06.106305\n",
      "157300 0:00:06.108042\n",
      "157400 0:00:06.109940\n",
      "157500 0:00:06.111192\n",
      "157600 0:00:06.112940\n",
      "157700 0:00:06.114936\n",
      "157800 0:00:06.116949\n",
      "157900 0:00:06.118510\n",
      "get 158000 0:00:06.120594\n",
      "158000 0:00:06.120730\n",
      "158100 0:00:06.122300\n",
      "158200 0:00:06.123812\n",
      "158300 0:00:06.125552\n",
      "158400 0:00:06.127325\n",
      "158500 0:00:06.128692\n",
      "158600 0:00:06.130039\n",
      "158700 0:00:06.131549\n",
      "158800 0:00:06.132980\n",
      "158900 0:00:06.134748\n",
      "get 159000 0:00:06.136613\n",
      "159000 0:00:06.136633\n",
      "159100 0:00:06.138136\n",
      "159200 0:00:06.139635\n",
      "159300 0:00:06.140990\n",
      "159400 0:00:06.142705\n",
      "159500 0:00:06.143854\n",
      "159600 0:00:06.145168\n",
      "159700 0:00:06.146732\n",
      "159800 0:00:06.148478\n",
      "159900 0:00:06.150314\n",
      "get 160000 0:00:06.151761\n",
      "160000 0:00:06.151792\n",
      "160100 0:00:06.153370\n",
      "160200 0:00:06.154805\n",
      "160300 0:00:06.156505\n",
      "160400 0:00:06.158098\n",
      "160500 0:00:06.159466\n",
      "160600 0:00:06.161077\n",
      "160700 0:00:06.162348\n",
      "160800 0:00:06.164198\n",
      "160900 0:00:06.165849\n",
      "get 161000 0:00:06.167533\n",
      "161000 0:00:06.167560\n",
      "161100 0:00:06.168982\n",
      "161200 0:00:06.170341\n",
      "161300 0:00:06.171776\n",
      "161400 0:00:06.173031\n",
      "161500 0:00:06.174688\n",
      "161600 0:00:06.176093\n",
      "161700 0:00:06.177556\n",
      "161800 0:00:06.179223\n",
      "161900 0:00:06.180251\n",
      "get 162000 0:00:06.181340\n",
      "162000 0:00:06.181370\n",
      "162100 0:00:06.183235\n",
      "162200 0:00:06.185055\n",
      "162300 0:00:06.187143\n",
      "162400 0:00:06.188988\n",
      "162500 0:00:06.191145\n",
      "162600 0:00:06.192892\n",
      "162700 0:00:06.194785\n",
      "162800 0:00:06.196730\n",
      "162900 0:00:06.198473\n",
      "get 163000 0:00:06.200467\n",
      "163000 0:00:06.200490\n",
      "163100 0:00:06.202518\n",
      "163200 0:00:06.204320\n",
      "163300 0:00:06.206389\n",
      "163400 0:00:06.208285\n",
      "163500 0:00:06.210160\n",
      "163600 0:00:06.212075\n",
      "163700 0:00:06.214035\n",
      "163800 0:00:06.216163\n",
      "163900 0:00:06.218041\n",
      "get 164000 0:00:06.220037\n",
      "164000 0:00:06.220068\n",
      "164100 0:00:06.222327\n",
      "164200 0:00:06.223958\n",
      "164300 0:00:06.226023\n",
      "164400 0:00:06.227968\n",
      "164500 0:00:06.230037\n",
      "164600 0:00:06.231927\n",
      "164700 0:00:06.233965\n",
      "164800 0:00:06.235950\n",
      "164900 0:00:06.237980\n",
      "get 165000 0:00:06.239979\n",
      "165000 0:00:06.240001\n",
      "165100 0:00:06.241939\n",
      "165200 0:00:06.243904\n",
      "165300 0:00:06.245953\n",
      "165400 0:00:06.248068\n",
      "165500 0:00:06.249924\n",
      "165600 0:00:06.251801\n",
      "165700 0:00:06.253788\n",
      "165800 0:00:06.255891\n",
      "165900 0:00:06.257860\n",
      "get 166000 0:00:06.259893\n",
      "166000 0:00:06.259926\n",
      "166100 0:00:06.261506\n",
      "166200 0:00:06.263359\n",
      "166300 0:00:06.265497\n",
      "166400 0:00:06.267464\n",
      "166500 0:00:06.269312\n",
      "166600 0:00:06.271504\n",
      "166700 0:00:06.273246\n",
      "166800 0:00:06.275271\n",
      "166900 0:00:06.277193\n",
      "get 167000 0:00:06.279096\n",
      "167000 0:00:06.279118\n",
      "167100 0:00:06.281118\n",
      "167200 0:00:06.282884\n",
      "167300 0:00:06.284740\n",
      "167400 0:00:06.286607\n",
      "167500 0:00:06.288363\n",
      "167600 0:00:06.290572\n",
      "167700 0:00:06.292020\n",
      "167800 0:00:06.293685\n",
      "167900 0:00:06.295663\n",
      "get 168000 0:00:06.297435\n",
      "168000 0:00:06.297466\n",
      "168100 0:00:06.299209\n",
      "168200 0:00:06.300947\n",
      "168300 0:00:06.302675\n",
      "168400 0:00:06.304467\n",
      "168500 0:00:06.306385\n",
      "168600 0:00:06.308541\n",
      "168700 0:00:06.310502\n",
      "168800 0:00:06.313152\n",
      "168900 0:00:06.315345\n",
      "get 169000 0:00:06.317222\n",
      "169000 0:00:06.317257\n",
      "169100 0:00:06.318764\n",
      "169200 0:00:06.320769\n",
      "169300 0:00:06.322772\n",
      "169400 0:00:06.324744\n",
      "169500 0:00:06.326898\n",
      "169600 0:00:06.329106\n",
      "169700 0:00:06.331013\n",
      "169800 0:00:06.333187\n",
      "169900 0:00:06.335362\n",
      "get 170000 0:00:06.336819\n",
      "170000 0:00:06.336855\n",
      "170100 0:00:06.338535\n",
      "170200 0:00:06.340902\n",
      "170300 0:00:06.342813\n",
      "170400 0:00:06.345114\n",
      "170500 0:00:06.346943\n",
      "170600 0:00:06.349214\n",
      "170700 0:00:06.351423\n",
      "170800 0:00:06.353462\n",
      "170900 0:00:06.355515\n",
      "get 171000 0:00:06.357720\n",
      "171000 0:00:06.357744\n",
      "171100 0:00:06.359758\n",
      "171200 0:00:06.361575\n",
      "171300 0:00:06.363222\n",
      "171400 0:00:06.365152\n",
      "171500 0:00:06.367522\n",
      "171600 0:00:06.369501\n",
      "171700 0:00:06.371545\n",
      "171800 0:00:06.373756\n",
      "171900 0:00:06.375563\n",
      "get 172000 0:00:06.377515\n",
      "172000 0:00:06.377540\n",
      "172100 0:00:06.379143\n",
      "172200 0:00:06.381043\n",
      "172300 0:00:06.383146\n",
      "172400 0:00:06.384956\n",
      "172500 0:00:06.386504\n",
      "172600 0:00:06.388507\n",
      "172700 0:00:06.390513\n",
      "172800 0:00:06.392380\n",
      "172900 0:00:06.394209\n",
      "get 173000 0:00:06.396288\n",
      "173000 0:00:06.396324\n",
      "173100 0:00:06.398258\n",
      "173200 0:00:06.400054\n",
      "173300 0:00:06.402194\n",
      "173400 0:00:06.404356\n",
      "173500 0:00:06.406073\n",
      "173600 0:00:06.408099\n",
      "173700 0:00:06.409814\n",
      "173800 0:00:06.411670\n",
      "173900 0:00:06.413930\n",
      "get 174000 0:00:06.415986\n",
      "174000 0:00:06.416017\n",
      "174100 0:00:06.418009\n",
      "174200 0:00:06.420018\n",
      "174300 0:00:06.421616\n",
      "174400 0:00:06.423552\n",
      "174500 0:00:06.425421\n",
      "174600 0:00:06.427198\n",
      "174700 0:00:06.429134\n",
      "174800 0:00:06.431148\n",
      "174900 0:00:06.432791\n",
      "get 175000 0:00:06.434828\n",
      "175000 0:00:06.434860\n",
      "175100 0:00:06.436403\n",
      "175200 0:00:06.438368\n",
      "175300 0:00:06.440152\n",
      "175400 0:00:06.441544\n",
      "175500 0:00:06.442971\n",
      "175600 0:00:06.444740\n",
      "175700 0:00:06.446248\n",
      "175800 0:00:06.447990\n",
      "175900 0:00:06.449276\n",
      "get 176000 0:00:06.450762\n",
      "176000 0:00:06.450788\n",
      "176100 0:00:06.452073\n",
      "176200 0:00:06.453368\n",
      "176300 0:00:06.455098\n",
      "176400 0:00:06.456457\n",
      "176500 0:00:06.458052\n",
      "176600 0:00:06.459599\n",
      "176700 0:00:06.461161\n",
      "176800 0:00:06.462768\n",
      "176900 0:00:06.464428\n",
      "get 177000 0:00:06.466066\n",
      "177000 0:00:06.466089\n",
      "177100 0:00:06.467465\n",
      "177200 0:00:06.468819\n",
      "177300 0:00:06.470173\n",
      "177400 0:00:06.471543\n",
      "177500 0:00:06.473136\n",
      "177600 0:00:06.474424\n",
      "177700 0:00:06.475930\n",
      "177800 0:00:06.477762\n",
      "177900 0:00:06.478995\n",
      "get 178000 0:00:06.480800\n",
      "178000 0:00:06.480824\n",
      "178100 0:00:06.483052\n",
      "178200 0:00:06.484489\n",
      "178300 0:00:06.486035\n",
      "178400 0:00:06.487830\n",
      "178500 0:00:06.489602\n",
      "178600 0:00:06.491103\n",
      "178700 0:00:06.492572\n",
      "178800 0:00:06.494340\n",
      "178900 0:00:06.496042\n",
      "get 179000 0:00:06.497723\n",
      "179000 0:00:06.497752\n",
      "179100 0:00:06.499419\n",
      "179200 0:00:06.501139\n",
      "179300 0:00:06.502956\n",
      "179400 0:00:06.505003\n",
      "179500 0:00:06.506945\n",
      "179600 0:00:06.508884\n",
      "179700 0:00:06.510909\n",
      "179800 0:00:06.512917\n",
      "179900 0:00:06.515012\n",
      "get 180000 0:00:06.517057\n",
      "180000 0:00:06.517091\n",
      "180100 0:00:06.519037\n",
      "180200 0:00:06.520685\n",
      "180300 0:00:06.522778\n",
      "180400 0:00:06.524824\n",
      "180500 0:00:06.526868\n",
      "180600 0:00:06.528507\n",
      "180700 0:00:06.530252\n",
      "180800 0:00:06.531923\n",
      "180900 0:00:06.533877\n",
      "get 181000 0:00:06.536028\n",
      "181000 0:00:06.536051\n",
      "181100 0:00:06.538057\n",
      "181200 0:00:06.540060\n",
      "181300 0:00:06.542202\n",
      "181400 0:00:06.543956\n",
      "181500 0:00:06.545634\n",
      "181600 0:00:06.547901\n",
      "181700 0:00:06.549779\n",
      "181800 0:00:06.551383\n",
      "181900 0:00:06.553367\n",
      "get 182000 0:00:06.555058\n",
      "182000 0:00:06.555084\n",
      "182100 0:00:06.557040\n",
      "182200 0:00:06.558982\n",
      "182300 0:00:06.560970\n",
      "182400 0:00:06.562567\n",
      "182500 0:00:06.564581\n",
      "182600 0:00:06.566618\n",
      "182700 0:00:06.567998\n",
      "182800 0:00:06.569703\n",
      "182900 0:00:06.571082\n",
      "get 183000 0:00:06.572791\n",
      "183000 0:00:06.572814\n",
      "183100 0:00:06.574347\n",
      "183200 0:00:06.575982\n",
      "183300 0:00:06.577820\n",
      "183400 0:00:06.579298\n",
      "183500 0:00:06.581070\n",
      "183600 0:00:06.582586\n",
      "183700 0:00:06.583875\n",
      "183800 0:00:06.585823\n",
      "183900 0:00:06.587689\n",
      "get 184000 0:00:06.589474\n",
      "184000 0:00:06.589507\n",
      "184100 0:00:06.590975\n",
      "184200 0:00:06.592682\n",
      "184300 0:00:06.594116\n",
      "184400 0:00:06.595687\n",
      "184500 0:00:06.597374\n",
      "184600 0:00:06.598620\n",
      "184700 0:00:06.600292\n",
      "184800 0:00:06.601883\n",
      "184900 0:00:06.603788\n",
      "get 185000 0:00:06.605417\n",
      "185000 0:00:06.605449\n",
      "185100 0:00:06.607256\n",
      "185200 0:00:06.609095\n",
      "185300 0:00:06.610723\n",
      "185400 0:00:06.612445\n",
      "185500 0:00:06.614260\n",
      "185600 0:00:06.616198\n",
      "185700 0:00:06.617612\n",
      "185800 0:00:06.619270\n",
      "185900 0:00:06.620897\n",
      "get 186000 0:00:06.622628\n",
      "186000 0:00:06.622652\n",
      "186100 0:00:06.624274\n",
      "186200 0:00:06.626012\n",
      "186300 0:00:06.627684\n",
      "186400 0:00:06.629323\n",
      "186500 0:00:06.631037\n",
      "186600 0:00:06.632882\n",
      "186700 0:00:06.634576\n",
      "186800 0:00:06.636296\n",
      "186900 0:00:06.638065\n",
      "get 187000 0:00:06.639740\n",
      "187000 0:00:06.639761\n",
      "187100 0:00:06.641570\n",
      "187200 0:00:06.643398\n",
      "187300 0:00:06.645080\n",
      "187400 0:00:06.646715\n",
      "187500 0:00:06.648298\n",
      "187600 0:00:06.649966\n",
      "187700 0:00:06.651847\n",
      "187800 0:00:06.653450\n",
      "187900 0:00:06.655150\n",
      "get 188000 0:00:06.656584\n",
      "188000 0:00:06.656617\n",
      "188100 0:00:06.658273\n",
      "188200 0:00:06.660140\n",
      "188300 0:00:06.661834\n",
      "188400 0:00:06.663554\n",
      "188500 0:00:06.665386\n",
      "188600 0:00:06.666797\n",
      "188700 0:00:06.668404\n",
      "188800 0:00:06.670335\n",
      "188900 0:00:06.671595\n",
      "get 189000 0:00:06.673433\n",
      "189000 0:00:06.673454\n",
      "189100 0:00:06.675413\n",
      "189200 0:00:06.677311\n",
      "189300 0:00:06.679660\n",
      "189400 0:00:06.681082\n",
      "189500 0:00:06.682921\n",
      "189600 0:00:06.684837\n",
      "189700 0:00:06.686923\n",
      "189800 0:00:06.688761\n",
      "189900 0:00:06.690666\n",
      "get 190000 0:00:06.692824\n",
      "190000 0:00:06.692849\n",
      "190100 0:00:06.694882\n",
      "190200 0:00:06.696849\n",
      "190300 0:00:06.698714\n",
      "190400 0:00:06.700730\n",
      "190500 0:00:06.702240\n",
      "190600 0:00:06.703725\n",
      "190700 0:00:06.705642\n",
      "190800 0:00:06.707300\n",
      "190900 0:00:06.709483\n",
      "get 191000 0:00:06.711313\n",
      "191000 0:00:06.711354\n",
      "191100 0:00:06.713501\n",
      "191200 0:00:06.715282\n",
      "191300 0:00:06.716718\n",
      "191400 0:00:06.718832\n",
      "191500 0:00:06.720836\n",
      "191600 0:00:06.722452\n",
      "191700 0:00:06.724283\n",
      "191800 0:00:06.726686\n",
      "191900 0:00:06.728548\n",
      "get 192000 0:00:06.730319\n",
      "192000 0:00:06.730348\n",
      "192100 0:00:06.732088\n",
      "192200 0:00:06.733917\n",
      "192300 0:00:06.735461\n",
      "192400 0:00:06.737181\n",
      "192500 0:00:06.738916\n",
      "192600 0:00:06.740419\n",
      "192700 0:00:06.742143\n",
      "192800 0:00:06.743632\n",
      "192900 0:00:06.745062\n",
      "get 193000 0:00:06.746615\n",
      "193000 0:00:06.746637\n",
      "193100 0:00:06.747986\n",
      "193200 0:00:06.749278\n",
      "193300 0:00:06.750542\n",
      "193400 0:00:06.751913\n",
      "193500 0:00:06.753465\n",
      "193600 0:00:06.755176\n",
      "193700 0:00:06.756680\n",
      "193800 0:00:06.758371\n",
      "193900 0:00:06.759526\n",
      "get 194000 0:00:06.760807\n",
      "194000 0:00:06.760833\n",
      "194100 0:00:06.762458\n",
      "194200 0:00:06.763818\n",
      "194300 0:00:06.765524\n",
      "194400 0:00:06.767263\n",
      "194500 0:00:06.768822\n",
      "194600 0:00:06.770638\n",
      "194700 0:00:06.772055\n",
      "194800 0:00:06.774009\n",
      "194900 0:00:06.775444\n",
      "get 195000 0:00:06.777091\n",
      "195000 0:00:06.777115\n",
      "195100 0:00:06.778501\n",
      "195200 0:00:06.780054\n",
      "195300 0:00:06.781508\n",
      "195400 0:00:06.782911\n",
      "195500 0:00:06.784429\n",
      "195600 0:00:06.785745\n",
      "195700 0:00:06.787283\n",
      "195800 0:00:06.788834\n",
      "195900 0:00:06.790516\n",
      "get 196000 0:00:06.792392\n",
      "196000 0:00:06.792417\n",
      "196100 0:00:06.793950\n",
      "196200 0:00:06.795636\n",
      "196300 0:00:06.797333\n",
      "196400 0:00:06.799150\n",
      "196500 0:00:06.801035\n",
      "196600 0:00:06.803440\n",
      "196700 0:00:06.804953\n",
      "196800 0:00:06.806549\n",
      "196900 0:00:06.807954\n",
      "get 197000 0:00:06.809563\n",
      "197000 0:00:06.809607\n",
      "197100 0:00:06.811489\n",
      "197200 0:00:06.812965\n",
      "197300 0:00:06.814645\n",
      "197400 0:00:06.816418\n",
      "197500 0:00:06.818173\n",
      "197600 0:00:06.819914\n",
      "197700 0:00:06.821694\n",
      "197800 0:00:06.823615\n",
      "197900 0:00:06.825356\n",
      "get 198000 0:00:06.827237\n",
      "198000 0:00:06.827260\n",
      "198100 0:00:06.828922\n",
      "198200 0:00:06.830676\n",
      "198300 0:00:06.832559\n",
      "198400 0:00:06.834466\n",
      "198500 0:00:06.836392\n",
      "198600 0:00:06.838175\n",
      "198700 0:00:06.840193\n",
      "198800 0:00:06.842082\n",
      "198900 0:00:06.844028\n",
      "get 199000 0:00:06.845655\n",
      "199000 0:00:06.845680\n",
      "199100 0:00:06.847584\n",
      "199200 0:00:06.849626\n",
      "199300 0:00:06.851356\n",
      "199400 0:00:06.853349\n",
      "199500 0:00:06.855212\n",
      "199600 0:00:06.857018\n",
      "199700 0:00:06.858874\n",
      "199800 0:00:06.860619\n",
      "199900 0:00:06.862528\n",
      "get 200000 0:00:09.581694\n",
      "200000 0:00:09.581941\n",
      "200100 0:00:09.584517\n",
      "200200 0:00:09.586583\n",
      "200300 0:00:09.588401\n",
      "200400 0:00:09.590097\n",
      "200500 0:00:09.591657\n",
      "200600 0:00:09.592970\n",
      "200700 0:00:09.594197\n",
      "200800 0:00:09.595581\n",
      "200900 0:00:09.597056\n",
      "get 201000 0:00:09.598890\n",
      "201000 0:00:09.598910\n",
      "201100 0:00:09.600170\n",
      "201200 0:00:09.601850\n",
      "201300 0:00:09.603333\n",
      "201400 0:00:09.605211\n",
      "201500 0:00:09.606964\n",
      "201600 0:00:09.608467\n",
      "201700 0:00:09.610137\n",
      "201800 0:00:09.611776\n",
      "201900 0:00:09.613442\n",
      "get 202000 0:00:09.615284\n",
      "202000 0:00:09.615305\n",
      "202100 0:00:09.616678\n",
      "202200 0:00:09.618062\n",
      "202300 0:00:09.619403\n",
      "202400 0:00:09.620557\n",
      "202500 0:00:09.621955\n",
      "202600 0:00:09.623398\n",
      "202700 0:00:09.625202\n",
      "202800 0:00:09.626848\n",
      "202900 0:00:09.628346\n",
      "get 203000 0:00:09.629993\n",
      "203000 0:00:09.630011\n",
      "203100 0:00:09.631506\n",
      "203200 0:00:09.633366\n",
      "203300 0:00:09.635404\n",
      "203400 0:00:09.637428\n",
      "203500 0:00:09.639447\n",
      "203600 0:00:09.641239\n",
      "203700 0:00:09.643014\n",
      "203800 0:00:09.644944\n",
      "203900 0:00:09.647022\n",
      "get 204000 0:00:09.649042\n",
      "204000 0:00:09.649060\n",
      "204100 0:00:09.650729\n",
      "204200 0:00:09.652610\n",
      "204300 0:00:09.654913\n",
      "204400 0:00:09.656595\n",
      "204500 0:00:09.658652\n",
      "204600 0:00:09.660781\n",
      "204700 0:00:09.662917\n",
      "204800 0:00:09.664994\n",
      "204900 0:00:09.667002\n",
      "get 205000 0:00:09.668956\n",
      "205000 0:00:09.668975\n",
      "205100 0:00:09.670918\n",
      "205200 0:00:09.673035\n",
      "205300 0:00:09.675168\n",
      "205400 0:00:09.677197\n",
      "205500 0:00:09.678988\n",
      "205600 0:00:09.681025\n",
      "205700 0:00:09.683146\n",
      "205800 0:00:09.684731\n",
      "205900 0:00:09.686522\n",
      "get 206000 0:00:09.688385\n",
      "206000 0:00:09.688403\n",
      "206100 0:00:09.690514\n",
      "206200 0:00:09.692533\n",
      "206300 0:00:09.694537\n",
      "206400 0:00:09.696521\n",
      "206500 0:00:09.698437\n",
      "206600 0:00:09.700429\n",
      "206700 0:00:09.702468\n",
      "206800 0:00:09.704362\n",
      "206900 0:00:09.706351\n",
      "get 207000 0:00:09.708339\n",
      "207000 0:00:09.708358\n",
      "207100 0:00:09.710346\n",
      "207200 0:00:09.712404\n",
      "207300 0:00:09.714314\n",
      "207400 0:00:09.716179\n",
      "207500 0:00:09.717951\n",
      "207600 0:00:09.719731\n",
      "207700 0:00:09.721631\n",
      "207800 0:00:09.723142\n",
      "207900 0:00:09.724845\n",
      "get 208000 0:00:09.726671\n",
      "208000 0:00:09.726689\n",
      "208100 0:00:09.728633\n",
      "208200 0:00:09.730373\n",
      "208300 0:00:09.732107\n",
      "208400 0:00:09.734025\n",
      "208500 0:00:09.736026\n",
      "208600 0:00:09.737509\n",
      "208700 0:00:09.739320\n",
      "208800 0:00:09.741044\n",
      "208900 0:00:09.742866\n",
      "get 209000 0:00:09.744801\n",
      "209000 0:00:09.744821\n",
      "209100 0:00:09.746632\n",
      "209200 0:00:09.748272\n",
      "209300 0:00:09.750036\n",
      "209400 0:00:09.751703\n",
      "209500 0:00:09.753471\n",
      "209600 0:00:09.754976\n",
      "209700 0:00:09.756455\n",
      "209800 0:00:09.758022\n",
      "209900 0:00:09.759737\n",
      "get 210000 0:00:09.761360\n",
      "210000 0:00:09.761398\n",
      "210100 0:00:09.762956\n",
      "210200 0:00:09.764718\n",
      "210300 0:00:09.766447\n",
      "210400 0:00:09.768039\n",
      "210500 0:00:09.769610\n",
      "210600 0:00:09.771360\n",
      "210700 0:00:09.773064\n",
      "210800 0:00:09.774776\n",
      "210900 0:00:09.776607\n",
      "get 211000 0:00:09.778257\n",
      "211000 0:00:09.778281\n",
      "211100 0:00:09.779829\n",
      "211200 0:00:09.781387\n",
      "211300 0:00:09.783475\n",
      "211400 0:00:09.785096\n",
      "211500 0:00:09.786868\n",
      "211600 0:00:09.788402\n",
      "211700 0:00:09.790178\n",
      "211800 0:00:09.791767\n",
      "211900 0:00:09.793349\n",
      "get 212000 0:00:09.794995\n",
      "212000 0:00:09.795018\n",
      "212100 0:00:09.796675\n",
      "212200 0:00:09.798159\n",
      "212300 0:00:09.799900\n",
      "212400 0:00:09.801447\n",
      "212500 0:00:09.803304\n",
      "212600 0:00:09.804927\n",
      "212700 0:00:09.806642\n",
      "212800 0:00:09.808293\n",
      "212900 0:00:09.810196\n",
      "get 213000 0:00:09.811815\n",
      "213000 0:00:09.811838\n",
      "213100 0:00:09.813571\n",
      "213200 0:00:09.815179\n",
      "213300 0:00:09.816739\n",
      "213400 0:00:09.818482\n",
      "213500 0:00:09.820133\n",
      "213600 0:00:09.821863\n",
      "213700 0:00:09.823784\n",
      "213800 0:00:09.825460\n",
      "213900 0:00:09.827094\n",
      "get 214000 0:00:09.828742\n",
      "214000 0:00:09.828782\n",
      "214100 0:00:09.830472\n",
      "214200 0:00:09.832142\n",
      "214300 0:00:09.834082\n",
      "214400 0:00:09.835654\n",
      "214500 0:00:09.837420\n",
      "214600 0:00:09.839183\n",
      "214700 0:00:09.840813\n",
      "214800 0:00:09.842451\n",
      "214900 0:00:09.844232\n",
      "get 215000 0:00:09.845984\n",
      "215000 0:00:09.846009\n",
      "215100 0:00:09.847556\n",
      "215200 0:00:09.849473\n",
      "215300 0:00:09.851330\n",
      "215400 0:00:09.853043\n",
      "215500 0:00:09.854883\n",
      "215600 0:00:09.856420\n",
      "215700 0:00:09.858769\n",
      "215800 0:00:09.860413\n",
      "215900 0:00:09.862206\n",
      "get 216000 0:00:09.864004\n",
      "216000 0:00:09.864028\n",
      "216100 0:00:09.865548\n",
      "216200 0:00:09.867430\n",
      "216300 0:00:09.869100\n",
      "216400 0:00:09.870717\n",
      "216500 0:00:09.872607\n",
      "216600 0:00:09.874304\n",
      "216700 0:00:09.876116\n",
      "216800 0:00:09.878081\n",
      "216900 0:00:09.879783\n",
      "get 217000 0:00:09.881374\n",
      "217000 0:00:09.881411\n",
      "217100 0:00:09.883086\n",
      "217200 0:00:09.884992\n",
      "217300 0:00:09.886543\n",
      "217400 0:00:09.888398\n",
      "217500 0:00:09.890137\n",
      "217600 0:00:09.891876\n",
      "217700 0:00:09.893570\n",
      "217800 0:00:09.895467\n",
      "217900 0:00:09.897140\n",
      "get 218000 0:00:09.899093\n",
      "218000 0:00:09.899125\n",
      "218100 0:00:09.900715\n",
      "218200 0:00:09.902368\n",
      "218300 0:00:09.904447\n",
      "218400 0:00:09.906567\n",
      "218500 0:00:09.908657\n",
      "218600 0:00:09.910588\n",
      "218700 0:00:09.912915\n",
      "218800 0:00:09.914595\n",
      "218900 0:00:09.916416\n",
      "get 219000 0:00:09.918310\n",
      "219000 0:00:09.918343\n",
      "219100 0:00:09.920263\n",
      "219200 0:00:09.922152\n",
      "219300 0:00:09.924172\n",
      "219400 0:00:09.925760\n",
      "219500 0:00:09.927680\n",
      "219600 0:00:09.929663\n",
      "219700 0:00:09.931645\n",
      "219800 0:00:09.933977\n",
      "219900 0:00:09.935476\n",
      "get 220000 0:00:09.937137\n",
      "220000 0:00:09.937175\n",
      "220100 0:00:09.938786\n",
      "220200 0:00:09.940263\n",
      "220300 0:00:09.942346\n",
      "220400 0:00:09.943926\n",
      "220500 0:00:09.945634\n",
      "220600 0:00:09.947268\n",
      "220700 0:00:09.948886\n",
      "220800 0:00:09.950738\n",
      "220900 0:00:09.952181\n",
      "get 221000 0:00:09.953647\n",
      "221000 0:00:09.953672\n",
      "221100 0:00:09.955271\n",
      "221200 0:00:09.957075\n",
      "221300 0:00:09.958644\n",
      "221400 0:00:09.960325\n",
      "221500 0:00:09.962102\n",
      "221600 0:00:09.963955\n",
      "221700 0:00:09.965535\n",
      "221800 0:00:09.967998\n",
      "221900 0:00:09.970059\n",
      "get 222000 0:00:09.971879\n",
      "222000 0:00:09.971910\n",
      "222100 0:00:09.973664\n",
      "222200 0:00:09.975456\n",
      "222300 0:00:09.977274\n",
      "222400 0:00:09.979506\n",
      "222500 0:00:09.981589\n",
      "222600 0:00:09.983416\n",
      "222700 0:00:09.985468\n",
      "222800 0:00:09.987388\n",
      "222900 0:00:09.989001\n",
      "get 223000 0:00:09.991070\n",
      "223000 0:00:09.991092\n",
      "223100 0:00:09.993062\n",
      "223200 0:00:09.995169\n",
      "223300 0:00:09.997453\n",
      "223400 0:00:09.999249\n",
      "223500 0:00:10.000925\n",
      "223600 0:00:10.002984\n",
      "223700 0:00:10.005279\n",
      "223800 0:00:10.007178\n",
      "223900 0:00:10.009473\n",
      "get 224000 0:00:10.011758\n",
      "224000 0:00:10.011789\n",
      "224100 0:00:10.013394\n",
      "224200 0:00:10.015287\n",
      "224300 0:00:10.017359\n",
      "224400 0:00:10.019687\n",
      "224500 0:00:10.021929\n",
      "224600 0:00:10.024264\n",
      "224700 0:00:10.026038\n",
      "224800 0:00:10.028284\n",
      "224900 0:00:10.030398\n",
      "get 225000 0:00:10.032646\n",
      "225000 0:00:10.032670\n",
      "225100 0:00:10.034816\n",
      "225200 0:00:10.037067\n",
      "225300 0:00:10.039502\n",
      "225400 0:00:10.041529\n",
      "225500 0:00:10.043597\n",
      "225600 0:00:10.045506\n",
      "225700 0:00:10.047380\n",
      "225800 0:00:10.049458\n",
      "225900 0:00:10.051481\n",
      "get 226000 0:00:10.053753\n",
      "226000 0:00:10.053774\n",
      "226100 0:00:10.055891\n",
      "226200 0:00:10.058060\n",
      "226300 0:00:10.060332\n",
      "226400 0:00:10.062450\n",
      "226500 0:00:10.064669\n",
      "226600 0:00:10.066901\n",
      "226700 0:00:10.068920\n",
      "226800 0:00:10.071151\n",
      "226900 0:00:10.073375\n",
      "get 227000 0:00:10.075605\n",
      "227000 0:00:10.075641\n",
      "227100 0:00:10.077690\n",
      "227200 0:00:10.079737\n",
      "227300 0:00:10.081665\n",
      "227400 0:00:10.083924\n",
      "227500 0:00:10.086122\n",
      "227600 0:00:10.088264\n",
      "227700 0:00:10.090564\n",
      "227800 0:00:10.092608\n",
      "227900 0:00:10.094806\n",
      "get 228000 0:00:10.096612\n",
      "228000 0:00:10.096632\n",
      "228100 0:00:10.098392\n",
      "228200 0:00:10.100519\n",
      "228300 0:00:10.102455\n",
      "228400 0:00:10.104893\n",
      "228500 0:00:10.107172\n",
      "228600 0:00:10.109464\n",
      "228700 0:00:10.111590\n",
      "228800 0:00:10.113412\n",
      "228900 0:00:10.115779\n",
      "get 229000 0:00:10.118012\n",
      "229000 0:00:10.118031\n",
      "229100 0:00:10.119987\n",
      "229200 0:00:10.122274\n",
      "229300 0:00:10.124536\n",
      "229400 0:00:10.126952\n",
      "229500 0:00:10.129215\n",
      "229600 0:00:10.131399\n",
      "229700 0:00:10.133596\n",
      "229800 0:00:10.135840\n",
      "229900 0:00:10.137842\n",
      "get 230000 0:00:10.140170\n",
      "230000 0:00:10.140189\n",
      "230100 0:00:10.142584\n",
      "230200 0:00:10.145020\n",
      "230300 0:00:10.147030\n",
      "230400 0:00:10.149149\n",
      "230500 0:00:10.151471\n",
      "230600 0:00:10.153516\n",
      "230700 0:00:10.155872\n",
      "230800 0:00:10.158031\n",
      "230900 0:00:10.160011\n",
      "get 231000 0:00:10.162302\n",
      "231000 0:00:10.162335\n",
      "231100 0:00:10.164388\n",
      "231200 0:00:10.166431\n",
      "231300 0:00:10.168681\n",
      "231400 0:00:10.170649\n",
      "231500 0:00:10.172966\n",
      "231600 0:00:10.175407\n",
      "231700 0:00:10.177597\n",
      "231800 0:00:10.179901\n",
      "231900 0:00:10.181802\n",
      "get 232000 0:00:10.184122\n",
      "232000 0:00:10.184156\n",
      "232100 0:00:10.186422\n",
      "232200 0:00:10.188578\n",
      "232300 0:00:10.190296\n",
      "232400 0:00:10.192016\n",
      "232500 0:00:10.194053\n",
      "232600 0:00:10.196003\n",
      "232700 0:00:10.197948\n",
      "232800 0:00:10.199837\n",
      "232900 0:00:10.201761\n",
      "get 233000 0:00:10.203771\n",
      "233000 0:00:10.203804\n",
      "233100 0:00:10.205629\n",
      "233200 0:00:10.207049\n",
      "233300 0:00:10.208682\n",
      "233400 0:00:10.210309\n",
      "233500 0:00:10.212021\n",
      "233600 0:00:10.213783\n",
      "233700 0:00:10.215651\n",
      "233800 0:00:10.217125\n",
      "233900 0:00:10.219063\n",
      "get 234000 0:00:10.220950\n",
      "234000 0:00:10.220972\n",
      "234100 0:00:10.222621\n",
      "234200 0:00:10.224440\n",
      "234300 0:00:10.226414\n",
      "234400 0:00:10.228220\n",
      "234500 0:00:10.229812\n",
      "234600 0:00:10.231643\n",
      "234700 0:00:10.233776\n",
      "234800 0:00:10.235845\n",
      "234900 0:00:10.237700\n",
      "get 235000 0:00:10.239552\n",
      "235000 0:00:10.239580\n",
      "235100 0:00:10.241368\n",
      "235200 0:00:10.243109\n",
      "235300 0:00:10.244871\n",
      "235400 0:00:10.246544\n",
      "235500 0:00:10.248566\n",
      "235600 0:00:10.250506\n",
      "235700 0:00:10.252197\n",
      "235800 0:00:10.254254\n",
      "235900 0:00:10.256544\n",
      "get 236000 0:00:10.258206\n",
      "236000 0:00:10.258231\n",
      "236100 0:00:10.260754\n",
      "236200 0:00:10.262436\n",
      "236300 0:00:10.264150\n",
      "236400 0:00:10.265702\n",
      "236500 0:00:10.267530\n",
      "236600 0:00:10.269699\n",
      "236700 0:00:10.271818\n",
      "236800 0:00:10.273708\n",
      "236900 0:00:10.275834\n",
      "get 237000 0:00:10.278012\n",
      "237000 0:00:10.278037\n",
      "237100 0:00:10.280217\n",
      "237200 0:00:10.282315\n",
      "237300 0:00:10.284487\n",
      "237400 0:00:10.286408\n",
      "237500 0:00:10.288642\n",
      "237600 0:00:10.290652\n",
      "237700 0:00:10.293121\n",
      "237800 0:00:10.295306\n",
      "237900 0:00:10.297406\n",
      "get 238000 0:00:10.299842\n",
      "238000 0:00:10.299871\n",
      "238100 0:00:10.301422\n",
      "238200 0:00:10.303614\n",
      "238300 0:00:10.305770\n",
      "238400 0:00:10.308042\n",
      "238500 0:00:10.310349\n",
      "238600 0:00:10.312530\n",
      "238700 0:00:10.314787\n",
      "238800 0:00:10.317155\n",
      "238900 0:00:10.319405\n",
      "get 239000 0:00:10.321581\n",
      "239000 0:00:10.321613\n",
      "239100 0:00:10.323930\n",
      "239200 0:00:10.326003\n",
      "239300 0:00:10.328400\n",
      "239400 0:00:10.330559\n",
      "239500 0:00:10.332889\n",
      "239600 0:00:10.334956\n",
      "239700 0:00:10.337283\n",
      "239800 0:00:10.339284\n",
      "239900 0:00:10.341335\n",
      "get 240000 0:00:10.343724\n",
      "240000 0:00:10.343756\n",
      "240100 0:00:10.345984\n",
      "240200 0:00:10.348597\n",
      "240300 0:00:10.350996\n",
      "240400 0:00:10.353796\n",
      "240500 0:00:10.356291\n",
      "240600 0:00:10.358629\n",
      "240700 0:00:10.361123\n",
      "240800 0:00:10.363787\n",
      "240900 0:00:10.366689\n",
      "get 241000 0:00:10.369045\n",
      "241000 0:00:10.369128\n",
      "241100 0:00:10.371577\n",
      "241200 0:00:10.374495\n",
      "241300 0:00:10.377291\n",
      "241400 0:00:10.379911\n",
      "241500 0:00:10.382965\n",
      "241600 0:00:10.385638\n",
      "241700 0:00:10.388425\n",
      "241800 0:00:10.390290\n",
      "241900 0:00:10.392629\n",
      "get 242000 0:00:10.394558\n",
      "242000 0:00:10.394642\n",
      "242100 0:00:10.396452\n",
      "242200 0:00:10.399055\n",
      "242300 0:00:10.401464\n",
      "242400 0:00:10.403253\n",
      "242500 0:00:10.405406\n",
      "242600 0:00:10.407780\n",
      "242700 0:00:10.409465\n",
      "242800 0:00:10.412648\n",
      "242900 0:00:10.414604\n",
      "get 243000 0:00:10.416830\n",
      "243000 0:00:10.416865\n",
      "243100 0:00:10.419393\n",
      "243200 0:00:10.421423\n",
      "243300 0:00:10.423197\n",
      "243400 0:00:10.425531\n",
      "243500 0:00:10.427715\n",
      "243600 0:00:10.429625\n",
      "243700 0:00:10.431633\n",
      "243800 0:00:10.434310\n",
      "243900 0:00:10.436448\n",
      "get 244000 0:00:10.438218\n",
      "244000 0:00:10.438253\n",
      "244100 0:00:10.440337\n",
      "244200 0:00:10.442522\n",
      "244300 0:00:10.444639\n",
      "244400 0:00:10.447114\n",
      "244500 0:00:10.449197\n",
      "244600 0:00:10.451704\n",
      "244700 0:00:10.453978\n",
      "244800 0:00:10.455898\n",
      "244900 0:00:10.457642\n",
      "get 245000 0:00:10.459735\n",
      "245000 0:00:10.459778\n",
      "245100 0:00:10.461923\n",
      "245200 0:00:10.464419\n",
      "245300 0:00:10.466240\n",
      "245400 0:00:10.467724\n",
      "245500 0:00:10.469296\n",
      "245600 0:00:10.470876\n",
      "245700 0:00:10.472670\n",
      "245800 0:00:10.474170\n",
      "245900 0:00:10.475903\n",
      "get 246000 0:00:10.477896\n",
      "246000 0:00:10.477934\n",
      "246100 0:00:10.479548\n",
      "246200 0:00:10.481325\n",
      "246300 0:00:10.482633\n",
      "246400 0:00:10.484292\n",
      "246500 0:00:10.485838\n",
      "246600 0:00:10.487260\n",
      "246700 0:00:10.488586\n",
      "246800 0:00:10.489955\n",
      "246900 0:00:10.491810\n",
      "get 247000 0:00:10.493482\n",
      "247000 0:00:10.493515\n",
      "247100 0:00:10.495319\n",
      "247200 0:00:10.496581\n",
      "247300 0:00:10.498338\n",
      "247400 0:00:10.500125\n",
      "247500 0:00:10.501835\n",
      "247600 0:00:10.503248\n",
      "247700 0:00:10.504788\n",
      "247800 0:00:10.506618\n",
      "247900 0:00:10.508175\n",
      "get 248000 0:00:10.509527\n",
      "248000 0:00:10.509560\n",
      "248100 0:00:10.511061\n",
      "248200 0:00:10.512790\n",
      "248300 0:00:10.514547\n",
      "248400 0:00:10.516552\n",
      "248500 0:00:10.518182\n",
      "248600 0:00:10.519637\n",
      "248700 0:00:10.520989\n",
      "248800 0:00:10.522741\n",
      "248900 0:00:10.524293\n",
      "get 249000 0:00:10.525921\n",
      "249000 0:00:10.525943\n",
      "249100 0:00:10.527833\n",
      "249200 0:00:10.529655\n",
      "249300 0:00:10.531187\n",
      "249400 0:00:10.532911\n",
      "249500 0:00:10.534482\n",
      "249600 0:00:10.535966\n",
      "249700 0:00:10.537812\n",
      "249800 0:00:10.539835\n",
      "249900 0:00:10.541754\n",
      "get 250000 0:00:10.543768\n",
      "250000 0:00:10.543796\n",
      "250100 0:00:10.545382\n",
      "250200 0:00:10.547075\n",
      "250300 0:00:10.549244\n",
      "250400 0:00:10.551013\n",
      "250500 0:00:10.552600\n",
      "250600 0:00:10.554320\n",
      "250700 0:00:10.556143\n",
      "250800 0:00:10.558150\n",
      "250900 0:00:10.559877\n",
      "get 251000 0:00:10.561542\n",
      "251000 0:00:10.561570\n",
      "251100 0:00:10.563366\n",
      "251200 0:00:10.565153\n",
      "251300 0:00:10.567264\n",
      "251400 0:00:10.569138\n",
      "251500 0:00:10.571032\n",
      "251600 0:00:10.572682\n",
      "251700 0:00:10.574481\n",
      "251800 0:00:10.576537\n",
      "251900 0:00:10.578521\n",
      "get 252000 0:00:10.580488\n",
      "252000 0:00:10.580514\n",
      "252100 0:00:10.582209\n",
      "252200 0:00:10.583715\n",
      "252300 0:00:10.585338\n",
      "252400 0:00:10.587591\n",
      "252500 0:00:10.590144\n",
      "252600 0:00:10.592238\n",
      "252700 0:00:10.593827\n",
      "252800 0:00:10.595589\n",
      "252900 0:00:10.597416\n",
      "get 253000 0:00:10.599031\n",
      "253000 0:00:10.599058\n",
      "253100 0:00:10.600850\n",
      "253200 0:00:10.602668\n",
      "253300 0:00:10.604617\n",
      "253400 0:00:10.606926\n",
      "253500 0:00:10.608923\n",
      "253600 0:00:10.610825\n",
      "253700 0:00:10.612607\n",
      "253800 0:00:10.614428\n",
      "253900 0:00:10.616076\n",
      "get 254000 0:00:10.618508\n",
      "254000 0:00:10.618542\n",
      "254100 0:00:10.620548\n",
      "254200 0:00:10.623064\n",
      "254300 0:00:10.625311\n",
      "254400 0:00:10.627722\n",
      "254500 0:00:10.630094\n",
      "254600 0:00:10.631958\n",
      "254700 0:00:10.634056\n",
      "254800 0:00:10.635599\n",
      "254900 0:00:10.637453\n",
      "get 255000 0:00:10.639699\n",
      "255000 0:00:10.639729\n",
      "255100 0:00:10.641715\n",
      "255200 0:00:10.643641\n",
      "255300 0:00:10.645363\n",
      "255400 0:00:10.647515\n",
      "255500 0:00:10.649438\n",
      "255600 0:00:10.651578\n",
      "255700 0:00:10.653561\n",
      "255800 0:00:10.655483\n",
      "255900 0:00:10.657115\n",
      "get 256000 0:00:10.658750\n",
      "256000 0:00:10.658780\n",
      "256100 0:00:10.660670\n",
      "256200 0:00:10.662775\n",
      "256300 0:00:10.664696\n",
      "256400 0:00:10.666706\n",
      "256500 0:00:10.668536\n",
      "256600 0:00:10.670367\n",
      "256700 0:00:10.672364\n",
      "256800 0:00:10.673846\n",
      "256900 0:00:10.675461\n",
      "get 257000 0:00:10.677158\n",
      "257000 0:00:10.677192\n",
      "257100 0:00:10.679036\n",
      "257200 0:00:10.680828\n",
      "257300 0:00:10.682709\n",
      "257400 0:00:10.684454\n",
      "257500 0:00:10.686069\n",
      "257600 0:00:10.688313\n",
      "257700 0:00:10.690125\n",
      "257800 0:00:10.691936\n",
      "257900 0:00:10.693526\n",
      "get 258000 0:00:10.695567\n",
      "258000 0:00:10.695593\n",
      "258100 0:00:10.697368\n",
      "258200 0:00:10.698996\n",
      "258300 0:00:10.700461\n",
      "258400 0:00:10.701952\n",
      "258500 0:00:10.703754\n",
      "258600 0:00:10.705360\n",
      "258700 0:00:10.707089\n",
      "258800 0:00:10.708840\n",
      "258900 0:00:10.710547\n",
      "get 259000 0:00:10.712060\n",
      "259000 0:00:10.712081\n",
      "259100 0:00:10.714335\n",
      "259200 0:00:10.716189\n",
      "259300 0:00:10.718086\n",
      "259400 0:00:10.719988\n",
      "259500 0:00:10.721739\n",
      "259600 0:00:10.723557\n",
      "259700 0:00:10.725511\n",
      "259800 0:00:10.727198\n",
      "259900 0:00:10.728959\n",
      "get 260000 0:00:10.730820\n",
      "260000 0:00:10.730841\n",
      "260100 0:00:10.732401\n",
      "260200 0:00:10.734498\n",
      "260300 0:00:10.736436\n",
      "260400 0:00:10.737775\n",
      "260500 0:00:10.739488\n",
      "260600 0:00:10.741129\n",
      "260700 0:00:10.742713\n",
      "260800 0:00:10.744412\n",
      "260900 0:00:10.746241\n",
      "get 261000 0:00:10.747892\n",
      "261000 0:00:10.747921\n",
      "261100 0:00:10.750070\n",
      "261200 0:00:10.751908\n",
      "261300 0:00:10.753354\n",
      "261400 0:00:10.755209\n",
      "261500 0:00:10.756665\n",
      "261600 0:00:10.758426\n",
      "261700 0:00:10.760140\n",
      "261800 0:00:10.761730\n",
      "261900 0:00:10.763505\n",
      "get 262000 0:00:10.765176\n",
      "262000 0:00:10.765218\n",
      "262100 0:00:10.766729\n",
      "262200 0:00:10.768370\n",
      "262300 0:00:10.769978\n",
      "262400 0:00:10.772160\n",
      "262500 0:00:10.773960\n",
      "262600 0:00:10.776138\n",
      "262700 0:00:10.777823\n",
      "262800 0:00:10.779660\n",
      "262900 0:00:10.781227\n",
      "get 263000 0:00:10.782959\n",
      "263000 0:00:10.782985\n",
      "263100 0:00:10.784914\n",
      "263200 0:00:10.786914\n",
      "263300 0:00:10.789185\n",
      "263400 0:00:10.791434\n",
      "263500 0:00:10.793493\n",
      "263600 0:00:10.795875\n",
      "263700 0:00:10.798548\n",
      "263800 0:00:10.800903\n",
      "263900 0:00:10.803616\n",
      "get 264000 0:00:10.806120\n",
      "264000 0:00:10.806143\n",
      "264100 0:00:10.808815\n",
      "264200 0:00:10.811485\n",
      "264300 0:00:10.813848\n",
      "264400 0:00:10.816343\n",
      "264500 0:00:10.819003\n",
      "264600 0:00:10.821499\n",
      "264700 0:00:10.824182\n",
      "264800 0:00:10.826308\n",
      "264900 0:00:10.828988\n",
      "get 265000 0:00:10.831499\n",
      "265000 0:00:10.831599\n",
      "265100 0:00:10.833928\n",
      "265200 0:00:10.836626\n",
      "265300 0:00:10.839275\n",
      "265400 0:00:10.841755\n",
      "265500 0:00:10.844429\n",
      "265600 0:00:10.847126\n",
      "265700 0:00:10.849724\n",
      "265800 0:00:10.852475\n",
      "265900 0:00:10.854964\n",
      "get 266000 0:00:10.857685\n",
      "266000 0:00:10.857706\n",
      "266100 0:00:10.859870\n",
      "266200 0:00:10.862314\n",
      "266300 0:00:10.864823\n",
      "266400 0:00:10.867658\n",
      "266500 0:00:10.870142\n",
      "266600 0:00:10.872866\n",
      "266700 0:00:10.875317\n",
      "266800 0:00:10.878012\n",
      "266900 0:00:10.880648\n",
      "get 267000 0:00:10.883206\n",
      "267000 0:00:10.883234\n",
      "267100 0:00:10.885975\n",
      "267200 0:00:10.887981\n",
      "267300 0:00:10.890692\n",
      "267400 0:00:10.893412\n",
      "267500 0:00:10.896646\n",
      "267600 0:00:10.899570\n",
      "267700 0:00:10.902391\n",
      "267800 0:00:10.904797\n",
      "267900 0:00:10.906985\n",
      "get 268000 0:00:10.909352\n",
      "268000 0:00:10.909383\n",
      "268100 0:00:10.911588\n",
      "268200 0:00:10.914005\n",
      "268300 0:00:10.916422\n",
      "268400 0:00:10.919138\n",
      "268500 0:00:10.922120\n",
      "268600 0:00:10.925752\n",
      "268700 0:00:10.929365\n",
      "268800 0:00:10.932170\n",
      "268900 0:00:10.934672\n",
      "get 269000 0:00:10.937655\n",
      "269000 0:00:10.937750\n",
      "269100 0:00:10.940620\n",
      "269200 0:00:10.943559\n",
      "269300 0:00:10.946177\n",
      "269400 0:00:10.947961\n",
      "269500 0:00:10.949946\n",
      "269600 0:00:10.951999\n",
      "269700 0:00:10.954756\n",
      "269800 0:00:10.957854\n",
      "269900 0:00:10.960925\n",
      "get 270000 0:00:10.963690\n",
      "270000 0:00:10.963737\n",
      "270100 0:00:10.966796\n",
      "270200 0:00:10.969606\n",
      "270300 0:00:10.972582\n",
      "270400 0:00:10.975693\n",
      "270500 0:00:10.978792\n",
      "270600 0:00:10.981228\n",
      "270700 0:00:10.984149\n",
      "270800 0:00:10.987114\n",
      "270900 0:00:10.990018\n",
      "get 271000 0:00:10.993011\n",
      "271000 0:00:10.993131\n",
      "271100 0:00:10.996192\n",
      "271200 0:00:10.999327\n",
      "271300 0:00:11.002464\n",
      "271400 0:00:11.005539\n",
      "271500 0:00:11.008408\n",
      "271600 0:00:11.011400\n",
      "271700 0:00:11.014529\n",
      "271800 0:00:11.017754\n",
      "271900 0:00:11.020444\n",
      "get 272000 0:00:11.023491\n",
      "272000 0:00:11.023511\n",
      "272100 0:00:11.026599\n",
      "272200 0:00:11.029277\n",
      "272300 0:00:11.032257\n",
      "272400 0:00:11.035344\n",
      "272500 0:00:11.038286\n",
      "272600 0:00:11.041349\n",
      "272700 0:00:11.044194\n",
      "272800 0:00:11.046908\n",
      "272900 0:00:11.049801\n",
      "get 273000 0:00:11.052729\n",
      "273000 0:00:11.052765\n",
      "273100 0:00:11.055892\n",
      "273200 0:00:11.058909\n",
      "273300 0:00:11.062101\n",
      "273400 0:00:11.065397\n",
      "273500 0:00:11.067686\n",
      "273600 0:00:11.070214\n",
      "273700 0:00:11.073111\n",
      "273800 0:00:11.075959\n",
      "273900 0:00:11.079179\n",
      "get 274000 0:00:11.082299\n",
      "274000 0:00:11.082334\n",
      "274100 0:00:11.084796\n",
      "274200 0:00:11.087650\n",
      "274300 0:00:11.090634\n",
      "274400 0:00:11.093730\n",
      "274500 0:00:11.096389\n",
      "274600 0:00:11.099341\n",
      "274700 0:00:11.102250\n",
      "274800 0:00:11.104946\n",
      "274900 0:00:11.107943\n",
      "get 275000 0:00:11.111053\n",
      "275000 0:00:11.111081\n",
      "275100 0:00:11.113868\n",
      "275200 0:00:11.116971\n",
      "275300 0:00:11.119679\n",
      "275400 0:00:11.122721\n",
      "275500 0:00:11.125595\n",
      "275600 0:00:11.128345\n",
      "275700 0:00:11.131138\n",
      "275800 0:00:11.134035\n",
      "275900 0:00:11.136876\n",
      "get 276000 0:00:11.139602\n",
      "276000 0:00:11.139626\n",
      "276100 0:00:11.142215\n",
      "276200 0:00:11.145062\n",
      "276300 0:00:11.147213\n",
      "276400 0:00:11.149691\n",
      "276500 0:00:11.152518\n",
      "276600 0:00:11.155135\n",
      "276700 0:00:11.158086\n",
      "276800 0:00:11.160977\n",
      "276900 0:00:11.163698\n",
      "get 277000 0:00:11.166495\n",
      "277000 0:00:11.166514\n",
      "277100 0:00:11.169291\n",
      "277200 0:00:11.172102\n",
      "277300 0:00:11.175320\n",
      "277400 0:00:11.178134\n",
      "277500 0:00:11.180942\n",
      "277600 0:00:11.183874\n",
      "277700 0:00:11.186769\n",
      "277800 0:00:11.189648\n",
      "277900 0:00:11.192468\n",
      "get 278000 0:00:11.195366\n",
      "278000 0:00:11.195487\n",
      "278100 0:00:11.198196\n",
      "278200 0:00:11.200827\n",
      "278300 0:00:11.203873\n",
      "278400 0:00:11.206727\n",
      "278500 0:00:11.209394\n",
      "278600 0:00:11.212259\n",
      "278700 0:00:11.215171\n",
      "278800 0:00:11.217999\n",
      "278900 0:00:11.220883\n",
      "get 279000 0:00:11.223780\n",
      "279000 0:00:11.223800\n",
      "279100 0:00:11.226800\n",
      "279200 0:00:11.229484\n",
      "279300 0:00:11.232707\n",
      "279400 0:00:11.235477\n",
      "279500 0:00:11.238073\n",
      "279600 0:00:11.241101\n",
      "279700 0:00:11.243802\n",
      "279800 0:00:11.246689\n",
      "279900 0:00:11.249493\n",
      "get 280000 0:00:11.252445\n",
      "280000 0:00:11.252490\n",
      "280100 0:00:11.255480\n",
      "280200 0:00:11.258492\n",
      "280300 0:00:11.260949\n",
      "280400 0:00:11.263935\n",
      "280500 0:00:11.266682\n",
      "280600 0:00:11.269889\n",
      "280700 0:00:11.273028\n",
      "280800 0:00:11.275540\n",
      "280900 0:00:11.278591\n",
      "get 281000 0:00:11.281027\n",
      "281000 0:00:11.281048\n",
      "281100 0:00:11.284001\n",
      "281200 0:00:11.286884\n",
      "281300 0:00:11.289900\n",
      "281400 0:00:11.292842\n",
      "281500 0:00:11.295713\n",
      "281600 0:00:11.298551\n",
      "281700 0:00:11.301321\n",
      "281800 0:00:11.304123\n",
      "281900 0:00:11.306945\n",
      "get 282000 0:00:11.309909\n",
      "282000 0:00:11.309930\n",
      "282100 0:00:11.312853\n",
      "282200 0:00:11.315867\n",
      "282300 0:00:11.318790\n",
      "282400 0:00:11.321411\n",
      "282500 0:00:11.324642\n",
      "282600 0:00:11.326887\n",
      "282700 0:00:11.329750\n",
      "282800 0:00:11.332557\n",
      "282900 0:00:11.335639\n",
      "get 283000 0:00:11.338489\n",
      "283000 0:00:11.338509\n",
      "283100 0:00:11.341500\n",
      "283200 0:00:11.344460\n",
      "283300 0:00:11.347123\n",
      "283400 0:00:11.349758\n",
      "283500 0:00:11.352569\n",
      "283600 0:00:11.355444\n",
      "283700 0:00:11.358469\n",
      "283800 0:00:11.361371\n",
      "283900 0:00:11.364128\n",
      "get 284000 0:00:11.366800\n",
      "284000 0:00:11.366831\n",
      "284100 0:00:11.369652\n",
      "284200 0:00:11.372376\n",
      "284300 0:00:11.375115\n",
      "284400 0:00:11.377892\n",
      "284500 0:00:11.381025\n",
      "284600 0:00:11.383730\n",
      "284700 0:00:11.386366\n",
      "284800 0:00:11.389353\n",
      "284900 0:00:11.392113\n",
      "get 285000 0:00:11.394930\n",
      "285000 0:00:11.395031\n",
      "285100 0:00:11.397638\n",
      "285200 0:00:11.400724\n",
      "285300 0:00:11.403764\n",
      "285400 0:00:11.405978\n",
      "285500 0:00:11.408462\n",
      "285600 0:00:11.411038\n",
      "285700 0:00:11.414015\n",
      "285800 0:00:11.416852\n",
      "285900 0:00:11.419425\n",
      "get 286000 0:00:11.422271\n",
      "286000 0:00:11.422291\n",
      "286100 0:00:11.425081\n",
      "286200 0:00:11.427877\n",
      "286300 0:00:11.430761\n",
      "286400 0:00:11.433676\n",
      "286500 0:00:11.436542\n",
      "286600 0:00:11.439453\n",
      "286700 0:00:11.442514\n",
      "286800 0:00:11.445332\n",
      "286900 0:00:11.447763\n",
      "get 287000 0:00:11.450705\n",
      "287000 0:00:11.450724\n",
      "287100 0:00:11.453762\n",
      "287200 0:00:11.456656\n",
      "287300 0:00:11.459392\n",
      "287400 0:00:11.462332\n",
      "287500 0:00:11.465299\n",
      "287600 0:00:11.468080\n",
      "287700 0:00:11.470907\n",
      "287800 0:00:11.473687\n",
      "287900 0:00:11.476334\n",
      "get 288000 0:00:11.479222\n",
      "288000 0:00:11.479242\n",
      "288100 0:00:11.482306\n",
      "288200 0:00:11.484643\n",
      "288300 0:00:11.487300\n",
      "288400 0:00:11.490310\n",
      "288500 0:00:11.493213\n",
      "288600 0:00:11.496426\n",
      "288700 0:00:11.498977\n",
      "288800 0:00:11.501803\n",
      "288900 0:00:11.504561\n",
      "get 289000 0:00:11.507595\n",
      "289000 0:00:11.507615\n",
      "289100 0:00:11.510470\n",
      "289200 0:00:11.513374\n",
      "289300 0:00:11.516335\n",
      "289400 0:00:11.519344\n",
      "289500 0:00:11.522317\n",
      "289600 0:00:11.525301\n",
      "289700 0:00:11.528226\n",
      "289800 0:00:11.531211\n",
      "289900 0:00:11.533936\n",
      "get 290000 0:00:11.536458\n",
      "290000 0:00:11.536479\n",
      "290100 0:00:11.539250\n",
      "290200 0:00:11.542515\n",
      "290300 0:00:11.545401\n",
      "290400 0:00:11.548349\n",
      "290500 0:00:11.551188\n",
      "290600 0:00:11.554030\n",
      "290700 0:00:11.556745\n",
      "290800 0:00:11.559726\n",
      "290900 0:00:11.562653\n",
      "get 291000 0:00:11.565322\n",
      "291000 0:00:11.565343\n",
      "291100 0:00:11.568128\n",
      "291200 0:00:11.571084\n",
      "291300 0:00:11.573887\n",
      "291400 0:00:11.576621\n",
      "291500 0:00:11.579682\n",
      "291600 0:00:11.582113\n",
      "291700 0:00:11.584844\n",
      "291800 0:00:11.587727\n",
      "291900 0:00:11.590697\n",
      "get 292000 0:00:11.593453\n",
      "292000 0:00:11.593530\n",
      "292100 0:00:11.596293\n",
      "292200 0:00:11.599285\n",
      "292300 0:00:11.602289\n",
      "292400 0:00:11.604999\n",
      "292500 0:00:11.607764\n",
      "292600 0:00:11.610598\n",
      "292700 0:00:11.613300\n",
      "292800 0:00:11.616443\n",
      "292900 0:00:11.619613\n",
      "get 293000 0:00:11.622611\n",
      "293000 0:00:11.622630\n",
      "293100 0:00:11.625461\n",
      "293200 0:00:11.628339\n",
      "293300 0:00:11.631430\n",
      "293400 0:00:11.634390\n",
      "293500 0:00:11.637427\n",
      "293600 0:00:11.640501\n",
      "293700 0:00:11.643295\n",
      "293800 0:00:11.646151\n",
      "293900 0:00:11.649077\n",
      "get 294000 0:00:11.651876\n",
      "294000 0:00:11.651931\n",
      "294100 0:00:11.654690\n",
      "294200 0:00:11.657644\n",
      "294300 0:00:11.660682\n",
      "294400 0:00:11.663666\n",
      "294500 0:00:11.666427\n",
      "294600 0:00:11.669288\n",
      "294700 0:00:11.672237\n",
      "294800 0:00:11.674994\n",
      "294900 0:00:11.677741\n",
      "get 295000 0:00:11.680522\n",
      "295000 0:00:11.680541\n",
      "295100 0:00:11.683367\n",
      "295200 0:00:11.686325\n",
      "295300 0:00:11.689181\n",
      "295400 0:00:11.692139\n",
      "295500 0:00:11.694830\n",
      "295600 0:00:11.697677\n",
      "295700 0:00:11.700445\n",
      "295800 0:00:11.703307\n",
      "295900 0:00:11.706365\n",
      "get 296000 0:00:11.709181\n",
      "296000 0:00:11.709201\n",
      "296100 0:00:11.712190\n",
      "296200 0:00:11.715053\n",
      "296300 0:00:11.718006\n",
      "296400 0:00:11.720756\n",
      "296500 0:00:11.723504\n",
      "296600 0:00:11.726296\n",
      "296700 0:00:11.729179\n",
      "296800 0:00:11.732169\n",
      "296900 0:00:11.735282\n",
      "get 297000 0:00:11.737834\n",
      "297000 0:00:11.737853\n",
      "297100 0:00:11.740662\n",
      "297200 0:00:11.743576\n",
      "297300 0:00:11.746380\n",
      "297400 0:00:11.749237\n",
      "297500 0:00:11.752051\n",
      "297600 0:00:11.754852\n",
      "297700 0:00:11.757777\n",
      "297800 0:00:11.760566\n",
      "297900 0:00:11.763578\n",
      "get 298000 0:00:11.766307\n",
      "298000 0:00:11.766325\n",
      "298100 0:00:11.769756\n",
      "298200 0:00:11.771831\n",
      "298300 0:00:11.774702\n",
      "298400 0:00:11.777685\n",
      "298500 0:00:11.780509\n",
      "298600 0:00:11.783156\n",
      "298700 0:00:11.785779\n",
      "298800 0:00:11.788939\n",
      "298900 0:00:11.792054\n",
      "get 299000 0:00:11.794976\n",
      "299000 0:00:11.794997\n",
      "299100 0:00:11.797826\n",
      "299200 0:00:11.800690\n",
      "299300 0:00:11.803547\n",
      "299400 0:00:11.806321\n",
      "299500 0:00:11.809019\n",
      "299600 0:00:11.811933\n",
      "299700 0:00:11.815119\n",
      "299800 0:00:11.817428\n",
      "299900 0:00:11.820193\n",
      "get 300000 0:00:14.838064\n",
      "300000 0:00:14.838350\n",
      "300100 0:00:14.840936\n",
      "300200 0:00:14.842348\n",
      "300300 0:00:14.844097\n",
      "300400 0:00:14.846108\n",
      "300500 0:00:14.848097\n",
      "300600 0:00:14.849702\n",
      "300700 0:00:14.851778\n",
      "300800 0:00:14.853613\n",
      "300900 0:00:14.855305\n",
      "get 301000 0:00:14.857321\n",
      "301000 0:00:14.857343\n",
      "301100 0:00:14.859177\n",
      "301200 0:00:14.860863\n",
      "301300 0:00:14.862670\n",
      "301400 0:00:14.864682\n",
      "301500 0:00:14.866633\n",
      "301600 0:00:14.868372\n",
      "301700 0:00:14.870452\n",
      "301800 0:00:14.872306\n",
      "301900 0:00:14.873994\n",
      "get 302000 0:00:14.875875\n",
      "302000 0:00:14.875896\n",
      "302100 0:00:14.877956\n",
      "302200 0:00:14.879831\n",
      "302300 0:00:14.881738\n",
      "302400 0:00:14.883484\n",
      "302500 0:00:14.885295\n",
      "302600 0:00:14.887312\n",
      "302700 0:00:14.889207\n",
      "302800 0:00:14.890619\n",
      "302900 0:00:14.892705\n",
      "get 303000 0:00:14.894625\n",
      "303000 0:00:14.894646\n",
      "303100 0:00:14.896532\n",
      "303200 0:00:14.898355\n",
      "303300 0:00:14.900030\n",
      "303400 0:00:14.901635\n",
      "303500 0:00:14.903487\n",
      "303600 0:00:14.905291\n",
      "303700 0:00:14.906756\n",
      "303800 0:00:14.908695\n",
      "303900 0:00:14.910608\n",
      "get 304000 0:00:14.911955\n",
      "304000 0:00:14.911973\n",
      "304100 0:00:14.913381\n",
      "304200 0:00:14.914835\n",
      "304300 0:00:14.916286\n",
      "304400 0:00:14.917751\n",
      "304500 0:00:14.919164\n",
      "304600 0:00:14.920991\n",
      "304700 0:00:14.922499\n",
      "304800 0:00:14.923956\n",
      "304900 0:00:14.925655\n",
      "get 305000 0:00:14.927273\n",
      "305000 0:00:14.927292\n",
      "305100 0:00:14.929434\n",
      "305200 0:00:14.931230\n",
      "305300 0:00:14.932688\n",
      "305400 0:00:14.933988\n",
      "305500 0:00:14.935647\n",
      "305600 0:00:14.937727\n",
      "305700 0:00:14.939238\n",
      "305800 0:00:14.940695\n",
      "305900 0:00:14.942063\n",
      "get 306000 0:00:14.943316\n",
      "306000 0:00:14.943335\n",
      "306100 0:00:14.944855\n",
      "306200 0:00:14.946825\n",
      "306300 0:00:14.948098\n",
      "306400 0:00:14.949481\n",
      "306500 0:00:14.951176\n",
      "306600 0:00:14.953553\n",
      "306700 0:00:14.955144\n",
      "306800 0:00:14.956886\n",
      "306900 0:00:14.958414\n",
      "get 307000 0:00:14.960135\n",
      "307000 0:00:14.960154\n",
      "307100 0:00:14.962034\n",
      "307200 0:00:14.963840\n",
      "307300 0:00:14.965847\n",
      "307400 0:00:14.967746\n",
      "307500 0:00:14.969863\n",
      "307600 0:00:14.971442\n",
      "307700 0:00:14.972863\n",
      "307800 0:00:14.974183\n",
      "307900 0:00:14.975881\n",
      "get 308000 0:00:14.977617\n",
      "308000 0:00:14.977636\n",
      "308100 0:00:14.979164\n",
      "308200 0:00:14.980713\n",
      "308300 0:00:14.982473\n",
      "308400 0:00:14.984238\n",
      "308500 0:00:14.985665\n",
      "308600 0:00:14.987163\n",
      "308700 0:00:14.988714\n",
      "308800 0:00:14.990194\n",
      "308900 0:00:14.991738\n",
      "get 309000 0:00:14.993436\n",
      "309000 0:00:14.993454\n",
      "309100 0:00:14.995022\n",
      "309200 0:00:14.996937\n",
      "309300 0:00:14.998756\n",
      "309400 0:00:15.000269\n",
      "309500 0:00:15.001800\n",
      "309600 0:00:15.003821\n",
      "309700 0:00:15.005957\n",
      "309800 0:00:15.007553\n",
      "309900 0:00:15.009368\n",
      "get 310000 0:00:15.011199\n",
      "310000 0:00:15.011218\n",
      "310100 0:00:15.013089\n",
      "310200 0:00:15.014681\n",
      "310300 0:00:15.016610\n",
      "310400 0:00:15.018753\n",
      "310500 0:00:15.020393\n",
      "310600 0:00:15.021943\n",
      "310700 0:00:15.023629\n",
      "310800 0:00:15.025455\n",
      "310900 0:00:15.026842\n",
      "get 311000 0:00:15.028632\n",
      "311000 0:00:15.028651\n",
      "311100 0:00:15.030571\n",
      "311200 0:00:15.032282\n",
      "311300 0:00:15.033909\n",
      "311400 0:00:15.035714\n",
      "311500 0:00:15.037464\n",
      "311600 0:00:15.039541\n",
      "311700 0:00:15.041253\n",
      "311800 0:00:15.043119\n",
      "311900 0:00:15.044669\n",
      "get 312000 0:00:15.046486\n",
      "312000 0:00:15.046519\n",
      "312100 0:00:15.048262\n",
      "312200 0:00:15.050076\n",
      "312300 0:00:15.051848\n",
      "312400 0:00:15.053850\n",
      "312500 0:00:15.055297\n",
      "312600 0:00:15.056850\n",
      "312700 0:00:15.058687\n",
      "312800 0:00:15.060422\n",
      "312900 0:00:15.062285\n",
      "get 313000 0:00:15.063916\n",
      "313000 0:00:15.063978\n",
      "313100 0:00:15.065715\n",
      "313200 0:00:15.067479\n",
      "313300 0:00:15.069222\n",
      "313400 0:00:15.070977\n",
      "313500 0:00:15.072695\n",
      "313600 0:00:15.074486\n",
      "313700 0:00:15.076166\n",
      "313800 0:00:15.078018\n",
      "313900 0:00:15.079911\n",
      "get 314000 0:00:15.081290\n",
      "314000 0:00:15.081311\n",
      "314100 0:00:15.083255\n",
      "314200 0:00:15.085043\n",
      "314300 0:00:15.086654\n",
      "314400 0:00:15.088515\n",
      "314500 0:00:15.090398\n",
      "314600 0:00:15.092077\n",
      "314700 0:00:15.093317\n",
      "314800 0:00:15.094776\n",
      "314900 0:00:15.096693\n",
      "get 315000 0:00:15.098193\n",
      "315000 0:00:15.098212\n",
      "315100 0:00:15.100052\n",
      "315200 0:00:15.101795\n",
      "315300 0:00:15.103643\n",
      "315400 0:00:15.105283\n",
      "315500 0:00:15.107187\n",
      "315600 0:00:15.108721\n",
      "315700 0:00:15.110319\n",
      "315800 0:00:15.112102\n",
      "315900 0:00:15.113770\n",
      "get 316000 0:00:15.115561\n",
      "316000 0:00:15.115580\n",
      "316100 0:00:15.117601\n",
      "316200 0:00:15.119117\n",
      "316300 0:00:15.120943\n",
      "316400 0:00:15.122549\n",
      "316500 0:00:15.124220\n",
      "316600 0:00:15.125717\n",
      "316700 0:00:15.127342\n",
      "316800 0:00:15.129324\n",
      "316900 0:00:15.131048\n",
      "get 317000 0:00:15.133093\n",
      "317000 0:00:15.133123\n",
      "317100 0:00:15.134828\n",
      "317200 0:00:15.136196\n",
      "317300 0:00:15.137922\n",
      "317400 0:00:15.139474\n",
      "317500 0:00:15.141004\n",
      "317600 0:00:15.142765\n",
      "317700 0:00:15.144717\n",
      "317800 0:00:15.146462\n",
      "317900 0:00:15.148188\n",
      "get 318000 0:00:15.149772\n",
      "318000 0:00:15.149801\n",
      "318100 0:00:15.151573\n",
      "318200 0:00:15.153253\n",
      "318300 0:00:15.154988\n",
      "318400 0:00:15.156822\n",
      "318500 0:00:15.158647\n",
      "318600 0:00:15.160337\n",
      "318700 0:00:15.162230\n",
      "318800 0:00:15.163959\n",
      "318900 0:00:15.165814\n",
      "get 319000 0:00:15.167516\n",
      "319000 0:00:15.167536\n",
      "319100 0:00:15.169275\n",
      "319200 0:00:15.170895\n",
      "319300 0:00:15.172579\n",
      "319400 0:00:15.174351\n",
      "319500 0:00:15.176025\n",
      "319600 0:00:15.177864\n",
      "319700 0:00:15.179629\n",
      "319800 0:00:15.181242\n",
      "319900 0:00:15.183234\n",
      "get 320000 0:00:15.184807\n",
      "320000 0:00:15.184825\n",
      "320100 0:00:15.186433\n",
      "320200 0:00:15.188355\n",
      "320300 0:00:15.189940\n",
      "320400 0:00:15.191774\n",
      "320500 0:00:15.193350\n",
      "320600 0:00:15.194988\n",
      "320700 0:00:15.196877\n",
      "320800 0:00:15.198536\n",
      "320900 0:00:15.200904\n",
      "get 321000 0:00:15.202261\n",
      "321000 0:00:15.202280\n",
      "321100 0:00:15.203820\n",
      "321200 0:00:15.205567\n",
      "321300 0:00:15.207380\n",
      "321400 0:00:15.208844\n",
      "321500 0:00:15.210675\n",
      "321600 0:00:15.212400\n",
      "321700 0:00:15.214036\n",
      "321800 0:00:15.215952\n",
      "321900 0:00:15.217510\n",
      "get 322000 0:00:15.219608\n",
      "322000 0:00:15.219628\n",
      "322100 0:00:15.221075\n",
      "322200 0:00:15.222946\n",
      "322300 0:00:15.224895\n",
      "322400 0:00:15.226563\n",
      "322500 0:00:15.228158\n",
      "322600 0:00:15.229947\n",
      "322700 0:00:15.231867\n",
      "322800 0:00:15.233913\n",
      "322900 0:00:15.235402\n",
      "get 323000 0:00:15.236861\n",
      "323000 0:00:15.236879\n",
      "323100 0:00:15.238680\n",
      "323200 0:00:15.240620\n",
      "323300 0:00:15.242219\n",
      "323400 0:00:15.244078\n",
      "323500 0:00:15.245628\n",
      "323600 0:00:15.247213\n",
      "323700 0:00:15.249015\n",
      "323800 0:00:15.250935\n",
      "323900 0:00:15.252578\n",
      "get 324000 0:00:15.254572\n",
      "324000 0:00:15.254592\n",
      "324100 0:00:15.256361\n",
      "324200 0:00:15.258228\n",
      "324300 0:00:15.259764\n",
      "324400 0:00:15.261586\n",
      "324500 0:00:15.263239\n",
      "324600 0:00:15.264922\n",
      "324700 0:00:15.266699\n",
      "324800 0:00:15.268485\n",
      "324900 0:00:15.270692\n",
      "get 325000 0:00:15.272238\n",
      "325000 0:00:15.272257\n",
      "325100 0:00:15.274170\n",
      "325200 0:00:15.275877\n",
      "325300 0:00:15.277408\n",
      "325400 0:00:15.279198\n",
      "325500 0:00:15.281092\n",
      "325600 0:00:15.282786\n",
      "325700 0:00:15.284468\n",
      "325800 0:00:15.286176\n",
      "325900 0:00:15.287960\n",
      "get 326000 0:00:15.289859\n",
      "326000 0:00:15.289927\n",
      "326100 0:00:15.291713\n",
      "326200 0:00:15.293392\n",
      "326300 0:00:15.295189\n",
      "326400 0:00:15.296988\n",
      "326500 0:00:15.299350\n",
      "326600 0:00:15.300887\n",
      "326700 0:00:15.302727\n",
      "326800 0:00:15.304041\n",
      "326900 0:00:15.305592\n",
      "get 327000 0:00:15.307196\n",
      "327000 0:00:15.307215\n",
      "327100 0:00:15.308909\n",
      "327200 0:00:15.310559\n",
      "327300 0:00:15.312423\n",
      "327400 0:00:15.314180\n",
      "327500 0:00:15.315829\n",
      "327600 0:00:15.317686\n",
      "327700 0:00:15.319416\n",
      "327800 0:00:15.321086\n",
      "327900 0:00:15.322922\n",
      "get 328000 0:00:15.324614\n",
      "328000 0:00:15.324689\n",
      "328100 0:00:15.326375\n",
      "328200 0:00:15.328066\n",
      "328300 0:00:15.329982\n",
      "328400 0:00:15.331634\n",
      "328500 0:00:15.333453\n",
      "328600 0:00:15.335121\n",
      "328700 0:00:15.337174\n",
      "328800 0:00:15.338949\n",
      "328900 0:00:15.340480\n",
      "get 329000 0:00:15.341861\n",
      "329000 0:00:15.341878\n",
      "329100 0:00:15.343496\n",
      "329200 0:00:15.345422\n",
      "329300 0:00:15.347282\n",
      "329400 0:00:15.348869\n",
      "329500 0:00:15.350568\n",
      "329600 0:00:15.352289\n",
      "329700 0:00:15.354220\n",
      "329800 0:00:15.355787\n",
      "329900 0:00:15.357853\n",
      "get 330000 0:00:15.359704\n",
      "330000 0:00:15.359927\n",
      "330100 0:00:15.361492\n",
      "330200 0:00:15.362853\n",
      "330300 0:00:15.364511\n",
      "330400 0:00:15.365972\n",
      "330500 0:00:15.367816\n",
      "330600 0:00:15.369543\n",
      "330700 0:00:15.371220\n",
      "330800 0:00:15.373051\n",
      "330900 0:00:15.374813\n",
      "get 331000 0:00:15.376449\n",
      "331000 0:00:15.376467\n",
      "331100 0:00:15.378016\n",
      "331200 0:00:15.380103\n",
      "331300 0:00:15.382113\n",
      "331400 0:00:15.383285\n",
      "331500 0:00:15.385086\n",
      "331600 0:00:15.386697\n",
      "331700 0:00:15.388752\n",
      "331800 0:00:15.390537\n",
      "331900 0:00:15.392147\n",
      "get 332000 0:00:15.393912\n",
      "332000 0:00:15.393961\n",
      "332100 0:00:15.395677\n",
      "332200 0:00:15.397280\n",
      "332300 0:00:15.399000\n",
      "332400 0:00:15.400839\n",
      "332500 0:00:15.402765\n",
      "332600 0:00:15.404626\n",
      "332700 0:00:15.406609\n",
      "332800 0:00:15.408894\n",
      "332900 0:00:15.411007\n",
      "get 333000 0:00:15.412978\n",
      "333000 0:00:15.413004\n",
      "333100 0:00:15.415101\n",
      "333200 0:00:15.417278\n",
      "333300 0:00:15.419507\n",
      "333400 0:00:15.421789\n",
      "333500 0:00:15.423931\n",
      "333600 0:00:15.426152\n",
      "333700 0:00:15.428376\n",
      "333800 0:00:15.430237\n",
      "333900 0:00:15.432318\n",
      "get 334000 0:00:15.434418\n",
      "334000 0:00:15.434438\n",
      "334100 0:00:15.436663\n",
      "334200 0:00:15.438908\n",
      "334300 0:00:15.440937\n",
      "334400 0:00:15.443206\n",
      "334500 0:00:15.445394\n",
      "334600 0:00:15.447611\n",
      "334700 0:00:15.449886\n",
      "334800 0:00:15.451949\n",
      "334900 0:00:15.453951\n",
      "get 335000 0:00:15.456030\n",
      "335000 0:00:15.456049\n",
      "335100 0:00:15.458121\n",
      "335200 0:00:15.460200\n",
      "335300 0:00:15.462333\n",
      "335400 0:00:15.464449\n",
      "335500 0:00:15.466728\n",
      "335600 0:00:15.468781\n",
      "335700 0:00:15.470903\n",
      "335800 0:00:15.472976\n",
      "335900 0:00:15.475285\n",
      "get 336000 0:00:15.477518\n",
      "336000 0:00:15.477536\n",
      "336100 0:00:15.479718\n",
      "336200 0:00:15.481711\n",
      "336300 0:00:15.484104\n",
      "336400 0:00:15.486177\n",
      "336500 0:00:15.488487\n",
      "336600 0:00:15.490721\n",
      "336700 0:00:15.492923\n",
      "336800 0:00:15.495117\n",
      "336900 0:00:15.497599\n",
      "get 337000 0:00:15.499818\n",
      "337000 0:00:15.499836\n",
      "337100 0:00:15.501759\n",
      "337200 0:00:15.504006\n",
      "337300 0:00:15.506724\n",
      "337400 0:00:15.508789\n",
      "337500 0:00:15.511148\n",
      "337600 0:00:15.513607\n",
      "337700 0:00:15.516107\n",
      "337800 0:00:15.518573\n",
      "337900 0:00:15.520996\n",
      "get 338000 0:00:15.523438\n",
      "338000 0:00:15.523457\n",
      "338100 0:00:15.525943\n",
      "338200 0:00:15.528473\n",
      "338300 0:00:15.530812\n",
      "338400 0:00:15.533242\n",
      "338500 0:00:15.535783\n",
      "338600 0:00:15.538260\n",
      "338700 0:00:15.540776\n",
      "338800 0:00:15.543362\n",
      "338900 0:00:15.545693\n",
      "get 339000 0:00:15.548379\n",
      "339000 0:00:15.548397\n",
      "339100 0:00:15.550827\n",
      "339200 0:00:15.553323\n",
      "339300 0:00:15.555628\n",
      "339400 0:00:15.558094\n",
      "339500 0:00:15.560374\n",
      "339600 0:00:15.563025\n",
      "339700 0:00:15.565750\n",
      "339800 0:00:15.567645\n",
      "339900 0:00:15.570056\n",
      "get 340000 0:00:15.572432\n",
      "340000 0:00:15.572475\n",
      "340100 0:00:15.575163\n",
      "340200 0:00:15.577741\n",
      "340300 0:00:15.579817\n",
      "340400 0:00:15.582006\n",
      "340500 0:00:15.584160\n",
      "340600 0:00:15.586626\n",
      "340700 0:00:15.589260\n",
      "340800 0:00:15.591874\n",
      "340900 0:00:15.594178\n",
      "get 341000 0:00:15.596628\n",
      "341000 0:00:15.596669\n",
      "341100 0:00:15.599230\n",
      "341200 0:00:15.601649\n",
      "341300 0:00:15.604006\n",
      "341400 0:00:15.606630\n",
      "341500 0:00:15.608937\n",
      "341600 0:00:15.611630\n",
      "341700 0:00:15.613082\n",
      "341800 0:00:15.615543\n",
      "341900 0:00:15.618124\n",
      "get 342000 0:00:15.620696\n",
      "342000 0:00:15.620802\n",
      "342100 0:00:15.623009\n",
      "342200 0:00:15.625445\n",
      "342300 0:00:15.628112\n",
      "342400 0:00:15.630506\n",
      "342500 0:00:15.632728\n",
      "342600 0:00:15.635269\n",
      "342700 0:00:15.637968\n",
      "342800 0:00:15.640437\n",
      "342900 0:00:15.642783\n",
      "get 343000 0:00:15.645510\n",
      "343000 0:00:15.645627\n",
      "343100 0:00:15.647988\n",
      "343200 0:00:15.650293\n",
      "343300 0:00:15.652501\n",
      "343400 0:00:15.654930\n",
      "343500 0:00:15.657250\n",
      "343600 0:00:15.659801\n",
      "343700 0:00:15.662687\n",
      "343800 0:00:15.664624\n",
      "343900 0:00:15.666869\n",
      "get 344000 0:00:15.669222\n",
      "344000 0:00:15.669240\n",
      "344100 0:00:15.671788\n",
      "344200 0:00:15.674214\n",
      "344300 0:00:15.676844\n",
      "344400 0:00:15.679062\n",
      "344500 0:00:15.681587\n",
      "344600 0:00:15.683949\n",
      "344700 0:00:15.686523\n",
      "344800 0:00:15.689122\n",
      "344900 0:00:15.691639\n",
      "get 345000 0:00:15.694152\n",
      "345000 0:00:15.694226\n",
      "345100 0:00:15.696828\n",
      "345200 0:00:15.699066\n",
      "345300 0:00:15.701362\n",
      "345400 0:00:15.703799\n",
      "345500 0:00:15.706326\n",
      "345600 0:00:15.708927\n",
      "345700 0:00:15.711121\n",
      "345800 0:00:15.713624\n",
      "345900 0:00:15.716061\n",
      "get 346000 0:00:15.718486\n",
      "346000 0:00:15.718506\n",
      "346100 0:00:15.720771\n",
      "346200 0:00:15.723198\n",
      "346300 0:00:15.725378\n",
      "346400 0:00:15.727450\n",
      "346500 0:00:15.729864\n",
      "346600 0:00:15.732284\n",
      "346700 0:00:15.734758\n",
      "346800 0:00:15.737106\n",
      "346900 0:00:15.739324\n",
      "get 347000 0:00:15.741517\n",
      "347000 0:00:15.741548\n",
      "347100 0:00:15.744503\n",
      "347200 0:00:15.746357\n",
      "347300 0:00:15.748771\n",
      "347400 0:00:15.751487\n",
      "347500 0:00:15.753916\n",
      "347600 0:00:15.756395\n",
      "347700 0:00:15.758892\n",
      "347800 0:00:15.761149\n",
      "347900 0:00:15.763646\n",
      "get 348000 0:00:15.766117\n",
      "348000 0:00:15.766137\n",
      "348100 0:00:15.768632\n",
      "348200 0:00:15.771363\n",
      "348300 0:00:15.773606\n",
      "348400 0:00:15.776146\n",
      "348500 0:00:15.778925\n",
      "348600 0:00:15.780754\n",
      "348700 0:00:15.783238\n",
      "348800 0:00:15.785671\n",
      "348900 0:00:15.788283\n",
      "get 349000 0:00:15.790643\n",
      "349000 0:00:15.790663\n",
      "349100 0:00:15.792719\n",
      "349200 0:00:15.795296\n",
      "349300 0:00:15.797658\n",
      "349400 0:00:15.800267\n",
      "349500 0:00:15.802763\n",
      "349600 0:00:15.805036\n",
      "349700 0:00:15.807600\n",
      "349800 0:00:15.810037\n",
      "349900 0:00:15.812630\n",
      "get 350000 0:00:15.815066\n",
      "350000 0:00:15.815135\n",
      "350100 0:00:15.817611\n",
      "350200 0:00:15.820169\n",
      "350300 0:00:15.822509\n",
      "350400 0:00:15.824964\n",
      "350500 0:00:15.827230\n",
      "350600 0:00:15.829502\n",
      "350700 0:00:15.832144\n",
      "350800 0:00:15.834552\n",
      "350900 0:00:15.836474\n",
      "get 351000 0:00:15.838730\n",
      "351000 0:00:15.838778\n",
      "351100 0:00:15.841207\n",
      "351200 0:00:15.843964\n",
      "351300 0:00:15.846406\n",
      "351400 0:00:15.848122\n",
      "351500 0:00:15.850146\n",
      "351600 0:00:15.852249\n",
      "351700 0:00:15.854729\n",
      "351800 0:00:15.857121\n",
      "351900 0:00:15.859299\n",
      "get 352000 0:00:15.861547\n",
      "352000 0:00:15.861574\n",
      "352100 0:00:15.863875\n",
      "352200 0:00:15.866214\n",
      "352300 0:00:15.868397\n",
      "352400 0:00:15.870650\n",
      "352500 0:00:15.872781\n",
      "352600 0:00:15.875179\n",
      "352700 0:00:15.877609\n",
      "352800 0:00:15.879548\n",
      "352900 0:00:15.882075\n",
      "get 353000 0:00:15.884177\n",
      "353000 0:00:15.884196\n",
      "353100 0:00:15.886223\n",
      "353200 0:00:15.888485\n",
      "353300 0:00:15.891036\n",
      "353400 0:00:15.893451\n",
      "353500 0:00:15.895752\n",
      "353600 0:00:15.898007\n",
      "353700 0:00:15.900205\n",
      "353800 0:00:15.902781\n",
      "353900 0:00:15.904535\n",
      "get 354000 0:00:15.906808\n",
      "354000 0:00:15.906828\n",
      "354100 0:00:15.909004\n",
      "354200 0:00:15.911365\n",
      "354300 0:00:15.913622\n",
      "354400 0:00:15.915735\n",
      "354500 0:00:15.917894\n",
      "354600 0:00:15.920127\n",
      "354700 0:00:15.922498\n",
      "354800 0:00:15.924662\n",
      "354900 0:00:15.926963\n",
      "get 355000 0:00:15.929382\n",
      "355000 0:00:15.929532\n",
      "355100 0:00:15.931647\n",
      "355200 0:00:15.933886\n",
      "355300 0:00:15.936346\n",
      "355400 0:00:15.938670\n",
      "355500 0:00:15.940982\n",
      "355600 0:00:15.943331\n",
      "355700 0:00:15.945509\n",
      "355800 0:00:15.947597\n",
      "355900 0:00:15.949524\n",
      "get 356000 0:00:15.951170\n",
      "356000 0:00:15.951189\n",
      "356100 0:00:15.952666\n",
      "356200 0:00:15.954176\n",
      "356300 0:00:15.955880\n",
      "356400 0:00:15.957569\n",
      "356500 0:00:15.959347\n",
      "356600 0:00:15.961163\n",
      "356700 0:00:15.963194\n",
      "356800 0:00:15.964836\n",
      "356900 0:00:15.967072\n",
      "get 357000 0:00:15.968939\n",
      "357000 0:00:15.969016\n",
      "357100 0:00:15.971143\n",
      "357200 0:00:15.973292\n",
      "357300 0:00:15.975675\n",
      "357400 0:00:15.977897\n",
      "357500 0:00:15.980554\n",
      "357600 0:00:15.982623\n",
      "357700 0:00:15.984360\n",
      "357800 0:00:15.986570\n",
      "357900 0:00:15.988639\n",
      "get 358000 0:00:15.990812\n",
      "358000 0:00:15.990831\n",
      "358100 0:00:15.993060\n",
      "358200 0:00:15.995106\n",
      "358300 0:00:15.997341\n",
      "358400 0:00:15.999380\n",
      "358500 0:00:16.001804\n",
      "358600 0:00:16.003650\n",
      "358700 0:00:16.005871\n",
      "358800 0:00:16.007918\n",
      "358900 0:00:16.009951\n",
      "get 359000 0:00:16.012304\n",
      "359000 0:00:16.012323\n",
      "359100 0:00:16.013957\n",
      "359200 0:00:16.016129\n",
      "359300 0:00:16.018185\n",
      "359400 0:00:16.020077\n",
      "359500 0:00:16.021782\n",
      "359600 0:00:16.023573\n",
      "359700 0:00:16.025175\n",
      "359800 0:00:16.027066\n",
      "359900 0:00:16.028640\n",
      "get 360000 0:00:16.030231\n",
      "360000 0:00:16.030251\n",
      "360100 0:00:16.032027\n",
      "360200 0:00:16.033546\n",
      "360300 0:00:16.035255\n",
      "360400 0:00:16.037185\n",
      "360500 0:00:16.038919\n",
      "360600 0:00:16.040571\n",
      "360700 0:00:16.042250\n",
      "360800 0:00:16.043844\n",
      "360900 0:00:16.045580\n",
      "get 361000 0:00:16.047609\n",
      "361000 0:00:16.047649\n",
      "361100 0:00:16.049022\n",
      "361200 0:00:16.050887\n",
      "361300 0:00:16.052539\n",
      "361400 0:00:16.054152\n",
      "361500 0:00:16.055854\n",
      "361600 0:00:16.058032\n",
      "361700 0:00:16.059474\n",
      "361800 0:00:16.061493\n",
      "361900 0:00:16.063228\n",
      "get 362000 0:00:16.064931\n",
      "362000 0:00:16.064950\n",
      "362100 0:00:16.066409\n",
      "362200 0:00:16.067984\n",
      "362300 0:00:16.069313\n",
      "362400 0:00:16.071087\n",
      "362500 0:00:16.072937\n",
      "362600 0:00:16.074542\n",
      "362700 0:00:16.076409\n",
      "362800 0:00:16.077932\n",
      "362900 0:00:16.079679\n",
      "get 363000 0:00:16.081242\n",
      "363000 0:00:16.081263\n",
      "363100 0:00:16.082719\n",
      "363200 0:00:16.084429\n",
      "363300 0:00:16.086714\n",
      "363400 0:00:16.088625\n",
      "363500 0:00:16.090225\n",
      "363600 0:00:16.091882\n",
      "363700 0:00:16.093296\n",
      "363800 0:00:16.094723\n",
      "363900 0:00:16.096716\n",
      "get 364000 0:00:16.098653\n",
      "364000 0:00:16.098674\n",
      "364100 0:00:16.100121\n",
      "364200 0:00:16.101797\n",
      "364300 0:00:16.103801\n",
      "364400 0:00:16.105367\n",
      "364500 0:00:16.106986\n",
      "364600 0:00:16.108732\n",
      "364700 0:00:16.110179\n",
      "364800 0:00:16.112088\n",
      "364900 0:00:16.113663\n",
      "get 365000 0:00:16.115476\n",
      "365000 0:00:16.115494\n",
      "365100 0:00:16.117521\n",
      "365200 0:00:16.119516\n",
      "365300 0:00:16.121103\n",
      "365400 0:00:16.122987\n",
      "365500 0:00:16.124636\n",
      "365600 0:00:16.126462\n",
      "365700 0:00:16.127905\n",
      "365800 0:00:16.129428\n",
      "365900 0:00:16.131022\n",
      "get 366000 0:00:16.132444\n",
      "366000 0:00:16.132465\n",
      "366100 0:00:16.134242\n",
      "366200 0:00:16.136007\n",
      "366300 0:00:16.137861\n",
      "366400 0:00:16.139879\n",
      "366500 0:00:16.141412\n",
      "366600 0:00:16.142841\n",
      "366700 0:00:16.144562\n",
      "366800 0:00:16.146256\n",
      "366900 0:00:16.148119\n",
      "get 367000 0:00:16.150090\n",
      "367000 0:00:16.150109\n",
      "367100 0:00:16.151427\n",
      "367200 0:00:16.153213\n",
      "367300 0:00:16.154873\n",
      "367400 0:00:16.156566\n",
      "367500 0:00:16.158366\n",
      "367600 0:00:16.160515\n",
      "367700 0:00:16.162048\n",
      "367800 0:00:16.163454\n",
      "367900 0:00:16.165091\n",
      "get 368000 0:00:16.166603\n",
      "368000 0:00:16.166621\n",
      "368100 0:00:16.168333\n",
      "368200 0:00:16.170156\n",
      "368300 0:00:16.171761\n",
      "368400 0:00:16.173505\n",
      "368500 0:00:16.175251\n",
      "368600 0:00:16.176909\n",
      "368700 0:00:16.178891\n",
      "368800 0:00:16.180617\n",
      "368900 0:00:16.182033\n",
      "get 369000 0:00:16.183728\n",
      "369000 0:00:16.183746\n",
      "369100 0:00:16.185357\n",
      "369200 0:00:16.187266\n",
      "369300 0:00:16.189180\n",
      "369400 0:00:16.190612\n",
      "369500 0:00:16.192363\n",
      "369600 0:00:16.193926\n",
      "369700 0:00:16.195791\n",
      "369800 0:00:16.197413\n",
      "369900 0:00:16.199045\n",
      "get 370000 0:00:16.200831\n",
      "370000 0:00:16.200853\n",
      "370100 0:00:16.202549\n",
      "370200 0:00:16.204594\n",
      "370300 0:00:16.206642\n",
      "370400 0:00:16.208696\n",
      "370500 0:00:16.211369\n",
      "370600 0:00:16.213445\n",
      "370700 0:00:16.215244\n",
      "370800 0:00:16.217050\n",
      "370900 0:00:16.219428\n",
      "get 371000 0:00:16.221381\n",
      "371000 0:00:16.221403\n",
      "371100 0:00:16.223632\n",
      "371200 0:00:16.225936\n",
      "371300 0:00:16.228490\n",
      "371400 0:00:16.230759\n",
      "371500 0:00:16.233088\n",
      "371600 0:00:16.235178\n",
      "371700 0:00:16.237009\n",
      "371800 0:00:16.238776\n",
      "371900 0:00:16.240527\n",
      "get 372000 0:00:16.242295\n",
      "372000 0:00:16.242335\n",
      "372100 0:00:16.244227\n",
      "372200 0:00:16.246154\n",
      "372300 0:00:16.247953\n",
      "372400 0:00:16.249899\n",
      "372500 0:00:16.251728\n",
      "372600 0:00:16.253640\n",
      "372700 0:00:16.255414\n",
      "372800 0:00:16.257393\n",
      "372900 0:00:16.259094\n",
      "get 373000 0:00:16.260812\n",
      "373000 0:00:16.260845\n",
      "373100 0:00:16.262654\n",
      "373200 0:00:16.264432\n",
      "373300 0:00:16.266103\n",
      "373400 0:00:16.268019\n",
      "373500 0:00:16.269969\n",
      "373600 0:00:16.271781\n",
      "373700 0:00:16.273604\n",
      "373800 0:00:16.275443\n",
      "373900 0:00:16.277264\n",
      "get 374000 0:00:16.279173\n",
      "374000 0:00:16.279198\n",
      "374100 0:00:16.280997\n",
      "374200 0:00:16.283073\n",
      "374300 0:00:16.284835\n",
      "374400 0:00:16.286148\n",
      "374500 0:00:16.287885\n",
      "374600 0:00:16.289288\n",
      "374700 0:00:16.290985\n",
      "374800 0:00:16.292908\n",
      "374900 0:00:16.294535\n",
      "get 375000 0:00:16.296246\n",
      "375000 0:00:16.296275\n",
      "375100 0:00:16.297683\n",
      "375200 0:00:16.299045\n",
      "375300 0:00:16.300421\n",
      "375400 0:00:16.302015\n",
      "375500 0:00:16.303525\n",
      "375600 0:00:16.305163\n",
      "375700 0:00:16.306850\n",
      "375800 0:00:16.308240\n",
      "375900 0:00:16.310017\n",
      "get 376000 0:00:16.311671\n",
      "376000 0:00:16.311702\n",
      "376100 0:00:16.313152\n",
      "376200 0:00:16.314747\n",
      "376300 0:00:16.316131\n",
      "376400 0:00:16.317673\n",
      "376500 0:00:16.319761\n",
      "376600 0:00:16.321346\n",
      "376700 0:00:16.322908\n",
      "376800 0:00:16.324416\n",
      "376900 0:00:16.326042\n",
      "get 377000 0:00:16.327650\n",
      "377000 0:00:16.327683\n",
      "377100 0:00:16.329273\n",
      "377200 0:00:16.331106\n",
      "377300 0:00:16.332822\n",
      "377400 0:00:16.334509\n",
      "377500 0:00:16.335978\n",
      "377600 0:00:16.337564\n",
      "377700 0:00:16.339022\n",
      "377800 0:00:16.340743\n",
      "377900 0:00:16.342293\n",
      "get 378000 0:00:16.343708\n",
      "378000 0:00:16.343733\n",
      "378100 0:00:16.345080\n",
      "378200 0:00:16.346572\n",
      "378300 0:00:16.348381\n",
      "378400 0:00:16.350126\n",
      "378500 0:00:16.351403\n",
      "378600 0:00:16.352747\n",
      "378700 0:00:16.354441\n",
      "378800 0:00:16.356098\n",
      "378900 0:00:16.357717\n",
      "get 379000 0:00:16.359234\n",
      "379000 0:00:16.359258\n",
      "379100 0:00:16.361185\n",
      "379200 0:00:16.362625\n",
      "379300 0:00:16.364169\n",
      "379400 0:00:16.365930\n",
      "379500 0:00:16.367371\n",
      "379600 0:00:16.369205\n",
      "379700 0:00:16.370721\n",
      "379800 0:00:16.372178\n",
      "379900 0:00:16.373783\n",
      "get 380000 0:00:16.375269\n",
      "380000 0:00:16.375294\n",
      "380100 0:00:16.376911\n",
      "380200 0:00:16.378699\n",
      "380300 0:00:16.380100\n",
      "380400 0:00:16.381834\n",
      "380500 0:00:16.383876\n",
      "380600 0:00:16.385489\n",
      "380700 0:00:16.387079\n",
      "380800 0:00:16.388694\n",
      "380900 0:00:16.390414\n",
      "get 381000 0:00:16.391911\n",
      "381000 0:00:16.391949\n",
      "381100 0:00:16.393740\n",
      "381200 0:00:16.395521\n",
      "381300 0:00:16.397107\n",
      "381400 0:00:16.398671\n",
      "381500 0:00:16.400296\n",
      "381600 0:00:16.401762\n",
      "381700 0:00:16.403341\n",
      "381800 0:00:16.405155\n",
      "381900 0:00:16.407371\n",
      "get 382000 0:00:16.408789\n",
      "382000 0:00:16.408819\n",
      "382100 0:00:16.410435\n",
      "382200 0:00:16.412192\n",
      "382300 0:00:16.413670\n",
      "382400 0:00:16.415144\n",
      "382500 0:00:16.416902\n",
      "382600 0:00:16.418808\n",
      "382700 0:00:16.420533\n",
      "382800 0:00:16.422200\n",
      "382900 0:00:16.423794\n",
      "get 383000 0:00:16.425725\n",
      "383000 0:00:16.425748\n",
      "383100 0:00:16.427305\n",
      "383200 0:00:16.429121\n",
      "383300 0:00:16.430646\n",
      "383400 0:00:16.432155\n",
      "383500 0:00:16.433645\n",
      "383600 0:00:16.435249\n",
      "383700 0:00:16.436927\n",
      "383800 0:00:16.438767\n",
      "383900 0:00:16.440455\n",
      "get 384000 0:00:16.441994\n",
      "384000 0:00:16.442018\n",
      "384100 0:00:16.443548\n",
      "384200 0:00:16.445493\n",
      "384300 0:00:16.447034\n",
      "384400 0:00:16.448593\n",
      "384500 0:00:16.450854\n",
      "384600 0:00:16.453141\n",
      "384700 0:00:16.455011\n",
      "384800 0:00:16.456565\n",
      "384900 0:00:16.458108\n",
      "get 385000 0:00:16.460030\n",
      "385000 0:00:16.460053\n",
      "385100 0:00:16.461518\n",
      "385200 0:00:16.463005\n",
      "385300 0:00:16.464307\n",
      "385400 0:00:16.465563\n",
      "385500 0:00:16.467128\n",
      "385600 0:00:16.468347\n",
      "385700 0:00:16.469613\n",
      "385800 0:00:16.471135\n",
      "385900 0:00:16.472218\n",
      "get 386000 0:00:16.473668\n",
      "386000 0:00:16.473691\n",
      "386100 0:00:16.475628\n",
      "386200 0:00:16.477466\n",
      "386300 0:00:16.479280\n",
      "386400 0:00:16.481197\n",
      "386500 0:00:16.482829\n",
      "386600 0:00:16.484615\n",
      "386700 0:00:16.486102\n",
      "386800 0:00:16.487367\n",
      "386900 0:00:16.489008\n",
      "get 387000 0:00:16.490224\n",
      "387000 0:00:16.490258\n",
      "387100 0:00:16.492114\n",
      "387200 0:00:16.493599\n",
      "387300 0:00:16.495271\n",
      "387400 0:00:16.496858\n",
      "387500 0:00:16.498611\n",
      "387600 0:00:16.500093\n",
      "387700 0:00:16.501750\n",
      "387800 0:00:16.503345\n",
      "387900 0:00:16.505046\n",
      "get 388000 0:00:16.506649\n",
      "388000 0:00:16.506675\n",
      "388100 0:00:16.508441\n",
      "388200 0:00:16.509806\n",
      "388300 0:00:16.511715\n",
      "388400 0:00:16.512998\n",
      "388500 0:00:16.514341\n",
      "388600 0:00:16.516224\n",
      "388700 0:00:16.518507\n",
      "388800 0:00:16.520689\n",
      "388900 0:00:16.522323\n",
      "get 389000 0:00:16.524820\n",
      "389000 0:00:16.524848\n",
      "389100 0:00:16.526949\n",
      "389200 0:00:16.529052\n",
      "389300 0:00:16.530807\n",
      "389400 0:00:16.532771\n",
      "389500 0:00:16.534646\n",
      "389600 0:00:16.536569\n",
      "389700 0:00:16.538624\n",
      "389800 0:00:16.540905\n",
      "389900 0:00:16.543000\n",
      "get 390000 0:00:16.544606\n",
      "390000 0:00:16.544629\n",
      "390100 0:00:16.546503\n",
      "390200 0:00:16.547854\n",
      "390300 0:00:16.549678\n",
      "390400 0:00:16.551303\n",
      "390500 0:00:16.553270\n",
      "390600 0:00:16.555133\n",
      "390700 0:00:16.556643\n",
      "390800 0:00:16.557812\n",
      "390900 0:00:16.559382\n",
      "get 391000 0:00:16.561151\n",
      "391000 0:00:16.561175\n",
      "391100 0:00:16.562679\n",
      "391200 0:00:16.563977\n",
      "391300 0:00:16.566122\n",
      "391400 0:00:16.567765\n",
      "391500 0:00:16.569517\n",
      "391600 0:00:16.571206\n",
      "391700 0:00:16.572587\n",
      "391800 0:00:16.574637\n",
      "391900 0:00:16.575918\n",
      "get 392000 0:00:16.577482\n",
      "392000 0:00:16.577515\n",
      "392100 0:00:16.578888\n",
      "392200 0:00:16.580203\n",
      "392300 0:00:16.581880\n",
      "392400 0:00:16.583244\n",
      "392500 0:00:16.584597\n",
      "392600 0:00:16.586224\n",
      "392700 0:00:16.587920\n",
      "392800 0:00:16.589964\n",
      "392900 0:00:16.591233\n",
      "get 393000 0:00:16.592578\n",
      "393000 0:00:16.592614\n",
      "393100 0:00:16.594165\n",
      "393200 0:00:16.595453\n",
      "393300 0:00:16.596687\n",
      "393400 0:00:16.598300\n",
      "393500 0:00:16.599620\n",
      "393600 0:00:16.601297\n",
      "393700 0:00:16.602837\n",
      "393800 0:00:16.604474\n",
      "393900 0:00:16.606296\n",
      "get 394000 0:00:16.608458\n",
      "394000 0:00:16.608494\n",
      "394100 0:00:16.609748\n",
      "394200 0:00:16.611384\n",
      "394300 0:00:16.613134\n",
      "394400 0:00:16.615938\n",
      "394500 0:00:16.617680\n",
      "394600 0:00:16.619452\n",
      "394700 0:00:16.621206\n",
      "394800 0:00:16.622619\n",
      "394900 0:00:16.624016\n",
      "get 395000 0:00:16.625463\n",
      "395000 0:00:16.625485\n",
      "395100 0:00:16.627155\n",
      "395200 0:00:16.628525\n",
      "395300 0:00:16.629962\n",
      "395400 0:00:16.631791\n",
      "395500 0:00:16.633769\n",
      "395600 0:00:16.635349\n",
      "395700 0:00:16.636933\n",
      "395800 0:00:16.638319\n",
      "395900 0:00:16.639911\n",
      "get 396000 0:00:16.641219\n",
      "396000 0:00:16.641252\n",
      "396100 0:00:16.643133\n",
      "396200 0:00:16.644631\n",
      "396300 0:00:16.646058\n",
      "396400 0:00:16.647665\n",
      "396500 0:00:16.649745\n",
      "396600 0:00:16.651275\n",
      "396700 0:00:16.653000\n",
      "396800 0:00:16.654482\n",
      "396900 0:00:16.655879\n",
      "get 397000 0:00:16.657315\n",
      "397000 0:00:16.657336\n",
      "397100 0:00:16.658673\n",
      "397200 0:00:16.660131\n",
      "397300 0:00:16.662072\n",
      "397400 0:00:16.663429\n",
      "397500 0:00:16.664749\n",
      "397600 0:00:16.666299\n",
      "397700 0:00:16.668101\n",
      "397800 0:00:16.669624\n",
      "397900 0:00:16.671113\n",
      "get 398000 0:00:16.673134\n",
      "398000 0:00:16.673201\n",
      "398100 0:00:16.675769\n",
      "398200 0:00:16.677339\n",
      "398300 0:00:16.678843\n",
      "398400 0:00:16.680053\n",
      "398500 0:00:16.681922\n",
      "398600 0:00:16.683681\n",
      "398700 0:00:16.685065\n",
      "398800 0:00:16.686430\n",
      "398900 0:00:16.687745\n",
      "get 399000 0:00:16.689336\n",
      "399000 0:00:16.689356\n",
      "399100 0:00:16.691238\n",
      "399200 0:00:16.692771\n",
      "399300 0:00:16.694495\n",
      "399400 0:00:16.696117\n",
      "399500 0:00:16.697868\n",
      "399600 0:00:16.699637\n",
      "399700 0:00:16.701097\n",
      "399800 0:00:16.702781\n",
      "399900 0:00:16.704205\n",
      "get 400000 0:00:19.420385\n",
      "400000 0:00:19.420611\n",
      "400100 0:00:19.423022\n",
      "400200 0:00:19.424761\n",
      "400300 0:00:19.426385\n",
      "400400 0:00:19.428038\n",
      "400500 0:00:19.429786\n",
      "400600 0:00:19.431457\n",
      "400700 0:00:19.432770\n",
      "400800 0:00:19.433983\n",
      "400900 0:00:19.435979\n",
      "get 401000 0:00:19.437213\n",
      "401000 0:00:19.437234\n",
      "401100 0:00:19.438769\n",
      "401200 0:00:19.440562\n",
      "401300 0:00:19.442201\n",
      "401400 0:00:19.443543\n",
      "401500 0:00:19.445069\n",
      "401600 0:00:19.446650\n",
      "401700 0:00:19.447975\n",
      "401800 0:00:19.449790\n",
      "401900 0:00:19.451417\n",
      "get 402000 0:00:19.453101\n",
      "402000 0:00:19.453126\n",
      "402100 0:00:19.454744\n",
      "402200 0:00:19.456420\n",
      "402300 0:00:19.458042\n",
      "402400 0:00:19.459643\n",
      "402500 0:00:19.461269\n",
      "402600 0:00:19.462721\n",
      "402700 0:00:19.464426\n",
      "402800 0:00:19.466185\n",
      "402900 0:00:19.467666\n",
      "get 403000 0:00:19.469075\n",
      "403000 0:00:19.469099\n",
      "403100 0:00:19.471314\n",
      "403200 0:00:19.473392\n",
      "403300 0:00:19.475392\n",
      "403400 0:00:19.477458\n",
      "403500 0:00:19.479444\n",
      "403600 0:00:19.481226\n",
      "403700 0:00:19.483260\n",
      "403800 0:00:19.485230\n",
      "403900 0:00:19.487427\n",
      "get 404000 0:00:19.489531\n",
      "404000 0:00:19.489554\n",
      "404100 0:00:19.491459\n",
      "404200 0:00:19.493323\n",
      "404300 0:00:19.495297\n",
      "404400 0:00:19.497239\n",
      "404500 0:00:19.499296\n",
      "404600 0:00:19.501167\n",
      "404700 0:00:19.503222\n",
      "404800 0:00:19.505201\n",
      "404900 0:00:19.507199\n",
      "get 405000 0:00:19.509141\n",
      "405000 0:00:19.509168\n",
      "405100 0:00:19.511286\n",
      "405200 0:00:19.512922\n",
      "405300 0:00:19.514889\n",
      "405400 0:00:19.516743\n",
      "405500 0:00:19.518603\n",
      "405600 0:00:19.520674\n",
      "405700 0:00:19.522692\n",
      "405800 0:00:19.524468\n",
      "405900 0:00:19.526389\n",
      "get 406000 0:00:19.527945\n",
      "406000 0:00:19.527966\n",
      "406100 0:00:19.529621\n",
      "406200 0:00:19.531697\n",
      "406300 0:00:19.533653\n",
      "406400 0:00:19.535715\n",
      "406500 0:00:19.537855\n",
      "406600 0:00:19.539847\n",
      "406700 0:00:19.541628\n",
      "406800 0:00:19.543679\n",
      "406900 0:00:19.545768\n",
      "get 407000 0:00:19.547799\n",
      "407000 0:00:19.547823\n",
      "407100 0:00:19.549600\n",
      "407200 0:00:19.551628\n",
      "407300 0:00:19.553243\n",
      "407400 0:00:19.555158\n",
      "407500 0:00:19.557200\n",
      "407600 0:00:19.558853\n",
      "407700 0:00:19.560464\n",
      "407800 0:00:19.562240\n",
      "407900 0:00:19.563864\n",
      "get 408000 0:00:19.565426\n",
      "408000 0:00:19.565448\n",
      "408100 0:00:19.567657\n",
      "408200 0:00:19.569529\n",
      "408300 0:00:19.571107\n",
      "408400 0:00:19.573315\n",
      "408500 0:00:19.575534\n",
      "408600 0:00:19.577255\n",
      "408700 0:00:19.578948\n",
      "408800 0:00:19.581095\n",
      "408900 0:00:19.582915\n",
      "get 409000 0:00:19.584553\n",
      "409000 0:00:19.584575\n",
      "409100 0:00:19.586489\n",
      "409200 0:00:19.588365\n",
      "409300 0:00:19.590526\n",
      "409400 0:00:19.592560\n",
      "409500 0:00:19.594206\n",
      "409600 0:00:19.596009\n",
      "409700 0:00:19.597447\n",
      "409800 0:00:19.599044\n",
      "409900 0:00:19.600544\n",
      "get 410000 0:00:19.602135\n",
      "410000 0:00:19.602158\n",
      "410100 0:00:19.603606\n",
      "410200 0:00:19.605418\n",
      "410300 0:00:19.607276\n",
      "410400 0:00:19.609046\n",
      "410500 0:00:19.610927\n",
      "410600 0:00:19.612616\n",
      "410700 0:00:19.614452\n",
      "410800 0:00:19.616277\n",
      "410900 0:00:19.618032\n",
      "get 411000 0:00:19.619736\n",
      "411000 0:00:19.619773\n",
      "411100 0:00:19.622021\n",
      "411200 0:00:19.623805\n",
      "411300 0:00:19.625469\n",
      "411400 0:00:19.627129\n",
      "411500 0:00:19.629090\n",
      "411600 0:00:19.630771\n",
      "411700 0:00:19.632675\n",
      "411800 0:00:19.634563\n",
      "411900 0:00:19.636155\n",
      "get 412000 0:00:19.638217\n",
      "412000 0:00:19.638332\n",
      "412100 0:00:19.640168\n",
      "412200 0:00:19.641927\n",
      "412300 0:00:19.643591\n",
      "412400 0:00:19.645288\n",
      "412500 0:00:19.647005\n",
      "412600 0:00:19.649029\n",
      "412700 0:00:19.650984\n",
      "412800 0:00:19.652808\n",
      "412900 0:00:19.655100\n",
      "get 413000 0:00:19.656845\n",
      "413000 0:00:19.656882\n",
      "413100 0:00:19.658615\n",
      "413200 0:00:19.660335\n",
      "413300 0:00:19.662113\n",
      "413400 0:00:19.663949\n",
      "413500 0:00:19.665897\n",
      "413600 0:00:19.667694\n",
      "413700 0:00:19.669485\n",
      "413800 0:00:19.671154\n",
      "413900 0:00:19.672698\n",
      "get 414000 0:00:19.674549\n",
      "414000 0:00:19.674584\n",
      "414100 0:00:19.676532\n",
      "414200 0:00:19.678406\n",
      "414300 0:00:19.680243\n",
      "414400 0:00:19.682039\n",
      "414500 0:00:19.683737\n",
      "414600 0:00:19.685359\n",
      "414700 0:00:19.687070\n",
      "414800 0:00:19.688995\n",
      "414900 0:00:19.691189\n",
      "get 415000 0:00:19.692830\n",
      "415000 0:00:19.692853\n",
      "415100 0:00:19.694316\n",
      "415200 0:00:19.696264\n",
      "415300 0:00:19.698053\n",
      "415400 0:00:19.700075\n",
      "415500 0:00:19.701883\n",
      "415600 0:00:19.703452\n",
      "415700 0:00:19.705457\n",
      "415800 0:00:19.707465\n",
      "415900 0:00:19.709460\n",
      "get 416000 0:00:19.711132\n",
      "416000 0:00:19.711156\n",
      "416100 0:00:19.713049\n",
      "416200 0:00:19.714776\n",
      "416300 0:00:19.716360\n",
      "416400 0:00:19.717859\n",
      "416500 0:00:19.719700\n",
      "416600 0:00:19.721374\n",
      "416700 0:00:19.723218\n",
      "416800 0:00:19.724996\n",
      "416900 0:00:19.726899\n",
      "get 417000 0:00:19.728755\n",
      "417000 0:00:19.728778\n",
      "417100 0:00:19.730599\n",
      "417200 0:00:19.732790\n",
      "417300 0:00:19.734642\n",
      "417400 0:00:19.736252\n",
      "417500 0:00:19.737806\n",
      "417600 0:00:19.739353\n",
      "417700 0:00:19.740872\n",
      "417800 0:00:19.742846\n",
      "417900 0:00:19.744813\n",
      "get 418000 0:00:19.746658\n",
      "418000 0:00:19.746684\n",
      "418100 0:00:19.748406\n",
      "418200 0:00:19.750463\n",
      "418300 0:00:19.752577\n",
      "418400 0:00:19.754530\n",
      "418500 0:00:19.756322\n",
      "418600 0:00:19.758021\n",
      "418700 0:00:19.760010\n",
      "418800 0:00:19.761788\n",
      "418900 0:00:19.763490\n",
      "get 419000 0:00:19.765112\n",
      "419000 0:00:19.765140\n",
      "419100 0:00:19.766979\n",
      "419200 0:00:19.768791\n",
      "419300 0:00:19.770518\n",
      "419400 0:00:19.772130\n",
      "419500 0:00:19.773791\n",
      "419600 0:00:19.775943\n",
      "419700 0:00:19.777997\n",
      "419800 0:00:19.779692\n",
      "419900 0:00:19.781417\n",
      "get 420000 0:00:19.783140\n",
      "420000 0:00:19.783178\n",
      "420100 0:00:19.784933\n",
      "420200 0:00:19.786428\n",
      "420300 0:00:19.788040\n",
      "420400 0:00:19.789460\n",
      "420500 0:00:19.791435\n",
      "420600 0:00:19.793086\n",
      "420700 0:00:19.794635\n",
      "420800 0:00:19.796264\n",
      "420900 0:00:19.798164\n",
      "get 421000 0:00:19.800074\n",
      "421000 0:00:19.800099\n",
      "421100 0:00:19.801950\n",
      "421200 0:00:19.804003\n",
      "421300 0:00:19.805728\n",
      "421400 0:00:19.807932\n",
      "421500 0:00:19.810215\n",
      "421600 0:00:19.811518\n",
      "421700 0:00:19.813513\n",
      "421800 0:00:19.815885\n",
      "421900 0:00:19.817813\n",
      "get 422000 0:00:19.819446\n",
      "422000 0:00:19.819473\n",
      "422100 0:00:19.821602\n",
      "422200 0:00:19.824060\n",
      "422300 0:00:19.825593\n",
      "422400 0:00:19.827520\n",
      "422500 0:00:19.829530\n",
      "422600 0:00:19.831031\n",
      "422700 0:00:19.832940\n",
      "422800 0:00:19.834409\n",
      "422900 0:00:19.836142\n",
      "get 423000 0:00:19.838029\n",
      "423000 0:00:19.838061\n",
      "423100 0:00:19.840475\n",
      "423200 0:00:19.843094\n",
      "423300 0:00:19.844982\n",
      "423400 0:00:19.846937\n",
      "423500 0:00:19.848327\n",
      "423600 0:00:19.850044\n",
      "423700 0:00:19.851514\n",
      "423800 0:00:19.853158\n",
      "423900 0:00:19.854800\n",
      "get 424000 0:00:19.856135\n",
      "424000 0:00:19.856157\n",
      "424100 0:00:19.857996\n",
      "424200 0:00:19.860245\n",
      "424300 0:00:19.862165\n",
      "424400 0:00:19.863977\n",
      "424500 0:00:19.866520\n",
      "424600 0:00:19.868484\n",
      "424700 0:00:19.870529\n",
      "424800 0:00:19.872419\n",
      "424900 0:00:19.874132\n",
      "get 425000 0:00:19.875655\n",
      "425000 0:00:19.875679\n",
      "425100 0:00:19.877526\n",
      "425200 0:00:19.879451\n",
      "425300 0:00:19.881721\n",
      "425400 0:00:19.883909\n",
      "425500 0:00:19.885235\n",
      "425600 0:00:19.886955\n",
      "425700 0:00:19.889169\n",
      "425800 0:00:19.891221\n",
      "425900 0:00:19.893367\n",
      "get 426000 0:00:19.895402\n",
      "426000 0:00:19.895425\n",
      "426100 0:00:19.897488\n",
      "426200 0:00:19.899608\n",
      "426300 0:00:19.901515\n",
      "426400 0:00:19.903255\n",
      "426500 0:00:19.904968\n",
      "426600 0:00:19.907132\n",
      "426700 0:00:19.908933\n",
      "426800 0:00:19.910506\n",
      "426900 0:00:19.912191\n",
      "get 427000 0:00:19.913970\n",
      "427000 0:00:19.914035\n",
      "427100 0:00:19.916114\n",
      "427200 0:00:19.917964\n",
      "427300 0:00:19.919748\n",
      "427400 0:00:19.921631\n",
      "427500 0:00:19.923661\n",
      "427600 0:00:19.926066\n",
      "427700 0:00:19.927980\n",
      "427800 0:00:19.930267\n",
      "427900 0:00:19.931725\n",
      "get 428000 0:00:19.933500\n",
      "428000 0:00:19.933527\n",
      "428100 0:00:19.935382\n",
      "428200 0:00:19.937370\n",
      "428300 0:00:19.939289\n",
      "428400 0:00:19.941104\n",
      "428500 0:00:19.943184\n",
      "428600 0:00:19.945134\n",
      "428700 0:00:19.946925\n",
      "428800 0:00:19.948773\n",
      "428900 0:00:19.950761\n",
      "get 429000 0:00:19.953052\n",
      "429000 0:00:19.953075\n",
      "429100 0:00:19.955116\n",
      "429200 0:00:19.957044\n",
      "429300 0:00:19.958761\n",
      "429400 0:00:19.960869\n",
      "429500 0:00:19.962699\n",
      "429600 0:00:19.964782\n",
      "429700 0:00:19.966393\n",
      "429800 0:00:19.967953\n",
      "429900 0:00:19.969580\n",
      "get 430000 0:00:19.971335\n",
      "430000 0:00:19.971367\n",
      "430100 0:00:19.973025\n",
      "430200 0:00:19.974426\n",
      "430300 0:00:19.975746\n",
      "430400 0:00:19.977460\n",
      "430500 0:00:19.979167\n",
      "430600 0:00:19.981007\n",
      "430700 0:00:19.982888\n",
      "430800 0:00:19.984535\n",
      "430900 0:00:19.986004\n",
      "get 431000 0:00:19.987763\n",
      "431000 0:00:19.987798\n",
      "431100 0:00:19.989596\n",
      "431200 0:00:19.991507\n",
      "431300 0:00:19.993254\n",
      "431400 0:00:19.995058\n",
      "431500 0:00:19.996724\n",
      "431600 0:00:19.998552\n",
      "431700 0:00:20.000260\n",
      "431800 0:00:20.002030\n",
      "431900 0:00:20.003934\n",
      "get 432000 0:00:20.005876\n",
      "432000 0:00:20.005917\n",
      "432100 0:00:20.007596\n",
      "432200 0:00:20.009224\n",
      "432300 0:00:20.011060\n",
      "432400 0:00:20.012683\n",
      "432500 0:00:20.014368\n",
      "432600 0:00:20.016103\n",
      "432700 0:00:20.017899\n",
      "432800 0:00:20.019771\n",
      "432900 0:00:20.021537\n",
      "get 433000 0:00:20.023647\n",
      "433000 0:00:20.023672\n",
      "433100 0:00:20.025514\n",
      "433200 0:00:20.027507\n",
      "433300 0:00:20.029078\n",
      "433400 0:00:20.031005\n",
      "433500 0:00:20.032965\n",
      "433600 0:00:20.034885\n",
      "433700 0:00:20.036654\n",
      "433800 0:00:20.038525\n",
      "433900 0:00:20.040520\n",
      "get 434000 0:00:20.042360\n",
      "434000 0:00:20.042384\n",
      "434100 0:00:20.044496\n",
      "434200 0:00:20.045955\n",
      "434300 0:00:20.047722\n",
      "434400 0:00:20.050082\n",
      "434500 0:00:20.051846\n",
      "434600 0:00:20.053747\n",
      "434700 0:00:20.055472\n",
      "434800 0:00:20.057283\n",
      "434900 0:00:20.059376\n",
      "get 435000 0:00:20.061091\n",
      "435000 0:00:20.061128\n",
      "435100 0:00:20.062953\n",
      "435200 0:00:20.065030\n",
      "435300 0:00:20.066904\n",
      "435400 0:00:20.068561\n",
      "435500 0:00:20.070032\n",
      "435600 0:00:20.071963\n",
      "435700 0:00:20.073800\n",
      "435800 0:00:20.075268\n",
      "435900 0:00:20.077135\n",
      "get 436000 0:00:20.079003\n",
      "436000 0:00:20.079028\n",
      "436100 0:00:20.080640\n",
      "436200 0:00:20.082304\n",
      "436300 0:00:20.084226\n",
      "436400 0:00:20.086042\n",
      "436500 0:00:20.088181\n",
      "436600 0:00:20.090114\n",
      "436700 0:00:20.091672\n",
      "436800 0:00:20.093522\n",
      "436900 0:00:20.095269\n",
      "get 437000 0:00:20.097273\n",
      "437000 0:00:20.097353\n",
      "437100 0:00:20.099087\n",
      "437200 0:00:20.100783\n",
      "437300 0:00:20.102328\n",
      "437400 0:00:20.104093\n",
      "437500 0:00:20.105717\n",
      "437600 0:00:20.107297\n",
      "437700 0:00:20.108949\n",
      "437800 0:00:20.110973\n",
      "437900 0:00:20.113026\n",
      "get 438000 0:00:20.114613\n",
      "438000 0:00:20.114648\n",
      "438100 0:00:20.116279\n",
      "438200 0:00:20.117990\n",
      "438300 0:00:20.119535\n",
      "438400 0:00:20.121601\n",
      "438500 0:00:20.123403\n",
      "438600 0:00:20.125160\n",
      "438700 0:00:20.127372\n",
      "438800 0:00:20.129399\n",
      "438900 0:00:20.130884\n",
      "get 439000 0:00:20.133168\n",
      "439000 0:00:20.133192\n",
      "439100 0:00:20.135011\n",
      "439200 0:00:20.137098\n",
      "439300 0:00:20.138720\n",
      "439400 0:00:20.140765\n",
      "439500 0:00:20.142606\n",
      "439600 0:00:20.144379\n",
      "439700 0:00:20.145840\n",
      "439800 0:00:20.147478\n",
      "439900 0:00:20.149537\n",
      "get 440000 0:00:20.151615\n",
      "440000 0:00:20.151648\n",
      "440100 0:00:20.153577\n",
      "440200 0:00:20.155246\n",
      "440300 0:00:20.157104\n",
      "440400 0:00:20.159074\n",
      "440500 0:00:20.160897\n",
      "440600 0:00:20.162590\n",
      "440700 0:00:20.164112\n",
      "440800 0:00:20.166115\n",
      "440900 0:00:20.167882\n",
      "get 441000 0:00:20.169445\n",
      "441000 0:00:20.169468\n",
      "441100 0:00:20.171130\n",
      "441200 0:00:20.172969\n",
      "441300 0:00:20.174808\n",
      "441400 0:00:20.176653\n",
      "441500 0:00:20.178567\n",
      "441600 0:00:20.180135\n",
      "441700 0:00:20.182012\n",
      "441800 0:00:20.183651\n",
      "441900 0:00:20.185584\n",
      "get 442000 0:00:20.187442\n",
      "442000 0:00:20.187480\n",
      "442100 0:00:20.189206\n",
      "442200 0:00:20.190978\n",
      "442300 0:00:20.192482\n",
      "442400 0:00:20.194077\n",
      "442500 0:00:20.195525\n",
      "442600 0:00:20.197173\n",
      "442700 0:00:20.198881\n",
      "442800 0:00:20.200384\n",
      "442900 0:00:20.202399\n",
      "get 443000 0:00:20.203987\n",
      "443000 0:00:20.204022\n",
      "443100 0:00:20.205502\n",
      "443200 0:00:20.207153\n",
      "443300 0:00:20.208894\n",
      "443400 0:00:20.210627\n",
      "443500 0:00:20.212123\n",
      "443600 0:00:20.213595\n",
      "443700 0:00:20.215249\n",
      "443800 0:00:20.217033\n",
      "443900 0:00:20.218940\n",
      "get 444000 0:00:20.220391\n",
      "444000 0:00:20.220467\n",
      "444100 0:00:20.221758\n",
      "444200 0:00:20.223576\n",
      "444300 0:00:20.225518\n",
      "444400 0:00:20.227319\n",
      "444500 0:00:20.229052\n",
      "444600 0:00:20.230919\n",
      "444700 0:00:20.232278\n",
      "444800 0:00:20.233991\n",
      "444900 0:00:20.235576\n",
      "get 445000 0:00:20.237212\n",
      "445000 0:00:20.237235\n",
      "445100 0:00:20.238726\n",
      "445200 0:00:20.240188\n",
      "445300 0:00:20.241939\n",
      "445400 0:00:20.243496\n",
      "445500 0:00:20.245083\n",
      "445600 0:00:20.246618\n",
      "445700 0:00:20.248756\n",
      "445800 0:00:20.250727\n",
      "445900 0:00:20.252424\n",
      "get 446000 0:00:20.254042\n",
      "446000 0:00:20.254065\n",
      "446100 0:00:20.255263\n",
      "446200 0:00:20.256888\n",
      "446300 0:00:20.258195\n",
      "446400 0:00:20.259926\n",
      "446500 0:00:20.261546\n",
      "446600 0:00:20.263359\n",
      "446700 0:00:20.264865\n",
      "446800 0:00:20.266444\n",
      "446900 0:00:20.267599\n",
      "get 447000 0:00:20.268960\n",
      "447000 0:00:20.268980\n",
      "447100 0:00:20.270739\n",
      "447200 0:00:20.272240\n",
      "447300 0:00:20.273866\n",
      "447400 0:00:20.275215\n",
      "447500 0:00:20.276522\n",
      "447600 0:00:20.277874\n",
      "447700 0:00:20.279434\n",
      "447800 0:00:20.281094\n",
      "447900 0:00:20.282901\n",
      "get 448000 0:00:20.284484\n",
      "448000 0:00:20.284505\n",
      "448100 0:00:20.286046\n",
      "448200 0:00:20.287971\n",
      "448300 0:00:20.289544\n",
      "448400 0:00:20.291313\n",
      "448500 0:00:20.293093\n",
      "448600 0:00:20.295335\n",
      "448700 0:00:20.297279\n",
      "448800 0:00:20.299199\n",
      "448900 0:00:20.301046\n",
      "get 449000 0:00:20.302804\n",
      "449000 0:00:20.302826\n",
      "449100 0:00:20.304405\n",
      "449200 0:00:20.306301\n",
      "449300 0:00:20.307968\n",
      "449400 0:00:20.309924\n",
      "449500 0:00:20.311595\n",
      "449600 0:00:20.313476\n",
      "449700 0:00:20.315227\n",
      "449800 0:00:20.317218\n",
      "449900 0:00:20.319077\n",
      "get 450000 0:00:20.321202\n",
      "450000 0:00:20.321235\n",
      "450100 0:00:20.322776\n",
      "450200 0:00:20.324590\n",
      "450300 0:00:20.326247\n",
      "450400 0:00:20.328038\n",
      "450500 0:00:20.329617\n",
      "450600 0:00:20.331435\n",
      "450700 0:00:20.333297\n",
      "450800 0:00:20.335097\n",
      "450900 0:00:20.336755\n",
      "get 451000 0:00:20.338276\n",
      "451000 0:00:20.338298\n",
      "451100 0:00:20.340228\n",
      "451200 0:00:20.342062\n",
      "451300 0:00:20.343671\n",
      "451400 0:00:20.345274\n",
      "451500 0:00:20.346807\n",
      "451600 0:00:20.348647\n",
      "451700 0:00:20.350059\n",
      "451800 0:00:20.351804\n",
      "451900 0:00:20.353183\n",
      "get 452000 0:00:20.354625\n",
      "452000 0:00:20.354652\n",
      "452100 0:00:20.356020\n",
      "452200 0:00:20.357530\n",
      "452300 0:00:20.359267\n",
      "452400 0:00:20.361119\n",
      "452500 0:00:20.362439\n",
      "452600 0:00:20.364020\n",
      "452700 0:00:20.365998\n",
      "452800 0:00:20.367435\n",
      "452900 0:00:20.369071\n",
      "get 453000 0:00:20.370550\n",
      "453000 0:00:20.370572\n",
      "453100 0:00:20.372377\n",
      "453200 0:00:20.373963\n",
      "453300 0:00:20.375628\n",
      "453400 0:00:20.377356\n",
      "453500 0:00:20.379331\n",
      "453600 0:00:20.381350\n",
      "453700 0:00:20.383199\n",
      "453800 0:00:20.384837\n",
      "453900 0:00:20.386233\n",
      "get 454000 0:00:20.387571\n",
      "454000 0:00:20.387592\n",
      "454100 0:00:20.389617\n",
      "454200 0:00:20.391381\n",
      "454300 0:00:20.393169\n",
      "454400 0:00:20.394416\n",
      "454500 0:00:20.396535\n",
      "454600 0:00:20.397930\n",
      "454700 0:00:20.399578\n",
      "454800 0:00:20.401601\n",
      "454900 0:00:20.403074\n",
      "get 455000 0:00:20.405155\n",
      "455000 0:00:20.405184\n",
      "455100 0:00:20.406877\n",
      "455200 0:00:20.408502\n",
      "455300 0:00:20.410406\n",
      "455400 0:00:20.411984\n",
      "455500 0:00:20.413691\n",
      "455600 0:00:20.415485\n",
      "455700 0:00:20.416922\n",
      "455800 0:00:20.418885\n",
      "455900 0:00:20.420586\n",
      "get 456000 0:00:20.422249\n",
      "456000 0:00:20.422279\n",
      "456100 0:00:20.424044\n",
      "456200 0:00:20.425743\n",
      "456300 0:00:20.427491\n",
      "456400 0:00:20.429495\n",
      "456500 0:00:20.431360\n",
      "456600 0:00:20.433063\n",
      "456700 0:00:20.434812\n",
      "456800 0:00:20.436431\n",
      "456900 0:00:20.438089\n",
      "get 457000 0:00:20.439510\n",
      "457000 0:00:20.439561\n",
      "457100 0:00:20.441063\n",
      "457200 0:00:20.442777\n",
      "457300 0:00:20.444335\n",
      "457400 0:00:20.446339\n",
      "457500 0:00:20.447836\n",
      "457600 0:00:20.449443\n",
      "457700 0:00:20.451356\n",
      "457800 0:00:20.453049\n",
      "457900 0:00:20.454513\n",
      "get 458000 0:00:20.456357\n",
      "458000 0:00:20.456389\n",
      "458100 0:00:20.458315\n",
      "458200 0:00:20.459747\n",
      "458300 0:00:20.461298\n",
      "458400 0:00:20.462897\n",
      "458500 0:00:20.464536\n",
      "458600 0:00:20.466549\n",
      "458700 0:00:20.468170\n",
      "458800 0:00:20.469907\n",
      "458900 0:00:20.471237\n",
      "get 459000 0:00:20.472792\n",
      "459000 0:00:20.472813\n",
      "459100 0:00:20.474063\n",
      "459200 0:00:20.476167\n",
      "459300 0:00:20.477935\n",
      "459400 0:00:20.479742\n",
      "459500 0:00:20.481184\n",
      "459600 0:00:20.483535\n",
      "459700 0:00:20.484819\n",
      "459800 0:00:20.486855\n",
      "459900 0:00:20.488458\n",
      "get 460000 0:00:20.489993\n",
      "460000 0:00:20.490024\n",
      "460100 0:00:20.491956\n",
      "460200 0:00:20.493831\n",
      "460300 0:00:20.495536\n",
      "460400 0:00:20.497182\n",
      "460500 0:00:20.499032\n",
      "460600 0:00:20.500651\n",
      "460700 0:00:20.502315\n",
      "460800 0:00:20.503667\n",
      "460900 0:00:20.505225\n",
      "get 461000 0:00:20.506821\n",
      "461000 0:00:20.506850\n",
      "461100 0:00:20.508747\n",
      "461200 0:00:20.510152\n",
      "461300 0:00:20.512052\n",
      "461400 0:00:20.513741\n",
      "461500 0:00:20.515548\n",
      "461600 0:00:20.517326\n",
      "461700 0:00:20.518887\n",
      "461800 0:00:20.520536\n",
      "461900 0:00:20.522128\n",
      "get 462000 0:00:20.523909\n",
      "462000 0:00:20.523935\n",
      "462100 0:00:20.525527\n",
      "462200 0:00:20.526948\n",
      "462300 0:00:20.528613\n",
      "462400 0:00:20.530503\n",
      "462500 0:00:20.532587\n",
      "462600 0:00:20.534283\n",
      "462700 0:00:20.535888\n",
      "462800 0:00:20.537669\n",
      "462900 0:00:20.539424\n",
      "get 463000 0:00:20.541037\n",
      "463000 0:00:20.541069\n",
      "463100 0:00:20.542869\n",
      "463200 0:00:20.544736\n",
      "463300 0:00:20.546189\n",
      "463400 0:00:20.548166\n",
      "463500 0:00:20.549849\n",
      "463600 0:00:20.551639\n",
      "463700 0:00:20.553443\n",
      "463800 0:00:20.554802\n",
      "463900 0:00:20.556855\n",
      "get 464000 0:00:20.558597\n",
      "464000 0:00:20.558633\n",
      "464100 0:00:20.559963\n",
      "464200 0:00:20.561445\n",
      "464300 0:00:20.562815\n",
      "464400 0:00:20.564772\n",
      "464500 0:00:20.566496\n",
      "464600 0:00:20.568231\n",
      "464700 0:00:20.569987\n",
      "464800 0:00:20.571622\n",
      "464900 0:00:20.573304\n",
      "get 465000 0:00:20.574896\n",
      "465000 0:00:20.574929\n",
      "465100 0:00:20.576648\n",
      "465200 0:00:20.578246\n",
      "465300 0:00:20.579768\n",
      "465400 0:00:20.581410\n",
      "465500 0:00:20.583445\n",
      "465600 0:00:20.585056\n",
      "465700 0:00:20.586692\n",
      "465800 0:00:20.588339\n",
      "465900 0:00:20.590275\n",
      "get 466000 0:00:20.591814\n",
      "466000 0:00:20.591840\n",
      "466100 0:00:20.593195\n",
      "466200 0:00:20.594575\n",
      "466300 0:00:20.596424\n",
      "466400 0:00:20.598427\n",
      "466500 0:00:20.599961\n",
      "466600 0:00:20.601888\n",
      "466700 0:00:20.603607\n",
      "466800 0:00:20.605351\n",
      "466900 0:00:20.606991\n",
      "get 467000 0:00:20.608511\n",
      "467000 0:00:20.608538\n",
      "467100 0:00:20.609842\n",
      "467200 0:00:20.611699\n",
      "467300 0:00:20.613502\n",
      "467400 0:00:20.615088\n",
      "467500 0:00:20.616647\n",
      "467600 0:00:20.618444\n",
      "467700 0:00:20.620168\n",
      "467800 0:00:20.621643\n",
      "467900 0:00:20.623155\n",
      "get 468000 0:00:20.625013\n",
      "468000 0:00:20.625043\n",
      "468100 0:00:20.626748\n",
      "468200 0:00:20.628490\n",
      "468300 0:00:20.630249\n",
      "468400 0:00:20.632320\n",
      "468500 0:00:20.633970\n",
      "468600 0:00:20.635500\n",
      "468700 0:00:20.637314\n",
      "468800 0:00:20.639277\n",
      "468900 0:00:20.640790\n",
      "get 469000 0:00:20.642410\n",
      "469000 0:00:20.642433\n",
      "469100 0:00:20.643967\n",
      "469200 0:00:20.645437\n",
      "469300 0:00:20.647391\n",
      "469400 0:00:20.648882\n",
      "469500 0:00:20.650789\n",
      "469600 0:00:20.652408\n",
      "469700 0:00:20.653965\n",
      "469800 0:00:20.655775\n",
      "469900 0:00:20.657422\n",
      "get 470000 0:00:20.659182\n",
      "470000 0:00:20.659239\n",
      "470100 0:00:20.660972\n",
      "470200 0:00:20.662903\n",
      "470300 0:00:20.664497\n",
      "470400 0:00:20.666198\n",
      "470500 0:00:20.667812\n",
      "470600 0:00:20.669600\n",
      "470700 0:00:20.671219\n",
      "470800 0:00:20.673170\n",
      "470900 0:00:20.674774\n",
      "get 471000 0:00:20.676488\n",
      "471000 0:00:20.676518\n",
      "471100 0:00:20.678109\n",
      "471200 0:00:20.679549\n",
      "471300 0:00:20.681066\n",
      "471400 0:00:20.683054\n",
      "471500 0:00:20.684548\n",
      "471600 0:00:20.686349\n",
      "471700 0:00:20.687980\n",
      "471800 0:00:20.689599\n",
      "471900 0:00:20.691423\n",
      "get 472000 0:00:20.693070\n",
      "472000 0:00:20.693091\n",
      "472100 0:00:20.694772\n",
      "472200 0:00:20.696516\n",
      "472300 0:00:20.698300\n",
      "472400 0:00:20.699723\n",
      "472500 0:00:20.701479\n",
      "472600 0:00:20.702978\n",
      "472700 0:00:20.704441\n",
      "472800 0:00:20.706400\n",
      "472900 0:00:20.707770\n",
      "get 473000 0:00:20.709470\n",
      "473000 0:00:20.709505\n",
      "473100 0:00:20.711216\n",
      "473200 0:00:20.713084\n",
      "473300 0:00:20.715217\n",
      "473400 0:00:20.716902\n",
      "473500 0:00:20.718636\n",
      "473600 0:00:20.720362\n",
      "473700 0:00:20.722149\n",
      "473800 0:00:20.723850\n",
      "473900 0:00:20.725360\n",
      "get 474000 0:00:20.726856\n",
      "474000 0:00:20.726879\n",
      "474100 0:00:20.728356\n",
      "474200 0:00:20.730551\n",
      "474300 0:00:20.732249\n",
      "474400 0:00:20.734252\n",
      "474500 0:00:20.736053\n",
      "474600 0:00:20.737974\n",
      "474700 0:00:20.739726\n",
      "474800 0:00:20.741295\n",
      "474900 0:00:20.742691\n",
      "get 475000 0:00:20.744373\n",
      "475000 0:00:20.744403\n",
      "475100 0:00:20.746250\n",
      "475200 0:00:20.748071\n",
      "475300 0:00:20.749886\n",
      "475400 0:00:20.751556\n",
      "475500 0:00:20.753114\n",
      "475600 0:00:20.755176\n",
      "475700 0:00:20.756855\n",
      "475800 0:00:20.758492\n",
      "475900 0:00:20.760217\n",
      "get 476000 0:00:20.761926\n",
      "476000 0:00:20.761951\n",
      "476100 0:00:20.763765\n",
      "476200 0:00:20.765507\n",
      "476300 0:00:20.767012\n",
      "476400 0:00:20.768679\n",
      "476500 0:00:20.770647\n",
      "476600 0:00:20.772042\n",
      "476700 0:00:20.773810\n",
      "476800 0:00:20.775777\n",
      "476900 0:00:20.777587\n",
      "get 477000 0:00:20.779256\n",
      "477000 0:00:20.779285\n",
      "477100 0:00:20.781247\n",
      "477200 0:00:20.782654\n",
      "477300 0:00:20.784602\n",
      "477400 0:00:20.786227\n",
      "477500 0:00:20.788149\n",
      "477600 0:00:20.790001\n",
      "477700 0:00:20.791695\n",
      "477800 0:00:20.793698\n",
      "477900 0:00:20.795562\n",
      "get 478000 0:00:20.797414\n",
      "478000 0:00:20.797449\n",
      "478100 0:00:20.799187\n",
      "478200 0:00:20.800956\n",
      "478300 0:00:20.802730\n",
      "478400 0:00:20.804415\n",
      "478500 0:00:20.805635\n",
      "478600 0:00:20.807451\n",
      "478700 0:00:20.809137\n",
      "478800 0:00:20.810653\n",
      "478900 0:00:20.812189\n",
      "get 479000 0:00:20.813938\n",
      "479000 0:00:20.813976\n",
      "479100 0:00:20.815627\n",
      "479200 0:00:20.817456\n",
      "479300 0:00:20.818963\n",
      "479400 0:00:20.820673\n",
      "479500 0:00:20.822380\n",
      "479600 0:00:20.824228\n",
      "479700 0:00:20.825693\n",
      "479800 0:00:20.827529\n",
      "479900 0:00:20.829859\n",
      "get 480000 0:00:20.831855\n",
      "480000 0:00:20.831916\n",
      "480100 0:00:20.833527\n",
      "480200 0:00:20.834824\n",
      "480300 0:00:20.836170\n",
      "480400 0:00:20.837702\n",
      "480500 0:00:20.839632\n",
      "480600 0:00:20.841196\n",
      "480700 0:00:20.842758\n",
      "480800 0:00:20.844225\n",
      "480900 0:00:20.845994\n",
      "get 481000 0:00:20.847555\n",
      "481000 0:00:20.847584\n",
      "481100 0:00:20.849027\n",
      "481200 0:00:20.850180\n",
      "481300 0:00:20.851427\n",
      "481400 0:00:20.853022\n",
      "481500 0:00:20.854569\n",
      "481600 0:00:20.856234\n",
      "481700 0:00:20.857753\n",
      "481800 0:00:20.859404\n",
      "481900 0:00:20.860778\n",
      "get 482000 0:00:20.862170\n",
      "482000 0:00:20.862197\n",
      "482100 0:00:20.863468\n",
      "482200 0:00:20.864924\n",
      "482300 0:00:20.866442\n",
      "482400 0:00:20.868202\n",
      "482500 0:00:20.870100\n",
      "482600 0:00:20.871720\n",
      "482700 0:00:20.873295\n",
      "482800 0:00:20.874911\n",
      "482900 0:00:20.876410\n",
      "get 483000 0:00:20.878448\n",
      "483000 0:00:20.878470\n",
      "483100 0:00:20.879977\n",
      "483200 0:00:20.881962\n",
      "483300 0:00:20.884258\n",
      "483400 0:00:20.885917\n",
      "483500 0:00:20.887567\n",
      "483600 0:00:20.889536\n",
      "483700 0:00:20.890863\n",
      "483800 0:00:20.892363\n",
      "483900 0:00:20.893951\n",
      "get 484000 0:00:20.895519\n",
      "484000 0:00:20.895540\n",
      "484100 0:00:20.897047\n",
      "484200 0:00:20.898876\n",
      "484300 0:00:20.900224\n",
      "484400 0:00:20.902131\n",
      "484500 0:00:20.903748\n",
      "484600 0:00:20.905504\n",
      "484700 0:00:20.906921\n",
      "484800 0:00:20.908765\n",
      "484900 0:00:20.910330\n",
      "get 485000 0:00:20.911597\n",
      "485000 0:00:20.911619\n",
      "485100 0:00:20.913270\n",
      "485200 0:00:20.915130\n",
      "485300 0:00:20.916726\n",
      "485400 0:00:20.918728\n",
      "485500 0:00:20.920393\n",
      "485600 0:00:20.921874\n",
      "485700 0:00:20.923504\n",
      "485800 0:00:20.925139\n",
      "485900 0:00:20.927217\n",
      "get 486000 0:00:20.928987\n",
      "486000 0:00:20.929017\n",
      "486100 0:00:20.930941\n",
      "486200 0:00:20.932998\n",
      "486300 0:00:20.934752\n",
      "486400 0:00:20.936649\n",
      "486500 0:00:20.938467\n",
      "486600 0:00:20.940488\n",
      "486700 0:00:20.942159\n",
      "486800 0:00:20.943624\n",
      "486900 0:00:20.945060\n",
      "get 487000 0:00:20.946347\n",
      "487000 0:00:20.946368\n",
      "487100 0:00:20.947863\n",
      "487200 0:00:20.950329\n",
      "487300 0:00:20.952176\n",
      "487400 0:00:20.953831\n",
      "487500 0:00:20.955140\n",
      "487600 0:00:20.956780\n",
      "487700 0:00:20.958907\n",
      "487800 0:00:20.960515\n",
      "487900 0:00:20.962333\n",
      "get 488000 0:00:20.964042\n",
      "488000 0:00:20.964078\n",
      "488100 0:00:20.965866\n",
      "488200 0:00:20.967590\n",
      "488300 0:00:20.969042\n",
      "488400 0:00:20.970638\n",
      "488500 0:00:20.972668\n",
      "488600 0:00:20.974315\n",
      "488700 0:00:20.975781\n",
      "488800 0:00:20.977562\n",
      "488900 0:00:20.978984\n",
      "get 489000 0:00:20.980995\n",
      "489000 0:00:20.981017\n",
      "489100 0:00:20.982762\n",
      "489200 0:00:20.984239\n",
      "489300 0:00:20.986129\n",
      "489400 0:00:20.987892\n",
      "489500 0:00:20.989574\n",
      "489600 0:00:20.991262\n",
      "489700 0:00:20.992933\n",
      "489800 0:00:20.994508\n",
      "489900 0:00:20.995873\n",
      "get 490000 0:00:20.997716\n",
      "490000 0:00:20.997744\n",
      "490100 0:00:20.999710\n",
      "490200 0:00:21.001529\n",
      "490300 0:00:21.003288\n",
      "490400 0:00:21.004946\n",
      "490500 0:00:21.006504\n",
      "490600 0:00:21.007778\n",
      "490700 0:00:21.009975\n",
      "490800 0:00:21.011684\n",
      "490900 0:00:21.013566\n",
      "get 491000 0:00:21.015375\n",
      "491000 0:00:21.015408\n",
      "491100 0:00:21.017134\n",
      "491200 0:00:21.018600\n",
      "491300 0:00:21.020433\n",
      "491400 0:00:21.021915\n",
      "491500 0:00:21.023713\n",
      "491600 0:00:21.025268\n",
      "491700 0:00:21.026873\n",
      "491800 0:00:21.028556\n",
      "491900 0:00:21.030057\n",
      "get 492000 0:00:21.032192\n",
      "492000 0:00:21.032220\n",
      "492100 0:00:21.034113\n",
      "492200 0:00:21.036011\n",
      "492300 0:00:21.037691\n",
      "492400 0:00:21.039353\n",
      "492500 0:00:21.040980\n",
      "492600 0:00:21.042714\n",
      "492700 0:00:21.044550\n",
      "492800 0:00:21.046569\n",
      "492900 0:00:21.048477\n",
      "get 493000 0:00:21.050317\n",
      "493000 0:00:21.050355\n",
      "493100 0:00:21.051770\n",
      "493200 0:00:21.053842\n",
      "493300 0:00:21.055671\n",
      "493400 0:00:21.057303\n",
      "493500 0:00:21.058875\n",
      "493600 0:00:21.060582\n",
      "493700 0:00:21.062157\n",
      "493800 0:00:21.063840\n",
      "493900 0:00:21.065137\n",
      "get 494000 0:00:21.066676\n",
      "494000 0:00:21.066703\n",
      "494100 0:00:21.068422\n",
      "494200 0:00:21.069985\n",
      "494300 0:00:21.071556\n",
      "494400 0:00:21.072969\n",
      "494500 0:00:21.074917\n",
      "494600 0:00:21.076906\n",
      "494700 0:00:21.078687\n",
      "494800 0:00:21.080391\n",
      "494900 0:00:21.082109\n",
      "get 495000 0:00:21.083685\n",
      "495000 0:00:21.083709\n",
      "495100 0:00:21.085608\n",
      "495200 0:00:21.087197\n",
      "495300 0:00:21.088633\n",
      "495400 0:00:21.090228\n",
      "495500 0:00:21.091616\n",
      "495600 0:00:21.093544\n",
      "495700 0:00:21.095100\n",
      "495800 0:00:21.097214\n",
      "495900 0:00:21.098551\n",
      "get 496000 0:00:21.100295\n",
      "496000 0:00:21.100317\n",
      "496100 0:00:21.101870\n",
      "496200 0:00:21.103690\n",
      "496300 0:00:21.105357\n",
      "496400 0:00:21.107165\n",
      "496500 0:00:21.108687\n",
      "496600 0:00:21.110608\n",
      "496700 0:00:21.112317\n",
      "496800 0:00:21.113977\n",
      "496900 0:00:21.115424\n",
      "get 497000 0:00:21.117437\n",
      "497000 0:00:21.117460\n",
      "497100 0:00:21.119233\n",
      "497200 0:00:21.120684\n",
      "497300 0:00:21.121985\n",
      "497400 0:00:21.123594\n",
      "497500 0:00:21.125171\n",
      "497600 0:00:21.126739\n",
      "497700 0:00:21.128542\n",
      "497800 0:00:21.130648\n",
      "497900 0:00:21.132493\n",
      "get 498000 0:00:21.134043\n",
      "498000 0:00:21.134066\n",
      "498100 0:00:21.135967\n",
      "498200 0:00:21.137422\n",
      "498300 0:00:21.138972\n",
      "498400 0:00:21.140924\n",
      "498500 0:00:21.142563\n",
      "498600 0:00:21.143976\n",
      "498700 0:00:21.145205\n",
      "498800 0:00:21.146836\n",
      "498900 0:00:21.148520\n",
      "get 499000 0:00:21.150067\n",
      "499000 0:00:21.150109\n",
      "499100 0:00:21.151706\n",
      "499200 0:00:21.153332\n",
      "499300 0:00:21.155033\n",
      "499400 0:00:21.156941\n",
      "499500 0:00:21.158525\n",
      "499600 0:00:21.160038\n",
      "499700 0:00:21.161583\n",
      "499800 0:00:21.163537\n",
      "499900 0:00:21.165301\n",
      "get 500000 0:00:23.912983\n",
      "500000 0:00:23.913242\n",
      "500100 0:00:23.915558\n",
      "500200 0:00:23.917323\n",
      "500300 0:00:23.919064\n",
      "500400 0:00:23.920661\n",
      "500500 0:00:23.922248\n",
      "500600 0:00:23.923987\n",
      "500700 0:00:23.925715\n",
      "500800 0:00:23.927196\n",
      "500900 0:00:23.928925\n",
      "get 501000 0:00:23.930706\n",
      "501000 0:00:23.930728\n",
      "501100 0:00:23.932209\n",
      "501200 0:00:23.933624\n",
      "501300 0:00:23.935397\n",
      "501400 0:00:23.937057\n",
      "501500 0:00:23.938814\n",
      "501600 0:00:23.940416\n",
      "501700 0:00:23.942217\n",
      "501800 0:00:23.943985\n",
      "501900 0:00:23.945237\n",
      "get 502000 0:00:23.946686\n",
      "502000 0:00:23.946708\n",
      "502100 0:00:23.948167\n",
      "502200 0:00:23.949685\n",
      "502300 0:00:23.951182\n",
      "502400 0:00:23.952839\n",
      "502500 0:00:23.954538\n",
      "502600 0:00:23.956205\n",
      "502700 0:00:23.958033\n",
      "502800 0:00:23.959856\n",
      "502900 0:00:23.961268\n",
      "get 503000 0:00:23.962614\n",
      "503000 0:00:23.962646\n",
      "503100 0:00:23.964149\n",
      "503200 0:00:23.965569\n",
      "503300 0:00:23.967499\n",
      "503400 0:00:23.969295\n",
      "503500 0:00:23.970849\n",
      "503600 0:00:23.972463\n",
      "503700 0:00:23.974132\n",
      "503800 0:00:23.975842\n",
      "503900 0:00:23.977426\n",
      "get 504000 0:00:23.979013\n",
      "504000 0:00:23.979036\n",
      "504100 0:00:23.981478\n",
      "504200 0:00:23.983543\n",
      "504300 0:00:23.985317\n",
      "504400 0:00:23.987627\n",
      "504500 0:00:23.989670\n",
      "504600 0:00:23.991931\n",
      "504700 0:00:23.993971\n",
      "504800 0:00:23.996058\n",
      "504900 0:00:23.997975\n",
      "get 505000 0:00:24.000039\n",
      "505000 0:00:24.000067\n",
      "505100 0:00:24.002146\n",
      "505200 0:00:24.004307\n",
      "505300 0:00:24.006544\n",
      "505400 0:00:24.008621\n",
      "505500 0:00:24.010719\n",
      "505600 0:00:24.012524\n",
      "505700 0:00:24.013951\n",
      "505800 0:00:24.015616\n",
      "505900 0:00:24.017385\n",
      "get 506000 0:00:24.018881\n",
      "506000 0:00:24.018914\n",
      "506100 0:00:24.021089\n",
      "506200 0:00:24.022623\n",
      "506300 0:00:24.024209\n",
      "506400 0:00:24.025570\n",
      "506500 0:00:24.027326\n",
      "506600 0:00:24.028794\n",
      "506700 0:00:24.030547\n",
      "506800 0:00:24.032477\n",
      "506900 0:00:24.033797\n",
      "get 507000 0:00:24.035364\n",
      "507000 0:00:24.035386\n",
      "507100 0:00:24.036858\n",
      "507200 0:00:24.038361\n",
      "507300 0:00:24.039888\n",
      "507400 0:00:24.042108\n",
      "507500 0:00:24.043757\n",
      "507600 0:00:24.045360\n",
      "507700 0:00:24.047031\n",
      "507800 0:00:24.049291\n",
      "507900 0:00:24.050953\n",
      "get 508000 0:00:24.052256\n",
      "508000 0:00:24.052279\n",
      "508100 0:00:24.053718\n",
      "508200 0:00:24.055342\n",
      "508300 0:00:24.057127\n",
      "508400 0:00:24.058595\n",
      "508500 0:00:24.060085\n",
      "508600 0:00:24.061765\n",
      "508700 0:00:24.063205\n",
      "508800 0:00:24.064676\n",
      "508900 0:00:24.066668\n",
      "get 509000 0:00:24.068352\n",
      "509000 0:00:24.068374\n",
      "509100 0:00:24.070219\n",
      "509200 0:00:24.071752\n",
      "509300 0:00:24.073495\n",
      "509400 0:00:24.075099\n",
      "509500 0:00:24.076870\n",
      "509600 0:00:24.079012\n",
      "509700 0:00:24.080747\n",
      "509800 0:00:24.082153\n",
      "509900 0:00:24.083764\n",
      "get 510000 0:00:24.085160\n",
      "510000 0:00:24.085194\n",
      "510100 0:00:24.086532\n",
      "510200 0:00:24.087851\n",
      "510300 0:00:24.089591\n",
      "510400 0:00:24.091397\n",
      "510500 0:00:24.092984\n",
      "510600 0:00:24.094466\n",
      "510700 0:00:24.096270\n",
      "510800 0:00:24.097699\n",
      "510900 0:00:24.099249\n",
      "get 511000 0:00:24.101071\n",
      "511000 0:00:24.101105\n",
      "511100 0:00:24.102598\n",
      "511200 0:00:24.104460\n",
      "511300 0:00:24.105939\n",
      "511400 0:00:24.107690\n",
      "511500 0:00:24.109248\n",
      "511600 0:00:24.111173\n",
      "511700 0:00:24.113088\n",
      "511800 0:00:24.114975\n",
      "511900 0:00:24.116782\n",
      "get 512000 0:00:24.118636\n",
      "512000 0:00:24.118662\n",
      "512100 0:00:24.120276\n",
      "512200 0:00:24.121698\n",
      "512300 0:00:24.123102\n",
      "512400 0:00:24.124612\n",
      "512500 0:00:24.126449\n",
      "512600 0:00:24.128276\n",
      "512700 0:00:24.129876\n",
      "512800 0:00:24.131504\n",
      "512900 0:00:24.132842\n",
      "get 513000 0:00:24.134077\n",
      "513000 0:00:24.134101\n",
      "513100 0:00:24.135450\n",
      "513200 0:00:24.137076\n",
      "513300 0:00:24.138644\n",
      "513400 0:00:24.140238\n",
      "513500 0:00:24.142205\n",
      "513600 0:00:24.143679\n",
      "513700 0:00:24.145314\n",
      "513800 0:00:24.146597\n",
      "513900 0:00:24.148623\n",
      "get 514000 0:00:24.150305\n",
      "514000 0:00:24.150327\n",
      "514100 0:00:24.151727\n",
      "514200 0:00:24.153123\n",
      "514300 0:00:24.154317\n",
      "514400 0:00:24.155990\n",
      "514500 0:00:24.157537\n",
      "514600 0:00:24.159034\n",
      "514700 0:00:24.160712\n",
      "514800 0:00:24.162364\n",
      "514900 0:00:24.163742\n",
      "get 515000 0:00:24.165161\n",
      "515000 0:00:24.165191\n",
      "515100 0:00:24.167068\n",
      "515200 0:00:24.168793\n",
      "515300 0:00:24.170707\n",
      "515400 0:00:24.172269\n",
      "515500 0:00:24.173697\n",
      "515600 0:00:24.175530\n",
      "515700 0:00:24.177195\n",
      "515800 0:00:24.178734\n",
      "515900 0:00:24.180345\n",
      "get 516000 0:00:24.181673\n",
      "516000 0:00:24.181696\n",
      "516100 0:00:24.183190\n",
      "516200 0:00:24.184790\n",
      "516300 0:00:24.186453\n",
      "516400 0:00:24.187981\n",
      "516500 0:00:24.189321\n",
      "516600 0:00:24.191052\n",
      "516700 0:00:24.192658\n",
      "516800 0:00:24.194156\n",
      "516900 0:00:24.195487\n",
      "get 517000 0:00:24.196897\n",
      "517000 0:00:24.196923\n",
      "517100 0:00:24.198515\n",
      "517200 0:00:24.199952\n",
      "517300 0:00:24.201260\n",
      "517400 0:00:24.203169\n",
      "517500 0:00:24.204801\n",
      "517600 0:00:24.206149\n",
      "517700 0:00:24.207908\n",
      "517800 0:00:24.209540\n",
      "517900 0:00:24.211152\n",
      "get 518000 0:00:24.212518\n",
      "518000 0:00:24.212541\n",
      "518100 0:00:24.214363\n",
      "518200 0:00:24.215732\n",
      "518300 0:00:24.217407\n",
      "518400 0:00:24.218900\n",
      "518500 0:00:24.220621\n",
      "518600 0:00:24.222339\n",
      "518700 0:00:24.223837\n",
      "518800 0:00:24.225641\n",
      "518900 0:00:24.227506\n",
      "get 519000 0:00:24.228872\n",
      "519000 0:00:24.228895\n",
      "519100 0:00:24.230455\n",
      "519200 0:00:24.231670\n",
      "519300 0:00:24.233388\n",
      "519400 0:00:24.235074\n",
      "519500 0:00:24.236819\n",
      "519600 0:00:24.238418\n",
      "519700 0:00:24.239852\n",
      "519800 0:00:24.241518\n",
      "519900 0:00:24.242978\n",
      "get 520000 0:00:24.244541\n",
      "520000 0:00:24.244579\n",
      "520100 0:00:24.246311\n",
      "520200 0:00:24.248034\n",
      "520300 0:00:24.249559\n",
      "520400 0:00:24.251197\n",
      "520500 0:00:24.252787\n",
      "520600 0:00:24.254605\n",
      "520700 0:00:24.256382\n",
      "520800 0:00:24.257940\n",
      "520900 0:00:24.259738\n",
      "get 521000 0:00:24.261464\n",
      "521000 0:00:24.261488\n",
      "521100 0:00:24.263282\n",
      "521200 0:00:24.265028\n",
      "521300 0:00:24.266447\n",
      "521400 0:00:24.268066\n",
      "521500 0:00:24.269488\n",
      "521600 0:00:24.271220\n",
      "521700 0:00:24.272835\n",
      "521800 0:00:24.274730\n",
      "521900 0:00:24.276239\n",
      "get 522000 0:00:24.277997\n",
      "522000 0:00:24.278030\n",
      "522100 0:00:24.279599\n",
      "522200 0:00:24.281061\n",
      "522300 0:00:24.282786\n",
      "522400 0:00:24.284614\n",
      "522500 0:00:24.286378\n",
      "522600 0:00:24.288025\n",
      "522700 0:00:24.289822\n",
      "522800 0:00:24.291834\n",
      "522900 0:00:24.293286\n",
      "get 523000 0:00:24.294855\n",
      "523000 0:00:24.294877\n",
      "523100 0:00:24.296252\n",
      "523200 0:00:24.298006\n",
      "523300 0:00:24.299424\n",
      "523400 0:00:24.301158\n",
      "523500 0:00:24.302573\n",
      "523600 0:00:24.304141\n",
      "523700 0:00:24.306015\n",
      "523800 0:00:24.307589\n",
      "523900 0:00:24.309088\n",
      "get 524000 0:00:24.310726\n",
      "524000 0:00:24.310749\n",
      "524100 0:00:24.312744\n",
      "524200 0:00:24.314387\n",
      "524300 0:00:24.316529\n",
      "524400 0:00:24.318286\n",
      "524500 0:00:24.319813\n",
      "524600 0:00:24.321092\n",
      "524700 0:00:24.322957\n",
      "524800 0:00:24.324390\n",
      "524900 0:00:24.326096\n",
      "get 525000 0:00:24.327845\n",
      "525000 0:00:24.327878\n",
      "525100 0:00:24.329347\n",
      "525200 0:00:24.331013\n",
      "525300 0:00:24.332883\n",
      "525400 0:00:24.334494\n",
      "525500 0:00:24.336075\n",
      "525600 0:00:24.337976\n",
      "525700 0:00:24.339858\n",
      "525800 0:00:24.341536\n",
      "525900 0:00:24.342917\n",
      "get 526000 0:00:24.344485\n",
      "526000 0:00:24.344518\n",
      "526100 0:00:24.346144\n",
      "526200 0:00:24.347944\n",
      "526300 0:00:24.349896\n",
      "526400 0:00:24.351527\n",
      "526500 0:00:24.353096\n",
      "526600 0:00:24.355028\n",
      "526700 0:00:24.356685\n",
      "526800 0:00:24.358340\n",
      "526900 0:00:24.360064\n",
      "get 527000 0:00:24.361697\n",
      "527000 0:00:24.361737\n",
      "527100 0:00:24.363965\n",
      "527200 0:00:24.365889\n",
      "527300 0:00:24.367693\n",
      "527400 0:00:24.369440\n",
      "527500 0:00:24.371538\n",
      "527600 0:00:24.373004\n",
      "527700 0:00:24.374945\n",
      "527800 0:00:24.376503\n",
      "527900 0:00:24.378213\n",
      "get 528000 0:00:24.379637\n",
      "528000 0:00:24.379660\n",
      "528100 0:00:24.381021\n",
      "528200 0:00:24.383082\n",
      "528300 0:00:24.384555\n",
      "528400 0:00:24.386166\n",
      "528500 0:00:24.387599\n",
      "528600 0:00:24.389198\n",
      "528700 0:00:24.390859\n",
      "528800 0:00:24.392639\n",
      "528900 0:00:24.394029\n",
      "get 529000 0:00:24.395585\n",
      "529000 0:00:24.395620\n",
      "529100 0:00:24.397112\n",
      "529200 0:00:24.398548\n",
      "529300 0:00:24.400006\n",
      "529400 0:00:24.401596\n",
      "529500 0:00:24.403490\n",
      "529600 0:00:24.405242\n",
      "529700 0:00:24.406668\n",
      "529800 0:00:24.407962\n",
      "529900 0:00:24.409448\n",
      "get 530000 0:00:24.410749\n",
      "530000 0:00:24.410773\n",
      "530100 0:00:24.412684\n",
      "530200 0:00:24.414416\n",
      "530300 0:00:24.416007\n",
      "530400 0:00:24.417540\n",
      "530500 0:00:24.419311\n",
      "530600 0:00:24.421160\n",
      "530700 0:00:24.422575\n",
      "530800 0:00:24.423941\n",
      "530900 0:00:24.425734\n",
      "get 531000 0:00:24.427549\n",
      "531000 0:00:24.427576\n",
      "531100 0:00:24.429215\n",
      "531200 0:00:24.430964\n",
      "531300 0:00:24.432927\n",
      "531400 0:00:24.434517\n",
      "531500 0:00:24.436236\n",
      "531600 0:00:24.438098\n",
      "531700 0:00:24.439710\n",
      "531800 0:00:24.441481\n",
      "531900 0:00:24.443195\n",
      "get 532000 0:00:24.444850\n",
      "532000 0:00:24.444884\n",
      "532100 0:00:24.446480\n",
      "532200 0:00:24.448455\n",
      "532300 0:00:24.450057\n",
      "532400 0:00:24.451864\n",
      "532500 0:00:24.453784\n",
      "532600 0:00:24.455477\n",
      "532700 0:00:24.457227\n",
      "532800 0:00:24.459194\n",
      "532900 0:00:24.460834\n",
      "get 533000 0:00:24.462639\n",
      "533000 0:00:24.462663\n",
      "533100 0:00:24.464364\n",
      "533200 0:00:24.466266\n",
      "533300 0:00:24.468044\n",
      "533400 0:00:24.470097\n",
      "533500 0:00:24.471779\n",
      "533600 0:00:24.473710\n",
      "533700 0:00:24.475272\n",
      "533800 0:00:24.476808\n",
      "533900 0:00:24.479074\n",
      "get 534000 0:00:24.480452\n",
      "534000 0:00:24.480486\n",
      "534100 0:00:24.481715\n",
      "534200 0:00:24.483470\n",
      "534300 0:00:24.485080\n",
      "534400 0:00:24.486994\n",
      "534500 0:00:24.488670\n",
      "534600 0:00:24.490541\n",
      "534700 0:00:24.492188\n",
      "534800 0:00:24.494131\n",
      "534900 0:00:24.496128\n",
      "get 535000 0:00:24.497694\n",
      "535000 0:00:24.497759\n",
      "535100 0:00:24.499376\n",
      "535200 0:00:24.501082\n",
      "535300 0:00:24.502775\n",
      "535400 0:00:24.504469\n",
      "535500 0:00:24.506397\n",
      "535600 0:00:24.508232\n",
      "535700 0:00:24.509878\n",
      "535800 0:00:24.511843\n",
      "535900 0:00:24.513366\n",
      "get 536000 0:00:24.515259\n",
      "536000 0:00:24.515289\n",
      "536100 0:00:24.517652\n",
      "536200 0:00:24.519315\n",
      "536300 0:00:24.520842\n",
      "536400 0:00:24.522667\n",
      "536500 0:00:24.524118\n",
      "536600 0:00:24.526091\n",
      "536700 0:00:24.527697\n",
      "536800 0:00:24.529845\n",
      "536900 0:00:24.531537\n",
      "get 537000 0:00:24.533355\n",
      "537000 0:00:24.533374\n",
      "537100 0:00:24.534996\n",
      "537200 0:00:24.536804\n",
      "537300 0:00:24.538758\n",
      "537400 0:00:24.540510\n",
      "537500 0:00:24.542198\n",
      "537600 0:00:24.543985\n",
      "537700 0:00:24.545762\n",
      "537800 0:00:24.547593\n",
      "537900 0:00:24.549318\n",
      "get 538000 0:00:24.551133\n",
      "538000 0:00:24.551156\n",
      "538100 0:00:24.553061\n",
      "538200 0:00:24.554892\n",
      "538300 0:00:24.556917\n",
      "538400 0:00:24.558288\n",
      "538500 0:00:24.560055\n",
      "538600 0:00:24.561754\n",
      "538700 0:00:24.563754\n",
      "538800 0:00:24.565257\n",
      "538900 0:00:24.567221\n",
      "get 539000 0:00:24.569091\n",
      "539000 0:00:24.569111\n",
      "539100 0:00:24.570334\n",
      "539200 0:00:24.572159\n",
      "539300 0:00:24.573950\n",
      "539400 0:00:24.575671\n",
      "539500 0:00:24.577334\n",
      "539600 0:00:24.579061\n",
      "539700 0:00:24.581014\n",
      "539800 0:00:24.583030\n",
      "539900 0:00:24.584457\n",
      "get 540000 0:00:24.586151\n",
      "540000 0:00:24.586183\n",
      "540100 0:00:24.587923\n",
      "540200 0:00:24.589726\n",
      "540300 0:00:24.591509\n",
      "540400 0:00:24.593476\n",
      "540500 0:00:24.595438\n",
      "540600 0:00:24.597336\n",
      "540700 0:00:24.599401\n",
      "540800 0:00:24.601361\n",
      "540900 0:00:24.603598\n",
      "get 541000 0:00:24.605006\n",
      "541000 0:00:24.605025\n",
      "541100 0:00:24.606890\n",
      "541200 0:00:24.608945\n",
      "541300 0:00:24.610684\n",
      "541400 0:00:24.612640\n",
      "541500 0:00:24.614697\n",
      "541600 0:00:24.616631\n",
      "541700 0:00:24.618518\n",
      "541800 0:00:24.619672\n",
      "541900 0:00:24.621449\n",
      "get 542000 0:00:24.623422\n",
      "542000 0:00:24.623443\n",
      "542100 0:00:24.625394\n",
      "542200 0:00:24.627261\n",
      "542300 0:00:24.628996\n",
      "542400 0:00:24.630779\n",
      "542500 0:00:24.632649\n",
      "542600 0:00:24.634914\n",
      "542700 0:00:24.636463\n",
      "542800 0:00:24.638407\n",
      "542900 0:00:24.640293\n",
      "get 543000 0:00:24.642134\n",
      "543000 0:00:24.642153\n",
      "543100 0:00:24.644021\n",
      "543200 0:00:24.645682\n",
      "543300 0:00:24.647869\n",
      "543400 0:00:24.649905\n",
      "543500 0:00:24.651394\n",
      "543600 0:00:24.653585\n",
      "543700 0:00:24.655554\n",
      "543800 0:00:24.657363\n",
      "543900 0:00:24.659253\n",
      "get 544000 0:00:24.661142\n",
      "544000 0:00:24.661162\n",
      "544100 0:00:24.663271\n",
      "544200 0:00:24.664890\n",
      "544300 0:00:24.666557\n",
      "544400 0:00:24.668541\n",
      "544500 0:00:24.670784\n",
      "544600 0:00:24.673234\n",
      "544700 0:00:24.675115\n",
      "544800 0:00:24.676698\n",
      "544900 0:00:24.678028\n",
      "get 545000 0:00:24.679588\n",
      "545000 0:00:24.679607\n",
      "545100 0:00:24.681729\n",
      "545200 0:00:24.683842\n",
      "545300 0:00:24.685635\n",
      "545400 0:00:24.687468\n",
      "545500 0:00:24.689229\n",
      "545600 0:00:24.690829\n",
      "545700 0:00:24.692537\n",
      "545800 0:00:24.693884\n",
      "545900 0:00:24.696029\n",
      "get 546000 0:00:24.697872\n",
      "546000 0:00:24.697891\n",
      "546100 0:00:24.699396\n",
      "546200 0:00:24.701915\n",
      "546300 0:00:24.703605\n",
      "546400 0:00:24.705035\n",
      "546500 0:00:24.706749\n",
      "546600 0:00:24.708944\n",
      "546700 0:00:24.710551\n",
      "546800 0:00:24.712529\n",
      "546900 0:00:24.714225\n",
      "get 547000 0:00:24.716089\n",
      "547000 0:00:24.716108\n",
      "547100 0:00:24.718454\n",
      "547200 0:00:24.720148\n",
      "547300 0:00:24.722129\n",
      "547400 0:00:24.723874\n",
      "547500 0:00:24.726001\n",
      "547600 0:00:24.727518\n",
      "547700 0:00:24.729210\n",
      "547800 0:00:24.730862\n",
      "547900 0:00:24.732515\n",
      "get 548000 0:00:24.734248\n",
      "548000 0:00:24.734267\n",
      "548100 0:00:24.736337\n",
      "548200 0:00:24.737748\n",
      "548300 0:00:24.739738\n",
      "548400 0:00:24.741700\n",
      "548500 0:00:24.743403\n",
      "548600 0:00:24.745380\n",
      "548700 0:00:24.747237\n",
      "548800 0:00:24.748906\n",
      "548900 0:00:24.750714\n",
      "get 549000 0:00:24.752530\n",
      "549000 0:00:24.752549\n",
      "549100 0:00:24.754495\n",
      "549200 0:00:24.756377\n",
      "549300 0:00:24.758396\n",
      "549400 0:00:24.759749\n",
      "549500 0:00:24.761322\n",
      "549600 0:00:24.762793\n",
      "549700 0:00:24.764363\n",
      "549800 0:00:24.766382\n",
      "549900 0:00:24.768397\n",
      "get 550000 0:00:24.770147\n",
      "550000 0:00:24.770167\n",
      "550100 0:00:24.772353\n",
      "550200 0:00:24.773568\n",
      "550300 0:00:24.775069\n",
      "550400 0:00:24.776588\n",
      "550500 0:00:24.778289\n",
      "550600 0:00:24.780205\n",
      "550700 0:00:24.782137\n",
      "550800 0:00:24.783900\n",
      "550900 0:00:24.785435\n",
      "get 551000 0:00:24.787472\n",
      "551000 0:00:24.787492\n",
      "551100 0:00:24.789375\n",
      "551200 0:00:24.791310\n",
      "551300 0:00:24.792821\n",
      "551400 0:00:24.794444\n",
      "551500 0:00:24.796190\n",
      "551600 0:00:24.797949\n",
      "551700 0:00:24.799706\n",
      "551800 0:00:24.801791\n",
      "551900 0:00:24.803704\n",
      "get 552000 0:00:24.805217\n",
      "552000 0:00:24.805236\n",
      "552100 0:00:24.806887\n",
      "552200 0:00:24.808809\n",
      "552300 0:00:24.810687\n",
      "552400 0:00:24.812356\n",
      "552500 0:00:24.813685\n",
      "552600 0:00:24.815553\n",
      "552700 0:00:24.817086\n",
      "552800 0:00:24.818976\n",
      "552900 0:00:24.820950\n",
      "get 553000 0:00:24.823022\n",
      "553000 0:00:24.823043\n",
      "553100 0:00:24.824870\n",
      "553200 0:00:24.827050\n",
      "553300 0:00:24.829046\n",
      "553400 0:00:24.830545\n",
      "553500 0:00:24.832327\n",
      "553600 0:00:24.834134\n",
      "553700 0:00:24.836143\n",
      "553800 0:00:24.837932\n",
      "553900 0:00:24.840068\n",
      "get 554000 0:00:24.841550\n",
      "554000 0:00:24.841568\n",
      "554100 0:00:24.844002\n",
      "554200 0:00:24.845666\n",
      "554300 0:00:24.846937\n",
      "554400 0:00:24.848580\n",
      "554500 0:00:24.850696\n",
      "554600 0:00:24.852369\n",
      "554700 0:00:24.854034\n",
      "554800 0:00:24.855939\n",
      "554900 0:00:24.857940\n",
      "get 555000 0:00:24.859735\n",
      "555000 0:00:24.859790\n",
      "555100 0:00:24.861965\n",
      "555200 0:00:24.863760\n",
      "555300 0:00:24.865261\n",
      "555400 0:00:24.866910\n",
      "555500 0:00:24.869125\n",
      "555600 0:00:24.870527\n",
      "555700 0:00:24.872073\n",
      "555800 0:00:24.874599\n",
      "555900 0:00:24.876057\n",
      "get 556000 0:00:24.877338\n",
      "556000 0:00:24.877357\n",
      "556100 0:00:24.879091\n",
      "556200 0:00:24.880938\n",
      "556300 0:00:24.882768\n",
      "556400 0:00:24.884461\n",
      "556500 0:00:24.886333\n",
      "556600 0:00:24.888117\n",
      "556700 0:00:24.889989\n",
      "556800 0:00:24.892079\n",
      "556900 0:00:24.893636\n",
      "get 557000 0:00:24.895170\n",
      "557000 0:00:24.895204\n",
      "557100 0:00:24.897174\n",
      "557200 0:00:24.899217\n",
      "557300 0:00:24.901092\n",
      "557400 0:00:24.903047\n",
      "557500 0:00:24.904778\n",
      "557600 0:00:24.906924\n",
      "557700 0:00:24.908228\n",
      "557800 0:00:24.910216\n",
      "557900 0:00:24.911751\n",
      "get 558000 0:00:24.913416\n",
      "558000 0:00:24.913435\n",
      "558100 0:00:24.915243\n",
      "558200 0:00:24.917025\n",
      "558300 0:00:24.918867\n",
      "558400 0:00:24.920919\n",
      "558500 0:00:24.922623\n",
      "558600 0:00:24.924508\n",
      "558700 0:00:24.926194\n",
      "558800 0:00:24.927988\n",
      "558900 0:00:24.929517\n",
      "get 559000 0:00:24.931158\n",
      "559000 0:00:24.931177\n",
      "559100 0:00:24.933207\n",
      "559200 0:00:24.935257\n",
      "559300 0:00:24.937000\n",
      "559400 0:00:24.939164\n",
      "559500 0:00:24.940838\n",
      "559600 0:00:24.942340\n",
      "559700 0:00:24.944279\n",
      "559800 0:00:24.946034\n",
      "559900 0:00:24.947933\n",
      "get 560000 0:00:24.949767\n",
      "560000 0:00:24.949785\n",
      "560100 0:00:24.951697\n",
      "560200 0:00:24.953416\n",
      "560300 0:00:24.955062\n",
      "560400 0:00:24.957084\n",
      "560500 0:00:24.958544\n",
      "560600 0:00:24.960566\n",
      "560700 0:00:24.962416\n",
      "560800 0:00:24.964978\n",
      "560900 0:00:24.966428\n",
      "get 561000 0:00:24.967752\n",
      "561000 0:00:24.967772\n",
      "561100 0:00:24.969574\n",
      "561200 0:00:24.971501\n",
      "561300 0:00:24.973137\n",
      "561400 0:00:24.974430\n",
      "561500 0:00:24.976724\n",
      "561600 0:00:24.978605\n",
      "561700 0:00:24.980282\n",
      "561800 0:00:24.982426\n",
      "561900 0:00:24.984519\n",
      "get 562000 0:00:24.987023\n",
      "562000 0:00:24.987060\n",
      "562100 0:00:24.989127\n",
      "562200 0:00:24.990822\n",
      "562300 0:00:24.993003\n",
      "562400 0:00:24.995398\n",
      "562500 0:00:24.997277\n",
      "562600 0:00:24.999779\n",
      "562700 0:00:25.002258\n",
      "562800 0:00:25.004589\n",
      "562900 0:00:25.006752\n",
      "get 563000 0:00:25.009300\n",
      "563000 0:00:25.009321\n",
      "563100 0:00:25.011464\n",
      "563200 0:00:25.013846\n",
      "563300 0:00:25.015983\n",
      "563400 0:00:25.018288\n",
      "563500 0:00:25.020449\n",
      "563600 0:00:25.022595\n",
      "563700 0:00:25.023964\n",
      "563800 0:00:25.026037\n",
      "563900 0:00:25.028478\n",
      "get 564000 0:00:25.030949\n",
      "564000 0:00:25.030973\n",
      "564100 0:00:25.033016\n",
      "564200 0:00:25.035140\n",
      "564300 0:00:25.037235\n",
      "564400 0:00:25.039362\n",
      "564500 0:00:25.041196\n",
      "564600 0:00:25.043146\n",
      "564700 0:00:25.045015\n",
      "564800 0:00:25.046795\n",
      "564900 0:00:25.048618\n",
      "get 565000 0:00:25.050423\n",
      "565000 0:00:25.050447\n",
      "565100 0:00:25.052496\n",
      "565200 0:00:25.054279\n",
      "565300 0:00:25.056195\n",
      "565400 0:00:25.058085\n",
      "565500 0:00:25.059931\n",
      "565600 0:00:25.061721\n",
      "565700 0:00:25.063671\n",
      "565800 0:00:25.065477\n",
      "565900 0:00:25.067275\n",
      "get 566000 0:00:25.069153\n",
      "566000 0:00:25.069177\n",
      "566100 0:00:25.070996\n",
      "566200 0:00:25.072838\n",
      "566300 0:00:25.074674\n",
      "566400 0:00:25.076541\n",
      "566500 0:00:25.078496\n",
      "566600 0:00:25.080282\n",
      "566700 0:00:25.082309\n",
      "566800 0:00:25.084290\n",
      "566900 0:00:25.086202\n",
      "get 567000 0:00:25.087904\n",
      "567000 0:00:25.087931\n",
      "567100 0:00:25.089619\n",
      "567200 0:00:25.091555\n",
      "567300 0:00:25.093277\n",
      "567400 0:00:25.095331\n",
      "567500 0:00:25.097523\n",
      "567600 0:00:25.099444\n",
      "567700 0:00:25.100969\n",
      "567800 0:00:25.103167\n",
      "567900 0:00:25.104946\n",
      "get 568000 0:00:25.106786\n",
      "568000 0:00:25.106809\n",
      "568100 0:00:25.108960\n",
      "568200 0:00:25.111009\n",
      "568300 0:00:25.112427\n",
      "568400 0:00:25.114535\n",
      "568500 0:00:25.116251\n",
      "568600 0:00:25.117926\n",
      "568700 0:00:25.119809\n",
      "568800 0:00:25.121718\n",
      "568900 0:00:25.123769\n",
      "get 569000 0:00:25.125682\n",
      "569000 0:00:25.125706\n",
      "569100 0:00:25.127597\n",
      "569200 0:00:25.129496\n",
      "569300 0:00:25.131282\n",
      "569400 0:00:25.132953\n",
      "569500 0:00:25.134784\n",
      "569600 0:00:25.136754\n",
      "569700 0:00:25.139076\n",
      "569800 0:00:25.140636\n",
      "569900 0:00:25.142596\n",
      "get 570000 0:00:25.144477\n",
      "570000 0:00:25.144505\n",
      "570100 0:00:25.146461\n",
      "570200 0:00:25.148394\n",
      "570300 0:00:25.150348\n",
      "570400 0:00:25.152356\n",
      "570500 0:00:25.153925\n",
      "570600 0:00:25.155838\n",
      "570700 0:00:25.157890\n",
      "570800 0:00:25.159666\n",
      "570900 0:00:25.160983\n",
      "get 571000 0:00:25.162988\n",
      "571000 0:00:25.163008\n",
      "571100 0:00:25.165221\n",
      "571200 0:00:25.166951\n",
      "571300 0:00:25.169139\n",
      "571400 0:00:25.170796\n",
      "571500 0:00:25.172455\n",
      "571600 0:00:25.174352\n",
      "571700 0:00:25.176246\n",
      "571800 0:00:25.178104\n",
      "571900 0:00:25.179946\n",
      "get 572000 0:00:25.181702\n",
      "572000 0:00:25.181738\n",
      "572100 0:00:25.183027\n",
      "572200 0:00:25.184743\n",
      "572300 0:00:25.186642\n",
      "572400 0:00:25.188231\n",
      "572500 0:00:25.190473\n",
      "572600 0:00:25.191769\n",
      "572700 0:00:25.193489\n",
      "572800 0:00:25.195347\n",
      "572900 0:00:25.197337\n",
      "get 573000 0:00:25.198807\n",
      "573000 0:00:25.198827\n",
      "573100 0:00:25.200326\n",
      "573200 0:00:25.202037\n",
      "573300 0:00:25.203568\n",
      "573400 0:00:25.205543\n",
      "573500 0:00:25.207229\n",
      "573600 0:00:25.208808\n",
      "573700 0:00:25.211040\n",
      "573800 0:00:25.212897\n",
      "573900 0:00:25.214961\n",
      "get 574000 0:00:25.216631\n",
      "574000 0:00:25.216650\n",
      "574100 0:00:25.218437\n",
      "574200 0:00:25.220760\n",
      "574300 0:00:25.222882\n",
      "574400 0:00:25.224568\n",
      "574500 0:00:25.226230\n",
      "574600 0:00:25.228211\n",
      "574700 0:00:25.229725\n",
      "574800 0:00:25.231179\n",
      "574900 0:00:25.232620\n",
      "get 575000 0:00:25.234428\n",
      "575000 0:00:25.234449\n",
      "575100 0:00:25.236556\n",
      "575200 0:00:25.238600\n",
      "575300 0:00:25.240951\n",
      "575400 0:00:25.243383\n",
      "575500 0:00:25.245373\n",
      "575600 0:00:25.247219\n",
      "575700 0:00:25.249107\n",
      "575800 0:00:25.251039\n",
      "575900 0:00:25.252832\n",
      "get 576000 0:00:25.254834\n",
      "576000 0:00:25.254904\n",
      "576100 0:00:25.256819\n",
      "576200 0:00:25.258896\n",
      "576300 0:00:25.260514\n",
      "576400 0:00:25.262548\n",
      "576500 0:00:25.264172\n",
      "576600 0:00:25.265879\n",
      "576700 0:00:25.267462\n",
      "576800 0:00:25.269323\n",
      "576900 0:00:25.271782\n",
      "get 577000 0:00:25.273790\n",
      "577000 0:00:25.273814\n",
      "577100 0:00:25.275885\n",
      "577200 0:00:25.278109\n",
      "577300 0:00:25.280009\n",
      "577400 0:00:25.282276\n",
      "577500 0:00:25.284214\n",
      "577600 0:00:25.286332\n",
      "577700 0:00:25.288407\n",
      "577800 0:00:25.290296\n",
      "577900 0:00:25.292214\n",
      "get 578000 0:00:25.294192\n",
      "578000 0:00:25.294222\n",
      "578100 0:00:25.296007\n",
      "578200 0:00:25.297872\n",
      "578300 0:00:25.299729\n",
      "578400 0:00:25.301470\n",
      "578500 0:00:25.303333\n",
      "578600 0:00:25.305185\n",
      "578700 0:00:25.307279\n",
      "578800 0:00:25.308867\n",
      "578900 0:00:25.310727\n",
      "get 579000 0:00:25.312178\n",
      "579000 0:00:25.312200\n",
      "579100 0:00:25.314275\n",
      "579200 0:00:25.316027\n",
      "579300 0:00:25.317830\n",
      "579400 0:00:25.319476\n",
      "579500 0:00:25.321139\n",
      "579600 0:00:25.323306\n",
      "579700 0:00:25.325132\n",
      "579800 0:00:25.326932\n",
      "579900 0:00:25.328926\n",
      "get 580000 0:00:25.331049\n",
      "580000 0:00:25.331078\n",
      "580100 0:00:25.332949\n",
      "580200 0:00:25.334629\n",
      "580300 0:00:25.336887\n",
      "580400 0:00:25.338815\n",
      "580500 0:00:25.340599\n",
      "580600 0:00:25.342623\n",
      "580700 0:00:25.344378\n",
      "580800 0:00:25.346005\n",
      "580900 0:00:25.347887\n",
      "get 581000 0:00:25.349437\n",
      "581000 0:00:25.349462\n",
      "581100 0:00:25.351182\n",
      "581200 0:00:25.353062\n",
      "581300 0:00:25.354776\n",
      "581400 0:00:25.356592\n",
      "581500 0:00:25.358378\n",
      "581600 0:00:25.360021\n",
      "581700 0:00:25.361786\n",
      "581800 0:00:25.363484\n",
      "581900 0:00:25.365277\n",
      "get 582000 0:00:25.367130\n",
      "582000 0:00:25.367167\n",
      "582100 0:00:25.368717\n",
      "582200 0:00:25.370682\n",
      "582300 0:00:25.372689\n",
      "582400 0:00:25.374442\n",
      "582500 0:00:25.376149\n",
      "582600 0:00:25.377973\n",
      "582700 0:00:25.379695\n",
      "582800 0:00:25.381570\n",
      "582900 0:00:25.383454\n",
      "get 583000 0:00:25.385238\n",
      "583000 0:00:25.385295\n",
      "583100 0:00:25.387056\n",
      "583200 0:00:25.389002\n",
      "583300 0:00:25.390854\n",
      "583400 0:00:25.392660\n",
      "583500 0:00:25.394228\n",
      "583600 0:00:25.396182\n",
      "583700 0:00:25.398153\n",
      "583800 0:00:25.399998\n",
      "583900 0:00:25.401854\n",
      "get 584000 0:00:25.403718\n",
      "584000 0:00:25.403761\n",
      "584100 0:00:25.405328\n",
      "584200 0:00:25.407138\n",
      "584300 0:00:25.409270\n",
      "584400 0:00:25.411089\n",
      "584500 0:00:25.412651\n",
      "584600 0:00:25.414584\n",
      "584700 0:00:25.416261\n",
      "584800 0:00:25.417917\n",
      "584900 0:00:25.419208\n",
      "get 585000 0:00:25.421419\n",
      "585000 0:00:25.421440\n",
      "585100 0:00:25.423148\n",
      "585200 0:00:25.424562\n",
      "585300 0:00:25.426301\n",
      "585400 0:00:25.427919\n",
      "585500 0:00:25.429590\n",
      "585600 0:00:25.431146\n",
      "585700 0:00:25.432735\n",
      "585800 0:00:25.434317\n",
      "585900 0:00:25.435778\n",
      "get 586000 0:00:25.437594\n",
      "586000 0:00:25.437613\n",
      "586100 0:00:25.439453\n",
      "586200 0:00:25.441286\n",
      "586300 0:00:25.442833\n",
      "586400 0:00:25.444973\n",
      "586500 0:00:25.446703\n",
      "586600 0:00:25.448480\n",
      "586700 0:00:25.450416\n",
      "586800 0:00:25.452055\n",
      "586900 0:00:25.453541\n",
      "get 587000 0:00:25.455418\n",
      "587000 0:00:25.455438\n",
      "587100 0:00:25.457529\n",
      "587200 0:00:25.459326\n",
      "587300 0:00:25.460968\n",
      "587400 0:00:25.462555\n",
      "587500 0:00:25.464340\n",
      "587600 0:00:25.466064\n",
      "587700 0:00:25.467674\n",
      "587800 0:00:25.469233\n",
      "587900 0:00:25.471329\n",
      "get 588000 0:00:25.472923\n",
      "588000 0:00:25.472942\n",
      "588100 0:00:25.474533\n",
      "588200 0:00:25.476167\n",
      "588300 0:00:25.478336\n",
      "588400 0:00:25.480271\n",
      "588500 0:00:25.481934\n",
      "588600 0:00:25.483437\n",
      "588700 0:00:25.485313\n",
      "588800 0:00:25.486866\n",
      "588900 0:00:25.488698\n",
      "get 589000 0:00:25.491155\n",
      "589000 0:00:25.491177\n",
      "589100 0:00:25.492801\n",
      "589200 0:00:25.494747\n",
      "589300 0:00:25.496352\n",
      "589400 0:00:25.498092\n",
      "589500 0:00:25.499923\n",
      "589600 0:00:25.501751\n",
      "589700 0:00:25.503635\n",
      "589800 0:00:25.505017\n",
      "589900 0:00:25.506909\n",
      "get 590000 0:00:25.508619\n",
      "590000 0:00:25.508640\n",
      "590100 0:00:25.510414\n",
      "590200 0:00:25.512154\n",
      "590300 0:00:25.513657\n",
      "590400 0:00:25.515165\n",
      "590500 0:00:25.516651\n",
      "590600 0:00:25.518743\n",
      "590700 0:00:25.520138\n",
      "590800 0:00:25.522184\n",
      "590900 0:00:25.524457\n",
      "get 591000 0:00:25.526237\n",
      "591000 0:00:25.526259\n",
      "591100 0:00:25.527841\n",
      "591200 0:00:25.529850\n",
      "591300 0:00:25.531308\n",
      "591400 0:00:25.533466\n",
      "591500 0:00:25.535528\n",
      "591600 0:00:25.537182\n",
      "591700 0:00:25.538714\n",
      "591800 0:00:25.540262\n",
      "591900 0:00:25.542038\n",
      "get 592000 0:00:25.543512\n",
      "592000 0:00:25.543532\n",
      "592100 0:00:25.545874\n",
      "592200 0:00:25.547709\n",
      "592300 0:00:25.549127\n",
      "592400 0:00:25.551402\n",
      "592500 0:00:25.553206\n",
      "592600 0:00:25.555219\n",
      "592700 0:00:25.557079\n",
      "592800 0:00:25.559250\n",
      "592900 0:00:25.561308\n",
      "get 593000 0:00:25.563232\n",
      "593000 0:00:25.563255\n",
      "593100 0:00:25.565381\n",
      "593200 0:00:25.567443\n",
      "593300 0:00:25.569277\n",
      "593400 0:00:25.570991\n",
      "593500 0:00:25.572899\n",
      "593600 0:00:25.574369\n",
      "593700 0:00:25.576056\n",
      "593800 0:00:25.578271\n",
      "593900 0:00:25.580145\n",
      "get 594000 0:00:25.582096\n",
      "594000 0:00:25.582122\n",
      "594100 0:00:25.584180\n",
      "594200 0:00:25.585943\n",
      "594300 0:00:25.587986\n",
      "594400 0:00:25.589892\n",
      "594500 0:00:25.591894\n",
      "594600 0:00:25.593869\n",
      "594700 0:00:25.595647\n",
      "594800 0:00:25.597448\n",
      "594900 0:00:25.599455\n",
      "get 595000 0:00:25.601425\n",
      "595000 0:00:25.601480\n",
      "595100 0:00:25.603479\n",
      "595200 0:00:25.605169\n",
      "595300 0:00:25.607200\n",
      "595400 0:00:25.609210\n",
      "595500 0:00:25.611123\n",
      "595600 0:00:25.612776\n",
      "595700 0:00:25.614570\n",
      "595800 0:00:25.616668\n",
      "595900 0:00:25.618592\n",
      "get 596000 0:00:25.620659\n",
      "596000 0:00:25.620684\n",
      "596100 0:00:25.622649\n",
      "596200 0:00:25.624446\n",
      "596300 0:00:25.626225\n",
      "596400 0:00:25.628229\n",
      "596500 0:00:25.630057\n",
      "596600 0:00:25.631764\n",
      "596700 0:00:25.633505\n",
      "596800 0:00:25.635101\n",
      "596900 0:00:25.636665\n",
      "get 597000 0:00:25.638490\n",
      "597000 0:00:25.638514\n",
      "597100 0:00:25.640169\n",
      "597200 0:00:25.641814\n",
      "597300 0:00:25.643525\n",
      "597400 0:00:25.645239\n",
      "597500 0:00:25.647115\n",
      "597600 0:00:25.648857\n",
      "597700 0:00:25.650643\n",
      "597800 0:00:25.652426\n",
      "597900 0:00:25.654208\n",
      "get 598000 0:00:25.656030\n",
      "598000 0:00:25.656065\n",
      "598100 0:00:25.657544\n",
      "598200 0:00:25.659078\n",
      "598300 0:00:25.660358\n",
      "598400 0:00:25.662127\n",
      "598500 0:00:25.664161\n",
      "598600 0:00:25.666080\n",
      "598700 0:00:25.667894\n",
      "598800 0:00:25.669703\n",
      "598900 0:00:25.671624\n",
      "get 599000 0:00:25.673487\n",
      "599000 0:00:25.673516\n",
      "599100 0:00:25.675103\n",
      "599200 0:00:25.676978\n",
      "599300 0:00:25.678812\n",
      "599400 0:00:25.680686\n",
      "599500 0:00:25.682759\n",
      "599600 0:00:25.684600\n",
      "599700 0:00:25.686474\n",
      "599800 0:00:25.688444\n",
      "599900 0:00:25.690212\n",
      "get 600000 0:00:28.440301\n",
      "600000 0:00:28.440658\n",
      "600100 0:00:28.443321\n",
      "600200 0:00:28.445249\n",
      "600300 0:00:28.447127\n",
      "600400 0:00:28.448932\n",
      "600500 0:00:28.450758\n",
      "600600 0:00:28.452617\n",
      "600700 0:00:28.454424\n",
      "600800 0:00:28.456187\n",
      "600900 0:00:28.457915\n",
      "get 601000 0:00:28.459720\n",
      "601000 0:00:28.459743\n",
      "601100 0:00:28.461581\n",
      "601200 0:00:28.463275\n",
      "601300 0:00:28.464972\n",
      "601400 0:00:28.466559\n",
      "601500 0:00:28.468596\n",
      "601600 0:00:28.470389\n",
      "601700 0:00:28.471750\n",
      "601800 0:00:28.473551\n",
      "601900 0:00:28.475265\n",
      "get 602000 0:00:28.477202\n",
      "602000 0:00:28.477231\n",
      "602100 0:00:28.479075\n",
      "602200 0:00:28.480795\n",
      "602300 0:00:28.482732\n",
      "602400 0:00:28.484524\n",
      "602500 0:00:28.486349\n",
      "602600 0:00:28.488073\n",
      "602700 0:00:28.489831\n",
      "602800 0:00:28.491604\n",
      "602900 0:00:28.493168\n",
      "get 603000 0:00:28.494950\n",
      "603000 0:00:28.494972\n",
      "603100 0:00:28.496885\n",
      "603200 0:00:28.498682\n",
      "603300 0:00:28.500698\n",
      "603400 0:00:28.502491\n",
      "603500 0:00:28.504377\n",
      "603600 0:00:28.506149\n",
      "603700 0:00:28.508034\n",
      "603800 0:00:28.509711\n",
      "603900 0:00:28.511537\n",
      "get 604000 0:00:28.513308\n",
      "604000 0:00:28.513331\n",
      "604100 0:00:28.515095\n",
      "604200 0:00:28.516794\n",
      "604300 0:00:28.518776\n",
      "604400 0:00:28.520519\n",
      "604500 0:00:28.522382\n",
      "604600 0:00:28.524377\n",
      "604700 0:00:28.526212\n",
      "604800 0:00:28.528150\n",
      "604900 0:00:28.529883\n",
      "get 605000 0:00:28.531424\n",
      "605000 0:00:28.531451\n",
      "605100 0:00:28.533001\n",
      "605200 0:00:28.534866\n",
      "605300 0:00:28.536799\n",
      "605400 0:00:28.538603\n",
      "605500 0:00:28.540430\n",
      "605600 0:00:28.542164\n",
      "605700 0:00:28.544082\n",
      "605800 0:00:28.545954\n",
      "605900 0:00:28.547809\n",
      "get 606000 0:00:28.549444\n",
      "606000 0:00:28.549465\n",
      "606100 0:00:28.551405\n",
      "606200 0:00:28.553394\n",
      "606300 0:00:28.554889\n",
      "606400 0:00:28.556526\n",
      "606500 0:00:28.558418\n",
      "606600 0:00:28.560232\n",
      "606700 0:00:28.561930\n",
      "606800 0:00:28.563853\n",
      "606900 0:00:28.565676\n",
      "get 607000 0:00:28.567412\n",
      "607000 0:00:28.567445\n",
      "607100 0:00:28.569214\n",
      "607200 0:00:28.571125\n",
      "607300 0:00:28.572930\n",
      "607400 0:00:28.574839\n",
      "607500 0:00:28.576874\n",
      "607600 0:00:28.578462\n",
      "607700 0:00:28.580086\n",
      "607800 0:00:28.581869\n",
      "607900 0:00:28.583885\n",
      "get 608000 0:00:28.585563\n",
      "608000 0:00:28.585586\n",
      "608100 0:00:28.587598\n",
      "608200 0:00:28.589574\n",
      "608300 0:00:28.591408\n",
      "608400 0:00:28.593271\n",
      "608500 0:00:28.595035\n",
      "608600 0:00:28.596751\n",
      "608700 0:00:28.598519\n",
      "608800 0:00:28.600321\n",
      "608900 0:00:28.602329\n",
      "get 609000 0:00:28.604275\n",
      "609000 0:00:28.604298\n",
      "609100 0:00:28.605999\n",
      "609200 0:00:28.607916\n",
      "609300 0:00:28.609470\n",
      "609400 0:00:28.611577\n",
      "609500 0:00:28.613216\n",
      "609600 0:00:28.615323\n",
      "609700 0:00:28.616978\n",
      "609800 0:00:28.618626\n",
      "609900 0:00:28.620430\n",
      "get 610000 0:00:28.622243\n",
      "610000 0:00:28.622271\n",
      "610100 0:00:28.624041\n",
      "610200 0:00:28.625853\n",
      "610300 0:00:28.627560\n",
      "610400 0:00:28.629502\n",
      "610500 0:00:28.631324\n",
      "610600 0:00:28.633110\n",
      "610700 0:00:28.634920\n",
      "610800 0:00:28.636753\n",
      "610900 0:00:28.638739\n",
      "get 611000 0:00:28.640451\n",
      "611000 0:00:28.640517\n",
      "611100 0:00:28.642751\n",
      "611200 0:00:28.644508\n",
      "611300 0:00:28.646157\n",
      "611400 0:00:28.647961\n",
      "611500 0:00:28.649619\n",
      "611600 0:00:28.651575\n",
      "611700 0:00:28.653608\n",
      "611800 0:00:28.655518\n",
      "611900 0:00:28.657276\n",
      "get 612000 0:00:28.658987\n",
      "612000 0:00:28.659011\n",
      "612100 0:00:28.660761\n",
      "612200 0:00:28.662650\n",
      "612300 0:00:28.664265\n",
      "612400 0:00:28.666064\n",
      "612500 0:00:28.667762\n",
      "612600 0:00:28.669726\n",
      "612700 0:00:28.671483\n",
      "612800 0:00:28.673157\n",
      "612900 0:00:28.674884\n",
      "get 613000 0:00:28.676448\n",
      "613000 0:00:28.676469\n",
      "613100 0:00:28.678347\n",
      "613200 0:00:28.680440\n",
      "613300 0:00:28.682322\n",
      "613400 0:00:28.683978\n",
      "613500 0:00:28.686122\n",
      "613600 0:00:28.688040\n",
      "613700 0:00:28.690110\n",
      "613800 0:00:28.691967\n",
      "613900 0:00:28.693975\n",
      "get 614000 0:00:28.695545\n",
      "614000 0:00:28.695574\n",
      "614100 0:00:28.697539\n",
      "614200 0:00:28.699591\n",
      "614300 0:00:28.701407\n",
      "614400 0:00:28.703407\n",
      "614500 0:00:28.704943\n",
      "614600 0:00:28.706529\n",
      "614700 0:00:28.708576\n",
      "614800 0:00:28.710310\n",
      "614900 0:00:28.712094\n",
      "get 615000 0:00:28.713829\n",
      "615000 0:00:28.713886\n",
      "615100 0:00:28.715628\n",
      "615200 0:00:28.717312\n",
      "615300 0:00:28.719090\n",
      "615400 0:00:28.720801\n",
      "615500 0:00:28.722508\n",
      "615600 0:00:28.724411\n",
      "615700 0:00:28.725938\n",
      "615800 0:00:28.727602\n",
      "615900 0:00:28.729294\n",
      "get 616000 0:00:28.731009\n",
      "616000 0:00:28.731034\n",
      "616100 0:00:28.732521\n",
      "616200 0:00:28.734432\n",
      "616300 0:00:28.736043\n",
      "616400 0:00:28.737688\n",
      "616500 0:00:28.739290\n",
      "616600 0:00:28.740922\n",
      "616700 0:00:28.742503\n",
      "616800 0:00:28.744128\n",
      "616900 0:00:28.745470\n",
      "get 617000 0:00:28.746955\n",
      "617000 0:00:28.746980\n",
      "617100 0:00:28.748809\n",
      "617200 0:00:28.750537\n",
      "617300 0:00:28.752396\n",
      "617400 0:00:28.754115\n",
      "617500 0:00:28.755943\n",
      "617600 0:00:28.757644\n",
      "617700 0:00:28.759307\n",
      "617800 0:00:28.760982\n",
      "617900 0:00:28.762474\n",
      "get 618000 0:00:28.764131\n",
      "618000 0:00:28.764156\n",
      "618100 0:00:28.765866\n",
      "618200 0:00:28.767666\n",
      "618300 0:00:28.769232\n",
      "618400 0:00:28.771335\n",
      "618500 0:00:28.773114\n",
      "618600 0:00:28.774842\n",
      "618700 0:00:28.776615\n",
      "618800 0:00:28.778590\n",
      "618900 0:00:28.780170\n",
      "get 619000 0:00:28.781785\n",
      "619000 0:00:28.781865\n",
      "619100 0:00:28.783471\n",
      "619200 0:00:28.784960\n",
      "619300 0:00:28.786711\n",
      "619400 0:00:28.788736\n",
      "619500 0:00:28.790140\n",
      "619600 0:00:28.791352\n",
      "619700 0:00:28.792762\n",
      "619800 0:00:28.794579\n",
      "619900 0:00:28.795979\n",
      "get 620000 0:00:28.797414\n",
      "620000 0:00:28.797436\n",
      "620100 0:00:28.798833\n",
      "620200 0:00:28.800191\n",
      "620300 0:00:28.801768\n",
      "620400 0:00:28.803308\n",
      "620500 0:00:28.804729\n",
      "620600 0:00:28.806710\n",
      "620700 0:00:28.808565\n",
      "620800 0:00:28.810505\n",
      "620900 0:00:28.812381\n",
      "get 621000 0:00:28.813871\n",
      "621000 0:00:28.813894\n",
      "621100 0:00:28.815288\n",
      "621200 0:00:28.816741\n",
      "621300 0:00:28.818691\n",
      "621400 0:00:28.819983\n",
      "621500 0:00:28.821605\n",
      "621600 0:00:28.823307\n",
      "621700 0:00:28.825065\n",
      "621800 0:00:28.826662\n",
      "621900 0:00:28.828224\n",
      "get 622000 0:00:28.829997\n",
      "622000 0:00:28.830025\n",
      "622100 0:00:28.831335\n",
      "622200 0:00:28.832745\n",
      "622300 0:00:28.833921\n",
      "622400 0:00:28.835706\n",
      "622500 0:00:28.837614\n",
      "622600 0:00:28.839464\n",
      "622700 0:00:28.841667\n",
      "622800 0:00:28.843616\n",
      "622900 0:00:28.845230\n",
      "get 623000 0:00:28.846771\n",
      "623000 0:00:28.846795\n",
      "623100 0:00:28.848682\n",
      "623200 0:00:28.850171\n",
      "623300 0:00:28.851416\n",
      "623400 0:00:28.853123\n",
      "623500 0:00:28.854759\n",
      "623600 0:00:28.856684\n",
      "623700 0:00:28.858694\n",
      "623800 0:00:28.860482\n",
      "623900 0:00:28.862389\n",
      "get 624000 0:00:28.864148\n",
      "624000 0:00:28.864186\n",
      "624100 0:00:28.866107\n",
      "624200 0:00:28.868035\n",
      "624300 0:00:28.869552\n",
      "624400 0:00:28.871142\n",
      "624500 0:00:28.873003\n",
      "624600 0:00:28.874927\n",
      "624700 0:00:28.876858\n",
      "624800 0:00:28.878710\n",
      "624900 0:00:28.880452\n",
      "get 625000 0:00:28.882260\n",
      "625000 0:00:28.882294\n",
      "625100 0:00:28.884117\n",
      "625200 0:00:28.885666\n",
      "625300 0:00:28.887315\n",
      "625400 0:00:28.889402\n",
      "625500 0:00:28.891062\n",
      "625600 0:00:28.892850\n",
      "625700 0:00:28.894632\n",
      "625800 0:00:28.896276\n",
      "625900 0:00:28.898082\n",
      "get 626000 0:00:28.899893\n",
      "626000 0:00:28.899916\n",
      "626100 0:00:28.901479\n",
      "626200 0:00:28.903213\n",
      "626300 0:00:28.904598\n",
      "626400 0:00:28.906706\n",
      "626500 0:00:28.908405\n",
      "626600 0:00:28.910136\n",
      "626700 0:00:28.911776\n",
      "626800 0:00:28.913478\n",
      "626900 0:00:28.915153\n",
      "get 627000 0:00:28.916456\n",
      "627000 0:00:28.916478\n",
      "627100 0:00:28.918177\n",
      "627200 0:00:28.919764\n",
      "627300 0:00:28.921439\n",
      "627400 0:00:28.923511\n",
      "627500 0:00:28.925482\n",
      "627600 0:00:28.927329\n",
      "627700 0:00:28.928861\n",
      "627800 0:00:28.930618\n",
      "627900 0:00:28.932189\n",
      "get 628000 0:00:28.933967\n",
      "628000 0:00:28.933988\n",
      "628100 0:00:28.936055\n",
      "628200 0:00:28.937949\n",
      "628300 0:00:28.939757\n",
      "628400 0:00:28.941799\n",
      "628500 0:00:28.943730\n",
      "628600 0:00:28.945543\n",
      "628700 0:00:28.947320\n",
      "628800 0:00:28.949097\n",
      "628900 0:00:28.950986\n",
      "get 629000 0:00:28.952463\n",
      "629000 0:00:28.952485\n",
      "629100 0:00:28.954015\n",
      "629200 0:00:28.955757\n",
      "629300 0:00:28.957236\n",
      "629400 0:00:28.958727\n",
      "629500 0:00:28.960173\n",
      "629600 0:00:28.961820\n",
      "629700 0:00:28.963111\n",
      "629800 0:00:28.964626\n",
      "629900 0:00:28.966429\n",
      "get 630000 0:00:28.967908\n",
      "630000 0:00:28.967938\n",
      "630100 0:00:28.970059\n",
      "630200 0:00:28.971730\n",
      "630300 0:00:28.973806\n",
      "630400 0:00:28.975641\n",
      "630500 0:00:28.977283\n",
      "630600 0:00:28.978415\n",
      "630700 0:00:28.980349\n",
      "630800 0:00:28.981722\n",
      "630900 0:00:28.983652\n",
      "get 631000 0:00:28.985250\n",
      "631000 0:00:28.985277\n",
      "631100 0:00:28.986720\n",
      "631200 0:00:28.988177\n",
      "631300 0:00:28.989815\n",
      "631400 0:00:28.991542\n",
      "631500 0:00:28.993768\n",
      "631600 0:00:28.995520\n",
      "631700 0:00:28.996977\n",
      "631800 0:00:28.998515\n",
      "631900 0:00:29.000303\n",
      "get 632000 0:00:29.002050\n",
      "632000 0:00:29.002073\n",
      "632100 0:00:29.003574\n",
      "632200 0:00:29.005007\n",
      "632300 0:00:29.006886\n",
      "632400 0:00:29.008510\n",
      "632500 0:00:29.010275\n",
      "632600 0:00:29.011697\n",
      "632700 0:00:29.013284\n",
      "632800 0:00:29.014759\n",
      "632900 0:00:29.016651\n",
      "get 633000 0:00:29.018299\n",
      "633000 0:00:29.018321\n",
      "633100 0:00:29.020107\n",
      "633200 0:00:29.021643\n",
      "633300 0:00:29.023082\n",
      "633400 0:00:29.025174\n",
      "633500 0:00:29.027076\n",
      "633600 0:00:29.029107\n",
      "633700 0:00:29.030459\n",
      "633800 0:00:29.032151\n",
      "633900 0:00:29.033691\n",
      "get 634000 0:00:29.035129\n",
      "634000 0:00:29.035201\n",
      "634100 0:00:29.036665\n",
      "634200 0:00:29.038235\n",
      "634300 0:00:29.039915\n",
      "634400 0:00:29.041422\n",
      "634500 0:00:29.042991\n",
      "634600 0:00:29.044769\n",
      "634700 0:00:29.046746\n",
      "634800 0:00:29.048260\n",
      "634900 0:00:29.050108\n",
      "get 635000 0:00:29.052201\n",
      "635000 0:00:29.052235\n",
      "635100 0:00:29.053890\n",
      "635200 0:00:29.056149\n",
      "635300 0:00:29.057762\n",
      "635400 0:00:29.060284\n",
      "635500 0:00:29.062023\n",
      "635600 0:00:29.063747\n",
      "635700 0:00:29.065912\n",
      "635800 0:00:29.067716\n",
      "635900 0:00:29.069437\n",
      "get 636000 0:00:29.071000\n",
      "636000 0:00:29.071027\n",
      "636100 0:00:29.072766\n",
      "636200 0:00:29.074618\n",
      "636300 0:00:29.076503\n",
      "636400 0:00:29.078258\n",
      "636500 0:00:29.080012\n",
      "636600 0:00:29.081697\n",
      "636700 0:00:29.083927\n",
      "636800 0:00:29.086049\n",
      "636900 0:00:29.087888\n",
      "get 637000 0:00:29.089640\n",
      "637000 0:00:29.089672\n",
      "637100 0:00:29.091556\n",
      "637200 0:00:29.093405\n",
      "637300 0:00:29.095217\n",
      "637400 0:00:29.096951\n",
      "637500 0:00:29.098954\n",
      "637600 0:00:29.100665\n",
      "637700 0:00:29.102610\n",
      "637800 0:00:29.104220\n",
      "637900 0:00:29.105884\n",
      "get 638000 0:00:29.107823\n",
      "638000 0:00:29.107857\n",
      "638100 0:00:29.109697\n",
      "638200 0:00:29.111367\n",
      "638300 0:00:29.113302\n",
      "638400 0:00:29.115003\n",
      "638500 0:00:29.116800\n",
      "638600 0:00:29.118658\n",
      "638700 0:00:29.120372\n",
      "638800 0:00:29.122265\n",
      "638900 0:00:29.123999\n",
      "get 639000 0:00:29.125929\n",
      "639000 0:00:29.125965\n",
      "639100 0:00:29.127659\n",
      "639200 0:00:29.129605\n",
      "639300 0:00:29.131522\n",
      "639400 0:00:29.132956\n",
      "639500 0:00:29.134439\n",
      "639600 0:00:29.136499\n",
      "639700 0:00:29.138164\n",
      "639800 0:00:29.139883\n",
      "639900 0:00:29.141785\n",
      "get 640000 0:00:29.143384\n",
      "640000 0:00:29.143408\n",
      "640100 0:00:29.145009\n",
      "640200 0:00:29.146434\n",
      "640300 0:00:29.148294\n",
      "640400 0:00:29.150186\n",
      "640500 0:00:29.151930\n",
      "640600 0:00:29.153830\n",
      "640700 0:00:29.155540\n",
      "640800 0:00:29.157379\n",
      "640900 0:00:29.158925\n",
      "get 641000 0:00:29.160960\n",
      "641000 0:00:29.160985\n",
      "641100 0:00:29.162624\n",
      "641200 0:00:29.164274\n",
      "641300 0:00:29.166223\n",
      "641400 0:00:29.167974\n",
      "641500 0:00:29.169880\n",
      "641600 0:00:29.171586\n",
      "641700 0:00:29.173503\n",
      "641800 0:00:29.175230\n",
      "641900 0:00:29.176932\n",
      "get 642000 0:00:29.178825\n",
      "642000 0:00:29.178849\n",
      "642100 0:00:29.180782\n",
      "642200 0:00:29.182581\n",
      "642300 0:00:29.184342\n",
      "642400 0:00:29.186109\n",
      "642500 0:00:29.187861\n",
      "642600 0:00:29.189738\n",
      "642700 0:00:29.191270\n",
      "642800 0:00:29.193123\n",
      "642900 0:00:29.195007\n",
      "get 643000 0:00:29.196942\n",
      "643000 0:00:29.196980\n",
      "643100 0:00:29.198849\n",
      "643200 0:00:29.200445\n",
      "643300 0:00:29.202098\n",
      "643400 0:00:29.203690\n",
      "643500 0:00:29.205271\n",
      "643600 0:00:29.207033\n",
      "643700 0:00:29.208792\n",
      "643800 0:00:29.210622\n",
      "643900 0:00:29.212703\n",
      "get 644000 0:00:29.214720\n",
      "644000 0:00:29.214755\n",
      "644100 0:00:29.216512\n",
      "644200 0:00:29.218368\n",
      "644300 0:00:29.220035\n",
      "644400 0:00:29.221825\n",
      "644500 0:00:29.223677\n",
      "644600 0:00:29.225596\n",
      "644700 0:00:29.227401\n",
      "644800 0:00:29.229246\n",
      "644900 0:00:29.230793\n",
      "get 645000 0:00:29.232705\n",
      "645000 0:00:29.232731\n",
      "645100 0:00:29.234560\n",
      "645200 0:00:29.236268\n",
      "645300 0:00:29.237989\n",
      "645400 0:00:29.239612\n",
      "645500 0:00:29.241369\n",
      "645600 0:00:29.243208\n",
      "645700 0:00:29.244841\n",
      "645800 0:00:29.247074\n",
      "645900 0:00:29.248856\n",
      "get 646000 0:00:29.250531\n",
      "646000 0:00:29.250558\n",
      "646100 0:00:29.252348\n",
      "646200 0:00:29.254152\n",
      "646300 0:00:29.255813\n",
      "646400 0:00:29.257780\n",
      "646500 0:00:29.259715\n",
      "646600 0:00:29.261327\n",
      "646700 0:00:29.262947\n",
      "646800 0:00:29.264889\n",
      "646900 0:00:29.266480\n",
      "get 647000 0:00:29.268260\n",
      "647000 0:00:29.268284\n",
      "647100 0:00:29.270260\n",
      "647200 0:00:29.272422\n",
      "647300 0:00:29.274411\n",
      "647400 0:00:29.276275\n",
      "647500 0:00:29.278090\n",
      "647600 0:00:29.279559\n",
      "647700 0:00:29.281404\n",
      "647800 0:00:29.283377\n",
      "647900 0:00:29.285092\n",
      "get 648000 0:00:29.287131\n",
      "648000 0:00:29.287160\n",
      "648100 0:00:29.288538\n",
      "648200 0:00:29.290389\n",
      "648300 0:00:29.292262\n",
      "648400 0:00:29.294345\n",
      "648500 0:00:29.296001\n",
      "648600 0:00:29.297610\n",
      "648700 0:00:29.299349\n",
      "648800 0:00:29.300893\n",
      "648900 0:00:29.303233\n",
      "get 649000 0:00:29.305187\n",
      "649000 0:00:29.305214\n",
      "649100 0:00:29.307107\n",
      "649200 0:00:29.308942\n",
      "649300 0:00:29.310621\n",
      "649400 0:00:29.312771\n",
      "649500 0:00:29.314528\n",
      "649600 0:00:29.316186\n",
      "649700 0:00:29.317856\n",
      "649800 0:00:29.319320\n",
      "649900 0:00:29.321298\n",
      "get 650000 0:00:29.322900\n",
      "650000 0:00:29.322926\n",
      "650100 0:00:29.324452\n",
      "650200 0:00:29.326243\n",
      "650300 0:00:29.328171\n",
      "650400 0:00:29.330312\n",
      "650500 0:00:29.331945\n",
      "650600 0:00:29.333880\n",
      "650700 0:00:29.335415\n",
      "650800 0:00:29.337111\n",
      "650900 0:00:29.339120\n",
      "get 651000 0:00:29.340599\n",
      "651000 0:00:29.340624\n",
      "651100 0:00:29.342213\n",
      "651200 0:00:29.344088\n",
      "651300 0:00:29.346204\n",
      "651400 0:00:29.347857\n",
      "651500 0:00:29.349350\n",
      "651600 0:00:29.351364\n",
      "651700 0:00:29.353329\n",
      "651800 0:00:29.355141\n",
      "651900 0:00:29.356906\n",
      "get 652000 0:00:29.358796\n",
      "652000 0:00:29.358852\n",
      "652100 0:00:29.360699\n",
      "652200 0:00:29.362378\n",
      "652300 0:00:29.364260\n",
      "652400 0:00:29.366256\n",
      "652500 0:00:29.368091\n",
      "652600 0:00:29.369932\n",
      "652700 0:00:29.371747\n",
      "652800 0:00:29.373156\n",
      "652900 0:00:29.374938\n",
      "get 653000 0:00:29.376467\n",
      "653000 0:00:29.376585\n",
      "653100 0:00:29.378487\n",
      "653200 0:00:29.379911\n",
      "653300 0:00:29.381838\n",
      "653400 0:00:29.383812\n",
      "653500 0:00:29.385941\n",
      "653600 0:00:29.387733\n",
      "653700 0:00:29.389290\n",
      "653800 0:00:29.390909\n",
      "653900 0:00:29.392913\n",
      "get 654000 0:00:29.394399\n",
      "654000 0:00:29.394436\n",
      "654100 0:00:29.396061\n",
      "654200 0:00:29.397925\n",
      "654300 0:00:29.399931\n",
      "654400 0:00:29.402123\n",
      "654500 0:00:29.403939\n",
      "654600 0:00:29.405917\n",
      "654700 0:00:29.407807\n",
      "654800 0:00:29.409612\n",
      "654900 0:00:29.411358\n",
      "get 655000 0:00:29.413220\n",
      "655000 0:00:29.413242\n",
      "655100 0:00:29.415052\n",
      "655200 0:00:29.416936\n",
      "655300 0:00:29.418391\n",
      "655400 0:00:29.420111\n",
      "655500 0:00:29.421493\n",
      "655600 0:00:29.423713\n",
      "655700 0:00:29.425301\n",
      "655800 0:00:29.426840\n",
      "655900 0:00:29.428910\n",
      "get 656000 0:00:29.430497\n",
      "656000 0:00:29.430516\n",
      "656100 0:00:29.432570\n",
      "656200 0:00:29.434428\n",
      "656300 0:00:29.436227\n",
      "656400 0:00:29.437900\n",
      "656500 0:00:29.439462\n",
      "656600 0:00:29.440998\n",
      "656700 0:00:29.442858\n",
      "656800 0:00:29.444355\n",
      "656900 0:00:29.445949\n",
      "get 657000 0:00:29.448079\n",
      "657000 0:00:29.448232\n",
      "657100 0:00:29.449929\n",
      "657200 0:00:29.451709\n",
      "657300 0:00:29.453214\n",
      "657400 0:00:29.454735\n",
      "657500 0:00:29.456488\n",
      "657600 0:00:29.457968\n",
      "657700 0:00:29.459612\n",
      "657800 0:00:29.461475\n",
      "657900 0:00:29.463175\n",
      "get 658000 0:00:29.464742\n",
      "658000 0:00:29.464760\n",
      "658100 0:00:29.466084\n",
      "658200 0:00:29.467644\n",
      "658300 0:00:29.469569\n",
      "658400 0:00:29.470984\n",
      "658500 0:00:29.472806\n",
      "658600 0:00:29.474475\n",
      "658700 0:00:29.476805\n",
      "658800 0:00:29.478474\n",
      "658900 0:00:29.479753\n",
      "get 659000 0:00:29.481080\n",
      "659000 0:00:29.481126\n",
      "659100 0:00:29.482654\n",
      "659200 0:00:29.484251\n",
      "659300 0:00:29.485898\n",
      "659400 0:00:29.487502\n",
      "659500 0:00:29.488958\n",
      "659600 0:00:29.490814\n",
      "659700 0:00:29.492469\n",
      "659800 0:00:29.494204\n",
      "659900 0:00:29.496027\n",
      "get 660000 0:00:29.497515\n",
      "660000 0:00:29.497533\n",
      "660100 0:00:29.499728\n",
      "660200 0:00:29.501681\n",
      "660300 0:00:29.503148\n",
      "660400 0:00:29.505115\n",
      "660500 0:00:29.507077\n",
      "660600 0:00:29.508760\n",
      "660700 0:00:29.510167\n",
      "660800 0:00:29.511629\n",
      "660900 0:00:29.513424\n",
      "get 661000 0:00:29.514889\n",
      "661000 0:00:29.514907\n",
      "661100 0:00:29.516279\n",
      "661200 0:00:29.517708\n",
      "661300 0:00:29.519330\n",
      "661400 0:00:29.520882\n",
      "661500 0:00:29.522552\n",
      "661600 0:00:29.524249\n",
      "661700 0:00:29.525666\n",
      "661800 0:00:29.527936\n",
      "661900 0:00:29.529409\n",
      "get 662000 0:00:29.530915\n",
      "662000 0:00:29.530957\n",
      "662100 0:00:29.532913\n",
      "662200 0:00:29.534949\n",
      "662300 0:00:29.536518\n",
      "662400 0:00:29.538369\n",
      "662500 0:00:29.540596\n",
      "662600 0:00:29.542516\n",
      "662700 0:00:29.544033\n",
      "662800 0:00:29.545823\n",
      "662900 0:00:29.547384\n",
      "get 663000 0:00:29.548986\n",
      "663000 0:00:29.549004\n",
      "663100 0:00:29.550563\n",
      "663200 0:00:29.551903\n",
      "663300 0:00:29.553809\n",
      "663400 0:00:29.555636\n",
      "663500 0:00:29.557283\n",
      "663600 0:00:29.559057\n",
      "663700 0:00:29.560944\n",
      "663800 0:00:29.562439\n",
      "663900 0:00:29.564366\n",
      "get 664000 0:00:29.566216\n",
      "664000 0:00:29.566235\n",
      "664100 0:00:29.567854\n",
      "664200 0:00:29.569569\n",
      "664300 0:00:29.571347\n",
      "664400 0:00:29.573547\n",
      "664500 0:00:29.575058\n",
      "664600 0:00:29.576809\n",
      "664700 0:00:29.578373\n",
      "664800 0:00:29.580193\n",
      "664900 0:00:29.581831\n",
      "get 665000 0:00:29.583456\n",
      "665000 0:00:29.583477\n",
      "665100 0:00:29.585579\n",
      "665200 0:00:29.587348\n",
      "665300 0:00:29.589054\n",
      "665400 0:00:29.590995\n",
      "665500 0:00:29.592402\n",
      "665600 0:00:29.593982\n",
      "665700 0:00:29.595763\n",
      "665800 0:00:29.597528\n",
      "665900 0:00:29.599466\n",
      "get 666000 0:00:29.601290\n",
      "666000 0:00:29.601309\n",
      "666100 0:00:29.602890\n",
      "666200 0:00:29.604506\n",
      "666300 0:00:29.606245\n",
      "666400 0:00:29.608096\n",
      "666500 0:00:29.609461\n",
      "666600 0:00:29.611361\n",
      "666700 0:00:29.612915\n",
      "666800 0:00:29.615077\n",
      "666900 0:00:29.616742\n",
      "get 667000 0:00:29.618937\n",
      "667000 0:00:29.618972\n",
      "667100 0:00:29.620881\n",
      "667200 0:00:29.622564\n",
      "667300 0:00:29.624066\n",
      "667400 0:00:29.625707\n",
      "667500 0:00:29.627522\n",
      "667600 0:00:29.629015\n",
      "667700 0:00:29.631162\n",
      "667800 0:00:29.632883\n",
      "667900 0:00:29.634795\n",
      "get 668000 0:00:29.636762\n",
      "668000 0:00:29.636781\n",
      "668100 0:00:29.638740\n",
      "668200 0:00:29.640323\n",
      "668300 0:00:29.642262\n",
      "668400 0:00:29.643965\n",
      "668500 0:00:29.645951\n",
      "668600 0:00:29.647647\n",
      "668700 0:00:29.649844\n",
      "668800 0:00:29.651604\n",
      "668900 0:00:29.653254\n",
      "get 669000 0:00:29.655310\n",
      "669000 0:00:29.655332\n",
      "669100 0:00:29.657235\n",
      "669200 0:00:29.659053\n",
      "669300 0:00:29.660803\n",
      "669400 0:00:29.662662\n",
      "669500 0:00:29.664536\n",
      "669600 0:00:29.666506\n",
      "669700 0:00:29.668369\n",
      "669800 0:00:29.670171\n",
      "669900 0:00:29.671980\n",
      "get 670000 0:00:29.673669\n",
      "670000 0:00:29.673694\n",
      "670100 0:00:29.675510\n",
      "670200 0:00:29.677119\n",
      "670300 0:00:29.678692\n",
      "670400 0:00:29.680755\n",
      "670500 0:00:29.682541\n",
      "670600 0:00:29.684305\n",
      "670700 0:00:29.686171\n",
      "670800 0:00:29.687768\n",
      "670900 0:00:29.689446\n",
      "get 671000 0:00:29.691100\n",
      "671000 0:00:29.691130\n",
      "671100 0:00:29.692772\n",
      "671200 0:00:29.694396\n",
      "671300 0:00:29.696453\n",
      "671400 0:00:29.698135\n",
      "671500 0:00:29.699630\n",
      "671600 0:00:29.701475\n",
      "671700 0:00:29.703130\n",
      "671800 0:00:29.704795\n",
      "671900 0:00:29.706297\n",
      "get 672000 0:00:29.708200\n",
      "672000 0:00:29.708233\n",
      "672100 0:00:29.710223\n",
      "672200 0:00:29.712005\n",
      "672300 0:00:29.713858\n",
      "672400 0:00:29.715778\n",
      "672500 0:00:29.717442\n",
      "672600 0:00:29.719208\n",
      "672700 0:00:29.720735\n",
      "672800 0:00:29.722502\n",
      "672900 0:00:29.724429\n",
      "get 673000 0:00:29.726005\n",
      "673000 0:00:29.726072\n",
      "673100 0:00:29.727750\n",
      "673200 0:00:29.729454\n",
      "673300 0:00:29.731114\n",
      "673400 0:00:29.732853\n",
      "673500 0:00:29.734750\n",
      "673600 0:00:29.736394\n",
      "673700 0:00:29.737844\n",
      "673800 0:00:29.739824\n",
      "673900 0:00:29.741506\n",
      "get 674000 0:00:29.742957\n",
      "674000 0:00:29.742988\n",
      "674100 0:00:29.744894\n",
      "674200 0:00:29.746624\n",
      "674300 0:00:29.748282\n",
      "674400 0:00:29.749871\n",
      "674500 0:00:29.751463\n",
      "674600 0:00:29.753094\n",
      "674700 0:00:29.754999\n",
      "674800 0:00:29.756511\n",
      "674900 0:00:29.758034\n",
      "get 675000 0:00:29.759636\n",
      "675000 0:00:29.759672\n",
      "675100 0:00:29.761185\n",
      "675200 0:00:29.762831\n",
      "675300 0:00:29.764653\n",
      "675400 0:00:29.766394\n",
      "675500 0:00:29.767929\n",
      "675600 0:00:29.769619\n",
      "675700 0:00:29.771189\n",
      "675800 0:00:29.772975\n",
      "675900 0:00:29.774721\n",
      "get 676000 0:00:29.776380\n",
      "676000 0:00:29.776404\n",
      "676100 0:00:29.778224\n",
      "676200 0:00:29.779938\n",
      "676300 0:00:29.781647\n",
      "676400 0:00:29.783509\n",
      "676500 0:00:29.785172\n",
      "676600 0:00:29.786942\n",
      "676700 0:00:29.788989\n",
      "676800 0:00:29.790454\n",
      "676900 0:00:29.792095\n",
      "get 677000 0:00:29.793828\n",
      "677000 0:00:29.793855\n",
      "677100 0:00:29.795882\n",
      "677200 0:00:29.797537\n",
      "677300 0:00:29.799115\n",
      "677400 0:00:29.800889\n",
      "677500 0:00:29.802525\n",
      "677600 0:00:29.804268\n",
      "677700 0:00:29.806098\n",
      "677800 0:00:29.807957\n",
      "677900 0:00:29.809654\n",
      "get 678000 0:00:29.811316\n",
      "678000 0:00:29.811339\n",
      "678100 0:00:29.813118\n",
      "678200 0:00:29.814984\n",
      "678300 0:00:29.816815\n",
      "678400 0:00:29.818666\n",
      "678500 0:00:29.820592\n",
      "678600 0:00:29.822278\n",
      "678700 0:00:29.824083\n",
      "678800 0:00:29.825728\n",
      "678900 0:00:29.827443\n",
      "get 679000 0:00:29.829174\n",
      "679000 0:00:29.829200\n",
      "679100 0:00:29.831279\n",
      "679200 0:00:29.832845\n",
      "679300 0:00:29.834347\n",
      "679400 0:00:29.836538\n",
      "679500 0:00:29.838484\n",
      "679600 0:00:29.839937\n",
      "679700 0:00:29.841235\n",
      "679800 0:00:29.842633\n",
      "679900 0:00:29.844412\n",
      "get 680000 0:00:29.845946\n",
      "680000 0:00:29.845966\n",
      "680100 0:00:29.847808\n",
      "680200 0:00:29.849645\n",
      "680300 0:00:29.851955\n",
      "680400 0:00:29.853582\n",
      "680500 0:00:29.854852\n",
      "680600 0:00:29.856092\n",
      "680700 0:00:29.857854\n",
      "680800 0:00:29.859599\n",
      "680900 0:00:29.861411\n",
      "get 681000 0:00:29.862609\n",
      "681000 0:00:29.862629\n",
      "681100 0:00:29.864303\n",
      "681200 0:00:29.865774\n",
      "681300 0:00:29.867455\n",
      "681400 0:00:29.869149\n",
      "681500 0:00:29.871076\n",
      "681600 0:00:29.872925\n",
      "681700 0:00:29.875211\n",
      "681800 0:00:29.876801\n",
      "681900 0:00:29.878182\n",
      "get 682000 0:00:29.880122\n",
      "682000 0:00:29.880211\n",
      "682100 0:00:29.881788\n",
      "682200 0:00:29.883178\n",
      "682300 0:00:29.885060\n",
      "682400 0:00:29.887103\n",
      "682500 0:00:29.888942\n",
      "682600 0:00:29.891477\n",
      "682700 0:00:29.893200\n",
      "682800 0:00:29.894654\n",
      "682900 0:00:29.896363\n",
      "get 683000 0:00:29.898042\n",
      "683000 0:00:29.898100\n",
      "683100 0:00:29.899982\n",
      "683200 0:00:29.902286\n",
      "683300 0:00:29.904199\n",
      "683400 0:00:29.906300\n",
      "683500 0:00:29.907884\n",
      "683600 0:00:29.909402\n",
      "683700 0:00:29.911488\n",
      "683800 0:00:29.913327\n",
      "683900 0:00:29.915005\n",
      "get 684000 0:00:29.916437\n",
      "684000 0:00:29.916473\n",
      "684100 0:00:29.918601\n",
      "684200 0:00:29.920615\n",
      "684300 0:00:29.922391\n",
      "684400 0:00:29.924147\n",
      "684500 0:00:29.926051\n",
      "684600 0:00:29.927882\n",
      "684700 0:00:29.929748\n",
      "684800 0:00:29.931345\n",
      "684900 0:00:29.933249\n",
      "get 685000 0:00:29.935011\n",
      "685000 0:00:29.935032\n",
      "685100 0:00:29.936684\n",
      "685200 0:00:29.938270\n",
      "685300 0:00:29.940016\n",
      "685400 0:00:29.942067\n",
      "685500 0:00:29.943980\n",
      "685600 0:00:29.945874\n",
      "685700 0:00:29.947585\n",
      "685800 0:00:29.949314\n",
      "685900 0:00:29.951306\n",
      "get 686000 0:00:29.952864\n",
      "686000 0:00:29.952883\n",
      "686100 0:00:29.954891\n",
      "686200 0:00:29.956672\n",
      "686300 0:00:29.958485\n",
      "686400 0:00:29.960321\n",
      "686500 0:00:29.961991\n",
      "686600 0:00:29.963746\n",
      "686700 0:00:29.965641\n",
      "686800 0:00:29.967448\n",
      "686900 0:00:29.969204\n",
      "get 687000 0:00:29.971008\n",
      "687000 0:00:29.971030\n",
      "687100 0:00:29.972822\n",
      "687200 0:00:29.974442\n",
      "687300 0:00:29.976329\n",
      "687400 0:00:29.978234\n",
      "687500 0:00:29.979939\n",
      "687600 0:00:29.981951\n",
      "687700 0:00:29.983574\n",
      "687800 0:00:29.985645\n",
      "687900 0:00:29.987373\n",
      "get 688000 0:00:29.989384\n",
      "688000 0:00:29.989403\n",
      "688100 0:00:29.991073\n",
      "688200 0:00:29.993112\n",
      "688300 0:00:29.994364\n",
      "688400 0:00:29.996177\n",
      "688500 0:00:29.998237\n",
      "688600 0:00:29.999841\n",
      "688700 0:00:30.001894\n",
      "688800 0:00:30.003716\n",
      "688900 0:00:30.005348\n",
      "get 689000 0:00:30.007652\n",
      "689000 0:00:30.007675\n",
      "689100 0:00:30.009152\n",
      "689200 0:00:30.010504\n",
      "689300 0:00:30.011856\n",
      "689400 0:00:30.013821\n",
      "689500 0:00:30.015377\n",
      "689600 0:00:30.016832\n",
      "689700 0:00:30.018869\n",
      "689800 0:00:30.020849\n",
      "689900 0:00:30.023007\n",
      "get 690000 0:00:30.025084\n",
      "690000 0:00:30.025106\n",
      "690100 0:00:30.026963\n",
      "690200 0:00:30.028673\n",
      "690300 0:00:30.030642\n",
      "690400 0:00:30.032241\n",
      "690500 0:00:30.034037\n",
      "690600 0:00:30.035879\n",
      "690700 0:00:30.037604\n",
      "690800 0:00:30.039543\n",
      "690900 0:00:30.041408\n",
      "get 691000 0:00:30.043288\n",
      "691000 0:00:30.043309\n",
      "691100 0:00:30.045055\n",
      "691200 0:00:30.046591\n",
      "691300 0:00:30.048862\n",
      "691400 0:00:30.050670\n",
      "691500 0:00:30.052678\n",
      "691600 0:00:30.054731\n",
      "691700 0:00:30.056546\n",
      "691800 0:00:30.058176\n",
      "691900 0:00:30.059958\n",
      "get 692000 0:00:30.061675\n",
      "692000 0:00:30.061697\n",
      "692100 0:00:30.063819\n",
      "692200 0:00:30.065457\n",
      "692300 0:00:30.067387\n",
      "692400 0:00:30.069269\n",
      "692500 0:00:30.070898\n",
      "692600 0:00:30.072624\n",
      "692700 0:00:30.074802\n",
      "692800 0:00:30.076875\n",
      "692900 0:00:30.078859\n",
      "get 693000 0:00:30.080491\n",
      "693000 0:00:30.080509\n",
      "693100 0:00:30.081954\n",
      "693200 0:00:30.083733\n",
      "693300 0:00:30.085365\n",
      "693400 0:00:30.087836\n",
      "693500 0:00:30.089291\n",
      "693600 0:00:30.091547\n",
      "693700 0:00:30.093256\n",
      "693800 0:00:30.095049\n",
      "693900 0:00:30.096895\n",
      "get 694000 0:00:30.098556\n",
      "694000 0:00:30.098574\n",
      "694100 0:00:30.100173\n",
      "694200 0:00:30.101795\n",
      "694300 0:00:30.104030\n",
      "694400 0:00:30.105888\n",
      "694500 0:00:30.107557\n",
      "694600 0:00:30.109357\n",
      "694700 0:00:30.111415\n",
      "694800 0:00:30.113374\n",
      "694900 0:00:30.115001\n",
      "get 695000 0:00:30.116952\n",
      "695000 0:00:30.116972\n",
      "695100 0:00:30.118788\n",
      "695200 0:00:30.120644\n",
      "695300 0:00:30.122495\n",
      "695400 0:00:30.124214\n",
      "695500 0:00:30.125918\n",
      "695600 0:00:30.127878\n",
      "695700 0:00:30.129578\n",
      "695800 0:00:30.131156\n",
      "695900 0:00:30.133076\n",
      "get 696000 0:00:30.134764\n",
      "696000 0:00:30.134782\n",
      "696100 0:00:30.136551\n",
      "696200 0:00:30.138398\n",
      "696300 0:00:30.139922\n",
      "696400 0:00:30.141869\n",
      "696500 0:00:30.143297\n",
      "696600 0:00:30.144668\n",
      "696700 0:00:30.146097\n",
      "696800 0:00:30.147671\n",
      "696900 0:00:30.149503\n",
      "get 697000 0:00:30.151069\n",
      "697000 0:00:30.151089\n",
      "697100 0:00:30.153616\n",
      "697200 0:00:30.155472\n",
      "697300 0:00:30.157686\n",
      "697400 0:00:30.159604\n",
      "697500 0:00:30.161457\n",
      "697600 0:00:30.163281\n",
      "697700 0:00:30.164774\n",
      "697800 0:00:30.166692\n",
      "697900 0:00:30.168431\n",
      "get 698000 0:00:30.170439\n",
      "698000 0:00:30.170458\n",
      "698100 0:00:30.172165\n",
      "698200 0:00:30.174554\n",
      "698300 0:00:30.176439\n",
      "698400 0:00:30.178290\n",
      "698500 0:00:30.180367\n",
      "698600 0:00:30.182290\n",
      "698700 0:00:30.184515\n",
      "698800 0:00:30.186395\n",
      "698900 0:00:30.188439\n",
      "get 699000 0:00:30.190497\n",
      "699000 0:00:30.190520\n",
      "699100 0:00:30.192643\n",
      "699200 0:00:30.194763\n",
      "699300 0:00:30.196498\n",
      "699400 0:00:30.198415\n",
      "699500 0:00:30.200591\n",
      "699600 0:00:30.202236\n",
      "699700 0:00:30.204353\n",
      "699800 0:00:30.206152\n",
      "699900 0:00:30.208214\n",
      "get 700000 0:00:33.003805\n",
      "700000 0:00:33.004184\n",
      "700100 0:00:33.006807\n",
      "700200 0:00:33.008882\n",
      "700300 0:00:33.011040\n",
      "700400 0:00:33.013057\n",
      "700500 0:00:33.015033\n",
      "700600 0:00:33.016925\n",
      "700700 0:00:33.019057\n",
      "700800 0:00:33.021315\n",
      "700900 0:00:33.023210\n",
      "get 701000 0:00:33.025161\n",
      "701000 0:00:33.025189\n",
      "701100 0:00:33.026993\n",
      "701200 0:00:33.029141\n",
      "701300 0:00:33.030792\n",
      "701400 0:00:33.033043\n",
      "701500 0:00:33.034640\n",
      "701600 0:00:33.036559\n",
      "701700 0:00:33.038465\n",
      "701800 0:00:33.040540\n",
      "701900 0:00:33.042345\n",
      "get 702000 0:00:33.044390\n",
      "702000 0:00:33.044418\n",
      "702100 0:00:33.046438\n",
      "702200 0:00:33.048370\n",
      "702300 0:00:33.050247\n",
      "702400 0:00:33.051644\n",
      "702500 0:00:33.053918\n",
      "702600 0:00:33.055794\n",
      "702700 0:00:33.057911\n",
      "702800 0:00:33.059606\n",
      "702900 0:00:33.061427\n",
      "get 703000 0:00:33.063354\n",
      "703000 0:00:33.063413\n",
      "703100 0:00:33.065222\n",
      "703200 0:00:33.067233\n",
      "703300 0:00:33.069310\n",
      "703400 0:00:33.071280\n",
      "703500 0:00:33.073229\n",
      "703600 0:00:33.075380\n",
      "703700 0:00:33.077139\n",
      "703800 0:00:33.079235\n",
      "703900 0:00:33.081164\n",
      "get 704000 0:00:33.082847\n",
      "704000 0:00:33.082875\n",
      "704100 0:00:33.084787\n",
      "704200 0:00:33.086683\n",
      "704300 0:00:33.088485\n",
      "704400 0:00:33.090510\n",
      "704500 0:00:33.092383\n",
      "704600 0:00:33.094408\n",
      "704700 0:00:33.096462\n",
      "704800 0:00:33.098448\n",
      "704900 0:00:33.100611\n",
      "get 705000 0:00:33.102129\n",
      "705000 0:00:33.102160\n",
      "705100 0:00:33.104060\n",
      "705200 0:00:33.105826\n",
      "705300 0:00:33.108071\n",
      "705400 0:00:33.109808\n",
      "705500 0:00:33.111796\n",
      "705600 0:00:33.113569\n",
      "705700 0:00:33.115525\n",
      "705800 0:00:33.117308\n",
      "705900 0:00:33.118992\n",
      "get 706000 0:00:33.120532\n",
      "706000 0:00:33.120560\n",
      "706100 0:00:33.122239\n",
      "706200 0:00:33.124294\n",
      "706300 0:00:33.126408\n",
      "706400 0:00:33.128311\n",
      "706500 0:00:33.130271\n",
      "706600 0:00:33.132407\n",
      "706700 0:00:33.134637\n",
      "706800 0:00:33.136670\n",
      "706900 0:00:33.138689\n",
      "get 707000 0:00:33.140827\n",
      "707000 0:00:33.140859\n",
      "707100 0:00:33.142721\n",
      "707200 0:00:33.144482\n",
      "707300 0:00:33.146348\n",
      "707400 0:00:33.148355\n",
      "707500 0:00:33.150224\n",
      "707600 0:00:33.152158\n",
      "707700 0:00:33.153866\n",
      "707800 0:00:33.155764\n",
      "707900 0:00:33.157425\n",
      "get 708000 0:00:33.159433\n",
      "708000 0:00:33.159458\n",
      "708100 0:00:33.161366\n",
      "708200 0:00:33.163319\n",
      "708300 0:00:33.165213\n",
      "708400 0:00:33.167261\n",
      "708500 0:00:33.168863\n",
      "708600 0:00:33.170709\n",
      "708700 0:00:33.172602\n",
      "708800 0:00:33.174611\n",
      "708900 0:00:33.176506\n",
      "get 709000 0:00:33.178096\n",
      "709000 0:00:33.178120\n",
      "709100 0:00:33.180199\n",
      "709200 0:00:33.182093\n",
      "709300 0:00:33.183952\n",
      "709400 0:00:33.185974\n",
      "709500 0:00:33.187818\n",
      "709600 0:00:33.189703\n",
      "709700 0:00:33.191513\n",
      "709800 0:00:33.193430\n",
      "709900 0:00:33.195463\n",
      "get 710000 0:00:33.197486\n",
      "710000 0:00:33.197521\n",
      "710100 0:00:33.199568\n",
      "710200 0:00:33.201390\n",
      "710300 0:00:33.203316\n",
      "710400 0:00:33.205563\n",
      "710500 0:00:33.207426\n",
      "710600 0:00:33.209227\n",
      "710700 0:00:33.210540\n",
      "710800 0:00:33.212107\n",
      "710900 0:00:33.213295\n",
      "get 711000 0:00:33.214946\n",
      "711000 0:00:33.214973\n",
      "711100 0:00:33.216410\n",
      "711200 0:00:33.218251\n",
      "711300 0:00:33.219824\n",
      "711400 0:00:33.221794\n",
      "711500 0:00:33.223322\n",
      "711600 0:00:33.225185\n",
      "711700 0:00:33.226719\n",
      "711800 0:00:33.228458\n",
      "711900 0:00:33.230237\n",
      "get 712000 0:00:33.231669\n",
      "712000 0:00:33.231700\n",
      "712100 0:00:33.233579\n",
      "712200 0:00:33.235792\n",
      "712300 0:00:33.237371\n",
      "712400 0:00:33.239148\n",
      "712500 0:00:33.240470\n",
      "712600 0:00:33.241786\n",
      "712700 0:00:33.243798\n",
      "712800 0:00:33.245356\n",
      "712900 0:00:33.247243\n",
      "get 713000 0:00:33.249097\n",
      "713000 0:00:33.249130\n",
      "713100 0:00:33.250732\n",
      "713200 0:00:33.252701\n",
      "713300 0:00:33.254693\n",
      "713400 0:00:33.256189\n",
      "713500 0:00:33.257816\n",
      "713600 0:00:33.259477\n",
      "713700 0:00:33.261284\n",
      "713800 0:00:33.263221\n",
      "713900 0:00:33.264758\n",
      "get 714000 0:00:33.266861\n",
      "714000 0:00:33.266882\n",
      "714100 0:00:33.268238\n",
      "714200 0:00:33.269570\n",
      "714300 0:00:33.271023\n",
      "714400 0:00:33.272503\n",
      "714500 0:00:33.273690\n",
      "714600 0:00:33.275340\n",
      "714700 0:00:33.277066\n",
      "714800 0:00:33.278829\n",
      "714900 0:00:33.280754\n",
      "get 715000 0:00:33.282740\n",
      "715000 0:00:33.282763\n",
      "715100 0:00:33.284470\n",
      "715200 0:00:33.286063\n",
      "715300 0:00:33.287564\n",
      "715400 0:00:33.288990\n",
      "715500 0:00:33.291124\n",
      "715600 0:00:33.292841\n",
      "715700 0:00:33.294656\n",
      "715800 0:00:33.296342\n",
      "715900 0:00:33.298030\n",
      "get 716000 0:00:33.299368\n",
      "716000 0:00:33.299396\n",
      "716100 0:00:33.301167\n",
      "716200 0:00:33.302966\n",
      "716300 0:00:33.304409\n",
      "716400 0:00:33.306299\n",
      "716500 0:00:33.308254\n",
      "716600 0:00:33.309744\n",
      "716700 0:00:33.311449\n",
      "716800 0:00:33.313045\n",
      "716900 0:00:33.314766\n",
      "get 717000 0:00:33.316296\n",
      "717000 0:00:33.316319\n",
      "717100 0:00:33.317842\n",
      "717200 0:00:33.319394\n",
      "717300 0:00:33.321087\n",
      "717400 0:00:33.322658\n",
      "717500 0:00:33.324238\n",
      "717600 0:00:33.325627\n",
      "717700 0:00:33.326946\n",
      "717800 0:00:33.328871\n",
      "717900 0:00:33.330646\n",
      "get 718000 0:00:33.332173\n",
      "718000 0:00:33.332196\n",
      "718100 0:00:33.333856\n",
      "718200 0:00:33.335273\n",
      "718300 0:00:33.336999\n",
      "718400 0:00:33.338454\n",
      "718500 0:00:33.340365\n",
      "718600 0:00:33.342156\n",
      "718700 0:00:33.343906\n",
      "718800 0:00:33.345684\n",
      "718900 0:00:33.347321\n",
      "get 719000 0:00:33.349409\n",
      "719000 0:00:33.349441\n",
      "719100 0:00:33.351084\n",
      "719200 0:00:33.352563\n",
      "719300 0:00:33.354103\n",
      "719400 0:00:33.355692\n",
      "719500 0:00:33.357407\n",
      "719600 0:00:33.358931\n",
      "719700 0:00:33.360662\n",
      "719800 0:00:33.362197\n",
      "719900 0:00:33.364092\n",
      "get 720000 0:00:33.365823\n",
      "720000 0:00:33.365845\n",
      "720100 0:00:33.367554\n",
      "720200 0:00:33.369416\n",
      "720300 0:00:33.371610\n",
      "720400 0:00:33.373182\n",
      "720500 0:00:33.374697\n",
      "720600 0:00:33.376559\n",
      "720700 0:00:33.377996\n",
      "720800 0:00:33.379590\n",
      "720900 0:00:33.381249\n",
      "get 721000 0:00:33.383053\n",
      "721000 0:00:33.383076\n",
      "721100 0:00:33.384435\n",
      "721200 0:00:33.386097\n",
      "721300 0:00:33.388028\n",
      "721400 0:00:33.389760\n",
      "721500 0:00:33.391544\n",
      "721600 0:00:33.393382\n",
      "721700 0:00:33.395433\n",
      "721800 0:00:33.397340\n",
      "721900 0:00:33.399083\n",
      "get 722000 0:00:33.400492\n",
      "722000 0:00:33.400523\n",
      "722100 0:00:33.402135\n",
      "722200 0:00:33.404087\n",
      "722300 0:00:33.405971\n",
      "722400 0:00:33.408048\n",
      "722500 0:00:33.409871\n",
      "722600 0:00:33.411424\n",
      "722700 0:00:33.412949\n",
      "722800 0:00:33.414848\n",
      "722900 0:00:33.416824\n",
      "get 723000 0:00:33.418491\n",
      "723000 0:00:33.418513\n",
      "723100 0:00:33.420278\n",
      "723200 0:00:33.422029\n",
      "723300 0:00:33.423815\n",
      "723400 0:00:33.425346\n",
      "723500 0:00:33.427106\n",
      "723600 0:00:33.428680\n",
      "723700 0:00:33.430301\n",
      "723800 0:00:33.431811\n",
      "723900 0:00:33.433950\n",
      "get 724000 0:00:33.435748\n",
      "724000 0:00:33.435791\n",
      "724100 0:00:33.437402\n",
      "724200 0:00:33.438878\n",
      "724300 0:00:33.440602\n",
      "724400 0:00:33.442500\n",
      "724500 0:00:33.444164\n",
      "724600 0:00:33.445751\n",
      "724700 0:00:33.447414\n",
      "724800 0:00:33.448974\n",
      "724900 0:00:33.450731\n",
      "get 725000 0:00:33.452849\n",
      "725000 0:00:33.452886\n",
      "725100 0:00:33.454774\n",
      "725200 0:00:33.456640\n",
      "725300 0:00:33.458266\n",
      "725400 0:00:33.459874\n",
      "725500 0:00:33.461911\n",
      "725600 0:00:33.463661\n",
      "725700 0:00:33.465403\n",
      "725800 0:00:33.467147\n",
      "725900 0:00:33.468804\n",
      "get 726000 0:00:33.470469\n",
      "726000 0:00:33.470491\n",
      "726100 0:00:33.472108\n",
      "726200 0:00:33.473614\n",
      "726300 0:00:33.475076\n",
      "726400 0:00:33.476595\n",
      "726500 0:00:33.478277\n",
      "726600 0:00:33.479894\n",
      "726700 0:00:33.482096\n",
      "726800 0:00:33.483700\n",
      "726900 0:00:33.485228\n",
      "get 727000 0:00:33.486555\n",
      "727000 0:00:33.486577\n",
      "727100 0:00:33.488437\n",
      "727200 0:00:33.489804\n",
      "727300 0:00:33.491217\n",
      "727400 0:00:33.492889\n",
      "727500 0:00:33.494496\n",
      "727600 0:00:33.496294\n",
      "727700 0:00:33.498677\n",
      "727800 0:00:33.500497\n",
      "727900 0:00:33.502054\n",
      "get 728000 0:00:33.503624\n",
      "728000 0:00:33.503648\n",
      "728100 0:00:33.505150\n",
      "728200 0:00:33.506894\n",
      "728300 0:00:33.508604\n",
      "728400 0:00:33.510126\n",
      "728500 0:00:33.511620\n",
      "728600 0:00:33.513278\n",
      "728700 0:00:33.514914\n",
      "728800 0:00:33.516838\n",
      "728900 0:00:33.518522\n",
      "get 729000 0:00:33.520084\n",
      "729000 0:00:33.520105\n",
      "729100 0:00:33.521425\n",
      "729200 0:00:33.522892\n",
      "729300 0:00:33.524335\n",
      "729400 0:00:33.525953\n",
      "729500 0:00:33.527439\n",
      "729600 0:00:33.528961\n",
      "729700 0:00:33.530974\n",
      "729800 0:00:33.532829\n",
      "729900 0:00:33.534278\n",
      "get 730000 0:00:33.535929\n",
      "730000 0:00:33.535958\n",
      "730100 0:00:33.537544\n",
      "730200 0:00:33.538984\n",
      "730300 0:00:33.540302\n",
      "730400 0:00:33.541929\n",
      "730500 0:00:33.544227\n",
      "730600 0:00:33.545686\n",
      "730700 0:00:33.547786\n",
      "730800 0:00:33.549286\n",
      "730900 0:00:33.551258\n",
      "get 731000 0:00:33.552591\n",
      "731000 0:00:33.552611\n",
      "731100 0:00:33.554088\n",
      "731200 0:00:33.555787\n",
      "731300 0:00:33.557524\n",
      "731400 0:00:33.559296\n",
      "731500 0:00:33.560934\n",
      "731600 0:00:33.562342\n",
      "731700 0:00:33.564308\n",
      "731800 0:00:33.566392\n",
      "731900 0:00:33.567995\n",
      "get 732000 0:00:33.569962\n",
      "732000 0:00:33.570036\n",
      "732100 0:00:33.571593\n",
      "732200 0:00:33.573434\n",
      "732300 0:00:33.574883\n",
      "732400 0:00:33.576515\n",
      "732500 0:00:33.578091\n",
      "732600 0:00:33.579580\n",
      "732700 0:00:33.581100\n",
      "732800 0:00:33.582696\n",
      "732900 0:00:33.584040\n",
      "get 733000 0:00:33.585692\n",
      "733000 0:00:33.585758\n",
      "733100 0:00:33.587333\n",
      "733200 0:00:33.588849\n",
      "733300 0:00:33.590708\n",
      "733400 0:00:33.592499\n",
      "733500 0:00:33.594135\n",
      "733600 0:00:33.595581\n",
      "733700 0:00:33.597285\n",
      "733800 0:00:33.598984\n",
      "733900 0:00:33.600796\n",
      "get 734000 0:00:33.602956\n",
      "734000 0:00:33.603054\n",
      "734100 0:00:33.604647\n",
      "734200 0:00:33.606264\n",
      "734300 0:00:33.608144\n",
      "734400 0:00:33.610259\n",
      "734500 0:00:33.611727\n",
      "734600 0:00:33.613227\n",
      "734700 0:00:33.614996\n",
      "734800 0:00:33.616923\n",
      "734900 0:00:33.618919\n",
      "get 735000 0:00:33.620613\n",
      "735000 0:00:33.620634\n",
      "735100 0:00:33.622494\n",
      "735200 0:00:33.624084\n",
      "735300 0:00:33.625814\n",
      "735400 0:00:33.627703\n",
      "735500 0:00:33.629692\n",
      "735600 0:00:33.631990\n",
      "735700 0:00:33.633546\n",
      "735800 0:00:33.635212\n",
      "735900 0:00:33.636668\n",
      "get 736000 0:00:33.638178\n",
      "736000 0:00:33.638207\n",
      "736100 0:00:33.639937\n",
      "736200 0:00:33.641663\n",
      "736300 0:00:33.643467\n",
      "736400 0:00:33.645201\n",
      "736500 0:00:33.646831\n",
      "736600 0:00:33.648623\n",
      "736700 0:00:33.650319\n",
      "736800 0:00:33.652285\n",
      "736900 0:00:33.654180\n",
      "get 737000 0:00:33.655708\n",
      "737000 0:00:33.655750\n",
      "737100 0:00:33.657688\n",
      "737200 0:00:33.659176\n",
      "737300 0:00:33.661722\n",
      "737400 0:00:33.663568\n",
      "737500 0:00:33.664782\n",
      "737600 0:00:33.666704\n",
      "737700 0:00:33.668169\n",
      "737800 0:00:33.669909\n",
      "737900 0:00:33.671818\n",
      "get 738000 0:00:33.673720\n",
      "738000 0:00:33.673743\n",
      "738100 0:00:33.675295\n",
      "738200 0:00:33.676992\n",
      "738300 0:00:33.678709\n",
      "738400 0:00:33.680490\n",
      "738500 0:00:33.682177\n",
      "738600 0:00:33.683819\n",
      "738700 0:00:33.685751\n",
      "738800 0:00:33.687431\n",
      "738900 0:00:33.689352\n",
      "get 739000 0:00:33.691394\n",
      "739000 0:00:33.691426\n",
      "739100 0:00:33.692808\n",
      "739200 0:00:33.694639\n",
      "739300 0:00:33.696315\n",
      "739400 0:00:33.697974\n",
      "739500 0:00:33.699771\n",
      "739600 0:00:33.701519\n",
      "739700 0:00:33.703436\n",
      "739800 0:00:33.705792\n",
      "739900 0:00:33.707891\n",
      "get 740000 0:00:33.709447\n",
      "740000 0:00:33.709475\n",
      "740100 0:00:33.711319\n",
      "740200 0:00:33.713282\n",
      "740300 0:00:33.714942\n",
      "740400 0:00:33.717044\n",
      "740500 0:00:33.718431\n",
      "740600 0:00:33.720390\n",
      "740700 0:00:33.721637\n",
      "740800 0:00:33.723532\n",
      "740900 0:00:33.725311\n",
      "get 741000 0:00:33.727012\n",
      "741000 0:00:33.727034\n",
      "741100 0:00:33.728686\n",
      "741200 0:00:33.730570\n",
      "741300 0:00:33.732220\n",
      "741400 0:00:33.734071\n",
      "741500 0:00:33.735530\n",
      "741600 0:00:33.737243\n",
      "741700 0:00:33.738853\n",
      "741800 0:00:33.740643\n",
      "741900 0:00:33.742183\n",
      "get 742000 0:00:33.743965\n",
      "742000 0:00:33.743987\n",
      "742100 0:00:33.745401\n",
      "742200 0:00:33.747459\n",
      "742300 0:00:33.748862\n",
      "742400 0:00:33.750359\n",
      "742500 0:00:33.752077\n",
      "742600 0:00:33.753892\n",
      "742700 0:00:33.755306\n",
      "742800 0:00:33.757053\n",
      "742900 0:00:33.758615\n",
      "get 743000 0:00:33.760133\n",
      "743000 0:00:33.760161\n",
      "743100 0:00:33.761740\n",
      "743200 0:00:33.763293\n",
      "743300 0:00:33.765484\n",
      "743400 0:00:33.767275\n",
      "743500 0:00:33.768939\n",
      "743600 0:00:33.770294\n",
      "743700 0:00:33.772008\n",
      "743800 0:00:33.773990\n",
      "743900 0:00:33.775853\n",
      "get 744000 0:00:33.777597\n",
      "744000 0:00:33.777624\n",
      "744100 0:00:33.779235\n",
      "744200 0:00:33.780924\n",
      "744300 0:00:33.782963\n",
      "744400 0:00:33.784587\n",
      "744500 0:00:33.787314\n",
      "744600 0:00:33.788456\n",
      "744700 0:00:33.789748\n",
      "744800 0:00:33.791751\n",
      "744900 0:00:33.793643\n",
      "get 745000 0:00:33.795065\n",
      "745000 0:00:33.795091\n",
      "745100 0:00:33.797114\n",
      "745200 0:00:33.799086\n",
      "745300 0:00:33.800879\n",
      "745400 0:00:33.802528\n",
      "745500 0:00:33.804373\n",
      "745600 0:00:33.806015\n",
      "745700 0:00:33.807460\n",
      "745800 0:00:33.809144\n",
      "745900 0:00:33.811199\n",
      "get 746000 0:00:33.812730\n",
      "746000 0:00:33.812759\n",
      "746100 0:00:33.814271\n",
      "746200 0:00:33.816172\n",
      "746300 0:00:33.818005\n",
      "746400 0:00:33.819652\n",
      "746500 0:00:33.821378\n",
      "746600 0:00:33.823126\n",
      "746700 0:00:33.825234\n",
      "746800 0:00:33.826710\n",
      "746900 0:00:33.828444\n",
      "get 747000 0:00:33.829989\n",
      "747000 0:00:33.830079\n",
      "747100 0:00:33.831692\n",
      "747200 0:00:33.833409\n",
      "747300 0:00:33.835189\n",
      "747400 0:00:33.837003\n",
      "747500 0:00:33.838524\n",
      "747600 0:00:33.840486\n",
      "747700 0:00:33.842219\n",
      "747800 0:00:33.844096\n",
      "747900 0:00:33.845816\n",
      "get 748000 0:00:33.847314\n",
      "748000 0:00:33.847337\n",
      "748100 0:00:33.849099\n",
      "748200 0:00:33.851022\n",
      "748300 0:00:33.852698\n",
      "748400 0:00:33.854481\n",
      "748500 0:00:33.856632\n",
      "748600 0:00:33.858165\n",
      "748700 0:00:33.860084\n",
      "748800 0:00:33.861487\n",
      "748900 0:00:33.863198\n",
      "get 749000 0:00:33.864560\n",
      "749000 0:00:33.864593\n",
      "749100 0:00:33.866127\n",
      "749200 0:00:33.868044\n",
      "749300 0:00:33.869805\n",
      "749400 0:00:33.871576\n",
      "749500 0:00:33.873083\n",
      "749600 0:00:33.874837\n",
      "749700 0:00:33.876635\n",
      "749800 0:00:33.878396\n",
      "749900 0:00:33.880220\n",
      "get 750000 0:00:33.882329\n",
      "750000 0:00:33.882351\n",
      "750100 0:00:33.883829\n",
      "750200 0:00:33.885720\n",
      "750300 0:00:33.887434\n",
      "750400 0:00:33.889115\n",
      "750500 0:00:33.890973\n",
      "750600 0:00:33.892628\n",
      "750700 0:00:33.894361\n",
      "750800 0:00:33.896235\n",
      "750900 0:00:33.897850\n",
      "get 751000 0:00:33.899708\n",
      "751000 0:00:33.899731\n",
      "751100 0:00:33.901438\n",
      "751200 0:00:33.903264\n",
      "751300 0:00:33.904871\n",
      "751400 0:00:33.906617\n",
      "751500 0:00:33.908377\n",
      "751600 0:00:33.910672\n",
      "751700 0:00:33.912225\n",
      "751800 0:00:33.913485\n",
      "751900 0:00:33.915390\n",
      "get 752000 0:00:33.917099\n",
      "752000 0:00:33.917123\n",
      "752100 0:00:33.918789\n",
      "752200 0:00:33.920504\n",
      "752300 0:00:33.922328\n",
      "752400 0:00:33.924055\n",
      "752500 0:00:33.925714\n",
      "752600 0:00:33.927306\n",
      "752700 0:00:33.929151\n",
      "752800 0:00:33.930783\n",
      "752900 0:00:33.932726\n",
      "get 753000 0:00:33.934296\n",
      "753000 0:00:33.934389\n",
      "753100 0:00:33.936387\n",
      "753200 0:00:33.938123\n",
      "753300 0:00:33.939857\n",
      "753400 0:00:33.941660\n",
      "753500 0:00:33.943247\n",
      "753600 0:00:33.945086\n",
      "753700 0:00:33.946449\n",
      "753800 0:00:33.948182\n",
      "753900 0:00:33.949868\n",
      "get 754000 0:00:33.951061\n",
      "754000 0:00:33.951082\n",
      "754100 0:00:33.952672\n",
      "754200 0:00:33.954143\n",
      "754300 0:00:33.955704\n",
      "754400 0:00:33.957712\n",
      "754500 0:00:33.959536\n",
      "754600 0:00:33.961441\n",
      "754700 0:00:33.962995\n",
      "754800 0:00:33.964837\n",
      "754900 0:00:33.966558\n",
      "get 755000 0:00:33.968411\n",
      "755000 0:00:33.968443\n",
      "755100 0:00:33.970239\n",
      "755200 0:00:33.972315\n",
      "755300 0:00:33.974356\n",
      "755400 0:00:33.976577\n",
      "755500 0:00:33.978734\n",
      "755600 0:00:33.981715\n",
      "755700 0:00:33.984248\n",
      "755800 0:00:33.986380\n",
      "755900 0:00:33.988778\n",
      "get 756000 0:00:33.990648\n",
      "756000 0:00:33.990679\n",
      "756100 0:00:33.992361\n",
      "756200 0:00:33.994353\n",
      "756300 0:00:33.996279\n",
      "756400 0:00:33.998471\n",
      "756500 0:00:34.000261\n",
      "756600 0:00:34.001878\n",
      "756700 0:00:34.003815\n",
      "756800 0:00:34.006067\n",
      "756900 0:00:34.007767\n",
      "get 757000 0:00:34.010074\n",
      "757000 0:00:34.010106\n",
      "757100 0:00:34.012104\n",
      "757200 0:00:34.014036\n",
      "757300 0:00:34.015856\n",
      "757400 0:00:34.017571\n",
      "757500 0:00:34.019164\n",
      "757600 0:00:34.021153\n",
      "757700 0:00:34.022882\n",
      "757800 0:00:34.024548\n",
      "757900 0:00:34.026401\n",
      "get 758000 0:00:34.028161\n",
      "758000 0:00:34.028189\n",
      "758100 0:00:34.029834\n",
      "758200 0:00:34.031698\n",
      "758300 0:00:34.033502\n",
      "758400 0:00:34.035324\n",
      "758500 0:00:34.037208\n",
      "758600 0:00:34.038932\n",
      "758700 0:00:34.040626\n",
      "758800 0:00:34.042218\n",
      "758900 0:00:34.044006\n",
      "get 759000 0:00:34.045567\n",
      "759000 0:00:34.045591\n",
      "759100 0:00:34.047342\n",
      "759200 0:00:34.048975\n",
      "759300 0:00:34.050723\n",
      "759400 0:00:34.052740\n",
      "759500 0:00:34.054349\n",
      "759600 0:00:34.056040\n",
      "759700 0:00:34.057833\n",
      "759800 0:00:34.059760\n",
      "759900 0:00:34.061426\n",
      "get 760000 0:00:34.063292\n",
      "760000 0:00:34.063328\n",
      "760100 0:00:34.065074\n",
      "760200 0:00:34.066785\n",
      "760300 0:00:34.068426\n",
      "760400 0:00:34.070133\n",
      "760500 0:00:34.071866\n",
      "760600 0:00:34.073678\n",
      "760700 0:00:34.075487\n",
      "760800 0:00:34.077233\n",
      "760900 0:00:34.078835\n",
      "get 761000 0:00:34.080475\n",
      "761000 0:00:34.080504\n",
      "761100 0:00:34.082476\n",
      "761200 0:00:34.083906\n",
      "761300 0:00:34.085468\n",
      "761400 0:00:34.087194\n",
      "761500 0:00:34.089399\n",
      "761600 0:00:34.091325\n",
      "761700 0:00:34.093033\n",
      "761800 0:00:34.094721\n",
      "761900 0:00:34.096467\n",
      "get 762000 0:00:34.098314\n",
      "762000 0:00:34.098340\n",
      "762100 0:00:34.100072\n",
      "762200 0:00:34.101771\n",
      "762300 0:00:34.103500\n",
      "762400 0:00:34.105052\n",
      "762500 0:00:34.107050\n",
      "762600 0:00:34.108767\n",
      "762700 0:00:34.110704\n",
      "762800 0:00:34.112460\n",
      "762900 0:00:34.114409\n",
      "get 763000 0:00:34.116307\n",
      "763000 0:00:34.116336\n",
      "763100 0:00:34.118210\n",
      "763200 0:00:34.119962\n",
      "763300 0:00:34.121518\n",
      "763400 0:00:34.123505\n",
      "763500 0:00:34.125234\n",
      "763600 0:00:34.127045\n",
      "763700 0:00:34.128433\n",
      "763800 0:00:34.130125\n",
      "763900 0:00:34.132098\n",
      "get 764000 0:00:34.134128\n",
      "764000 0:00:34.134160\n",
      "764100 0:00:34.135961\n",
      "764200 0:00:34.137679\n",
      "764300 0:00:34.139300\n",
      "764400 0:00:34.141302\n",
      "764500 0:00:34.142901\n",
      "764600 0:00:34.144355\n",
      "764700 0:00:34.146404\n",
      "764800 0:00:34.148254\n",
      "764900 0:00:34.149891\n",
      "get 765000 0:00:34.151697\n",
      "765000 0:00:34.151720\n",
      "765100 0:00:34.153271\n",
      "765200 0:00:34.155127\n",
      "765300 0:00:34.156873\n",
      "765400 0:00:34.158652\n",
      "765500 0:00:34.160554\n",
      "765600 0:00:34.162252\n",
      "765700 0:00:34.164052\n",
      "765800 0:00:34.166114\n",
      "765900 0:00:34.167794\n",
      "get 766000 0:00:34.169397\n",
      "766000 0:00:34.169420\n",
      "766100 0:00:34.171417\n",
      "766200 0:00:34.173171\n",
      "766300 0:00:34.175059\n",
      "766400 0:00:34.176724\n",
      "766500 0:00:34.178404\n",
      "766600 0:00:34.180309\n",
      "766700 0:00:34.181899\n",
      "766800 0:00:34.183543\n",
      "766900 0:00:34.185396\n",
      "get 767000 0:00:34.187135\n",
      "767000 0:00:34.187160\n",
      "767100 0:00:34.188939\n",
      "767200 0:00:34.190877\n",
      "767300 0:00:34.192326\n",
      "767400 0:00:34.194130\n",
      "767500 0:00:34.196061\n",
      "767600 0:00:34.197793\n",
      "767700 0:00:34.199566\n",
      "767800 0:00:34.201064\n",
      "767900 0:00:34.202670\n",
      "get 768000 0:00:34.204521\n",
      "768000 0:00:34.204550\n",
      "768100 0:00:34.206528\n",
      "768200 0:00:34.208332\n",
      "768300 0:00:34.210338\n",
      "768400 0:00:34.211888\n",
      "768500 0:00:34.213974\n",
      "768600 0:00:34.215729\n",
      "768700 0:00:34.217368\n",
      "768800 0:00:34.219355\n",
      "768900 0:00:34.221224\n",
      "get 769000 0:00:34.222985\n",
      "769000 0:00:34.223010\n",
      "769100 0:00:34.224461\n",
      "769200 0:00:34.225948\n",
      "769300 0:00:34.227333\n",
      "769400 0:00:34.229142\n",
      "769500 0:00:34.230455\n",
      "769600 0:00:34.232342\n",
      "769700 0:00:34.233719\n",
      "769800 0:00:34.235339\n",
      "769900 0:00:34.236926\n",
      "get 770000 0:00:34.239192\n",
      "770000 0:00:34.239213\n",
      "770100 0:00:34.241264\n",
      "770200 0:00:34.243092\n",
      "770300 0:00:34.244846\n",
      "770400 0:00:34.246686\n",
      "770500 0:00:34.248209\n",
      "770600 0:00:34.250006\n",
      "770700 0:00:34.251713\n",
      "770800 0:00:34.253329\n",
      "770900 0:00:34.255017\n",
      "get 771000 0:00:34.256749\n",
      "771000 0:00:34.256795\n",
      "771100 0:00:34.258231\n",
      "771200 0:00:34.259992\n",
      "771300 0:00:34.261905\n",
      "771400 0:00:34.263686\n",
      "771500 0:00:34.265639\n",
      "771600 0:00:34.267438\n",
      "771700 0:00:34.268947\n",
      "771800 0:00:34.270833\n",
      "771900 0:00:34.272307\n",
      "get 772000 0:00:34.273859\n",
      "772000 0:00:34.273877\n",
      "772100 0:00:34.275571\n",
      "772200 0:00:34.276829\n",
      "772300 0:00:34.278659\n",
      "772400 0:00:34.280547\n",
      "772500 0:00:34.281726\n",
      "772600 0:00:34.283224\n",
      "772700 0:00:34.285094\n",
      "772800 0:00:34.287082\n",
      "772900 0:00:34.288600\n",
      "get 773000 0:00:34.290574\n",
      "773000 0:00:34.290593\n",
      "773100 0:00:34.292208\n",
      "773200 0:00:34.293746\n",
      "773300 0:00:34.296421\n",
      "773400 0:00:34.298382\n",
      "773500 0:00:34.299955\n",
      "773600 0:00:34.301632\n",
      "773700 0:00:34.303350\n",
      "773800 0:00:34.304924\n",
      "773900 0:00:34.306361\n",
      "get 774000 0:00:34.308061\n",
      "774000 0:00:34.308083\n",
      "774100 0:00:34.309768\n",
      "774200 0:00:34.311524\n",
      "774300 0:00:34.312889\n",
      "774400 0:00:34.314381\n",
      "774500 0:00:34.316311\n",
      "774600 0:00:34.317861\n",
      "774700 0:00:34.320099\n",
      "774800 0:00:34.321300\n",
      "774900 0:00:34.322952\n",
      "get 775000 0:00:34.324432\n",
      "775000 0:00:34.324560\n",
      "775100 0:00:34.326090\n",
      "775200 0:00:34.327921\n",
      "775300 0:00:34.329710\n",
      "775400 0:00:34.331177\n",
      "775500 0:00:34.332835\n",
      "775600 0:00:34.334166\n",
      "775700 0:00:34.335850\n",
      "775800 0:00:34.337337\n",
      "775900 0:00:34.339316\n",
      "get 776000 0:00:34.340694\n",
      "776000 0:00:34.340715\n",
      "776100 0:00:34.342270\n",
      "776200 0:00:34.343771\n",
      "776300 0:00:34.346135\n",
      "776400 0:00:34.348416\n",
      "776500 0:00:34.350112\n",
      "776600 0:00:34.351868\n",
      "776700 0:00:34.353634\n",
      "776800 0:00:34.355626\n",
      "776900 0:00:34.357286\n",
      "get 777000 0:00:34.358876\n",
      "777000 0:00:34.358896\n",
      "777100 0:00:34.360564\n",
      "777200 0:00:34.362351\n",
      "777300 0:00:34.363517\n",
      "777400 0:00:34.365827\n",
      "777500 0:00:34.368371\n",
      "777600 0:00:34.370008\n",
      "777700 0:00:34.371973\n",
      "777800 0:00:34.373386\n",
      "777900 0:00:34.374992\n",
      "get 778000 0:00:34.376592\n",
      "778000 0:00:34.376611\n",
      "778100 0:00:34.378183\n",
      "778200 0:00:34.379973\n",
      "778300 0:00:34.381584\n",
      "778400 0:00:34.383327\n",
      "778500 0:00:34.384959\n",
      "778600 0:00:34.386699\n",
      "778700 0:00:34.388838\n",
      "778800 0:00:34.390773\n",
      "778900 0:00:34.392751\n",
      "get 779000 0:00:34.394276\n",
      "779000 0:00:34.394297\n",
      "779100 0:00:34.395877\n",
      "779200 0:00:34.397624\n",
      "779300 0:00:34.399106\n",
      "779400 0:00:34.400707\n",
      "779500 0:00:34.402608\n",
      "779600 0:00:34.403758\n",
      "779700 0:00:34.405361\n",
      "779800 0:00:34.407063\n",
      "779900 0:00:34.408568\n",
      "get 780000 0:00:34.409851\n",
      "780000 0:00:34.409870\n",
      "780100 0:00:34.411849\n",
      "780200 0:00:34.413673\n",
      "780300 0:00:34.415717\n",
      "780400 0:00:34.417622\n",
      "780500 0:00:34.419042\n",
      "780600 0:00:34.420693\n",
      "780700 0:00:34.422302\n",
      "780800 0:00:34.423786\n",
      "780900 0:00:34.425287\n",
      "get 781000 0:00:34.427031\n",
      "781000 0:00:34.427053\n",
      "781100 0:00:34.429036\n",
      "781200 0:00:34.430539\n",
      "781300 0:00:34.432182\n",
      "781400 0:00:34.433715\n",
      "781500 0:00:34.435634\n",
      "781600 0:00:34.437342\n",
      "781700 0:00:34.438856\n",
      "781800 0:00:34.441017\n",
      "781900 0:00:34.442797\n",
      "get 782000 0:00:34.444919\n",
      "782000 0:00:34.444965\n",
      "782100 0:00:34.447155\n",
      "782200 0:00:34.448783\n",
      "782300 0:00:34.451233\n",
      "782400 0:00:34.452639\n",
      "782500 0:00:34.454964\n",
      "782600 0:00:34.456406\n",
      "782700 0:00:34.458122\n",
      "782800 0:00:34.459523\n",
      "782900 0:00:34.461804\n",
      "get 783000 0:00:34.464135\n",
      "783000 0:00:34.464155\n",
      "783100 0:00:34.466567\n",
      "783200 0:00:34.468924\n",
      "783300 0:00:34.471476\n",
      "783400 0:00:34.473736\n",
      "783500 0:00:34.475943\n",
      "783600 0:00:34.478639\n",
      "783700 0:00:34.481078\n",
      "783800 0:00:34.483627\n",
      "783900 0:00:34.485657\n",
      "get 784000 0:00:34.487147\n",
      "784000 0:00:34.487176\n",
      "784100 0:00:34.489519\n",
      "784200 0:00:34.491933\n",
      "784300 0:00:34.494496\n",
      "784400 0:00:34.496416\n",
      "784500 0:00:34.498791\n",
      "784600 0:00:34.501106\n",
      "784700 0:00:34.503472\n",
      "784800 0:00:34.505718\n",
      "784900 0:00:34.508134\n",
      "get 785000 0:00:34.510364\n",
      "785000 0:00:34.510383\n",
      "785100 0:00:34.512523\n",
      "785200 0:00:34.514877\n",
      "785300 0:00:34.517189\n",
      "785400 0:00:34.519751\n",
      "785500 0:00:34.522136\n",
      "785600 0:00:34.524324\n",
      "785700 0:00:34.526856\n",
      "785800 0:00:34.529078\n",
      "785900 0:00:34.531337\n",
      "get 786000 0:00:34.533515\n",
      "786000 0:00:34.533534\n",
      "786100 0:00:34.535678\n",
      "786200 0:00:34.538135\n",
      "786300 0:00:34.540538\n",
      "786400 0:00:34.542764\n",
      "786500 0:00:34.545203\n",
      "786600 0:00:34.547183\n",
      "786700 0:00:34.549577\n",
      "786800 0:00:34.552081\n",
      "786900 0:00:34.554174\n",
      "get 787000 0:00:34.556641\n",
      "787000 0:00:34.556662\n",
      "787100 0:00:34.559047\n",
      "787200 0:00:34.561462\n",
      "787300 0:00:34.563819\n",
      "787400 0:00:34.566006\n",
      "787500 0:00:34.568409\n",
      "787600 0:00:34.570901\n",
      "787700 0:00:34.572816\n",
      "787800 0:00:34.574509\n",
      "787900 0:00:34.576428\n",
      "get 788000 0:00:34.578237\n",
      "788000 0:00:34.578258\n",
      "788100 0:00:34.580431\n",
      "788200 0:00:34.582687\n",
      "788300 0:00:34.584542\n",
      "788400 0:00:34.587065\n",
      "788500 0:00:34.589006\n",
      "788600 0:00:34.591363\n",
      "788700 0:00:34.593271\n",
      "788800 0:00:34.595252\n",
      "788900 0:00:34.597291\n",
      "get 789000 0:00:34.599491\n",
      "789000 0:00:34.599530\n",
      "789100 0:00:34.601437\n",
      "789200 0:00:34.603135\n",
      "789300 0:00:34.605061\n",
      "789400 0:00:34.607010\n",
      "789500 0:00:34.608684\n",
      "789600 0:00:34.610353\n",
      "789700 0:00:34.612247\n",
      "789800 0:00:34.614221\n",
      "789900 0:00:34.616052\n",
      "get 790000 0:00:34.618312\n",
      "790000 0:00:34.618399\n",
      "790100 0:00:34.620130\n",
      "790200 0:00:34.621660\n",
      "790300 0:00:34.623386\n",
      "790400 0:00:34.625173\n",
      "790500 0:00:34.626891\n",
      "790600 0:00:34.628651\n",
      "790700 0:00:34.630322\n",
      "790800 0:00:34.632069\n",
      "790900 0:00:34.633878\n",
      "get 791000 0:00:34.635560\n",
      "791000 0:00:34.635594\n",
      "791100 0:00:34.637264\n",
      "791200 0:00:34.638939\n",
      "791300 0:00:34.640660\n",
      "791400 0:00:34.642301\n",
      "791500 0:00:34.644055\n",
      "791600 0:00:34.645833\n",
      "791700 0:00:34.647519\n",
      "791800 0:00:34.649249\n",
      "791900 0:00:34.651018\n",
      "get 792000 0:00:34.652782\n",
      "792000 0:00:34.652818\n",
      "792100 0:00:34.654570\n",
      "792200 0:00:34.656210\n",
      "792300 0:00:34.657892\n",
      "792400 0:00:34.659475\n",
      "792500 0:00:34.661095\n",
      "792600 0:00:34.662754\n",
      "792700 0:00:34.664533\n",
      "792800 0:00:34.666283\n",
      "792900 0:00:34.667807\n",
      "get 793000 0:00:34.669423\n",
      "793000 0:00:34.669450\n",
      "793100 0:00:34.671159\n",
      "793200 0:00:34.672850\n",
      "793300 0:00:34.674440\n",
      "793400 0:00:34.676206\n",
      "793500 0:00:34.677873\n",
      "793600 0:00:34.679421\n",
      "793700 0:00:34.681076\n",
      "793800 0:00:34.682395\n",
      "793900 0:00:34.683429\n",
      "get 794000 0:00:34.684502\n",
      "794000 0:00:34.684520\n",
      "794100 0:00:34.685550\n",
      "794200 0:00:34.686589\n",
      "dumped\n",
      "get 0 0:00:37.911186\n",
      "0 0:00:37.912857\n",
      "100 0:00:37.916819\n",
      "200 0:00:37.919886\n",
      "300 0:00:37.922975\n",
      "400 0:00:37.925778\n",
      "500 0:00:37.928127\n",
      "600 0:00:37.930607\n",
      "700 0:00:37.933115\n",
      "800 0:00:37.935646\n",
      "900 0:00:37.938159\n",
      "get 1000 0:00:37.940676\n",
      "1000 0:00:37.940712\n",
      "1100 0:00:37.943256\n",
      "1200 0:00:37.945830\n",
      "1300 0:00:37.948353\n",
      "1400 0:00:37.950945\n",
      "1500 0:00:37.953522\n",
      "1600 0:00:37.956133\n",
      "1700 0:00:37.958697\n",
      "1800 0:00:37.961213\n",
      "1900 0:00:37.963798\n",
      "get 2000 0:00:37.966362\n",
      "2000 0:00:37.966403\n",
      "2100 0:00:37.968998\n",
      "2200 0:00:37.971570\n",
      "2300 0:00:37.974261\n",
      "2400 0:00:37.977063\n",
      "2500 0:00:37.979835\n",
      "2600 0:00:37.982508\n",
      "2700 0:00:37.985210\n",
      "2800 0:00:37.987365\n",
      "2900 0:00:37.989502\n",
      "get 3000 0:00:37.991735\n",
      "3000 0:00:37.991771\n",
      "3100 0:00:37.993973\n",
      "3200 0:00:37.996173\n",
      "3300 0:00:37.998602\n",
      "3400 0:00:38.000897\n",
      "3500 0:00:38.003132\n",
      "3600 0:00:38.005384\n",
      "3700 0:00:38.007581\n",
      "3800 0:00:38.009802\n",
      "3900 0:00:38.012015\n",
      "get 4000 0:00:38.014234\n",
      "4000 0:00:38.014267\n",
      "4100 0:00:38.016487\n",
      "4200 0:00:38.018678\n",
      "4300 0:00:38.020856\n",
      "4400 0:00:38.022929\n",
      "4500 0:00:38.025042\n",
      "4600 0:00:38.027129\n",
      "4700 0:00:38.029183\n",
      "4800 0:00:38.031153\n",
      "4900 0:00:38.033077\n",
      "get 5000 0:00:38.035104\n",
      "5000 0:00:38.035149\n",
      "5100 0:00:38.037245\n",
      "5200 0:00:38.039225\n",
      "5300 0:00:38.041226\n",
      "5400 0:00:38.043220\n",
      "5500 0:00:38.045353\n",
      "5600 0:00:38.047293\n",
      "5700 0:00:38.049280\n",
      "5800 0:00:38.051314\n",
      "5900 0:00:38.053349\n",
      "get 6000 0:00:38.055442\n",
      "6000 0:00:38.055476\n",
      "6100 0:00:38.057568\n",
      "6200 0:00:38.059515\n",
      "6300 0:00:38.061497\n",
      "6400 0:00:38.063509\n",
      "6500 0:00:38.065562\n",
      "6600 0:00:38.067605\n",
      "6700 0:00:38.069750\n",
      "6800 0:00:38.071716\n",
      "6900 0:00:38.073716\n",
      "get 7000 0:00:38.075797\n",
      "7000 0:00:38.075831\n",
      "7100 0:00:38.077895\n",
      "7200 0:00:38.079854\n",
      "7300 0:00:38.081805\n",
      "7400 0:00:38.083896\n",
      "7500 0:00:38.085990\n",
      "7600 0:00:38.088064\n",
      "7700 0:00:38.090179\n",
      "7800 0:00:38.092104\n",
      "7900 0:00:38.094239\n",
      "get 8000 0:00:38.096252\n",
      "8000 0:00:38.096282\n",
      "8100 0:00:38.098306\n",
      "8200 0:00:38.100412\n",
      "8300 0:00:38.102538\n",
      "8400 0:00:38.104784\n",
      "8500 0:00:38.106918\n",
      "8600 0:00:38.108933\n",
      "8700 0:00:38.111004\n",
      "8800 0:00:38.113873\n",
      "8900 0:00:38.115728\n",
      "get 9000 0:00:38.117420\n",
      "9000 0:00:38.117446\n",
      "9100 0:00:38.119028\n",
      "9200 0:00:38.120870\n",
      "9300 0:00:38.122565\n",
      "9400 0:00:38.124220\n",
      "9500 0:00:38.125953\n",
      "9600 0:00:38.127604\n",
      "9700 0:00:38.129207\n",
      "9800 0:00:38.130800\n",
      "9900 0:00:38.132434\n",
      "get 10000 0:00:38.134071\n",
      "10000 0:00:38.134095\n",
      "10100 0:00:38.136095\n",
      "10200 0:00:38.138205\n",
      "10300 0:00:38.140269\n",
      "10400 0:00:38.142422\n",
      "10500 0:00:38.144612\n",
      "10600 0:00:38.146742\n",
      "10700 0:00:38.148947\n",
      "10800 0:00:38.151116\n",
      "10900 0:00:38.153226\n",
      "get 11000 0:00:38.155294\n",
      "11000 0:00:38.155341\n",
      "11100 0:00:38.157462\n",
      "11200 0:00:38.159631\n",
      "11300 0:00:38.161601\n",
      "11400 0:00:38.163630\n",
      "11500 0:00:38.165683\n",
      "11600 0:00:38.167826\n",
      "11700 0:00:38.170063\n",
      "11800 0:00:38.172162\n",
      "11900 0:00:38.174257\n",
      "get 12000 0:00:38.176358\n",
      "12000 0:00:38.176401\n",
      "12100 0:00:38.178476\n",
      "12200 0:00:38.180626\n",
      "12300 0:00:38.182792\n",
      "12400 0:00:38.184917\n",
      "12500 0:00:38.187101\n",
      "12600 0:00:38.189231\n",
      "12700 0:00:38.191350\n",
      "12800 0:00:38.193453\n",
      "12900 0:00:38.195571\n",
      "get 13000 0:00:38.197655\n",
      "13000 0:00:38.197689\n",
      "13100 0:00:38.199750\n",
      "13200 0:00:38.201799\n",
      "13300 0:00:38.203948\n",
      "13400 0:00:38.205969\n",
      "13500 0:00:38.208082\n",
      "13600 0:00:38.210193\n",
      "13700 0:00:38.212320\n",
      "13800 0:00:38.214344\n",
      "13900 0:00:38.216448\n",
      "get 14000 0:00:38.218600\n",
      "14000 0:00:38.218635\n",
      "14100 0:00:38.220750\n",
      "14200 0:00:38.222850\n",
      "14300 0:00:38.224968\n",
      "14400 0:00:38.227121\n",
      "14500 0:00:38.229227\n",
      "14600 0:00:38.231359\n",
      "14700 0:00:38.233509\n",
      "14800 0:00:38.235616\n",
      "14900 0:00:38.237606\n",
      "get 15000 0:00:38.239607\n",
      "15000 0:00:38.239640\n",
      "15100 0:00:38.241722\n",
      "15200 0:00:38.243784\n",
      "15300 0:00:38.245815\n",
      "15400 0:00:38.247875\n",
      "15500 0:00:38.249887\n",
      "15600 0:00:38.251908\n",
      "15700 0:00:38.253974\n",
      "15800 0:00:38.256035\n",
      "15900 0:00:38.258070\n",
      "get 16000 0:00:38.260125\n",
      "16000 0:00:38.260168\n",
      "16100 0:00:38.262248\n",
      "16200 0:00:38.264323\n",
      "16300 0:00:38.266540\n",
      "16400 0:00:38.268670\n",
      "16500 0:00:38.270858\n",
      "16600 0:00:38.272889\n",
      "16700 0:00:38.275031\n",
      "16800 0:00:38.277107\n",
      "16900 0:00:38.279204\n",
      "get 17000 0:00:38.281246\n",
      "17000 0:00:38.281276\n",
      "17100 0:00:38.283469\n",
      "17200 0:00:38.285586\n",
      "17300 0:00:38.287691\n",
      "17400 0:00:38.289794\n",
      "17500 0:00:38.291927\n",
      "17600 0:00:38.294054\n",
      "17700 0:00:38.296139\n",
      "17800 0:00:38.298255\n",
      "17900 0:00:38.300347\n",
      "get 18000 0:00:38.302413\n",
      "18000 0:00:38.302448\n",
      "18100 0:00:38.304551\n",
      "18200 0:00:38.306675\n",
      "18300 0:00:38.308772\n",
      "18400 0:00:38.310999\n",
      "18500 0:00:38.313162\n",
      "18600 0:00:38.315734\n",
      "18700 0:00:38.317993\n",
      "18800 0:00:38.320186\n",
      "18900 0:00:38.322326\n",
      "get 19000 0:00:38.324425\n",
      "19000 0:00:38.324455\n",
      "19100 0:00:38.326640\n",
      "19200 0:00:38.328754\n",
      "19300 0:00:38.330721\n",
      "19400 0:00:38.332796\n",
      "19500 0:00:38.334875\n",
      "19600 0:00:38.337035\n",
      "19700 0:00:38.339121\n",
      "19800 0:00:38.341188\n",
      "19900 0:00:38.343278\n",
      "get 20000 0:00:38.345330\n",
      "20000 0:00:38.345373\n",
      "20100 0:00:38.347497\n",
      "20200 0:00:38.349564\n",
      "20300 0:00:38.351690\n",
      "20400 0:00:38.353722\n",
      "20500 0:00:38.355926\n",
      "20600 0:00:38.358011\n",
      "20700 0:00:38.360137\n",
      "20800 0:00:38.362227\n",
      "20900 0:00:38.364397\n",
      "get 21000 0:00:38.366423\n",
      "21000 0:00:38.366468\n",
      "21100 0:00:38.368498\n",
      "21200 0:00:38.370508\n",
      "21300 0:00:38.372503\n",
      "21400 0:00:38.374606\n",
      "21500 0:00:38.376676\n",
      "21600 0:00:38.378751\n",
      "21700 0:00:38.380757\n",
      "21800 0:00:38.382852\n",
      "21900 0:00:38.385002\n",
      "get 22000 0:00:38.387042\n",
      "22000 0:00:38.387078\n",
      "22100 0:00:38.389213\n",
      "22200 0:00:38.391321\n",
      "22300 0:00:38.393358\n",
      "22400 0:00:38.395452\n",
      "22500 0:00:38.397551\n",
      "22600 0:00:38.399687\n",
      "22700 0:00:38.401804\n",
      "22800 0:00:38.403874\n",
      "22900 0:00:38.406033\n",
      "get 23000 0:00:38.408156\n",
      "23000 0:00:38.408189\n",
      "23100 0:00:38.410312\n",
      "23200 0:00:38.412442\n",
      "23300 0:00:38.414519\n",
      "23400 0:00:38.416630\n",
      "23500 0:00:38.418732\n",
      "23600 0:00:38.420840\n",
      "23700 0:00:38.423001\n",
      "23800 0:00:38.425023\n",
      "23900 0:00:38.427092\n",
      "get 24000 0:00:38.429171\n",
      "24000 0:00:38.429201\n",
      "24100 0:00:38.431290\n",
      "24200 0:00:38.433397\n",
      "24300 0:00:38.435517\n",
      "24400 0:00:38.437775\n",
      "24500 0:00:38.439851\n",
      "24600 0:00:38.441914\n",
      "24700 0:00:38.443945\n",
      "24800 0:00:38.446037\n",
      "24900 0:00:38.448124\n",
      "get 25000 0:00:38.450234\n",
      "25000 0:00:38.450264\n",
      "25100 0:00:38.452280\n",
      "25200 0:00:38.454405\n",
      "25300 0:00:38.456469\n",
      "25400 0:00:38.458636\n",
      "25500 0:00:38.460697\n",
      "25600 0:00:38.462737\n",
      "25700 0:00:38.464810\n",
      "25800 0:00:38.466953\n",
      "25900 0:00:38.469128\n",
      "get 26000 0:00:38.471209\n",
      "26000 0:00:38.471239\n",
      "26100 0:00:38.473321\n",
      "26200 0:00:38.475378\n",
      "26300 0:00:38.477443\n",
      "26400 0:00:38.479474\n",
      "26500 0:00:38.481531\n",
      "26600 0:00:38.483607\n",
      "26700 0:00:38.485732\n",
      "26800 0:00:38.487829\n",
      "26900 0:00:38.490000\n",
      "get 27000 0:00:38.492047\n",
      "27000 0:00:38.492078\n",
      "27100 0:00:38.494226\n",
      "27200 0:00:38.496199\n",
      "27300 0:00:38.498313\n",
      "27400 0:00:38.500577\n",
      "27500 0:00:38.502633\n",
      "27600 0:00:38.504766\n",
      "27700 0:00:38.506913\n",
      "27800 0:00:38.509026\n",
      "27900 0:00:38.511108\n",
      "get 28000 0:00:38.513134\n",
      "28000 0:00:38.513172\n",
      "28100 0:00:38.515204\n",
      "28200 0:00:38.517726\n",
      "28300 0:00:38.519882\n",
      "28400 0:00:38.522014\n",
      "28500 0:00:38.524119\n",
      "28600 0:00:38.526171\n",
      "28700 0:00:38.528275\n",
      "28800 0:00:38.530411\n",
      "28900 0:00:38.532476\n",
      "get 29000 0:00:38.534527\n",
      "29000 0:00:38.534567\n",
      "29100 0:00:38.536677\n",
      "29200 0:00:38.538775\n",
      "29300 0:00:38.540943\n",
      "29400 0:00:38.543000\n",
      "29500 0:00:38.545084\n",
      "29600 0:00:38.547206\n",
      "29700 0:00:38.549335\n",
      "29800 0:00:38.551542\n",
      "29900 0:00:38.553687\n",
      "get 30000 0:00:38.555767\n",
      "30000 0:00:38.555809\n",
      "30100 0:00:38.557968\n",
      "30200 0:00:38.560101\n",
      "30300 0:00:38.562286\n",
      "30400 0:00:38.564392\n",
      "30500 0:00:38.566492\n",
      "30600 0:00:38.568541\n",
      "30700 0:00:38.570658\n",
      "30800 0:00:38.572654\n",
      "30900 0:00:38.574819\n",
      "get 31000 0:00:38.576889\n",
      "31000 0:00:38.576933\n",
      "31100 0:00:38.579069\n",
      "31200 0:00:38.581257\n",
      "31300 0:00:38.583388\n",
      "31400 0:00:38.585516\n",
      "31500 0:00:38.587640\n",
      "31600 0:00:38.589709\n",
      "31700 0:00:38.591884\n",
      "31800 0:00:38.594029\n",
      "31900 0:00:38.596127\n",
      "get 32000 0:00:38.598125\n",
      "32000 0:00:38.598155\n",
      "32100 0:00:38.600216\n",
      "32200 0:00:38.602302\n",
      "32300 0:00:38.604343\n",
      "32400 0:00:38.606485\n",
      "32500 0:00:38.608565\n",
      "32600 0:00:38.610700\n",
      "32700 0:00:38.612852\n",
      "32800 0:00:38.614978\n",
      "32900 0:00:38.617107\n",
      "get 33000 0:00:38.619249\n",
      "33000 0:00:38.619289\n",
      "33100 0:00:38.621404\n",
      "33200 0:00:38.623482\n",
      "33300 0:00:38.625614\n",
      "33400 0:00:38.627774\n",
      "33500 0:00:38.629827\n",
      "33600 0:00:38.631954\n",
      "33700 0:00:38.634079\n",
      "33800 0:00:38.635740\n",
      "33900 0:00:38.637476\n",
      "get 34000 0:00:38.639129\n",
      "34000 0:00:38.639156\n",
      "34100 0:00:38.640790\n",
      "34200 0:00:38.642464\n",
      "34300 0:00:38.644076\n",
      "34400 0:00:38.645941\n",
      "34500 0:00:38.647563\n",
      "34600 0:00:38.649067\n",
      "34700 0:00:38.650890\n",
      "34800 0:00:38.652396\n",
      "34900 0:00:38.653941\n",
      "get 35000 0:00:38.655711\n",
      "35000 0:00:38.655737\n",
      "35100 0:00:38.657354\n",
      "35200 0:00:38.658956\n",
      "35300 0:00:38.660468\n",
      "35400 0:00:38.662116\n",
      "35500 0:00:38.663721\n",
      "35600 0:00:38.665398\n",
      "35700 0:00:38.666974\n",
      "35800 0:00:38.668763\n",
      "35900 0:00:38.670291\n",
      "get 36000 0:00:38.672069\n",
      "36000 0:00:38.672092\n",
      "36100 0:00:38.674206\n",
      "36200 0:00:38.676416\n",
      "36300 0:00:38.679282\n",
      "36400 0:00:38.682205\n",
      "36500 0:00:38.684661\n",
      "36600 0:00:38.687098\n",
      "36700 0:00:38.689553\n",
      "36800 0:00:38.691999\n",
      "36900 0:00:38.694203\n",
      "get 37000 0:00:38.696044\n",
      "37000 0:00:38.696074\n",
      "37100 0:00:38.697800\n",
      "37200 0:00:38.699583\n",
      "37300 0:00:38.701482\n",
      "37400 0:00:38.703370\n",
      "37500 0:00:38.705275\n",
      "37600 0:00:38.707020\n",
      "37700 0:00:38.708929\n",
      "37800 0:00:38.710684\n",
      "37900 0:00:38.712501\n",
      "get 38000 0:00:38.714356\n",
      "38000 0:00:38.714380\n",
      "38100 0:00:38.716261\n",
      "38200 0:00:38.718208\n",
      "38300 0:00:38.720638\n",
      "38400 0:00:38.722603\n",
      "38500 0:00:38.724613\n",
      "38600 0:00:38.726743\n",
      "38700 0:00:38.728626\n",
      "38800 0:00:38.730618\n",
      "38900 0:00:38.732533\n",
      "get 39000 0:00:38.735076\n",
      "39000 0:00:38.735103\n",
      "39100 0:00:38.736686\n",
      "39200 0:00:38.738721\n",
      "39300 0:00:38.740625\n",
      "39400 0:00:38.742534\n",
      "39500 0:00:38.744567\n",
      "39600 0:00:38.746670\n",
      "39700 0:00:38.748562\n",
      "39800 0:00:38.750414\n",
      "39900 0:00:38.752593\n",
      "get 40000 0:00:38.754530\n",
      "40000 0:00:38.754561\n",
      "40100 0:00:38.756483\n",
      "40200 0:00:38.758562\n",
      "40300 0:00:38.760483\n",
      "40400 0:00:38.762374\n",
      "40500 0:00:38.764325\n",
      "40600 0:00:38.766126\n",
      "40700 0:00:38.767987\n",
      "40800 0:00:38.770045\n",
      "40900 0:00:38.771915\n",
      "get 41000 0:00:38.773757\n",
      "41000 0:00:38.773791\n",
      "41100 0:00:38.775643\n",
      "41200 0:00:38.777446\n",
      "41300 0:00:38.779259\n",
      "41400 0:00:38.781109\n",
      "41500 0:00:38.783061\n",
      "41600 0:00:38.784938\n",
      "41700 0:00:38.786880\n",
      "41800 0:00:38.789073\n",
      "41900 0:00:38.791032\n",
      "get 42000 0:00:38.792918\n",
      "42000 0:00:38.792952\n",
      "42100 0:00:38.795006\n",
      "42200 0:00:38.796941\n",
      "42300 0:00:38.799192\n",
      "42400 0:00:38.801513\n",
      "42500 0:00:38.803797\n",
      "42600 0:00:38.806008\n",
      "42700 0:00:38.808109\n",
      "42800 0:00:38.810474\n",
      "42900 0:00:38.812619\n",
      "get 43000 0:00:38.814741\n",
      "43000 0:00:38.814784\n",
      "43100 0:00:38.816773\n",
      "43200 0:00:38.819111\n",
      "43300 0:00:38.821055\n",
      "43400 0:00:38.822852\n",
      "43500 0:00:38.825008\n",
      "43600 0:00:38.827070\n",
      "43700 0:00:38.829407\n",
      "43800 0:00:38.831192\n",
      "43900 0:00:38.833799\n",
      "get 44000 0:00:38.835450\n",
      "44000 0:00:38.835491\n",
      "44100 0:00:38.837348\n",
      "44200 0:00:38.839446\n",
      "44300 0:00:38.841800\n",
      "44400 0:00:38.843684\n",
      "44500 0:00:38.845654\n",
      "44600 0:00:38.847734\n",
      "44700 0:00:38.849880\n",
      "44800 0:00:38.851968\n",
      "44900 0:00:38.854127\n",
      "get 45000 0:00:38.856210\n",
      "45000 0:00:38.856248\n",
      "45100 0:00:38.858297\n",
      "45200 0:00:38.860814\n",
      "45300 0:00:38.863316\n",
      "45400 0:00:38.865794\n",
      "45500 0:00:38.868334\n",
      "45600 0:00:38.870894\n",
      "45700 0:00:38.873393\n",
      "45800 0:00:38.875892\n",
      "45900 0:00:38.878525\n",
      "get 46000 0:00:38.881390\n",
      "46000 0:00:38.881436\n",
      "46100 0:00:38.883987\n",
      "46200 0:00:38.886568\n",
      "46300 0:00:38.889318\n",
      "46400 0:00:38.892025\n",
      "46500 0:00:38.894681\n",
      "46600 0:00:38.897362\n",
      "46700 0:00:38.900036\n",
      "46800 0:00:38.902641\n",
      "46900 0:00:38.905304\n",
      "get 47000 0:00:38.907949\n",
      "47000 0:00:38.907997\n",
      "47100 0:00:38.910638\n",
      "47200 0:00:38.913297\n",
      "47300 0:00:38.916000\n",
      "47400 0:00:38.918620\n",
      "47500 0:00:38.921683\n",
      "47600 0:00:38.924429\n",
      "47700 0:00:38.926995\n",
      "47800 0:00:38.929622\n",
      "47900 0:00:38.932262\n",
      "get 48000 0:00:38.934862\n",
      "48000 0:00:38.934898\n",
      "48100 0:00:38.937554\n",
      "48200 0:00:38.940202\n",
      "48300 0:00:38.942835\n",
      "48400 0:00:38.945512\n",
      "48500 0:00:38.948188\n",
      "48600 0:00:38.950849\n",
      "48700 0:00:38.953499\n",
      "48800 0:00:38.956191\n",
      "48900 0:00:38.958812\n",
      "get 49000 0:00:38.961508\n",
      "49000 0:00:38.961541\n",
      "49100 0:00:38.964182\n",
      "49200 0:00:38.966818\n",
      "49300 0:00:38.969441\n",
      "49400 0:00:38.972369\n",
      "49500 0:00:38.974573\n",
      "49600 0:00:38.977224\n",
      "49700 0:00:38.979846\n",
      "49800 0:00:38.982455\n",
      "49900 0:00:38.985109\n",
      "get 50000 0:00:38.987739\n",
      "50000 0:00:38.987781\n",
      "50100 0:00:38.990258\n",
      "50200 0:00:38.992131\n",
      "50300 0:00:38.993846\n",
      "50400 0:00:38.995824\n",
      "50500 0:00:38.997703\n",
      "50600 0:00:38.999670\n",
      "50700 0:00:39.001574\n",
      "50800 0:00:39.003483\n",
      "50900 0:00:39.005319\n",
      "get 51000 0:00:39.007050\n",
      "51000 0:00:39.007083\n",
      "51100 0:00:39.008984\n",
      "51200 0:00:39.010891\n",
      "51300 0:00:39.012791\n",
      "51400 0:00:39.014697\n",
      "51500 0:00:39.016561\n",
      "51600 0:00:39.018480\n",
      "51700 0:00:39.020443\n",
      "51800 0:00:39.022260\n",
      "51900 0:00:39.023998\n",
      "get 52000 0:00:39.025673\n",
      "52000 0:00:39.025700\n",
      "52100 0:00:39.027542\n",
      "52200 0:00:39.029400\n",
      "52300 0:00:39.031352\n",
      "52400 0:00:39.033262\n",
      "52500 0:00:39.035162\n",
      "52600 0:00:39.037132\n",
      "52700 0:00:39.039037\n",
      "52800 0:00:39.040818\n",
      "52900 0:00:39.042529\n",
      "get 53000 0:00:39.044282\n",
      "53000 0:00:39.044310\n",
      "53100 0:00:39.046173\n",
      "53200 0:00:39.047882\n",
      "53300 0:00:39.049920\n",
      "53400 0:00:39.051873\n",
      "53500 0:00:39.053800\n",
      "53600 0:00:39.055554\n",
      "53700 0:00:39.057282\n",
      "53800 0:00:39.059005\n",
      "53900 0:00:39.060923\n",
      "get 54000 0:00:39.062791\n",
      "54000 0:00:39.062821\n",
      "54100 0:00:39.064739\n",
      "54200 0:00:39.066606\n",
      "54300 0:00:39.068480\n",
      "54400 0:00:39.070330\n",
      "54500 0:00:39.072235\n",
      "54600 0:00:39.074005\n",
      "54700 0:00:39.075812\n",
      "54800 0:00:39.077646\n",
      "54900 0:00:39.079515\n",
      "get 55000 0:00:39.081400\n",
      "55000 0:00:39.081428\n",
      "55100 0:00:39.083459\n",
      "55200 0:00:39.085319\n",
      "55300 0:00:39.087078\n",
      "55400 0:00:39.088853\n",
      "55500 0:00:39.090565\n",
      "55600 0:00:39.092813\n",
      "55700 0:00:39.094712\n",
      "55800 0:00:39.096496\n",
      "55900 0:00:39.098214\n",
      "get 56000 0:00:39.099957\n",
      "56000 0:00:39.099984\n",
      "56100 0:00:39.101850\n",
      "56200 0:00:39.103468\n",
      "56300 0:00:39.105138\n",
      "56400 0:00:39.106857\n",
      "56500 0:00:39.108599\n",
      "56600 0:00:39.110308\n",
      "56700 0:00:39.111974\n",
      "56800 0:00:39.113940\n",
      "56900 0:00:39.115859\n",
      "get 57000 0:00:39.117635\n",
      "57000 0:00:39.117661\n",
      "57100 0:00:39.119486\n",
      "57200 0:00:39.121270\n",
      "57300 0:00:39.123705\n",
      "57400 0:00:39.125649\n",
      "57500 0:00:39.127547\n",
      "57600 0:00:39.129321\n",
      "57700 0:00:39.131086\n",
      "57800 0:00:39.133132\n",
      "57900 0:00:39.134980\n",
      "get 58000 0:00:39.136799\n",
      "58000 0:00:39.136824\n",
      "58100 0:00:39.138423\n",
      "58200 0:00:39.140443\n",
      "58300 0:00:39.142200\n",
      "58400 0:00:39.143843\n",
      "58500 0:00:39.145489\n",
      "58600 0:00:39.147185\n",
      "58700 0:00:39.148980\n",
      "58800 0:00:39.150799\n",
      "58900 0:00:39.152555\n",
      "get 59000 0:00:39.154409\n",
      "59000 0:00:39.154436\n",
      "59100 0:00:39.156132\n",
      "59200 0:00:39.157849\n",
      "59300 0:00:39.159626\n",
      "59400 0:00:39.161538\n",
      "59500 0:00:39.163219\n",
      "59600 0:00:39.164830\n",
      "59700 0:00:39.166471\n",
      "59800 0:00:39.168095\n",
      "59900 0:00:39.169752\n",
      "get 60000 0:00:39.171364\n",
      "60000 0:00:39.171390\n",
      "60100 0:00:39.173131\n",
      "60200 0:00:39.175012\n",
      "60300 0:00:39.176708\n",
      "60400 0:00:39.178299\n",
      "60500 0:00:39.179978\n",
      "60600 0:00:39.181728\n",
      "60700 0:00:39.183373\n",
      "60800 0:00:39.185019\n",
      "60900 0:00:39.186687\n",
      "get 61000 0:00:39.188373\n",
      "61000 0:00:39.188399\n",
      "61100 0:00:39.190022\n",
      "61200 0:00:39.191717\n",
      "61300 0:00:39.193391\n",
      "61400 0:00:39.195013\n",
      "61500 0:00:39.196877\n",
      "61600 0:00:39.198565\n",
      "61700 0:00:39.200276\n",
      "61800 0:00:39.201976\n",
      "61900 0:00:39.204003\n",
      "get 62000 0:00:39.205670\n",
      "62000 0:00:39.205696\n",
      "62100 0:00:39.207276\n",
      "62200 0:00:39.209221\n",
      "62300 0:00:39.210905\n",
      "62400 0:00:39.212870\n",
      "62500 0:00:39.215158\n",
      "62600 0:00:39.216881\n",
      "62700 0:00:39.218498\n",
      "62800 0:00:39.220080\n",
      "62900 0:00:39.221671\n",
      "get 63000 0:00:39.223472\n",
      "63000 0:00:39.223498\n",
      "63100 0:00:39.225350\n",
      "63200 0:00:39.227184\n",
      "63300 0:00:39.229033\n",
      "63400 0:00:39.230647\n",
      "63500 0:00:39.232198\n",
      "63600 0:00:39.233842\n",
      "63700 0:00:39.235451\n",
      "63800 0:00:39.237131\n",
      "63900 0:00:39.238707\n",
      "get 64000 0:00:39.240377\n",
      "64000 0:00:39.240405\n",
      "64100 0:00:39.242257\n",
      "64200 0:00:39.244332\n",
      "64300 0:00:39.246084\n",
      "64400 0:00:39.247926\n",
      "64500 0:00:39.249889\n",
      "64600 0:00:39.251591\n",
      "64700 0:00:39.253407\n",
      "64800 0:00:39.255129\n",
      "64900 0:00:39.256991\n",
      "get 65000 0:00:39.258812\n",
      "65000 0:00:39.258841\n",
      "65100 0:00:39.260537\n",
      "65200 0:00:39.262369\n",
      "65300 0:00:39.264237\n",
      "65400 0:00:39.266011\n",
      "65500 0:00:39.267775\n",
      "65600 0:00:39.269465\n",
      "65700 0:00:39.271086\n",
      "65800 0:00:39.272765\n",
      "65900 0:00:39.274484\n",
      "get 66000 0:00:39.276232\n",
      "66000 0:00:39.276260\n",
      "66100 0:00:39.277983\n",
      "66200 0:00:39.279624\n",
      "66300 0:00:39.281390\n",
      "66400 0:00:39.283076\n",
      "66500 0:00:39.284634\n",
      "66600 0:00:39.286286\n",
      "66700 0:00:39.287928\n",
      "66800 0:00:39.289579\n",
      "66900 0:00:39.291662\n",
      "get 67000 0:00:39.293415\n",
      "67000 0:00:39.293440\n",
      "67100 0:00:39.295347\n",
      "67200 0:00:39.296925\n",
      "67300 0:00:39.298505\n",
      "67400 0:00:39.300631\n",
      "67500 0:00:39.302543\n",
      "67600 0:00:39.304213\n",
      "67700 0:00:39.305762\n",
      "67800 0:00:39.307500\n",
      "67900 0:00:39.309265\n",
      "get 68000 0:00:39.310809\n",
      "68000 0:00:39.310832\n",
      "68100 0:00:39.312487\n",
      "68200 0:00:39.314047\n",
      "68300 0:00:39.315624\n",
      "68400 0:00:39.317299\n",
      "68500 0:00:39.318946\n",
      "68600 0:00:39.320620\n",
      "68700 0:00:39.322476\n",
      "68800 0:00:39.325054\n",
      "68900 0:00:39.326825\n",
      "get 69000 0:00:39.328594\n",
      "69000 0:00:39.328648\n",
      "69100 0:00:39.330443\n",
      "69200 0:00:39.332195\n",
      "69300 0:00:39.333943\n",
      "69400 0:00:39.335514\n",
      "69500 0:00:39.337324\n",
      "69600 0:00:39.338942\n",
      "69700 0:00:39.340523\n",
      "69800 0:00:39.342433\n",
      "69900 0:00:39.344170\n",
      "get 70000 0:00:39.345889\n",
      "70000 0:00:39.345923\n",
      "70100 0:00:39.347644\n",
      "70200 0:00:39.349335\n",
      "70300 0:00:39.351173\n",
      "70400 0:00:39.353423\n",
      "70500 0:00:39.355146\n",
      "70600 0:00:39.356805\n",
      "70700 0:00:39.358599\n",
      "70800 0:00:39.360389\n",
      "70900 0:00:39.362236\n",
      "get 71000 0:00:39.363950\n",
      "71000 0:00:39.363974\n",
      "71100 0:00:39.365675\n",
      "71200 0:00:39.367595\n",
      "71300 0:00:39.369256\n",
      "71400 0:00:39.371183\n",
      "71500 0:00:39.372842\n",
      "71600 0:00:39.374669\n",
      "71700 0:00:39.376521\n",
      "71800 0:00:39.378273\n",
      "71900 0:00:39.379891\n",
      "get 72000 0:00:39.381655\n",
      "72000 0:00:39.381681\n",
      "72100 0:00:39.383543\n",
      "72200 0:00:39.385261\n",
      "72300 0:00:39.387182\n",
      "72400 0:00:39.388995\n",
      "72500 0:00:39.390828\n",
      "72600 0:00:39.392639\n",
      "72700 0:00:39.394486\n",
      "72800 0:00:39.396251\n",
      "72900 0:00:39.397826\n",
      "get 73000 0:00:39.399459\n",
      "73000 0:00:39.399487\n",
      "73100 0:00:39.401265\n",
      "73200 0:00:39.402945\n",
      "73300 0:00:39.404527\n",
      "73400 0:00:39.406469\n",
      "73500 0:00:39.408225\n",
      "73600 0:00:39.409867\n",
      "73700 0:00:39.411699\n",
      "73800 0:00:39.413415\n",
      "73900 0:00:39.415079\n",
      "get 74000 0:00:39.416775\n",
      "74000 0:00:39.416801\n",
      "74100 0:00:39.418727\n",
      "74200 0:00:39.420417\n",
      "74300 0:00:39.422216\n",
      "74400 0:00:39.423907\n",
      "74500 0:00:39.425539\n",
      "74600 0:00:39.427226\n",
      "74700 0:00:39.428938\n",
      "74800 0:00:39.430837\n",
      "74900 0:00:39.432566\n",
      "get 75000 0:00:39.434300\n",
      "75000 0:00:39.434325\n",
      "75100 0:00:39.436003\n",
      "75200 0:00:39.437741\n",
      "75300 0:00:39.439437\n",
      "75400 0:00:39.441461\n",
      "75500 0:00:39.443240\n",
      "75600 0:00:39.445133\n",
      "75700 0:00:39.446720\n",
      "75800 0:00:39.448325\n",
      "75900 0:00:39.450073\n",
      "get 76000 0:00:39.451978\n",
      "76000 0:00:39.452001\n",
      "76100 0:00:39.453807\n",
      "76200 0:00:39.455740\n",
      "76300 0:00:39.457489\n",
      "76400 0:00:39.459160\n",
      "76500 0:00:39.460778\n",
      "76600 0:00:39.462539\n",
      "76700 0:00:39.464089\n",
      "76800 0:00:39.465604\n",
      "76900 0:00:39.467496\n",
      "get 77000 0:00:39.469023\n",
      "77000 0:00:39.469047\n",
      "77100 0:00:39.470775\n",
      "77200 0:00:39.472601\n",
      "77300 0:00:39.474552\n",
      "77400 0:00:39.476126\n",
      "77500 0:00:39.477899\n",
      "77600 0:00:39.479626\n",
      "77700 0:00:39.481749\n",
      "77800 0:00:39.483397\n",
      "77900 0:00:39.485353\n",
      "get 78000 0:00:39.487212\n",
      "78000 0:00:39.487240\n",
      "78100 0:00:39.489053\n",
      "78200 0:00:39.490710\n",
      "78300 0:00:39.492711\n",
      "78400 0:00:39.494526\n",
      "78500 0:00:39.496184\n",
      "78600 0:00:39.497811\n",
      "78700 0:00:39.499443\n",
      "78800 0:00:39.501494\n",
      "78900 0:00:39.503313\n",
      "get 79000 0:00:39.505186\n",
      "79000 0:00:39.505211\n",
      "79100 0:00:39.507756\n",
      "79200 0:00:39.509311\n",
      "79300 0:00:39.510905\n",
      "79400 0:00:39.512716\n",
      "79500 0:00:39.514341\n",
      "79600 0:00:39.516029\n",
      "79700 0:00:39.517943\n",
      "79800 0:00:39.519797\n",
      "79900 0:00:39.521581\n",
      "get 80000 0:00:39.523268\n",
      "80000 0:00:39.523294\n",
      "80100 0:00:39.524875\n",
      "80200 0:00:39.527204\n",
      "80300 0:00:39.529080\n",
      "80400 0:00:39.530973\n",
      "80500 0:00:39.532587\n",
      "80600 0:00:39.534280\n",
      "80700 0:00:39.535984\n",
      "80800 0:00:39.537606\n",
      "80900 0:00:39.539540\n",
      "get 81000 0:00:39.541253\n",
      "81000 0:00:39.541278\n",
      "81100 0:00:39.543215\n",
      "81200 0:00:39.545045\n",
      "81300 0:00:39.546812\n",
      "81400 0:00:39.548367\n",
      "81500 0:00:39.550391\n",
      "81600 0:00:39.552114\n",
      "81700 0:00:39.553801\n",
      "81800 0:00:39.555598\n",
      "81900 0:00:39.557367\n",
      "get 82000 0:00:39.559102\n",
      "82000 0:00:39.559127\n",
      "82100 0:00:39.560838\n",
      "82200 0:00:39.562572\n",
      "82300 0:00:39.564432\n",
      "82400 0:00:39.566143\n",
      "82500 0:00:39.567815\n",
      "82600 0:00:39.569637\n",
      "82700 0:00:39.571309\n",
      "82800 0:00:39.572925\n",
      "82900 0:00:39.575062\n",
      "get 83000 0:00:39.576912\n",
      "83000 0:00:39.576972\n",
      "83100 0:00:39.578707\n",
      "83200 0:00:39.580415\n",
      "83300 0:00:39.582019\n",
      "83400 0:00:39.584182\n",
      "83500 0:00:39.585858\n",
      "83600 0:00:39.587825\n",
      "83700 0:00:39.589420\n",
      "83800 0:00:39.591230\n",
      "83900 0:00:39.592975\n",
      "get 84000 0:00:39.594648\n",
      "84000 0:00:39.594675\n",
      "84100 0:00:39.596361\n",
      "84200 0:00:39.598044\n",
      "84300 0:00:39.599968\n",
      "84400 0:00:39.601865\n",
      "84500 0:00:39.603606\n",
      "84600 0:00:39.605311\n",
      "84700 0:00:39.606939\n",
      "84800 0:00:39.608500\n",
      "84900 0:00:39.610322\n",
      "get 85000 0:00:39.611978\n",
      "85000 0:00:39.612006\n",
      "85100 0:00:39.613620\n",
      "85200 0:00:39.615273\n",
      "85300 0:00:39.617303\n",
      "85400 0:00:39.619283\n",
      "85500 0:00:39.620988\n",
      "85600 0:00:39.622839\n",
      "85700 0:00:39.624558\n",
      "85800 0:00:39.626269\n",
      "85900 0:00:39.627945\n",
      "get 86000 0:00:39.629596\n",
      "86000 0:00:39.629621\n",
      "86100 0:00:39.631540\n",
      "86200 0:00:39.633239\n",
      "86300 0:00:39.635020\n",
      "86400 0:00:39.636792\n",
      "86500 0:00:39.638483\n",
      "86600 0:00:39.640052\n",
      "86700 0:00:39.641834\n",
      "86800 0:00:39.643685\n",
      "86900 0:00:39.645463\n",
      "get 87000 0:00:39.647388\n",
      "87000 0:00:39.647417\n",
      "87100 0:00:39.649153\n",
      "87200 0:00:39.650765\n",
      "87300 0:00:39.652333\n",
      "87400 0:00:39.654252\n",
      "87500 0:00:39.655852\n",
      "87600 0:00:39.657583\n",
      "87700 0:00:39.659445\n",
      "87800 0:00:39.661060\n",
      "87900 0:00:39.663113\n",
      "get 88000 0:00:39.664784\n",
      "88000 0:00:39.664808\n",
      "88100 0:00:39.666729\n",
      "88200 0:00:39.668419\n",
      "88300 0:00:39.670214\n",
      "88400 0:00:39.672090\n",
      "88500 0:00:39.673773\n",
      "88600 0:00:39.675835\n",
      "88700 0:00:39.677606\n",
      "88800 0:00:39.679490\n",
      "88900 0:00:39.681166\n",
      "get 89000 0:00:39.683945\n",
      "89000 0:00:39.683967\n",
      "89100 0:00:39.685699\n",
      "89200 0:00:39.687401\n",
      "89300 0:00:39.689203\n",
      "89400 0:00:39.690975\n",
      "89500 0:00:39.692875\n",
      "89600 0:00:39.694666\n",
      "89700 0:00:39.696579\n",
      "89800 0:00:39.698457\n",
      "89900 0:00:39.700174\n",
      "get 90000 0:00:39.701957\n",
      "90000 0:00:39.701984\n",
      "90100 0:00:39.703805\n",
      "90200 0:00:39.705554\n",
      "90300 0:00:39.707396\n",
      "90400 0:00:39.709082\n",
      "90500 0:00:39.710787\n",
      "90600 0:00:39.712375\n",
      "90700 0:00:39.714273\n",
      "90800 0:00:39.715927\n",
      "90900 0:00:39.717660\n",
      "get 91000 0:00:39.719257\n",
      "91000 0:00:39.719283\n",
      "91100 0:00:39.720875\n",
      "91200 0:00:39.722672\n",
      "91300 0:00:39.724416\n",
      "91400 0:00:39.726356\n",
      "91500 0:00:39.728683\n",
      "91600 0:00:39.730735\n",
      "91700 0:00:39.732649\n",
      "91800 0:00:39.734520\n",
      "91900 0:00:39.736365\n",
      "get 92000 0:00:39.738031\n",
      "92000 0:00:39.738057\n",
      "92100 0:00:39.739731\n",
      "92200 0:00:39.741397\n",
      "92300 0:00:39.743372\n",
      "92400 0:00:39.745082\n",
      "92500 0:00:39.746896\n",
      "92600 0:00:39.748551\n",
      "92700 0:00:39.750183\n",
      "92800 0:00:39.752076\n",
      "92900 0:00:39.753930\n",
      "get 93000 0:00:39.755699\n",
      "93000 0:00:39.755727\n",
      "93100 0:00:39.757351\n",
      "93200 0:00:39.759022\n",
      "93300 0:00:39.761068\n",
      "93400 0:00:39.762873\n",
      "93500 0:00:39.764608\n",
      "93600 0:00:39.766288\n",
      "93700 0:00:39.768015\n",
      "93800 0:00:39.769783\n",
      "93900 0:00:39.771444\n",
      "get 94000 0:00:39.773078\n",
      "94000 0:00:39.773102\n",
      "94100 0:00:39.775058\n",
      "94200 0:00:39.776808\n",
      "94300 0:00:39.778439\n",
      "94400 0:00:39.780104\n",
      "94500 0:00:39.781699\n",
      "94600 0:00:39.783413\n",
      "94700 0:00:39.784940\n",
      "94800 0:00:39.786714\n",
      "94900 0:00:39.788386\n",
      "get 95000 0:00:39.789972\n",
      "95000 0:00:39.789995\n",
      "95100 0:00:39.791706\n",
      "95200 0:00:39.793428\n",
      "95300 0:00:39.795164\n",
      "95400 0:00:39.797067\n",
      "95500 0:00:39.798709\n",
      "95600 0:00:39.800326\n",
      "95700 0:00:39.802010\n",
      "95800 0:00:39.804027\n",
      "95900 0:00:39.806006\n",
      "get 96000 0:00:39.807912\n",
      "96000 0:00:39.807938\n",
      "96100 0:00:39.809933\n",
      "96200 0:00:39.811649\n",
      "96300 0:00:39.813617\n",
      "96400 0:00:39.815554\n",
      "96500 0:00:39.817327\n",
      "96600 0:00:39.819206\n",
      "96700 0:00:39.820984\n",
      "96800 0:00:39.822742\n",
      "96900 0:00:39.824633\n",
      "get 97000 0:00:39.826251\n",
      "97000 0:00:39.826275\n",
      "97100 0:00:39.828243\n",
      "97200 0:00:39.830018\n",
      "97300 0:00:39.831716\n",
      "97400 0:00:39.833492\n",
      "97500 0:00:39.835279\n",
      "97600 0:00:39.836943\n",
      "97700 0:00:39.838836\n",
      "97800 0:00:39.840526\n",
      "97900 0:00:39.842358\n",
      "get 98000 0:00:39.844099\n",
      "98000 0:00:39.844124\n",
      "98100 0:00:39.845840\n",
      "98200 0:00:39.847465\n",
      "98300 0:00:39.849292\n",
      "98400 0:00:39.850981\n",
      "98500 0:00:39.852805\n",
      "98600 0:00:39.854426\n",
      "98700 0:00:39.856042\n",
      "98800 0:00:39.857818\n",
      "98900 0:00:39.859526\n",
      "get 99000 0:00:39.861274\n",
      "99000 0:00:39.861298\n",
      "99100 0:00:39.863055\n",
      "99200 0:00:39.864765\n",
      "99300 0:00:39.866346\n",
      "99400 0:00:39.868415\n",
      "99500 0:00:39.870432\n",
      "99600 0:00:39.872376\n",
      "99700 0:00:39.873948\n",
      "99800 0:00:39.875713\n",
      "99900 0:00:39.877619\n",
      "get 100000 0:00:42.881510\n",
      "100000 0:00:42.881744\n",
      "100100 0:00:42.883901\n",
      "100200 0:00:42.885587\n",
      "100300 0:00:42.887482\n",
      "100400 0:00:42.889252\n",
      "100500 0:00:42.890990\n",
      "100600 0:00:42.892716\n",
      "100700 0:00:42.894871\n",
      "100800 0:00:42.896632\n",
      "100900 0:00:42.898535\n",
      "get 101000 0:00:42.900525\n",
      "101000 0:00:42.900545\n",
      "101100 0:00:42.902603\n",
      "101200 0:00:42.904378\n",
      "101300 0:00:42.906117\n",
      "101400 0:00:42.907738\n",
      "101500 0:00:42.909733\n",
      "101600 0:00:42.911578\n",
      "101700 0:00:42.913619\n",
      "101800 0:00:42.915178\n",
      "101900 0:00:42.917496\n",
      "get 102000 0:00:42.919700\n",
      "102000 0:00:42.919719\n",
      "102100 0:00:42.921540\n",
      "102200 0:00:42.923546\n",
      "102300 0:00:42.925840\n",
      "102400 0:00:42.927671\n",
      "102500 0:00:42.929321\n",
      "102600 0:00:42.930353\n",
      "102700 0:00:42.932005\n",
      "102800 0:00:42.934148\n",
      "102900 0:00:42.935738\n",
      "get 103000 0:00:42.937115\n",
      "103000 0:00:42.937135\n",
      "103100 0:00:42.939215\n",
      "103200 0:00:42.941593\n",
      "103300 0:00:42.943966\n",
      "103400 0:00:42.946433\n",
      "103500 0:00:42.948865\n",
      "103600 0:00:42.951308\n",
      "103700 0:00:42.953671\n",
      "103800 0:00:42.956074\n",
      "103900 0:00:42.958087\n",
      "get 104000 0:00:42.960558\n",
      "104000 0:00:42.960577\n",
      "104100 0:00:42.963004\n",
      "104200 0:00:42.965406\n",
      "104300 0:00:42.967822\n",
      "104400 0:00:42.970030\n",
      "104500 0:00:42.972611\n",
      "104600 0:00:42.974533\n",
      "104700 0:00:42.976825\n",
      "104800 0:00:42.979296\n",
      "104900 0:00:42.981815\n",
      "get 105000 0:00:42.984325\n",
      "105000 0:00:42.984345\n",
      "105100 0:00:42.986795\n",
      "105200 0:00:42.988558\n",
      "105300 0:00:42.991103\n",
      "105400 0:00:42.993014\n",
      "105500 0:00:42.994816\n",
      "105600 0:00:42.996696\n",
      "105700 0:00:42.998821\n",
      "105800 0:00:43.000333\n",
      "105900 0:00:43.002276\n",
      "get 106000 0:00:43.003986\n",
      "106000 0:00:43.004006\n",
      "106100 0:00:43.005754\n",
      "106200 0:00:43.007416\n",
      "106300 0:00:43.009317\n",
      "106400 0:00:43.011063\n",
      "106500 0:00:43.013227\n",
      "106600 0:00:43.015002\n",
      "106700 0:00:43.016640\n",
      "106800 0:00:43.018130\n",
      "106900 0:00:43.020083\n",
      "get 107000 0:00:43.021721\n",
      "107000 0:00:43.021739\n",
      "107100 0:00:43.023613\n",
      "107200 0:00:43.025559\n",
      "107300 0:00:43.027444\n",
      "107400 0:00:43.029239\n",
      "107500 0:00:43.030746\n",
      "107600 0:00:43.032528\n",
      "107700 0:00:43.034290\n",
      "107800 0:00:43.035997\n",
      "107900 0:00:43.037844\n",
      "get 108000 0:00:43.039794\n",
      "108000 0:00:43.039814\n",
      "108100 0:00:43.041109\n",
      "108200 0:00:43.042916\n",
      "108300 0:00:43.044379\n",
      "108400 0:00:43.046230\n",
      "108500 0:00:43.048016\n",
      "108600 0:00:43.049885\n",
      "108700 0:00:43.051498\n",
      "108800 0:00:43.053078\n",
      "108900 0:00:43.055221\n",
      "get 109000 0:00:43.056768\n",
      "109000 0:00:43.056788\n",
      "109100 0:00:43.058598\n",
      "109200 0:00:43.060330\n",
      "109300 0:00:43.062574\n",
      "109400 0:00:43.064235\n",
      "109500 0:00:43.065968\n",
      "109600 0:00:43.067696\n",
      "109700 0:00:43.069446\n",
      "109800 0:00:43.071175\n",
      "109900 0:00:43.072914\n",
      "get 110000 0:00:43.074818\n",
      "110000 0:00:43.074837\n",
      "110100 0:00:43.076546\n",
      "110200 0:00:43.078264\n",
      "110300 0:00:43.080192\n",
      "110400 0:00:43.081551\n",
      "110500 0:00:43.083590\n",
      "110600 0:00:43.085715\n",
      "110700 0:00:43.087384\n",
      "110800 0:00:43.089179\n",
      "110900 0:00:43.090597\n",
      "get 111000 0:00:43.092497\n",
      "111000 0:00:43.092519\n",
      "111100 0:00:43.094203\n",
      "111200 0:00:43.095675\n",
      "111300 0:00:43.097549\n",
      "111400 0:00:43.099355\n",
      "111500 0:00:43.101039\n",
      "111600 0:00:43.102701\n",
      "111700 0:00:43.104437\n",
      "111800 0:00:43.106373\n",
      "111900 0:00:43.107529\n",
      "get 112000 0:00:43.109569\n",
      "112000 0:00:43.109639\n",
      "112100 0:00:43.111415\n",
      "112200 0:00:43.113096\n",
      "112300 0:00:43.115169\n",
      "112400 0:00:43.116564\n",
      "112500 0:00:43.118184\n",
      "112600 0:00:43.119764\n",
      "112700 0:00:43.121193\n",
      "112800 0:00:43.122949\n",
      "112900 0:00:43.124571\n",
      "get 113000 0:00:43.126383\n",
      "113000 0:00:43.126403\n",
      "113100 0:00:43.128161\n",
      "113200 0:00:43.129440\n",
      "113300 0:00:43.131176\n",
      "113400 0:00:43.133428\n",
      "113500 0:00:43.135282\n",
      "113600 0:00:43.136742\n",
      "113700 0:00:43.138472\n",
      "113800 0:00:43.140048\n",
      "113900 0:00:43.141692\n",
      "get 114000 0:00:43.143466\n",
      "114000 0:00:43.143486\n",
      "114100 0:00:43.145159\n",
      "114200 0:00:43.146701\n",
      "114300 0:00:43.148350\n",
      "114400 0:00:43.149924\n",
      "114500 0:00:43.151830\n",
      "114600 0:00:43.153347\n",
      "114700 0:00:43.154987\n",
      "114800 0:00:43.156564\n",
      "114900 0:00:43.158421\n",
      "get 115000 0:00:43.160306\n",
      "115000 0:00:43.160325\n",
      "115100 0:00:43.162031\n",
      "115200 0:00:43.163778\n",
      "115300 0:00:43.165619\n",
      "115400 0:00:43.167002\n",
      "115500 0:00:43.168698\n",
      "115600 0:00:43.170706\n",
      "115700 0:00:43.172214\n",
      "115800 0:00:43.173713\n",
      "115900 0:00:43.175446\n",
      "get 116000 0:00:43.177011\n",
      "116000 0:00:43.177030\n",
      "116100 0:00:43.178859\n",
      "116200 0:00:43.180801\n",
      "116300 0:00:43.182698\n",
      "116400 0:00:43.184508\n",
      "116500 0:00:43.186762\n",
      "116600 0:00:43.188027\n",
      "116700 0:00:43.189705\n",
      "116800 0:00:43.191172\n",
      "116900 0:00:43.193092\n",
      "get 117000 0:00:43.195001\n",
      "117000 0:00:43.195020\n",
      "117100 0:00:43.196751\n",
      "117200 0:00:43.198706\n",
      "117300 0:00:43.200351\n",
      "117400 0:00:43.202157\n",
      "117500 0:00:43.204016\n",
      "117600 0:00:43.205426\n",
      "117700 0:00:43.206990\n",
      "117800 0:00:43.208722\n",
      "117900 0:00:43.210435\n",
      "get 118000 0:00:43.212244\n",
      "118000 0:00:43.212333\n",
      "118100 0:00:43.214057\n",
      "118200 0:00:43.215484\n",
      "118300 0:00:43.217297\n",
      "118400 0:00:43.218874\n",
      "118500 0:00:43.220453\n",
      "118600 0:00:43.221892\n",
      "118700 0:00:43.223572\n",
      "118800 0:00:43.225117\n",
      "118900 0:00:43.227234\n",
      "get 119000 0:00:43.228724\n",
      "119000 0:00:43.228744\n",
      "119100 0:00:43.230283\n",
      "119200 0:00:43.232121\n",
      "119300 0:00:43.233510\n",
      "119400 0:00:43.235247\n",
      "119500 0:00:43.237010\n",
      "119600 0:00:43.238857\n",
      "119700 0:00:43.240636\n",
      "119800 0:00:43.242647\n",
      "119900 0:00:43.244132\n",
      "get 120000 0:00:43.245751\n",
      "120000 0:00:43.245770\n",
      "120100 0:00:43.247203\n",
      "120200 0:00:43.248816\n",
      "120300 0:00:43.250846\n",
      "120400 0:00:43.252500\n",
      "120500 0:00:43.254188\n",
      "120600 0:00:43.255855\n",
      "120700 0:00:43.257566\n",
      "120800 0:00:43.259158\n",
      "120900 0:00:43.261432\n",
      "get 121000 0:00:43.262637\n",
      "121000 0:00:43.262656\n",
      "121100 0:00:43.264554\n",
      "121200 0:00:43.266611\n",
      "121300 0:00:43.268413\n",
      "121400 0:00:43.270282\n",
      "121500 0:00:43.272068\n",
      "121600 0:00:43.273556\n",
      "121700 0:00:43.275513\n",
      "121800 0:00:43.277383\n",
      "121900 0:00:43.278831\n",
      "get 122000 0:00:43.280520\n",
      "122000 0:00:43.280539\n",
      "122100 0:00:43.281789\n",
      "122200 0:00:43.283072\n",
      "122300 0:00:43.284972\n",
      "122400 0:00:43.286623\n",
      "122500 0:00:43.288078\n",
      "122600 0:00:43.289688\n",
      "122700 0:00:43.291437\n",
      "122800 0:00:43.293146\n",
      "122900 0:00:43.294779\n",
      "get 123000 0:00:43.296351\n",
      "123000 0:00:43.296370\n",
      "123100 0:00:43.297992\n",
      "123200 0:00:43.299787\n",
      "123300 0:00:43.301867\n",
      "123400 0:00:43.303516\n",
      "123500 0:00:43.304965\n",
      "123600 0:00:43.306551\n",
      "123700 0:00:43.308296\n",
      "123800 0:00:43.310003\n",
      "123900 0:00:43.311780\n",
      "get 124000 0:00:43.313411\n",
      "124000 0:00:43.313430\n",
      "124100 0:00:43.315115\n",
      "124200 0:00:43.316965\n",
      "124300 0:00:43.318840\n",
      "124400 0:00:43.320539\n",
      "124500 0:00:43.322297\n",
      "124600 0:00:43.323853\n",
      "124700 0:00:43.325318\n",
      "124800 0:00:43.327033\n",
      "124900 0:00:43.328753\n",
      "get 125000 0:00:43.330739\n",
      "125000 0:00:43.330760\n",
      "125100 0:00:43.332237\n",
      "125200 0:00:43.334148\n",
      "125300 0:00:43.336240\n",
      "125400 0:00:43.337974\n",
      "125500 0:00:43.339610\n",
      "125600 0:00:43.341142\n",
      "125700 0:00:43.342851\n",
      "125800 0:00:43.344488\n",
      "125900 0:00:43.346073\n",
      "get 126000 0:00:43.347633\n",
      "126000 0:00:43.347652\n",
      "126100 0:00:43.349395\n",
      "126200 0:00:43.351040\n",
      "126300 0:00:43.352625\n",
      "126400 0:00:43.354352\n",
      "126500 0:00:43.356319\n",
      "126600 0:00:43.358149\n",
      "126700 0:00:43.359706\n",
      "126800 0:00:43.361375\n",
      "126900 0:00:43.363041\n",
      "get 127000 0:00:43.364873\n",
      "127000 0:00:43.364892\n",
      "127100 0:00:43.366660\n",
      "127200 0:00:43.368765\n",
      "127300 0:00:43.370249\n",
      "127400 0:00:43.372120\n",
      "127500 0:00:43.373503\n",
      "127600 0:00:43.375405\n",
      "127700 0:00:43.377126\n",
      "127800 0:00:43.379022\n",
      "127900 0:00:43.380844\n",
      "get 128000 0:00:43.382439\n",
      "128000 0:00:43.382459\n",
      "128100 0:00:43.384006\n",
      "128200 0:00:43.385628\n",
      "128300 0:00:43.387505\n",
      "128400 0:00:43.389106\n",
      "128500 0:00:43.390803\n",
      "128600 0:00:43.392455\n",
      "128700 0:00:43.394153\n",
      "128800 0:00:43.395990\n",
      "128900 0:00:43.397624\n",
      "get 129000 0:00:43.399159\n",
      "129000 0:00:43.399179\n",
      "129100 0:00:43.401093\n",
      "129200 0:00:43.402869\n",
      "129300 0:00:43.404541\n",
      "129400 0:00:43.406210\n",
      "129500 0:00:43.408041\n",
      "129600 0:00:43.409873\n",
      "129700 0:00:43.411658\n",
      "129800 0:00:43.413378\n",
      "129900 0:00:43.415139\n",
      "get 130000 0:00:43.416665\n",
      "130000 0:00:43.416685\n",
      "130100 0:00:43.418382\n",
      "130200 0:00:43.420168\n",
      "130300 0:00:43.421558\n",
      "130400 0:00:43.423427\n",
      "130500 0:00:43.425331\n",
      "130600 0:00:43.426998\n",
      "130700 0:00:43.428789\n",
      "130800 0:00:43.430647\n",
      "130900 0:00:43.432200\n",
      "get 131000 0:00:43.433804\n",
      "131000 0:00:43.433823\n",
      "131100 0:00:43.435730\n",
      "131200 0:00:43.437219\n",
      "131300 0:00:43.439220\n",
      "131400 0:00:43.441124\n",
      "131500 0:00:43.443118\n",
      "131600 0:00:43.444302\n",
      "131700 0:00:43.445618\n",
      "131800 0:00:43.447412\n",
      "131900 0:00:43.449044\n",
      "get 132000 0:00:43.450670\n",
      "132000 0:00:43.450701\n",
      "132100 0:00:43.452556\n",
      "132200 0:00:43.454493\n",
      "132300 0:00:43.456066\n",
      "132400 0:00:43.458138\n",
      "132500 0:00:43.459667\n",
      "132600 0:00:43.461052\n",
      "132700 0:00:43.462892\n",
      "132800 0:00:43.464196\n",
      "132900 0:00:43.465856\n",
      "get 133000 0:00:43.467272\n",
      "133000 0:00:43.467291\n",
      "133100 0:00:43.468858\n",
      "133200 0:00:43.470629\n",
      "133300 0:00:43.472433\n",
      "133400 0:00:43.474012\n",
      "133500 0:00:43.475570\n",
      "133600 0:00:43.477217\n",
      "133700 0:00:43.479128\n",
      "133800 0:00:43.480897\n",
      "133900 0:00:43.482814\n",
      "get 134000 0:00:43.484444\n",
      "134000 0:00:43.484465\n",
      "134100 0:00:43.486221\n",
      "134200 0:00:43.487849\n",
      "134300 0:00:43.489637\n",
      "134400 0:00:43.491354\n",
      "134500 0:00:43.493146\n",
      "134600 0:00:43.494819\n",
      "134700 0:00:43.496477\n",
      "134800 0:00:43.498016\n",
      "134900 0:00:43.500013\n",
      "get 135000 0:00:43.501656\n",
      "135000 0:00:43.501675\n",
      "135100 0:00:43.503221\n",
      "135200 0:00:43.505060\n",
      "135300 0:00:43.507195\n",
      "135400 0:00:43.508726\n",
      "135500 0:00:43.510802\n",
      "135600 0:00:43.512675\n",
      "135700 0:00:43.514772\n",
      "135800 0:00:43.516103\n",
      "135900 0:00:43.517814\n",
      "get 136000 0:00:43.519258\n",
      "136000 0:00:43.519279\n",
      "136100 0:00:43.521339\n",
      "136200 0:00:43.522905\n",
      "136300 0:00:43.524643\n",
      "136400 0:00:43.526579\n",
      "136500 0:00:43.529018\n",
      "136600 0:00:43.530409\n",
      "136700 0:00:43.532271\n",
      "136800 0:00:43.533942\n",
      "136900 0:00:43.535451\n",
      "get 137000 0:00:43.536889\n",
      "137000 0:00:43.536909\n",
      "137100 0:00:43.538781\n",
      "137200 0:00:43.540441\n",
      "137300 0:00:43.542216\n",
      "137400 0:00:43.544113\n",
      "137500 0:00:43.545621\n",
      "137600 0:00:43.547584\n",
      "137700 0:00:43.549291\n",
      "137800 0:00:43.550990\n",
      "137900 0:00:43.552578\n",
      "get 138000 0:00:43.554164\n",
      "138000 0:00:43.554183\n",
      "138100 0:00:43.555999\n",
      "138200 0:00:43.558094\n",
      "138300 0:00:43.560191\n",
      "138400 0:00:43.561588\n",
      "138500 0:00:43.563166\n",
      "138600 0:00:43.565114\n",
      "138700 0:00:43.566506\n",
      "138800 0:00:43.568399\n",
      "138900 0:00:43.570211\n",
      "get 139000 0:00:43.571761\n",
      "139000 0:00:43.571781\n",
      "139100 0:00:43.573331\n",
      "139200 0:00:43.575170\n",
      "139300 0:00:43.576781\n",
      "139400 0:00:43.578702\n",
      "139500 0:00:43.580318\n",
      "139600 0:00:43.582024\n",
      "139700 0:00:43.583654\n",
      "139800 0:00:43.585193\n",
      "139900 0:00:43.586684\n",
      "get 140000 0:00:43.588078\n",
      "140000 0:00:43.588098\n",
      "140100 0:00:43.589977\n",
      "140200 0:00:43.591728\n",
      "140300 0:00:43.593524\n",
      "140400 0:00:43.594993\n",
      "140500 0:00:43.596915\n",
      "140600 0:00:43.598817\n",
      "140700 0:00:43.600745\n",
      "140800 0:00:43.602142\n",
      "140900 0:00:43.604178\n",
      "get 141000 0:00:43.605676\n",
      "141000 0:00:43.605694\n",
      "141100 0:00:43.607480\n",
      "141200 0:00:43.609028\n",
      "141300 0:00:43.611148\n",
      "141400 0:00:43.613049\n",
      "141500 0:00:43.615262\n",
      "141600 0:00:43.616908\n",
      "141700 0:00:43.618933\n",
      "141800 0:00:43.621048\n",
      "141900 0:00:43.622467\n",
      "get 142000 0:00:43.623928\n",
      "142000 0:00:43.623973\n",
      "142100 0:00:43.625566\n",
      "142200 0:00:43.627359\n",
      "142300 0:00:43.628582\n",
      "142400 0:00:43.630287\n",
      "142500 0:00:43.632201\n",
      "142600 0:00:43.634146\n",
      "142700 0:00:43.636139\n",
      "142800 0:00:43.638521\n",
      "142900 0:00:43.639990\n",
      "get 143000 0:00:43.642176\n",
      "143000 0:00:43.642195\n",
      "143100 0:00:43.644237\n",
      "143200 0:00:43.645557\n",
      "143300 0:00:43.647106\n",
      "143400 0:00:43.648861\n",
      "143500 0:00:43.650343\n",
      "143600 0:00:43.651984\n",
      "143700 0:00:43.653538\n",
      "143800 0:00:43.655395\n",
      "143900 0:00:43.657234\n",
      "get 144000 0:00:43.658950\n",
      "144000 0:00:43.658972\n",
      "144100 0:00:43.660604\n",
      "144200 0:00:43.662369\n",
      "144300 0:00:43.664016\n",
      "144400 0:00:43.665784\n",
      "144500 0:00:43.667551\n",
      "144600 0:00:43.669752\n",
      "144700 0:00:43.671233\n",
      "144800 0:00:43.672650\n",
      "144900 0:00:43.674318\n",
      "get 145000 0:00:43.676304\n",
      "145000 0:00:43.676322\n",
      "145100 0:00:43.677833\n",
      "145200 0:00:43.679277\n",
      "145300 0:00:43.681154\n",
      "145400 0:00:43.682960\n",
      "145500 0:00:43.684288\n",
      "145600 0:00:43.686305\n",
      "145700 0:00:43.688553\n",
      "145800 0:00:43.690600\n",
      "145900 0:00:43.692414\n",
      "get 146000 0:00:43.693757\n",
      "146000 0:00:43.693777\n",
      "146100 0:00:43.695453\n",
      "146200 0:00:43.697176\n",
      "146300 0:00:43.698703\n",
      "146400 0:00:43.700269\n",
      "146500 0:00:43.701996\n",
      "146600 0:00:43.703586\n",
      "146700 0:00:43.705325\n",
      "146800 0:00:43.707279\n",
      "146900 0:00:43.708764\n",
      "get 147000 0:00:43.710452\n",
      "147000 0:00:43.710471\n",
      "147100 0:00:43.712165\n",
      "147200 0:00:43.714099\n",
      "147300 0:00:43.715919\n",
      "147400 0:00:43.717933\n",
      "147500 0:00:43.719549\n",
      "147600 0:00:43.721467\n",
      "147700 0:00:43.723130\n",
      "147800 0:00:43.724978\n",
      "147900 0:00:43.726542\n",
      "get 148000 0:00:43.728361\n",
      "148000 0:00:43.728381\n",
      "148100 0:00:43.730217\n",
      "148200 0:00:43.731614\n",
      "148300 0:00:43.733691\n",
      "148400 0:00:43.735299\n",
      "148500 0:00:43.736596\n",
      "148600 0:00:43.738390\n",
      "148700 0:00:43.740253\n",
      "148800 0:00:43.741869\n",
      "148900 0:00:43.743597\n",
      "get 149000 0:00:43.745145\n",
      "149000 0:00:43.745164\n",
      "149100 0:00:43.746472\n",
      "149200 0:00:43.748142\n",
      "149300 0:00:43.750188\n",
      "149400 0:00:43.752092\n",
      "149500 0:00:43.754131\n",
      "149600 0:00:43.756121\n",
      "149700 0:00:43.757948\n",
      "149800 0:00:43.759593\n",
      "149900 0:00:43.761252\n",
      "get 150000 0:00:43.763000\n",
      "150000 0:00:43.763019\n",
      "150100 0:00:43.764569\n",
      "150200 0:00:43.766008\n",
      "150300 0:00:43.767913\n",
      "150400 0:00:43.769217\n",
      "150500 0:00:43.770757\n",
      "150600 0:00:43.772352\n",
      "150700 0:00:43.774108\n",
      "150800 0:00:43.775649\n",
      "150900 0:00:43.777235\n",
      "get 151000 0:00:43.779117\n",
      "151000 0:00:43.779138\n",
      "151100 0:00:43.780609\n",
      "151200 0:00:43.782138\n",
      "151300 0:00:43.784021\n",
      "151400 0:00:43.785463\n",
      "151500 0:00:43.787157\n",
      "151600 0:00:43.789149\n",
      "151700 0:00:43.791044\n",
      "151800 0:00:43.792795\n",
      "151900 0:00:43.794872\n",
      "get 152000 0:00:43.796516\n",
      "152000 0:00:43.796535\n",
      "152100 0:00:43.798458\n",
      "152200 0:00:43.800370\n",
      "152300 0:00:43.801722\n",
      "152400 0:00:43.803466\n",
      "152500 0:00:43.805104\n",
      "152600 0:00:43.807029\n",
      "152700 0:00:43.808628\n",
      "152800 0:00:43.810291\n",
      "152900 0:00:43.812041\n",
      "get 153000 0:00:43.813791\n",
      "153000 0:00:43.813810\n",
      "153100 0:00:43.815794\n",
      "153200 0:00:43.817476\n",
      "153300 0:00:43.819209\n",
      "153400 0:00:43.820847\n",
      "153500 0:00:43.822651\n",
      "153600 0:00:43.824450\n",
      "153700 0:00:43.826295\n",
      "153800 0:00:43.827993\n",
      "153900 0:00:43.829663\n",
      "get 154000 0:00:43.831671\n",
      "154000 0:00:43.831690\n",
      "154100 0:00:43.833490\n",
      "154200 0:00:43.834978\n",
      "154300 0:00:43.836482\n",
      "154400 0:00:43.838053\n",
      "154500 0:00:43.839794\n",
      "154600 0:00:43.841612\n",
      "154700 0:00:43.843343\n",
      "154800 0:00:43.845312\n",
      "154900 0:00:43.847284\n",
      "get 155000 0:00:43.848705\n",
      "155000 0:00:43.848723\n",
      "155100 0:00:43.850491\n",
      "155200 0:00:43.852009\n",
      "155300 0:00:43.854056\n",
      "155400 0:00:43.855822\n",
      "155500 0:00:43.857559\n",
      "155600 0:00:43.859699\n",
      "155700 0:00:43.861365\n",
      "155800 0:00:43.862858\n",
      "155900 0:00:43.864829\n",
      "get 156000 0:00:43.866698\n",
      "156000 0:00:43.866717\n",
      "156100 0:00:43.868346\n",
      "156200 0:00:43.869917\n",
      "156300 0:00:43.871678\n",
      "156400 0:00:43.873622\n",
      "156500 0:00:43.875319\n",
      "156600 0:00:43.876914\n",
      "156700 0:00:43.878668\n",
      "156800 0:00:43.880436\n",
      "156900 0:00:43.882148\n",
      "get 157000 0:00:43.883847\n",
      "157000 0:00:43.883866\n",
      "157100 0:00:43.885649\n",
      "157200 0:00:43.887405\n",
      "157300 0:00:43.889586\n",
      "157400 0:00:43.891216\n",
      "157500 0:00:43.893035\n",
      "157600 0:00:43.894653\n",
      "157700 0:00:43.896779\n",
      "157800 0:00:43.898566\n",
      "157900 0:00:43.900231\n",
      "get 158000 0:00:43.902125\n",
      "158000 0:00:43.902145\n",
      "158100 0:00:43.903744\n",
      "158200 0:00:43.905478\n",
      "158300 0:00:43.907371\n",
      "158400 0:00:43.908902\n",
      "158500 0:00:43.910642\n",
      "158600 0:00:43.912455\n",
      "158700 0:00:43.914387\n",
      "158800 0:00:43.916216\n",
      "158900 0:00:43.918205\n",
      "get 159000 0:00:43.920406\n",
      "159000 0:00:43.920496\n",
      "159100 0:00:43.922303\n",
      "159200 0:00:43.924008\n",
      "159300 0:00:43.925455\n",
      "159400 0:00:43.927813\n",
      "159500 0:00:43.929750\n",
      "159600 0:00:43.931483\n",
      "159700 0:00:43.932971\n",
      "159800 0:00:43.934906\n",
      "159900 0:00:43.936398\n",
      "get 160000 0:00:43.938003\n",
      "160000 0:00:43.938022\n",
      "160100 0:00:43.939820\n",
      "160200 0:00:43.941312\n",
      "160300 0:00:43.943311\n",
      "160400 0:00:43.945433\n",
      "160500 0:00:43.946803\n",
      "160600 0:00:43.948618\n",
      "160700 0:00:43.950142\n",
      "160800 0:00:43.951916\n",
      "160900 0:00:43.953686\n",
      "get 161000 0:00:43.955139\n",
      "161000 0:00:43.955158\n",
      "161100 0:00:43.957425\n",
      "161200 0:00:43.958973\n",
      "161300 0:00:43.960570\n",
      "161400 0:00:43.962444\n",
      "161500 0:00:43.964495\n",
      "161600 0:00:43.966207\n",
      "161700 0:00:43.967835\n",
      "161800 0:00:43.969448\n",
      "161900 0:00:43.971133\n",
      "get 162000 0:00:43.972989\n",
      "162000 0:00:43.973007\n",
      "162100 0:00:43.974587\n",
      "162200 0:00:43.976431\n",
      "162300 0:00:43.978132\n",
      "162400 0:00:43.980380\n",
      "162500 0:00:43.982092\n",
      "162600 0:00:43.983923\n",
      "162700 0:00:43.985232\n",
      "162800 0:00:43.987058\n",
      "162900 0:00:43.989236\n",
      "get 163000 0:00:43.991197\n",
      "163000 0:00:43.991217\n",
      "dumped\n"
     ]
    }
   ],
   "source": [
    "LMDB_NAME = \"author_100_emb_weighted\"\n",
    "lc = LMDBClient(LMDB_NAME)\n",
    "start_time = datetime.now()\n",
    "\n",
    "\"\"\"\n",
    "This class generates triplets of author embeddings to train global model\n",
    "\"\"\"\n",
    "\n",
    "data_gen = TripletsGenerator(train_scale=1000000)\n",
    "data_gen.dump_triplets(role='train')\n",
    "data_gen.dump_triplets(role='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac911e4-26aa-4730-8d25-8e184cd12427",
   "metadata": {},
   "source": [
    "# 3. Global model_Global model stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7fd76e-12b9-40e0-b6e8-9b41bf1d2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triplet:\n",
    "    @staticmethod\n",
    "    def l2Norm(x):\n",
    "        return tf.math.l2_normalize(x, axis=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean_distance(vects):\n",
    "        x, y = vects\n",
    "        return tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "    \n",
    "    @staticmethod\n",
    "    def triplet_loss(_, y_pred):\n",
    "        # Make sure that the shape of y_pred is correctly understood as [batch_size, 2], the first one is the positive sample distance and the second one is the negative sample distance\n",
    "        margin = 1.0\n",
    "        pos_dist = y_pred[:, 0]  # Positive sample distance\n",
    "        neg_dist = y_pred[:, 1]  # Negative sample distance\n",
    "        # Calculate triplet loss\n",
    "        loss = K.mean(K.maximum(pos_dist - neg_dist + margin, 0.0))\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(_, y_pred):\n",
    "        # Judging accuracy based on distance\n",
    "        pos_dist = y_pred[:, 0]\n",
    "        neg_dist = y_pred[:, 1]\n",
    "        return K.mean(pos_dist < neg_dist)\n",
    "\n",
    "triplet = Triplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2a8ae8-e575-49ab-8bbd-5273b35da476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eval_utils:\n",
    "    @staticmethod\n",
    "    def pairwise_precision_recall_f1(preds, truths):\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        n_samples = len(preds)\n",
    "        for i in range(n_samples - 1):\n",
    "            pred_i = preds[i]\n",
    "            for j in range(i + 1, n_samples):\n",
    "                pred_j = preds[j]\n",
    "                if pred_i == pred_j:\n",
    "                    if truths[i] == truths[j]:\n",
    "                        tp += 1\n",
    "                    else:\n",
    "                        fp += 1\n",
    "                elif truths[i] == truths[j]:\n",
    "                    fn += 1\n",
    "        tp_plus_fp = tp + fp\n",
    "        tp_plus_fn = tp + fn\n",
    "        if tp_plus_fp == 0:\n",
    "            precision = 0.\n",
    "        else:\n",
    "            precision = tp / tp_plus_fp\n",
    "        if tp_plus_fn == 0:\n",
    "            recall = 0.\n",
    "        else:\n",
    "            recall = tp / tp_plus_fn\n",
    "\n",
    "        if not precision or not recall:\n",
    "            f1 = 0.\n",
    "        else:\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return precision, recall, f1\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_f1(prec, rec):\n",
    "        return 2 * prec * rec / (prec + rec) if (prec + rec) else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_hidden_output(model, inp):\n",
    "        # The first input of the original model is the anchor input\n",
    "        anchor_input = model.input[0]  # Get the anchor point input of the original model\n",
    "        intermediate_layer_model = Model(inputs=anchor_input, outputs=model.layers[5].output)\n",
    "        # Make sure the input provided is a list, even if it only has one element\n",
    "        intermediate_output = intermediate_layer_model([inp])\n",
    "        return intermediate_output\n",
    "        \n",
    "    @staticmethod\n",
    "    def predict(anchor_emb, test_embs):\n",
    "        score1 = np.linalg.norm(anchor_emb - test_embs[0])\n",
    "        score2 = np.linalg.norm(anchor_emb - test_embs[1])\n",
    "        return [score1, score2]\n",
    "\n",
    "    @staticmethod\n",
    "    def full_auc(model, test_triplets):\n",
    "    \n",
    "        \"\"\"\n",
    "        Measure AUC for model and ground truth on all items.\n",
    "    \n",
    "        Returns:\n",
    "        - float AUC\n",
    "        \"\"\"\n",
    "    \n",
    "        grnds = []\n",
    "        preds = []\n",
    "        preds_before = []\n",
    "        embs_anchor, embs_pos, embs_neg = test_triplets\n",
    "    \n",
    "        inter_embs_anchor = eval_utils.get_hidden_output(model, embs_anchor)\n",
    "        inter_embs_pos = eval_utils.get_hidden_output(model, embs_pos)\n",
    "        inter_embs_neg = eval_utils.get_hidden_output(model, embs_neg)\n",
    "        # print(inter_embs_pos.shape)\n",
    "    \n",
    "        accs = []\n",
    "        accs_before = []\n",
    "    \n",
    "        for i, e in enumerate(inter_embs_anchor):\n",
    "            if i % 10000 == 0:\n",
    "                print('test', i)\n",
    "    \n",
    "            emb_anchor = e\n",
    "            emb_pos = inter_embs_pos[i]\n",
    "            emb_neg = inter_embs_neg[i]\n",
    "            test_embs = np.array([emb_pos, emb_neg])\n",
    "    \n",
    "            emb_anchor_before = embs_anchor[i]\n",
    "            emb_pos_before = embs_pos[i]\n",
    "            emb_neg_before = embs_neg[i]\n",
    "            test_embs_before = np.array([emb_pos_before, emb_neg_before])\n",
    "    \n",
    "            predictions = eval_utils.predict(emb_anchor, test_embs)\n",
    "            predictions_before = eval_utils.predict(emb_anchor_before, test_embs_before)\n",
    "\n",
    "            acc_before = 1 if predictions_before[0] < predictions_before[1] else 0\n",
    "            acc = 1 if predictions[0] < predictions[1] else 0\n",
    "            accs_before.append(acc_before)\n",
    "            accs.append(acc)\n",
    "    \n",
    "            grnd = [0, 1]\n",
    "            grnds += grnd\n",
    "            preds += predictions\n",
    "            preds_before += predictions_before\n",
    "    \n",
    "        auc_before = roc_auc_score(grnds, preds_before)\n",
    "        auc = roc_auc_score(grnds, preds)\n",
    "        print('test accuracy before', np.mean(accs_before))\n",
    "        print('test accuracy after', np.mean(accs))\n",
    "    \n",
    "        print('test AUC before', auc_before)\n",
    "        print('test AUC after', auc)\n",
    "        return auc\n",
    "        \n",
    "eval_utils = eval_utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ed0e9a9-889e-4a22-b761-5db0bc6c8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom layer to replace the Lambda layer\n",
    "@register_keras_serializable()\n",
    "class custom_stack_layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(custom_stack_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        pos_dist, neg_dist = inputs\n",
    "        return tf.stack([pos_dist, neg_dist], axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = list(input_shape[0])\n",
    "        return (shape[0], 2, 1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(custom_stack_layer, self).get_config()\n",
    "        return config\n",
    "        \n",
    "@register_keras_serializable()\n",
    "class custom_lambda_layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(custom_lambda_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        pos_dist, neg_dist = inputs\n",
    "        return tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(inputs[0] - inputs[1]), axis=1, keepdims=True), tf.keras.backend.epsilon()))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(custom_lambda_layer, self).get_config()\n",
    "        return config\n",
    "        \n",
    "@register_keras_serializable()\n",
    "class l2_Norm_layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(l2_Norm_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return triplet.l2Norm(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f51439a-9691-4d35-8415-15cf1965de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "global metric learning model\n",
    "\"\"\"\n",
    "\n",
    "EMB_DIM = 768\n",
    "#@tf.keras.utils.register_keras_serializable()\n",
    "#@staticmethod\n",
    "def cal_output_shape(input_shape):\n",
    "    shape = list(input_shape[0])\n",
    "    assert len(shape) == 2  # only valid for 2D tensors\n",
    "    shape[-1] *= 2\n",
    "    return tuple(shape)\n",
    "    \n",
    "class GlobalTripletModel:\n",
    "\n",
    "    def __init__(self, data_scale, EMB_DIM=768):\n",
    "        self.data_scale = data_scale\n",
    "        self.EMB_DIM = EMB_DIM\n",
    "        self.train_triplets_dir = join(settings.OUT_DIR, 'triplets-{}'.format(self.data_scale))\n",
    "        self.test_triplets_dir = join(settings.OUT_DIR, 'test-triplets')\n",
    "        self.train_triplet_files_num = self.get_triplets_files_num(self.train_triplets_dir)\n",
    "        self.test_triplet_files_num = self.get_triplets_files_num(self.test_triplets_dir)\n",
    "        print('test file num', self.test_triplet_files_num)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_triplets_files_num(path_dir):\n",
    "        files = []\n",
    "        for f in os.listdir(path_dir):\n",
    "            if f.startswith('anchor_embs_'):\n",
    "                files.append(f)\n",
    "        return len(files)\n",
    "\n",
    "    def load_batch_triplets(self, f_idx, role='train'):\n",
    "        if role == 'train':\n",
    "            cur_dir = self.train_triplets_dir\n",
    "        else:\n",
    "            cur_dir = self.test_triplets_dir\n",
    "        X1 = data_utils.load_data(cur_dir, 'anchor_embs_{}_{}.pkl'.format(role, f_idx))\n",
    "        X2 = data_utils.load_data(cur_dir, 'pos_embs_{}_{}.pkl'.format(role, f_idx))\n",
    "        X3 = data_utils.load_data(cur_dir, 'neg_embs_{}_{}.pkl'.format(role, f_idx))\n",
    "        return X1, X2, X3\n",
    "\n",
    "    def load_triplets_data(self, role='train'):\n",
    "        X1 = np.empty([0, EMB_DIM])\n",
    "        X2 = np.empty([0, EMB_DIM])\n",
    "        X3 = np.empty([0, EMB_DIM])\n",
    "        if role == 'train':\n",
    "            f_num = self.train_triplet_files_num\n",
    "        else:\n",
    "            f_num = self.test_triplet_files_num\n",
    "        for i in range(f_num):\n",
    "            print('load', i)\n",
    "            x1_batch, x2_batch, x3_batch = self.load_batch_triplets(i, role)\n",
    "            p = np.random.permutation(len(x1_batch))\n",
    "            x1_batch = np.array(x1_batch)[p]\n",
    "            x2_batch = np.array(x2_batch)[p]\n",
    "            x3_batch = np.array(x3_batch)[p]\n",
    "            X1 = np.concatenate((X1, x1_batch))\n",
    "            X2 = np.concatenate((X2, x2_batch))\n",
    "            X3 = np.concatenate((X3, x3_batch))\n",
    "        return X1, X2, X3\n",
    "\n",
    "    @staticmethod\n",
    "    def create_triplet_model(EMB_DIM=768):\n",
    "        emb_anchor = Input(shape=(EMB_DIM,), name='anchor_input')\n",
    "        emb_pos = Input(shape=(EMB_DIM,), name='pos_input')\n",
    "        emb_neg = Input(shape=(EMB_DIM,), name='neg_input')\n",
    "\n",
    "        # shared layers\n",
    "        layer1 = Dense(128, activation='relu', name='first_emb_layer')\n",
    "        layer2 = Dense(64, activation='relu', name='last_emb_layer')\n",
    "        norm_layer = l2_Norm_layer(name='norm_layer')\n",
    "        \n",
    "        encoded_emb = norm_layer(layer2(layer1(emb_anchor)))\n",
    "        encoded_emb_pos = norm_layer(layer2(layer1(emb_pos)))\n",
    "        encoded_emb_neg = norm_layer(layer2(layer1(emb_neg)))\n",
    "\n",
    "        lambda_layer = custom_lambda_layer(name='distance_layer')\n",
    "        pos_dist = lambda_layer([encoded_emb, encoded_emb_pos])\n",
    "        neg_dist = lambda_layer([encoded_emb, encoded_emb_neg])\n",
    "\n",
    "        stack_layer = custom_stack_layer(name='stack_layer')\n",
    "        stacked_dists = stack_layer([pos_dist, neg_dist])\n",
    "        \n",
    "        model = Model(inputs=[emb_anchor, emb_pos, emb_neg], outputs=stacked_dists)\n",
    "        model.compile(loss=triplet.triplet_loss, optimizer=Adam(learning_rate=0.01), metrics=[triplet.accuracy])\n",
    "\n",
    "       \n",
    "    \n",
    "        inter_layer = Model(inputs=model.inputs, outputs=model.get_layer('norm_layer').output)\n",
    "    \n",
    "        return model, inter_layer\n",
    "\n",
    "    def load_triplets_model(self):\n",
    "        model_dir = join(settings.OUT_DIR, 'model')\n",
    "        model_filename = os.path.join(model_dir, 'model-triplets-{}.json'.format(self.data_scale))\n",
    "\n",
    "        with open(model_filename, 'r') as json_file:\n",
    "            model_json_string = json_file.read() \n",
    "\n",
    "    \n",
    "        # Define custom_objects dictionary, containing all custom functions\n",
    "        custom_objects = {\n",
    "            'custom_lambda_layer': custom_lambda_layer,\n",
    "            'custom_stack_layer': custom_stack_layer,\n",
    "            'l2Norm': triplet.l2Norm,\n",
    "            'euclidean_distance': triplet.euclidean_distance,\n",
    "            'triplet_loss': triplet.triplet_loss,\n",
    "            'accuracy': triplet.accuracy,\n",
    "            'cal_output_shape': cal_output_shape\n",
    "        }\n",
    "\n",
    "        # Load the model using the read JSON string and custom objects\n",
    "        loaded_model = model_from_json(model_json_string, custom_objects=custom_objects)\n",
    "        weights_filename = os.path.join(model_dir, 'model-triplets-{}.weights.h5'.format(self.data_scale))\n",
    "        loaded_model.load_weights(weights_filename)\n",
    "        return loaded_model\n",
    "\n",
    "    def train_triplets_model(self):\n",
    "        X1, X2, X3 = self.load_triplets_data()\n",
    "        n_triplets = len(X1)\n",
    "        print('loaded')\n",
    "        model, inter_model = self.create_triplet_model()\n",
    "        # print(model.summary())\n",
    "\n",
    "        X_anchor, X_pos, X_neg = X1, X2, X3\n",
    "        X = {'anchor_input': X_anchor, 'pos_input': X_pos, 'neg_input': X_neg}\n",
    "        model.fit(X, np.ones((n_triplets, 2)), batch_size=64, epochs=5, shuffle=True, validation_split=0.2)\n",
    "\n",
    "        model_json = model.to_json()\n",
    "        model_dir = join(settings.OUT_DIR, 'model')\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        with open(join(model_dir, 'model-triplets-{}.json'.format(self.data_scale)), 'w') as wf:\n",
    "            wf.write(model_json)\n",
    "        model.save_weights(join(model_dir, 'model-triplets-{}.weights.h5'.format(self.data_scale)))\n",
    "\n",
    "        test_triplets = self.load_triplets_data(role='test')\n",
    "        auc_score = eval_utils.full_auc(model, test_triplets)\n",
    "        # print('AUC', auc_score)\n",
    "\n",
    "        loaded_model = self.load_triplets_model()\n",
    "        print('triplets model loaded')\n",
    "        auc_score = eval_utils.full_auc(loaded_model, test_triplets)\n",
    "\n",
    "    def evaluate_triplet_model(self):\n",
    "        test_triplets = self.load_triplets_data(role='test')\n",
    "        loaded_model = self.load_triplets_model()\n",
    "        print('triplets model loaded')\n",
    "        auc_score = eval_utils.full_auc(loaded_model, test_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bc9d589-bf11-44e2-ae24-7ea44249eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test file num 2\n",
      "load 0\n",
      "load 1\n",
      "load 2\n",
      "load 3\n",
      "load 4\n",
      "load 5\n",
      "load 6\n",
      "load 7\n",
      "loaded\n",
      "Epoch 1/5\n",
      " 111/9929 [..............................] - ETA: 13s - loss: 0.7284 - accuracy: 0.7248  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 15:13:47.453468: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f2a50008f60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-02 15:13:47.453498: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Host, Default Version\n",
      "2024-05-02 15:13:47.457975: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-02 15:13:47.494699: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9929/9929 [==============================] - 18s 2ms/step - loss: 0.5473 - accuracy: 0.7858 - val_loss: 0.5346 - val_accuracy: 0.7844\n",
      "Epoch 2/5\n",
      "9929/9929 [==============================] - 15s 2ms/step - loss: 0.5440 - accuracy: 0.7831 - val_loss: 0.8255 - val_accuracy: 0.6040\n",
      "Epoch 3/5\n",
      "9929/9929 [==============================] - 16s 2ms/step - loss: 0.5660 - accuracy: 0.7664 - val_loss: 0.6249 - val_accuracy: 0.7283\n",
      "Epoch 4/5\n",
      "9929/9929 [==============================] - 15s 2ms/step - loss: 0.6273 - accuracy: 0.7315 - val_loss: 0.6158 - val_accuracy: 0.7345\n",
      "Epoch 5/5\n",
      "9929/9929 [==============================] - 16s 2ms/step - loss: 0.6189 - accuracy: 0.7338 - val_loss: 0.6291 - val_accuracy: 0.7116\n",
      "load 0\n",
      "load 1\n",
      "test 0\n",
      "test 10000\n",
      "test 20000\n",
      "test 30000\n",
      "test 40000\n",
      "test 50000\n",
      "test 60000\n",
      "test 70000\n",
      "test 80000\n",
      "test 90000\n",
      "test 100000\n",
      "test 110000\n",
      "test 120000\n",
      "test 130000\n",
      "test 140000\n",
      "test 150000\n",
      "test 160000\n",
      "test accuracy before 0.7387686779935374\n",
      "test accuracy after 0.7289889817466109\n",
      "test AUC before 0.7120272900484129\n",
      "test AUC after 0.7473216180172486\n",
      "triplets model loaded\n",
      "test 0\n",
      "test 10000\n",
      "test 20000\n",
      "test 30000\n",
      "test 40000\n",
      "test 50000\n",
      "test 60000\n",
      "test 70000\n",
      "test 80000\n",
      "test 90000\n",
      "test 100000\n",
      "test 110000\n",
      "test 120000\n",
      "test 130000\n",
      "test 140000\n",
      "test 150000\n",
      "test 160000\n",
      "test accuracy before 0.7387686779935374\n",
      "test accuracy after 0.7289889817466109\n",
      "test AUC before 0.7120272900484129\n",
      "test AUC after 0.7473216180172486\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "global_model = GlobalTripletModel(data_scale=1000000)\n",
    "global_model.train_triplets_model()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b629c-7f59-499e-9ed7-f1e2578cc529",
   "metadata": {},
   "source": [
    "# 4. Global model_Prepare_local_data stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87548104-57e5-45c4-862e-cefe8c934a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF_THRESHOLD = 32  # small data\n",
    "IDF_THRESHOLD = 10\n",
    "\n",
    "\n",
    "def dump_inter_emb():\n",
    "    \"\"\"\n",
    "    dump hidden embedding via trained global model for local model to use\n",
    "    \"\"\"\n",
    "    LMDB_NAME = \"author_100_emb_weighted\"\n",
    "    lc_input = LMDBClient(LMDB_NAME)\n",
    "    INTER_LMDB_NAME = 'author_triplets.emb'\n",
    "    lc_inter = LMDBClient(INTER_LMDB_NAME)\n",
    "    global_model = GlobalTripletModel(data_scale=1000000)\n",
    "    trained_global_model = global_model.load_triplets_model()\n",
    "    name_to_pubs_test = data_utils.load_json(settings.GLOBAL_DATA_DIR, 'name_to_pubs_test_100.json')\n",
    "    for name in name_to_pubs_test:\n",
    "        print('name', name)\n",
    "        name_data = name_to_pubs_test[name]\n",
    "        embs_input = []\n",
    "        pids = []\n",
    "        for i, aid in enumerate(name_data.keys()):\n",
    "            if len(name_data[aid]) < 5:  # n_pubs of current author is too small\n",
    "                continue\n",
    "            for pid in name_data[aid]:\n",
    "                cur_emb = lc_input.get(pid)\n",
    "                if cur_emb is None:\n",
    "                    continue\n",
    "                embs_input.append(cur_emb)\n",
    "                pids.append(pid)\n",
    "        embs_input = np.stack(embs_input)\n",
    "        inter_embs = eval_utils.get_hidden_output(trained_global_model, embs_input)\n",
    "        for i, pid_ in enumerate(pids):\n",
    "            lc_inter.set(pid_, inter_embs[i])\n",
    "\n",
    "\n",
    "def gen_local_data(idf_threshold=10):\n",
    "    \"\"\"\n",
    "    generate local data (including paper features and paper network) for each associated name\n",
    "    :param idf_threshold: threshold for determining whether there exists an edge between two papers (for this demo we set 29)\n",
    "    \"\"\"\n",
    "    name_to_pubs_test = data_utils.load_json(settings.GLOBAL_DATA_DIR, 'name_to_pubs_test_100.json')\n",
    "    idf = data_utils.load_data(settings.GLOBAL_DATA_DIR, 'feature_idf.pkl')\n",
    "    INTER_LMDB_NAME = 'author_triplets.emb'\n",
    "    lc_inter = LMDBClient(INTER_LMDB_NAME)\n",
    "    LMDB_AUTHOR_FEATURE = \"pub_authors_feature\"\n",
    "    lc_feature = LMDBClient(LMDB_AUTHOR_FEATURE)\n",
    "    graph_dir = join(settings.DATA_DIR, 'local', 'graph-{}'.format(idf_threshold))\n",
    "    os.makedirs(graph_dir, exist_ok=True)\n",
    "    for i, name in enumerate(name_to_pubs_test):\n",
    "        print(i, name)\n",
    "        cur_person_dict = name_to_pubs_test[name]\n",
    "        pids_set = set()\n",
    "        pids = []\n",
    "        pids2label = {}\n",
    "\n",
    "        # generate content\n",
    "        wf_content = open(join(graph_dir, '{}_pubs_content.txt'.format(name)), 'w')\n",
    "        for i, aid in enumerate(cur_person_dict):\n",
    "            items = cur_person_dict[aid]\n",
    "            if len(items) < 5:\n",
    "                continue\n",
    "            for pid in items:\n",
    "                pids2label[pid] = aid\n",
    "                pids.append(pid)\n",
    "        shuffle(pids)\n",
    "        for pid in pids:\n",
    "            cur_pub_emb = lc_inter.get(pid)\n",
    "            if cur_pub_emb is not None:\n",
    "                cur_pub_emb = list(map(str, cur_pub_emb))\n",
    "                pids_set.add(pid)\n",
    "                wf_content.write('{}\\t'.format(pid))\n",
    "                wf_content.write('\\t'.join(cur_pub_emb))\n",
    "                wf_content.write('\\t{}\\n'.format(pids2label[pid]))\n",
    "        wf_content.close()\n",
    "\n",
    "        # generate network\n",
    "        pids_filter = list(pids_set)\n",
    "        n_pubs = len(pids_filter)\n",
    "        print('n_pubs', n_pubs)\n",
    "        wf_network = open(join(graph_dir, '{}_pubs_network.txt'.format(name)), 'w')\n",
    "        for i in range(n_pubs-1):\n",
    "            if i % 10 == 0:\n",
    "                print(i)\n",
    "            author_feature1 = set(lc_feature.get(pids_filter[i]))\n",
    "            for j in range(i+1, n_pubs):\n",
    "                author_feature2 = set(lc_feature.get(pids_filter[j]))\n",
    "                common_features = author_feature1.intersection(author_feature2)\n",
    "                idf_sum = 0\n",
    "                for f in common_features:\n",
    "                    idf_sum += idf.get(f, idf_threshold)\n",
    "                    # print(f, idf.get(f, idf_threshold))\n",
    "                if idf_sum >= idf_threshold:\n",
    "                    wf_network.write('{}\\t{}\\n'.format(pids_filter[i], pids_filter[j]))\n",
    "        wf_network.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf61183-e66a-4edb-820e-6a726ec6d7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB directory: /home/netdb/project/Name_Disambiguation_BERT/data/lmdb/author_100_emb_weighted\n",
      "LMDB directory: /home/netdb/project/Name_Disambiguation_BERT/data/lmdb/author_triplets.emb\n",
      "test file num 2\n",
      "name hongbin_liang\n",
      "name j_yu\n",
      "name s_yu\n",
      "name he_ping_li\n",
      "name song_chen\n",
      "name jia_xu\n",
      "name lixin_tang\n",
      "name guotong_du\n",
      "name chih_ming_lin\n",
      "name wensheng_yang\n",
      "name fosong_wang\n",
      "name guorong_chen\n",
      "name bo_ai\n",
      "name yongjian_tang\n",
      "name qi_hu\n",
      "name ping_fu\n",
      "name geng_yang\n",
      "name li_zhu_wu\n",
      "name da_xing\n",
      "name jian_guo_he\n",
      "name xiaobing_luo\n",
      "name yang_shen\n",
      "name lan_sun\n",
      "name jian_du\n",
      "name jianhua_lu\n",
      "name jing_luo\n",
      "name zhigang_zeng\n",
      "name jian_feng\n",
      "name hua_fu\n",
      "name mei_xu\n",
      "name ruijin_liao\n",
      "name yu_ming_wang\n",
      "name bo_hong\n",
      "name weiming_zhu\n",
      "name xu_xu\n",
      "name rong_yu\n",
      "name yong_tian\n",
      "name lu_han\n",
      "name lin_huang\n",
      "name kexin_xu\n",
      "name wei_quan\n",
      "name tao_deng\n",
      "name hongbin_li\n",
      "name hua_bai\n",
      "name mei_ling_chen\n",
      "name yanqing_wang\n",
      "name xu_dong_zhang\n",
      "name qiang_shi\n",
      "name min_zheng\n",
      "name yue_tian\n",
      "name jian_shao\n",
      "name tian_chen\n",
      "name yan_yan_li\n",
      "name xue_qin\n",
      "name ming_shi\n",
      "name chunyan_liu\n",
      "name shuai_yuan\n",
      "name wen_chang_chen\n",
      "name minghui_li\n",
      "name kwok_fai_so\n",
      "name yin_shi\n",
      "name rong_lu\n",
      "name s_lin\n",
      "name shengkai_gong\n",
      "name yong_qing_huang\n",
      "name jie_jiang\n",
      "name xiao_ning_zhang\n",
      "name chun_hung_lin\n",
      "name xiao_juan_li\n",
      "name fei_sun\n",
      "name yongqing_huang\n",
      "name zhifeng_liu\n",
      "name su_zeng\n",
      "name lei_song\n",
      "name mian_chen\n",
      "name fuchu_he\n",
      "name shuang_song\n",
      "name hongtao_liu\n",
      "name c_y_huang\n",
      "name s_c_wu\n",
      "name wei_jun_zhang\n",
      "name jiamo_fu\n",
      "name hong_fan\n",
      "name h_xie\n",
      "name dan_sun\n",
      "name shui_wang\n",
      "name feng_teng\n",
      "name j_h_lin\n",
      "name lili_ma\n",
      "name yongqing_li\n",
      "name yin_wu\n",
      "name junling_wang\n",
      "name jian_ping_wang\n",
      "name ping_sun\n",
      "name jianqiang_yi\n",
      "name philip_kam_tao_li\n",
      "name dandan_zhang\n",
      "name yun_zhou\n",
      "name weiwei_li\n",
      "name xi_huang\n",
      "LMDB directory: /home/netdb/project/Name_Disambiguation_BERT/data/lmdb/author_triplets.emb\n",
      "LMDB directory: /home/netdb/project/Name_Disambiguation_BERT/data/lmdb/pub_authors_feature\n",
      "0 hongbin_liang\n",
      "n_pubs 183\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "1 j_yu\n",
      "n_pubs 265\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "2 s_yu\n",
      "n_pubs 61\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "3 he_ping_li\n",
      "n_pubs 234\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "4 song_chen\n",
      "n_pubs 598\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "5 jia_xu\n",
      "n_pubs 609\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "6 lixin_tang\n",
      "n_pubs 195\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "7 guotong_du\n",
      "n_pubs 220\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "8 chih_ming_lin\n",
      "n_pubs 162\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "9 wensheng_yang\n",
      "n_pubs 276\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "10 fosong_wang\n",
      "n_pubs 356\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "11 guorong_chen\n",
      "n_pubs 335\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "12 bo_ai\n",
      "n_pubs 225\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "13 yongjian_tang\n",
      "n_pubs 198\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "14 qi_hu\n",
      "n_pubs 173\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "15 ping_fu\n",
      "n_pubs 287\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "16 geng_yang\n",
      "n_pubs 292\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "17 li_zhu_wu\n",
      "n_pubs 224\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "18 da_xing\n",
      "n_pubs 596\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "19 jian_guo_he\n",
      "n_pubs 185\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "20 xiaobing_luo\n",
      "n_pubs 196\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "21 yang_shen\n",
      "n_pubs 485\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "22 lan_sun\n",
      "n_pubs 181\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "23 jian_du\n",
      "n_pubs 162\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "24 jianhua_lu\n",
      "n_pubs 361\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "25 jing_luo\n",
      "n_pubs 469\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "26 zhigang_zeng\n",
      "n_pubs 235\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "27 jian_feng\n",
      "n_pubs 465\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "28 hua_fu\n",
      "n_pubs 415\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "29 mei_xu\n",
      "n_pubs 98\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "30 ruijin_liao\n",
      "n_pubs 236\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "31 yu_ming_wang\n",
      "n_pubs 227\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "32 bo_hong\n",
      "n_pubs 321\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "33 weiming_zhu\n",
      "n_pubs 160\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "34 xu_xu\n",
      "n_pubs 555\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "35 rong_yu\n",
      "n_pubs 273\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "36 yong_tian\n",
      "n_pubs 262\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "37 lu_han\n",
      "n_pubs 366\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "38 lin_huang\n",
      "n_pubs 553\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "39 kexin_xu\n",
      "n_pubs 203\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "40 wei_quan\n",
      "n_pubs 174\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "41 tao_deng\n",
      "n_pubs 306\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "42 hongbin_li\n",
      "n_pubs 286\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "43 hua_bai\n",
      "n_pubs 351\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "44 mei_ling_chen\n",
      "n_pubs 161\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "45 yanqing_wang\n",
      "n_pubs 107\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "46 xu_dong_zhang\n",
      "n_pubs 224\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "47 qiang_shi\n",
      "n_pubs 241\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "48 min_zheng\n",
      "n_pubs 523\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "49 yue_tian\n",
      "n_pubs 127\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "50 jian_shao\n",
      "n_pubs 162\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "51 tian_chen\n",
      "n_pubs 180\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "52 yan_yan_li\n",
      "n_pubs 103\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "53 xue_qin\n",
      "n_pubs 224\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "54 ming_shi\n",
      "n_pubs 262\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "55 chunyan_liu\n",
      "n_pubs 222\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "56 shuai_yuan\n",
      "n_pubs 310\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "57 wen_chang_chen\n",
      "n_pubs 301\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "58 minghui_li\n",
      "n_pubs 173\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "59 kwok_fai_so\n",
      "n_pubs 229\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "60 yin_shi\n",
      "n_pubs 169\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "61 rong_lu\n",
      "n_pubs 205\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "62 s_lin\n",
      "n_pubs 71\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "63 shengkai_gong\n",
      "n_pubs 238\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "64 yong_qing_huang\n",
      "n_pubs 196\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "65 jie_jiang\n",
      "n_pubs 558\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "66 xiao_ning_zhang\n",
      "n_pubs 186\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "67 chun_hung_lin\n",
      "n_pubs 217\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "68 xiao_juan_li\n",
      "n_pubs 100\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "69 fei_sun\n",
      "n_pubs 357\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "70 yongqing_huang\n",
      "n_pubs 352\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "71 zhifeng_liu\n",
      "n_pubs 340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "72 su_zeng\n",
      "n_pubs 253\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "73 lei_song\n",
      "n_pubs 700\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "74 mian_chen\n",
      "n_pubs 185\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "75 fuchu_he\n",
      "n_pubs 274\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "76 shuang_song\n",
      "n_pubs 154\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "77 hongtao_liu\n",
      "n_pubs 328\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "78 c_y_huang\n",
      "n_pubs 203\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "79 s_c_wu\n",
      "n_pubs 208\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "80 wei_jun_zhang\n",
      "n_pubs 200\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "81 jiamo_fu\n",
      "n_pubs 364\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "82 hong_fan\n",
      "n_pubs 366\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "83 h_xie\n",
      "n_pubs 105\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "84 dan_sun\n",
      "n_pubs 204\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "85 shui_wang\n",
      "n_pubs 305\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "86 feng_teng\n",
      "n_pubs 228\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "87 j_h_lin\n",
      "n_pubs 189\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "88 lili_ma\n",
      "n_pubs 133\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "89 yongqing_li\n",
      "n_pubs 150\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "90 yin_wu\n",
      "n_pubs 179\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "91 junling_wang\n",
      "n_pubs 157\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "92 jian_ping_wang\n",
      "n_pubs 399\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "93 ping_sun\n",
      "n_pubs 524\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "94 jianqiang_yi\n",
      "n_pubs 226\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "95 philip_kam_tao_li\n",
      "n_pubs 245\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "96 dandan_zhang\n",
      "n_pubs 162\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "97 yun_zhou\n",
      "n_pubs 607\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "98 weiwei_li\n",
      "n_pubs 259\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "99 xi_huang\n",
      "n_pubs 269\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "dump_inter_emb()\n",
    "gen_local_data(idf_threshold=IDF_THRESHOLD)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a3c04-0011-4fe0-8564-bb9e902e2fb6",
   "metadata": {},
   "source": [
    "# 5. Local model_train stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d09595b-6693-4bcb-896f-52c47df26833",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "hidden1 = 128\n",
    "hidden2 = 64\n",
    "weight_decay = 0.0\n",
    "dropout = 0.0\n",
    "model = 'gcn_vae'\n",
    "dataset_name = 'hui_fang'\n",
    "is_sparse = 0\n",
    "model_str = model \n",
    "name_str = dataset_name \n",
    "\n",
    "# Get current time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d63efd59-6cb5-4f57-86b3-345e80b2a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using optimizers from tf.keras.optimizers\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "def compute_loss(preds, labels, model=None, num_nodes=None, pos_weight=1.0, norm=1.0):\n",
    "    \"\"\"\n",
    "    Computes the loss function given predictions and labels.\n",
    "    If model is not None and is a VAE model, the KL divergence loss is added.\n",
    "    \"\"\"\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=preds, labels=labels, pos_weight=pos_weight))\n",
    "    cost = norm * cross_entropy_loss\n",
    "\n",
    "    if model is not None and hasattr(model, 'z_mean') and hasattr(model, 'z_log_std'):\n",
    "        kl_divergence = (0.5 / num_nodes) * tf.reduce_mean(tf.reduce_sum(1 + 2 * model.z_log_std - tf.square(model.z_mean) - tf.square(tf.exp(model.z_log_std)), 1))\n",
    "        cost -= kl_divergence\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c690dbd7-9081-4ba7-84f3-a3f22cd66d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_na_dir = join(settings.DATA_DIR, 'local', 'graph-{}'.format(IDF_THRESHOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02dcd175-82eb-4d35-aa89-908d856e4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_float_from_tensor_string(tensor_string):\n",
    "    # Update regex to match floating point numbers more accurately\n",
    "    match = re.search(r\"\\((-?\\d+(\\.\\d+)?),\", tensor_string)\n",
    "    if match:\n",
    "        return float(match.group(1))  # Convert the first matched group to float\n",
    "    return 0.0  # If no match is found, returns 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d434879c-d4e1-4dd4-a36e-3f55a0176e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input_data:\n",
    "    def __init__(self, idf_threshold):\n",
    "        self.local_na_dir = join(settings.DATA_DIR, 'local', 'graph-{}'.format(idf_threshold))\n",
    "\n",
    "    def encode_labels(self, labels):\n",
    "        classes = set(labels)\n",
    "        classes_dict = {c: i for i, c in enumerate(classes)}\n",
    "        return list(map(lambda x: classes_dict[x], labels))\n",
    "\n",
    "    def load_local_data(self, name='default_name'):\n",
    "        # Load local paper network dataset\n",
    "        print('Loading {} dataset...'.format(name), 'path=', self.local_na_dir)\n",
    "\n",
    "        idx_features_labels = np.genfromtxt(join(self.local_na_dir, \"{}_pubs_content.txt\".format(name)), dtype=np.dtype(str))\n",
    "        # Use helper functions to process each eigenvalue\n",
    "        features = np.array([[extract_float_from_tensor_string(value) for value in row[1:-1]] for row in idx_features_labels], dtype=np.float32)\n",
    "        labels = self.encode_labels(idx_features_labels[:, -1])\n",
    "\n",
    "        # build graph\n",
    "        idx = np.array(idx_features_labels[:, 0], dtype=str)  \n",
    "        idx_map = {j: i for i, j in enumerate(idx)}\n",
    "        edges_unordered = np.genfromtxt(join(self.local_na_dir, \"{}_pubs_network.txt\".format(name)), dtype=np.dtype(str))\n",
    "        edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                         dtype=np.int32).reshape(edges_unordered.shape)\n",
    "        adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                            shape=(features.shape[0], features.shape[0]), dtype=np.float32)\n",
    "\n",
    "        # build symmetric adjacency matrix\n",
    "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "        print('Dataset has {} nodes, {} edges, {} features.'.format(adj.shape[0], edges.shape[0], features.shape[1]))\n",
    "\n",
    "        return adj, features, labels\n",
    "        \n",
    "idf_threshold = 10\n",
    "input_data = Input_data(idf_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89185312-1426-4333-8650-efac4469f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
    "    initialization.\n",
    "    \"\"\"\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
    "                                maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98104153-0ab6-444a-817b-d5798342f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dae8715f-964d-4c13-8bef-e2390936cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs\n",
    "    \"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
    "    \"\"\"\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "\n",
    "class Layer(tf.keras.layers.Layer):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Layer, self).__init__(**kwargs)\n",
    "        self.issparse = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            outputs = self._call(inputs)\n",
    "            return outputs\n",
    "\n",
    "class GraphConvolution(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate = 0., activation=tf.nn.relu, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "        self.weight = self.add_weight(shape=(input_dim, output_dim), initializer='glorot_uniform', name='kernel')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, adj_components = inputs\n",
    "        indices = tf.cast(adj_components.indices, dtype=tf.int64)  # Get the indices of a sparse tensor\n",
    "        values = adj_components.values  # Get the values of a sparse tensor\n",
    "        dense_shape = adj_components.dense_shape  # Get the shape of a sparse tensor\n",
    "\n",
    "        adj = tf.SparseTensor(indices=indices, values=values, dense_shape=dense_shape)  # Create a SparseTensor using the converted indices\n",
    "\n",
    "        # print(\"Type of adj in GraphConvolution:\", type(adj))  # Confirm the type of adj\n",
    "        \n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=self.dropout_rate)\n",
    "        \n",
    "        x = tf.matmul(x, self.weight)\n",
    "        x = tf.sparse.sparse_dense_matmul(adj, x)\n",
    "        \n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GraphConvolutionSparse(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, features_nonzero, dropout_rate=0., activation=tf.nn.relu, **kwargs):\n",
    "        super(GraphConvolutionSparse, self).__init__(**kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "        self.features_nonzero = features_nonzero\n",
    "        self.weight = self.add_weight(shape=(input_dim, output_dim), initializer='glorot_uniform', name='kernel')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, adj = inputs\n",
    "\n",
    "        if training:\n",
    "            x = dropout_sparse(x, 1 - self.dropout_rate, self.features_nonzero)\n",
    "        \n",
    "        x = tf.sparse.sparse_dense_matmul(x, self.weight)\n",
    "        x = tf.sparse.sparse_dense_matmul(adj, x)\n",
    "        \n",
    "        if self.activation is not None:\n",
    "            outputs = self.activation(x)\n",
    "        return outputs\n",
    "\n",
    "class InnerProductDecoder(tf.keras.layers.Layer):\n",
    "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout_rate=0., activation=tf.nn.sigmoid, **kwargs):\n",
    "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if training and self.dropout_rate > 0:\n",
    "            inputs = tf.nn.dropout(inputs, rate=self.dropout_rate)\n",
    "        x = tf.transpose(inputs)\n",
    "        x = tf.matmul(inputs, x)\n",
    "        outputs = tf.reshape(x, [-1])\n",
    "        return self.activation(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af3d6b2e-cb0d-4920-b274-1d503d2866d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1_units = 128\n",
    "hidden2_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56e4449c-79e7-4ade-be8d-11c111abeee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, name=None, logging=False, **kwargs):\n",
    "        super(Model, self).__init__(name=name, **kwargs)\n",
    "        self.logging = logging\n",
    "        self.issparse = False  # Set as needed to mark whether the input is sparse\n",
    "\n",
    "    def get_embeddings(self, inputs):\n",
    "        features, adj = inputs\n",
    "        if hasattr(self, 'graph_conv1') and hasattr(self, 'graph_conv_mean'):\n",
    "            x = features\n",
    "            hidden1 = self.graph_conv1((x, adj), training=False)\n",
    "            z_mean = self.graph_conv_mean((hidden1, adj), training=False)\n",
    "            return z_mean\n",
    "        else:\n",
    "            raise NotImplementedError(\"The model does not implement the required layers for get_embeddings.\")\n",
    "            \n",
    "class GCNModelAE(Model):\n",
    "    def __init__(self, num_features, hidden1_units, hidden2_units, dropout_rate=0., name='GCNModelAE', **kwargs):\n",
    "        super(GCNModelAE, self).__init__(name=name, **kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # Define model layer\n",
    "        self.graph_conv1 = GraphConvolution(num_features, hidden1_units, activation=tf.nn.relu)\n",
    "        self.graph_conv2 = GraphConvolution(hidden1_units, hidden2_units)\n",
    "        self.inner_product_decoder = InnerProductDecoder()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        features, adj = inputs\n",
    "        x = features\n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=self.dropout_rate)\n",
    "        hidden1 = self.graph_conv1((x, adj), training=training)\n",
    "        embeddings = self.graph_conv2((hidden1, adj), training=training)\n",
    "        reconstructions = self.inner_product_decoder(embeddings, training=training)\n",
    "        return reconstructions\n",
    "\n",
    "class GCNModelVAE(Model):\n",
    "    def __init__(self, num_features, hidden1_units, hidden2_units, num_nodes, dropout_rate=0., name='GCNModelVAE', **kwargs):\n",
    "        super(GCNModelVAE, self).__init__(name=name, **kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_nodes = num_nodes\n",
    "        # Define model layer\n",
    "        self.graph_conv1 = GraphConvolution(input_dim=num_features, output_dim=hidden1_units, activation=tf.nn.relu)\n",
    "        self.graph_conv_mean = GraphConvolution(input_dim=hidden1_units, output_dim=hidden2_units)\n",
    "        self.graph_conv_log_std = GraphConvolution(input_dim=hidden1_units, output_dim=hidden2_units)\n",
    "        self.inner_product_decoder = InnerProductDecoder()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        features, adj = inputs\n",
    "        x = features\n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=self.dropout_rate)\n",
    "        hidden1 = self.graph_conv1([x, adj], training=training)\n",
    "        z_mean = self.graph_conv_mean([hidden1, adj], training=training)\n",
    "        z_log_std = self.graph_conv_log_std([hidden1, adj], training=training)\n",
    "        z = z_mean + tf.random.normal([self.num_nodes, hidden2_units]) * tf.exp(z_log_std)\n",
    "        reconstructions = self.inner_product_decoder(z, training=training)\n",
    "        return reconstructions\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deb2ea64-d4e6-45de-a6d8-0c4610084671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sparse_to_tuple(self, sparse_mx):\n",
    "        if not sp.isspmatrix_coo(sparse_mx):\n",
    "            sparse_mx = sparse_mx.tocoo()\n",
    "        coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "        values = sparse_mx.data\n",
    "        shape = sparse_mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    def normalize_vectors(self, vectors):\n",
    "        scaler = StandardScaler()\n",
    "        vectors_norm = scaler.fit_transform(vectors)\n",
    "        return vectors_norm\n",
    "\n",
    "    def preprocess_graph(self, adj):\n",
    "        adj = sp.coo_matrix(adj)\n",
    "        adj_ = adj + sp.eye(adj.shape[0])\n",
    "        rowsum = np.array(adj_.sum(1))\n",
    "        degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "        adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "        return self.sparse_to_tuple(adj_normalized)\n",
    "\n",
    "    def gen_train_edges(self, adj):\n",
    "        adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "        adj.eliminate_zeros()\n",
    "        assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "        adj_triu = sp.triu(adj)\n",
    "        adj_tuple = self.sparse_to_tuple(adj_triu)\n",
    "        edges = adj_tuple[0]\n",
    "        data = np.ones(edges.shape[0])\n",
    "        adj_train = sp.csr_matrix((data, (edges[:, 0], edges[:, 1])), shape=adj.shape)\n",
    "        adj_train = adj_train + adj_train.T\n",
    "        return adj_train\n",
    "\n",
    "    def cal_pos_weight(self, adj):\n",
    "        pos_edges_num = adj.nnz\n",
    "        return (adj.shape[0] * adj.shape[0] - pos_edges_num) / pos_edges_num\n",
    "\n",
    "preprocessing = Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0107f73-091f-405f-8b12-15d780f52d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "def clustering(embeddings, num_clusters):\n",
    "    # Check if there are any NaN values in the embeddings\n",
    "    if np.isnan(embeddings).any():\n",
    "        # Handle NaN values. Here, we simply remove them, but you could also impute them\n",
    "        embeddings = embeddings[~np.isnan(embeddings).any(axis=1)]\n",
    "        # If after removing NaN values there are fewer rows than num_clusters, adjust num_clusters\n",
    "        num_clusters = min(num_clusters, len(embeddings))\n",
    "    \n",
    "    # Proceed with clustering if there are enough samples\n",
    "    if len(embeddings) > 1:\n",
    "        model = AgglomerativeClustering(n_clusters=num_clusters).fit(embeddings)\n",
    "        return model.labels_\n",
    "    else:\n",
    "        # Handle the case where there are not enough samples left after removing NaN values\n",
    "        print(\"Not enough samples left after removing NaN values for clustering\")\n",
    "        return None  # or handle this case as per your requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4589e9f5-914a-4ccd-b330-9a72bc038b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae_for_na(name):\n",
    "    \"\"\"\n",
    "    train and evaluate disambiguation results for a specific name\n",
    "    :param name: author name\n",
    "    :return: evaluation results\n",
    "    \"\"\"\n",
    "    \n",
    "    adj, features, labels = input_data.load_local_data(name=name)\n",
    "\n",
    "    adj_orig = adj - sp.diags([adj.diagonal()], [0])\n",
    "    adj_orig.eliminate_zeros()\n",
    "    adj_train = preprocessing.gen_train_edges(adj)\n",
    "    adj_norm = preprocessing.preprocess_graph(adj_train)\n",
    "    num_nodes = adj.shape[0]\n",
    "    is_sparse = sp.issparse(features)  # is_sparse is defined here\n",
    "    features = preprocessing.normalize_vectors(features) if not is_sparse else features.todense()\n",
    "\n",
    "    features_tensor = tf.convert_to_tensor(features, dtype=tf.float32)\n",
    "    adj_tensor = tf.SparseTensor(indices=adj_norm[0], values=adj_norm[1], dense_shape=adj_norm[2])\n",
    "\n",
    "    if model_str == 'gcn_ae':\n",
    "        model = GCNModelAE(num_features=features.shape[1], hidden1_units=128, hidden2_units=64, dropout_rate=0.5)\n",
    "    elif model_str == 'gcn_vae':\n",
    "        model = GCNModelVAE(num_features=features.shape[1], hidden1_units=128, hidden2_units=64, num_nodes=num_nodes, dropout_rate=0.5)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_str\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t = time.time()\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructions = model([features_tensor, adj_tensor], training=True)\n",
    "            adj_orig_dense = tf.sparse.to_dense(tf.sparse.reorder(tf.SparseTensor(indices=np.array(adj_orig.nonzero()).T, values=adj_orig.data, dense_shape=adj_orig.shape)))\n",
    "            loss = compute_loss(tf.reshape(reconstructions, adj_orig_dense.shape), adj_orig_dense)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        print(f\"Epoch: {epoch+1}, train_loss: {loss.numpy():.5f}, time: {time.time() - t:.5f}\")\n",
    "\n",
    "    # Get the embed and process it later\n",
    "    emb = model.get_embeddings((features_tensor, adj_tensor))\n",
    "    n_clusters = len(set(labels))\n",
    "    emb_norm = preprocessing.normalize_vectors(emb)\n",
    "    clusters_pred = clustering(emb_norm, num_clusters=n_clusters)\n",
    "\n",
    "    # Check if clusters_pred is None before calling pairwise_precision_recall_f1\n",
    "    if clusters_pred is not None:\n",
    "        prec, rec, f1 = eval_utils.pairwise_precision_recall_f1(clusters_pred, labels)\n",
    "    else:\n",
    "        # If clusters_pred is None, sets the evaluation metric to its default value\n",
    "        prec, rec, f1 = 0, 0, 0\n",
    "        print('Warning: Since clusters_pred is None, precision, recall, and F1 score are set to 0.')\n",
    "\n",
    "    print('pairwise precision', '{:.5f}'.format(prec), 'recall', '{:.5f}'.format(rec), 'f1', '{:.5f}'.format(f1))\n",
    "    return [prec, rec, f1], num_nodes, n_clusters\n",
    "\n",
    "def load_test_names():\n",
    "    return data_utils.load_json(settings.DATA_DIR, 'test_name_list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26425293-03ed-4e31-9687-3d590540f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hongbin_liang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 183 nodes, 6072 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.81994, time: 0.13154\n",
      "Epoch: 2, train_loss: 0.81368, time: 0.01521\n",
      "Epoch: 3, train_loss: 0.82009, time: 0.01390\n",
      "Epoch: 4, train_loss: 0.81639, time: 0.01369\n",
      "Epoch: 5, train_loss: 0.82292, time: 0.01344\n",
      "Epoch: 6, train_loss: 0.81985, time: 0.01350\n",
      "Epoch: 7, train_loss: 0.81800, time: 0.01351\n",
      "Epoch: 8, train_loss: 0.81865, time: 0.01302\n",
      "Epoch: 9, train_loss: 0.81894, time: 0.01321\n",
      "Epoch: 10, train_loss: 0.82019, time: 0.01368\n",
      "Epoch: 11, train_loss: 0.81856, time: 0.01330\n",
      "Epoch: 12, train_loss: 0.82143, time: 0.01336\n",
      "Epoch: 13, train_loss: 0.81891, time: 0.01353\n",
      "Epoch: 14, train_loss: 0.82093, time: 0.01359\n",
      "Epoch: 15, train_loss: 0.81888, time: 0.01378\n",
      "Epoch: 16, train_loss: 0.81624, time: 0.01380\n",
      "Epoch: 17, train_loss: 0.81790, time: 0.01293\n",
      "Epoch: 18, train_loss: 0.81946, time: 0.01355\n",
      "Epoch: 19, train_loss: 0.81900, time: 0.01356\n",
      "Epoch: 20, train_loss: 0.81631, time: 0.01383\n",
      "Epoch: 21, train_loss: 0.81780, time: 0.01357\n",
      "Epoch: 22, train_loss: 0.81562, time: 0.01301\n",
      "Epoch: 23, train_loss: 0.81764, time: 0.01349\n",
      "Epoch: 24, train_loss: 0.81890, time: 0.01343\n",
      "Epoch: 25, train_loss: 0.81466, time: 0.01300\n",
      "Epoch: 26, train_loss: 0.81917, time: 0.01367\n",
      "Epoch: 27, train_loss: 0.81634, time: 0.01334\n",
      "Epoch: 28, train_loss: 0.82360, time: 0.01325\n",
      "Epoch: 29, train_loss: 0.81458, time: 0.01351\n",
      "Epoch: 30, train_loss: 0.81713, time: 0.01360\n",
      "Epoch: 31, train_loss: 0.81911, time: 0.01429\n",
      "Epoch: 32, train_loss: 0.81346, time: 0.01342\n",
      "Epoch: 33, train_loss: 0.81760, time: 0.01346\n",
      "Epoch: 34, train_loss: 0.81499, time: 0.01357\n",
      "Epoch: 35, train_loss: 0.82056, time: 0.01336\n",
      "Epoch: 36, train_loss: 0.81043, time: 0.01365\n",
      "Epoch: 37, train_loss: 0.80586, time: 0.01371\n",
      "Epoch: 38, train_loss: 0.81883, time: 0.01314\n",
      "Epoch: 39, train_loss: 0.80976, time: 0.01323\n",
      "Epoch: 40, train_loss: 0.80476, time: 0.01310\n",
      "Epoch: 41, train_loss: 0.81977, time: 0.01294\n",
      "Epoch: 42, train_loss: 0.81441, time: 0.01317\n",
      "Epoch: 43, train_loss: 0.81067, time: 0.01352\n",
      "Epoch: 44, train_loss: 0.80114, time: 0.01305\n",
      "Epoch: 45, train_loss: 0.80508, time: 0.01365\n",
      "Epoch: 46, train_loss: 0.80036, time: 0.01396\n",
      "Epoch: 47, train_loss: 0.80037, time: 0.01343\n",
      "Epoch: 48, train_loss: 0.81040, time: 0.01349\n",
      "Epoch: 49, train_loss: 0.81467, time: 0.01350\n",
      "Epoch: 50, train_loss: 0.80327, time: 0.01333\n",
      "Epoch: 51, train_loss: 0.80900, time: 0.01352\n",
      "Epoch: 52, train_loss: 0.81637, time: 0.01334\n",
      "Epoch: 53, train_loss: 0.80531, time: 0.01346\n",
      "Epoch: 54, train_loss: 0.81273, time: 0.01400\n",
      "Epoch: 55, train_loss: 0.79242, time: 0.01338\n",
      "Epoch: 56, train_loss: 0.80560, time: 0.01363\n",
      "Epoch: 57, train_loss: 0.80528, time: 0.01335\n",
      "Epoch: 58, train_loss: 0.80308, time: 0.01380\n",
      "Epoch: 59, train_loss: 0.80923, time: 0.01369\n",
      "Epoch: 60, train_loss: 0.79762, time: 0.01350\n",
      "Epoch: 61, train_loss: 0.81336, time: 0.01380\n",
      "Epoch: 62, train_loss: 0.80513, time: 0.01349\n",
      "Epoch: 63, train_loss: 0.79913, time: 0.01341\n",
      "Epoch: 64, train_loss: 0.80435, time: 0.01374\n",
      "Epoch: 65, train_loss: 0.79742, time: 0.01376\n",
      "Epoch: 66, train_loss: 0.80330, time: 0.01339\n",
      "Epoch: 67, train_loss: 0.79922, time: 0.01335\n",
      "Epoch: 68, train_loss: 0.80686, time: 0.01343\n",
      "Epoch: 69, train_loss: 0.79411, time: 0.01340\n",
      "Epoch: 70, train_loss: 0.80539, time: 0.01347\n",
      "Epoch: 71, train_loss: 0.80778, time: 0.01343\n",
      "Epoch: 72, train_loss: 0.80689, time: 0.01368\n",
      "Epoch: 73, train_loss: 0.80300, time: 0.01361\n",
      "Epoch: 74, train_loss: 0.80191, time: 0.01336\n",
      "Epoch: 75, train_loss: 0.80707, time: 0.01370\n",
      "Epoch: 76, train_loss: 0.80243, time: 0.01410\n",
      "Epoch: 77, train_loss: 0.81126, time: 0.01365\n",
      "Epoch: 78, train_loss: 0.80615, time: 0.01353\n",
      "Epoch: 79, train_loss: 0.79099, time: 0.01356\n",
      "Epoch: 80, train_loss: 0.78823, time: 0.01341\n",
      "Epoch: 81, train_loss: 0.79243, time: 0.01305\n",
      "Epoch: 82, train_loss: 0.78169, time: 0.01347\n",
      "Epoch: 83, train_loss: 0.81323, time: 0.01393\n",
      "Epoch: 84, train_loss: 0.81751, time: 0.01378\n",
      "Epoch: 85, train_loss: 0.82026, time: 0.01350\n",
      "Epoch: 86, train_loss: 0.80220, time: 0.01335\n",
      "Epoch: 87, train_loss: 0.79384, time: 0.01368\n",
      "Epoch: 88, train_loss: 0.79735, time: 0.01370\n",
      "Epoch: 89, train_loss: 0.78650, time: 0.01368\n",
      "Epoch: 90, train_loss: 0.79821, time: 0.01339\n",
      "Epoch: 91, train_loss: 0.80011, time: 0.01337\n",
      "Epoch: 92, train_loss: 0.79785, time: 0.01331\n",
      "Epoch: 93, train_loss: 0.80424, time: 0.01351\n",
      "Epoch: 94, train_loss: 0.78697, time: 0.01377\n",
      "Epoch: 95, train_loss: 0.79720, time: 0.01338\n",
      "Epoch: 96, train_loss: 0.77407, time: 0.01357\n",
      "Epoch: 97, train_loss: 0.79349, time: 0.01355\n",
      "Epoch: 98, train_loss: 0.79478, time: 0.01328\n",
      "Epoch: 99, train_loss: 0.80169, time: 0.01345\n",
      "Epoch: 100, train_loss: 0.78826, time: 0.01369\n",
      "Epoch: 101, train_loss: 0.79717, time: 0.01352\n",
      "Epoch: 102, train_loss: 0.78741, time: 0.01350\n",
      "Epoch: 103, train_loss: 0.78321, time: 0.01342\n",
      "Epoch: 104, train_loss: 0.78817, time: 0.01369\n",
      "Epoch: 105, train_loss: 0.81104, time: 0.01334\n",
      "Epoch: 106, train_loss: 0.78190, time: 0.01383\n",
      "Epoch: 107, train_loss: 0.80971, time: 0.01332\n",
      "Epoch: 108, train_loss: 0.81767, time: 0.01348\n",
      "Epoch: 109, train_loss: 0.78613, time: 0.01320\n",
      "Epoch: 110, train_loss: 0.81151, time: 0.01327\n",
      "Epoch: 111, train_loss: 0.83010, time: 0.01359\n",
      "Epoch: 112, train_loss: 0.78425, time: 0.01348\n",
      "Epoch: 113, train_loss: 0.80606, time: 0.01345\n",
      "Epoch: 114, train_loss: 0.79869, time: 0.01346\n",
      "Epoch: 115, train_loss: 0.80561, time: 0.01323\n",
      "Epoch: 116, train_loss: 0.79953, time: 0.01366\n",
      "Epoch: 117, train_loss: 0.81909, time: 0.01368\n",
      "Epoch: 118, train_loss: 0.80162, time: 0.01360\n",
      "Epoch: 119, train_loss: 0.79567, time: 0.01353\n",
      "Epoch: 120, train_loss: 0.78925, time: 0.01324\n",
      "Epoch: 121, train_loss: 0.81175, time: 0.01385\n",
      "Epoch: 122, train_loss: 0.80407, time: 0.01358\n",
      "Epoch: 123, train_loss: 0.80262, time: 0.01341\n",
      "Epoch: 124, train_loss: 0.79262, time: 0.01360\n",
      "Epoch: 125, train_loss: 0.80515, time: 0.01339\n",
      "Epoch: 126, train_loss: 0.79763, time: 0.01335\n",
      "Epoch: 127, train_loss: 0.79328, time: 0.01388\n",
      "Epoch: 128, train_loss: 0.79085, time: 0.01351\n",
      "Epoch: 129, train_loss: 0.81059, time: 0.01376\n",
      "Epoch: 130, train_loss: 0.81321, time: 0.01345\n",
      "Epoch: 131, train_loss: 0.80140, time: 0.01328\n",
      "Epoch: 132, train_loss: 0.79459, time: 0.01355\n",
      "Epoch: 133, train_loss: 0.79289, time: 0.01358\n",
      "Epoch: 134, train_loss: 0.78875, time: 0.01348\n",
      "Epoch: 135, train_loss: 0.78880, time: 0.01365\n",
      "Epoch: 136, train_loss: 0.81386, time: 0.01392\n",
      "Epoch: 137, train_loss: 0.81058, time: 0.01374\n",
      "Epoch: 138, train_loss: 0.79571, time: 0.01348\n",
      "Epoch: 139, train_loss: 0.78924, time: 0.01330\n",
      "Epoch: 140, train_loss: 0.79433, time: 0.01341\n",
      "Epoch: 141, train_loss: 0.81279, time: 0.01354\n",
      "Epoch: 142, train_loss: 0.80989, time: 0.01375\n",
      "Epoch: 143, train_loss: 0.80294, time: 0.01325\n",
      "Epoch: 144, train_loss: 0.78795, time: 0.01339\n",
      "Epoch: 145, train_loss: 0.79978, time: 0.01339\n",
      "Epoch: 146, train_loss: 0.80495, time: 0.01373\n",
      "Epoch: 147, train_loss: 0.80688, time: 0.01336\n",
      "Epoch: 148, train_loss: 0.80497, time: 0.01354\n",
      "Epoch: 149, train_loss: 0.81349, time: 0.01356\n",
      "Epoch: 150, train_loss: 0.80471, time: 0.01357\n",
      "Epoch: 151, train_loss: 0.79197, time: 0.01355\n",
      "Epoch: 152, train_loss: 0.79592, time: 0.01365\n",
      "Epoch: 153, train_loss: 0.79653, time: 0.01344\n",
      "Epoch: 154, train_loss: 0.78433, time: 0.01332\n",
      "Epoch: 155, train_loss: 0.81703, time: 0.01339\n",
      "Epoch: 156, train_loss: 0.80184, time: 0.01368\n",
      "Epoch: 157, train_loss: 0.79101, time: 0.01332\n",
      "Epoch: 158, train_loss: 0.79690, time: 0.01336\n",
      "Epoch: 159, train_loss: 0.78925, time: 0.01313\n",
      "Epoch: 160, train_loss: 0.79814, time: 0.01354\n",
      "Epoch: 161, train_loss: 0.80286, time: 0.01315\n",
      "Epoch: 162, train_loss: 0.81258, time: 0.01327\n",
      "Epoch: 163, train_loss: 0.79480, time: 0.01359\n",
      "Epoch: 164, train_loss: 0.79614, time: 0.01339\n",
      "Epoch: 165, train_loss: 0.80126, time: 0.01327\n",
      "Epoch: 166, train_loss: 0.78751, time: 0.01335\n",
      "Epoch: 167, train_loss: 0.80931, time: 0.01346\n",
      "Epoch: 168, train_loss: 0.80863, time: 0.01353\n",
      "Epoch: 169, train_loss: 0.80327, time: 0.01310\n",
      "Epoch: 170, train_loss: 0.79189, time: 0.01356\n",
      "Epoch: 171, train_loss: 0.78805, time: 0.01357\n",
      "Epoch: 172, train_loss: 0.81341, time: 0.01352\n",
      "Epoch: 173, train_loss: 0.79225, time: 0.01360\n",
      "Epoch: 174, train_loss: 0.78755, time: 0.01356\n",
      "Epoch: 175, train_loss: 0.80152, time: 0.01330\n",
      "Epoch: 176, train_loss: 0.78209, time: 0.01352\n",
      "Epoch: 177, train_loss: 0.80397, time: 0.01346\n",
      "Epoch: 178, train_loss: 0.79424, time: 0.01356\n",
      "Epoch: 179, train_loss: 0.81913, time: 0.01352\n",
      "Epoch: 180, train_loss: 0.81187, time: 0.01360\n",
      "Epoch: 181, train_loss: 0.79048, time: 0.01369\n",
      "Epoch: 182, train_loss: 0.79974, time: 0.01387\n",
      "Epoch: 183, train_loss: 0.79328, time: 0.01377\n",
      "Epoch: 184, train_loss: 0.79852, time: 0.01336\n",
      "Epoch: 185, train_loss: 0.81600, time: 0.01297\n",
      "Epoch: 186, train_loss: 0.80419, time: 0.01325\n",
      "Epoch: 187, train_loss: 0.79484, time: 0.01308\n",
      "Epoch: 188, train_loss: 0.79764, time: 0.01318\n",
      "Epoch: 189, train_loss: 0.79152, time: 0.01295\n",
      "Epoch: 190, train_loss: 0.78402, time: 0.01304\n",
      "Epoch: 191, train_loss: 0.80022, time: 0.01290\n",
      "Epoch: 192, train_loss: 0.79811, time: 0.01292\n",
      "Epoch: 193, train_loss: 0.78770, time: 0.01284\n",
      "Epoch: 194, train_loss: 0.79619, time: 0.01328\n",
      "Epoch: 195, train_loss: 0.80160, time: 0.01355\n",
      "Epoch: 196, train_loss: 0.79795, time: 0.01321\n",
      "Epoch: 197, train_loss: 0.80315, time: 0.01366\n",
      "Epoch: 198, train_loss: 0.79293, time: 0.01346\n",
      "Epoch: 199, train_loss: 0.78984, time: 0.01376\n",
      "Epoch: 200, train_loss: 0.79278, time: 0.01354\n",
      "pairwise precision 0.90085 recall 0.34376 f1 0.49763\n",
      "average until now [0.9008461131676362, 0.3437594591867622, 0.49762652450157013]\n",
      "1 names 77.99315166473389 avg time 77.99315166473389\n",
      "Loading j_yu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 265 nodes, 1072 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99351, time: 0.11027\n",
      "Epoch: 2, train_loss: 0.98754, time: 0.01322\n",
      "Epoch: 3, train_loss: 0.98527, time: 0.01167\n",
      "Epoch: 4, train_loss: 0.98589, time: 0.01143\n",
      "Epoch: 5, train_loss: 0.98370, time: 0.01138\n",
      "Epoch: 6, train_loss: 0.98641, time: 0.01114\n",
      "Epoch: 7, train_loss: 0.98489, time: 0.01104\n",
      "Epoch: 8, train_loss: 0.98546, time: 0.01146\n",
      "Epoch: 9, train_loss: 0.98635, time: 0.01115\n",
      "Epoch: 10, train_loss: 0.98410, time: 0.01135\n",
      "Epoch: 11, train_loss: 0.98818, time: 0.01113\n",
      "Epoch: 12, train_loss: 0.98377, time: 0.01123\n",
      "Epoch: 13, train_loss: 0.98454, time: 0.01172\n",
      "Epoch: 14, train_loss: 0.98520, time: 0.01178\n",
      "Epoch: 15, train_loss: 0.98518, time: 0.01133\n",
      "Epoch: 16, train_loss: 0.98454, time: 0.01165\n",
      "Epoch: 17, train_loss: 0.98568, time: 0.01165\n",
      "Epoch: 18, train_loss: 0.98363, time: 0.01157\n",
      "Epoch: 19, train_loss: 0.98312, time: 0.01208\n",
      "Epoch: 20, train_loss: 0.98823, time: 0.01120\n",
      "Epoch: 21, train_loss: 0.98564, time: 0.01119\n",
      "Epoch: 22, train_loss: 0.98441, time: 0.01133\n",
      "Epoch: 23, train_loss: 0.98471, time: 0.01104\n",
      "Epoch: 24, train_loss: 0.98656, time: 0.01128\n",
      "Epoch: 25, train_loss: 0.98571, time: 0.01157\n",
      "Epoch: 26, train_loss: 0.98220, time: 0.01078\n",
      "Epoch: 27, train_loss: 0.98552, time: 0.01142\n",
      "Epoch: 28, train_loss: 0.98301, time: 0.01094\n",
      "Epoch: 29, train_loss: 0.98181, time: 0.01143\n",
      "Epoch: 30, train_loss: 0.98330, time: 0.01109\n",
      "Epoch: 31, train_loss: 0.98412, time: 0.01110\n",
      "Epoch: 32, train_loss: 0.98631, time: 0.01115\n",
      "Epoch: 33, train_loss: 0.98360, time: 0.01117\n",
      "Epoch: 34, train_loss: 0.98173, time: 0.01190\n",
      "Epoch: 35, train_loss: 0.98525, time: 0.01126\n",
      "Epoch: 36, train_loss: 0.98427, time: 0.01146\n",
      "Epoch: 37, train_loss: 0.98070, time: 0.01195\n",
      "Epoch: 38, train_loss: 0.98416, time: 0.01163\n",
      "Epoch: 39, train_loss: 0.98405, time: 0.01134\n",
      "Epoch: 40, train_loss: 0.98665, time: 0.01150\n",
      "Epoch: 41, train_loss: 0.98433, time: 0.01133\n",
      "Epoch: 42, train_loss: 0.98300, time: 0.01137\n",
      "Epoch: 43, train_loss: 0.98389, time: 0.01136\n",
      "Epoch: 44, train_loss: 0.98477, time: 0.01121\n",
      "Epoch: 45, train_loss: 0.98579, time: 0.01109\n",
      "Epoch: 46, train_loss: 0.98296, time: 0.01140\n",
      "Epoch: 47, train_loss: 0.98680, time: 0.01122\n",
      "Epoch: 48, train_loss: 0.98316, time: 0.01129\n",
      "Epoch: 49, train_loss: 0.98627, time: 0.01123\n",
      "Epoch: 50, train_loss: 0.98551, time: 0.01140\n",
      "Epoch: 51, train_loss: 0.98386, time: 0.01113\n",
      "Epoch: 52, train_loss: 0.98299, time: 0.01114\n",
      "Epoch: 53, train_loss: 0.98370, time: 0.01129\n",
      "Epoch: 54, train_loss: 0.98577, time: 0.01096\n",
      "Epoch: 55, train_loss: 0.98543, time: 0.01159\n",
      "Epoch: 56, train_loss: 0.98491, time: 0.01171\n",
      "Epoch: 57, train_loss: 0.98369, time: 0.01166\n",
      "Epoch: 58, train_loss: 0.98380, time: 0.01157\n",
      "Epoch: 59, train_loss: 0.98850, time: 0.01131\n",
      "Epoch: 60, train_loss: 0.98643, time: 0.01152\n",
      "Epoch: 61, train_loss: 0.98275, time: 0.01141\n",
      "Epoch: 62, train_loss: 0.98264, time: 0.01127\n",
      "Epoch: 63, train_loss: 0.98677, time: 0.01102\n",
      "Epoch: 64, train_loss: 0.98300, time: 0.01164\n",
      "Epoch: 65, train_loss: 0.98206, time: 0.01143\n",
      "Epoch: 66, train_loss: 0.98109, time: 0.01150\n",
      "Epoch: 67, train_loss: 0.98634, time: 0.01138\n",
      "Epoch: 68, train_loss: 0.98252, time: 0.01154\n",
      "Epoch: 69, train_loss: 0.98263, time: 0.01109\n",
      "Epoch: 70, train_loss: 0.98303, time: 0.01143\n",
      "Epoch: 71, train_loss: 0.98349, time: 0.01137\n",
      "Epoch: 72, train_loss: 0.98117, time: 0.01134\n",
      "Epoch: 73, train_loss: 0.98172, time: 0.01160\n",
      "Epoch: 74, train_loss: 0.98164, time: 0.01136\n",
      "Epoch: 75, train_loss: 0.98367, time: 0.01109\n",
      "Epoch: 76, train_loss: 0.98468, time: 0.01142\n",
      "Epoch: 77, train_loss: 0.98613, time: 0.01146\n",
      "Epoch: 78, train_loss: 0.98634, time: 0.01148\n",
      "Epoch: 79, train_loss: 0.98180, time: 0.01084\n",
      "Epoch: 80, train_loss: 0.98201, time: 0.01122\n",
      "Epoch: 81, train_loss: 0.98295, time: 0.01130\n",
      "Epoch: 82, train_loss: 0.98128, time: 0.01130\n",
      "Epoch: 83, train_loss: 0.98131, time: 0.01122\n",
      "Epoch: 84, train_loss: 0.98299, time: 0.01129\n",
      "Epoch: 85, train_loss: 0.98383, time: 0.01140\n",
      "Epoch: 86, train_loss: 0.98355, time: 0.01143\n",
      "Epoch: 87, train_loss: 0.98148, time: 0.01122\n",
      "Epoch: 88, train_loss: 0.98259, time: 0.01105\n",
      "Epoch: 89, train_loss: 0.98197, time: 0.01124\n",
      "Epoch: 90, train_loss: 0.98371, time: 0.01134\n",
      "Epoch: 91, train_loss: 0.98417, time: 0.01159\n",
      "Epoch: 92, train_loss: 0.98245, time: 0.01143\n",
      "Epoch: 93, train_loss: 0.98152, time: 0.01133\n",
      "Epoch: 94, train_loss: 0.98375, time: 0.01131\n",
      "Epoch: 95, train_loss: 0.98237, time: 0.01139\n",
      "Epoch: 96, train_loss: 0.98360, time: 0.01100\n",
      "Epoch: 97, train_loss: 0.98291, time: 0.01153\n",
      "Epoch: 98, train_loss: 0.98569, time: 0.01133\n",
      "Epoch: 99, train_loss: 0.98332, time: 0.01149\n",
      "Epoch: 100, train_loss: 0.98385, time: 0.01150\n",
      "Epoch: 101, train_loss: 0.98197, time: 0.01102\n",
      "Epoch: 102, train_loss: 0.98261, time: 0.01135\n",
      "Epoch: 103, train_loss: 0.98306, time: 0.01124\n",
      "Epoch: 104, train_loss: 0.98558, time: 0.01122\n",
      "Epoch: 105, train_loss: 0.98429, time: 0.01122\n",
      "Epoch: 106, train_loss: 0.98581, time: 0.01151\n",
      "Epoch: 107, train_loss: 0.98244, time: 0.01135\n",
      "Epoch: 108, train_loss: 0.98642, time: 0.01103\n",
      "Epoch: 109, train_loss: 0.98302, time: 0.01162\n",
      "Epoch: 110, train_loss: 0.98443, time: 0.01131\n",
      "Epoch: 111, train_loss: 0.98390, time: 0.01170\n",
      "Epoch: 112, train_loss: 0.98301, time: 0.01106\n",
      "Epoch: 113, train_loss: 0.98198, time: 0.01121\n",
      "Epoch: 114, train_loss: 0.98220, time: 0.01143\n",
      "Epoch: 115, train_loss: 0.98407, time: 0.01141\n",
      "Epoch: 116, train_loss: 0.98363, time: 0.01133\n",
      "Epoch: 117, train_loss: 0.98460, time: 0.01107\n",
      "Epoch: 118, train_loss: 0.98373, time: 0.01136\n",
      "Epoch: 119, train_loss: 0.98118, time: 0.01106\n",
      "Epoch: 120, train_loss: 0.98243, time: 0.01091\n",
      "Epoch: 121, train_loss: 0.98245, time: 0.01116\n",
      "Epoch: 122, train_loss: 0.98530, time: 0.01140\n",
      "Epoch: 123, train_loss: 0.98442, time: 0.01127\n",
      "Epoch: 124, train_loss: 0.98435, time: 0.01120\n",
      "Epoch: 125, train_loss: 0.98442, time: 0.01120\n",
      "Epoch: 126, train_loss: 0.98380, time: 0.01124\n",
      "Epoch: 127, train_loss: 0.98315, time: 0.01170\n",
      "Epoch: 128, train_loss: 0.98228, time: 0.01134\n",
      "Epoch: 129, train_loss: 0.98354, time: 0.01114\n",
      "Epoch: 130, train_loss: 0.98289, time: 0.01171\n",
      "Epoch: 131, train_loss: 0.98265, time: 0.01136\n",
      "Epoch: 132, train_loss: 0.98374, time: 0.01138\n",
      "Epoch: 133, train_loss: 0.98286, time: 0.01138\n",
      "Epoch: 134, train_loss: 0.98248, time: 0.01146\n",
      "Epoch: 135, train_loss: 0.98558, time: 0.01142\n",
      "Epoch: 136, train_loss: 0.98407, time: 0.01117\n",
      "Epoch: 137, train_loss: 0.98311, time: 0.01108\n",
      "Epoch: 138, train_loss: 0.98352, time: 0.01140\n",
      "Epoch: 139, train_loss: 0.98262, time: 0.01128\n",
      "Epoch: 140, train_loss: 0.98528, time: 0.01133\n",
      "Epoch: 141, train_loss: 0.98159, time: 0.01122\n",
      "Epoch: 142, train_loss: 0.98343, time: 0.01133\n",
      "Epoch: 143, train_loss: 0.98386, time: 0.01098\n",
      "Epoch: 144, train_loss: 0.98207, time: 0.01095\n",
      "Epoch: 145, train_loss: 0.98368, time: 0.01170\n",
      "Epoch: 146, train_loss: 0.98545, time: 0.01151\n",
      "Epoch: 147, train_loss: 0.98481, time: 0.01142\n",
      "Epoch: 148, train_loss: 0.98374, time: 0.01133\n",
      "Epoch: 149, train_loss: 0.98171, time: 0.01084\n",
      "Epoch: 150, train_loss: 0.98175, time: 0.01097\n",
      "Epoch: 151, train_loss: 0.98451, time: 0.01108\n",
      "Epoch: 152, train_loss: 0.98698, time: 0.01088\n",
      "Epoch: 153, train_loss: 0.98124, time: 0.01134\n",
      "Epoch: 154, train_loss: 0.98272, time: 0.01101\n",
      "Epoch: 155, train_loss: 0.98383, time: 0.01086\n",
      "Epoch: 156, train_loss: 0.98276, time: 0.01126\n",
      "Epoch: 157, train_loss: 0.98446, time: 0.01123\n",
      "Epoch: 158, train_loss: 0.98117, time: 0.01126\n",
      "Epoch: 159, train_loss: 0.98305, time: 0.01136\n",
      "Epoch: 160, train_loss: 0.98253, time: 0.01121\n",
      "Epoch: 161, train_loss: 0.98321, time: 0.01138\n",
      "Epoch: 162, train_loss: 0.98457, time: 0.01112\n",
      "Epoch: 163, train_loss: 0.98333, time: 0.01150\n",
      "Epoch: 164, train_loss: 0.98139, time: 0.01141\n",
      "Epoch: 165, train_loss: 0.98378, time: 0.01152\n",
      "Epoch: 166, train_loss: 0.98252, time: 0.01134\n",
      "Epoch: 167, train_loss: 0.98416, time: 0.01148\n",
      "Epoch: 168, train_loss: 0.98258, time: 0.01124\n",
      "Epoch: 169, train_loss: 0.98509, time: 0.01164\n",
      "Epoch: 170, train_loss: 0.98346, time: 0.01170\n",
      "Epoch: 171, train_loss: 0.98353, time: 0.01149\n",
      "Epoch: 172, train_loss: 0.98338, time: 0.01148\n",
      "Epoch: 173, train_loss: 0.98278, time: 0.01093\n",
      "Epoch: 174, train_loss: 0.98390, time: 0.01165\n",
      "Epoch: 175, train_loss: 0.98269, time: 0.01130\n",
      "Epoch: 176, train_loss: 0.98093, time: 0.01128\n",
      "Epoch: 177, train_loss: 0.98370, time: 0.01135\n",
      "Epoch: 178, train_loss: 0.98445, time: 0.01119\n",
      "Epoch: 179, train_loss: 0.98273, time: 0.01139\n",
      "Epoch: 180, train_loss: 0.98452, time: 0.01128\n",
      "Epoch: 181, train_loss: 0.98436, time: 0.01221\n",
      "Epoch: 182, train_loss: 0.98360, time: 0.01157\n",
      "Epoch: 183, train_loss: 0.98550, time: 0.01121\n",
      "Epoch: 184, train_loss: 0.98227, time: 0.01145\n",
      "Epoch: 185, train_loss: 0.98510, time: 0.01123\n",
      "Epoch: 186, train_loss: 0.98292, time: 0.01147\n",
      "Epoch: 187, train_loss: 0.98407, time: 0.01148\n",
      "Epoch: 188, train_loss: 0.98354, time: 0.01107\n",
      "Epoch: 189, train_loss: 0.98519, time: 0.01107\n",
      "Epoch: 190, train_loss: 0.98470, time: 0.01144\n",
      "Epoch: 191, train_loss: 0.98409, time: 0.01099\n",
      "Epoch: 192, train_loss: 0.98683, time: 0.01148\n",
      "Epoch: 193, train_loss: 0.98392, time: 0.01112\n",
      "Epoch: 194, train_loss: 0.98269, time: 0.01108\n",
      "Epoch: 195, train_loss: 0.98492, time: 0.01138\n",
      "Epoch: 196, train_loss: 0.98267, time: 0.01128\n",
      "Epoch: 197, train_loss: 0.98448, time: 0.01119\n",
      "Epoch: 198, train_loss: 0.98455, time: 0.01138\n",
      "Epoch: 199, train_loss: 0.98521, time: 0.01212\n",
      "Epoch: 200, train_loss: 0.98329, time: 0.01147\n",
      "pairwise precision 0.05241 recall 0.80590 f1 0.09841\n",
      "average until now [0.476625985454111, 0.5748287912555795, 0.521141460704462]\n",
      "2 names 80.41872644424438 avg time 40.20936322212219\n",
      "Loading s_yu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 61 nodes, 132 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.96273, time: 0.10646\n",
      "Epoch: 2, train_loss: 0.97318, time: 0.00894\n",
      "Epoch: 3, train_loss: 0.97541, time: 0.00813\n",
      "Epoch: 4, train_loss: 0.98193, time: 0.00802\n",
      "Epoch: 5, train_loss: 0.96994, time: 0.00799\n",
      "Epoch: 6, train_loss: 0.96939, time: 0.00798\n",
      "Epoch: 7, train_loss: 0.96160, time: 0.00805\n",
      "Epoch: 8, train_loss: 0.96758, time: 0.00860\n",
      "Epoch: 9, train_loss: 0.97921, time: 0.00823\n",
      "Epoch: 10, train_loss: 0.96982, time: 0.00804\n",
      "Epoch: 11, train_loss: 0.95967, time: 0.00796\n",
      "Epoch: 12, train_loss: 0.96777, time: 0.00797\n",
      "Epoch: 13, train_loss: 0.97834, time: 0.00798\n",
      "Epoch: 14, train_loss: 0.96707, time: 0.00801\n",
      "Epoch: 15, train_loss: 0.96547, time: 0.00799\n",
      "Epoch: 16, train_loss: 0.97520, time: 0.00799\n",
      "Epoch: 17, train_loss: 0.97972, time: 0.00800\n",
      "Epoch: 18, train_loss: 0.96593, time: 0.00798\n",
      "Epoch: 19, train_loss: 0.97534, time: 0.00816\n",
      "Epoch: 20, train_loss: 0.95888, time: 0.00800\n",
      "Epoch: 21, train_loss: 0.97025, time: 0.00797\n",
      "Epoch: 22, train_loss: 0.97280, time: 0.00802\n",
      "Epoch: 23, train_loss: 0.96958, time: 0.00805\n",
      "Epoch: 24, train_loss: 0.96466, time: 0.00794\n",
      "Epoch: 25, train_loss: 0.95647, time: 0.00809\n",
      "Epoch: 26, train_loss: 0.97939, time: 0.00802\n",
      "Epoch: 27, train_loss: 0.95681, time: 0.00803\n",
      "Epoch: 28, train_loss: 0.98011, time: 0.00801\n",
      "Epoch: 29, train_loss: 0.97495, time: 0.00801\n",
      "Epoch: 30, train_loss: 0.97003, time: 0.00795\n",
      "Epoch: 31, train_loss: 0.97336, time: 0.00796\n",
      "Epoch: 32, train_loss: 0.95904, time: 0.00796\n",
      "Epoch: 33, train_loss: 0.96395, time: 0.00803\n",
      "Epoch: 34, train_loss: 0.97160, time: 0.00849\n",
      "Epoch: 35, train_loss: 0.96820, time: 0.00831\n",
      "Epoch: 36, train_loss: 0.95708, time: 0.00795\n",
      "Epoch: 37, train_loss: 0.96517, time: 0.00799\n",
      "Epoch: 38, train_loss: 0.97082, time: 0.00796\n",
      "Epoch: 39, train_loss: 0.96493, time: 0.00787\n",
      "Epoch: 40, train_loss: 0.96369, time: 0.00810\n",
      "Epoch: 41, train_loss: 0.97041, time: 0.00793\n",
      "Epoch: 42, train_loss: 0.97570, time: 0.00794\n",
      "Epoch: 43, train_loss: 0.96914, time: 0.00792\n",
      "Epoch: 44, train_loss: 0.97923, time: 0.00794\n",
      "Epoch: 45, train_loss: 0.98235, time: 0.00788\n",
      "Epoch: 46, train_loss: 0.97966, time: 0.00789\n",
      "Epoch: 47, train_loss: 0.97516, time: 0.00790\n",
      "Epoch: 48, train_loss: 0.97185, time: 0.00789\n",
      "Epoch: 49, train_loss: 0.97122, time: 0.00792\n",
      "Epoch: 50, train_loss: 0.96049, time: 0.00782\n",
      "Epoch: 51, train_loss: 0.98932, time: 0.00783\n",
      "Epoch: 52, train_loss: 0.95917, time: 0.00790\n",
      "Epoch: 53, train_loss: 0.97692, time: 0.00795\n",
      "Epoch: 54, train_loss: 0.97625, time: 0.00788\n",
      "Epoch: 55, train_loss: 0.97064, time: 0.00796\n",
      "Epoch: 56, train_loss: 0.99539, time: 0.00808\n",
      "Epoch: 57, train_loss: 0.96359, time: 0.00830\n",
      "Epoch: 58, train_loss: 0.95742, time: 0.00801\n",
      "Epoch: 59, train_loss: 0.96835, time: 0.00801\n",
      "Epoch: 60, train_loss: 0.97266, time: 0.00834\n",
      "Epoch: 61, train_loss: 0.97291, time: 0.00808\n",
      "Epoch: 62, train_loss: 0.97628, time: 0.00804\n",
      "Epoch: 63, train_loss: 0.97794, time: 0.00811\n",
      "Epoch: 64, train_loss: 0.96719, time: 0.00791\n",
      "Epoch: 65, train_loss: 0.95718, time: 0.00792\n",
      "Epoch: 66, train_loss: 0.97666, time: 0.00782\n",
      "Epoch: 67, train_loss: 0.97205, time: 0.00784\n",
      "Epoch: 68, train_loss: 0.96142, time: 0.00785\n",
      "Epoch: 69, train_loss: 0.97889, time: 0.00787\n",
      "Epoch: 70, train_loss: 0.96913, time: 0.00792\n",
      "Epoch: 71, train_loss: 0.96478, time: 0.00789\n",
      "Epoch: 72, train_loss: 0.96747, time: 0.00784\n",
      "Epoch: 73, train_loss: 0.96968, time: 0.00787\n",
      "Epoch: 74, train_loss: 0.97882, time: 0.00788\n",
      "Epoch: 75, train_loss: 0.97653, time: 0.00791\n",
      "Epoch: 76, train_loss: 0.96021, time: 0.00785\n",
      "Epoch: 77, train_loss: 0.97264, time: 0.00794\n",
      "Epoch: 78, train_loss: 0.96971, time: 0.00792\n",
      "Epoch: 79, train_loss: 0.96890, time: 0.00782\n",
      "Epoch: 80, train_loss: 0.97242, time: 0.00788\n",
      "Epoch: 81, train_loss: 0.96529, time: 0.00786\n",
      "Epoch: 82, train_loss: 0.96983, time: 0.00783\n",
      "Epoch: 83, train_loss: 0.97180, time: 0.00782\n",
      "Epoch: 84, train_loss: 0.96622, time: 0.00789\n",
      "Epoch: 85, train_loss: 0.97046, time: 0.00778\n",
      "Epoch: 86, train_loss: 0.96865, time: 0.00834\n",
      "Epoch: 87, train_loss: 0.97107, time: 0.00810\n",
      "Epoch: 88, train_loss: 0.97135, time: 0.00793\n",
      "Epoch: 89, train_loss: 0.98570, time: 0.00794\n",
      "Epoch: 90, train_loss: 0.96420, time: 0.00793\n",
      "Epoch: 91, train_loss: 0.96749, time: 0.00788\n",
      "Epoch: 92, train_loss: 0.96892, time: 0.00785\n",
      "Epoch: 93, train_loss: 0.96414, time: 0.00801\n",
      "Epoch: 94, train_loss: 0.97851, time: 0.00789\n",
      "Epoch: 95, train_loss: 0.97985, time: 0.00790\n",
      "Epoch: 96, train_loss: 0.96376, time: 0.00783\n",
      "Epoch: 97, train_loss: 0.97922, time: 0.00790\n",
      "Epoch: 98, train_loss: 0.97056, time: 0.00791\n",
      "Epoch: 99, train_loss: 0.97808, time: 0.00789\n",
      "Epoch: 100, train_loss: 0.98333, time: 0.00791\n",
      "Epoch: 101, train_loss: 0.97826, time: 0.00783\n",
      "Epoch: 102, train_loss: 0.96191, time: 0.00792\n",
      "Epoch: 103, train_loss: 0.96193, time: 0.00798\n",
      "Epoch: 104, train_loss: 0.96305, time: 0.00799\n",
      "Epoch: 105, train_loss: 0.96613, time: 0.00794\n",
      "Epoch: 106, train_loss: 0.96802, time: 0.00782\n",
      "Epoch: 107, train_loss: 0.96578, time: 0.00789\n",
      "Epoch: 108, train_loss: 0.96659, time: 0.00795\n",
      "Epoch: 109, train_loss: 0.96589, time: 0.00797\n",
      "Epoch: 110, train_loss: 0.97832, time: 0.00804\n",
      "Epoch: 111, train_loss: 0.96247, time: 0.00793\n",
      "Epoch: 112, train_loss: 0.96953, time: 0.00857\n",
      "Epoch: 113, train_loss: 0.96625, time: 0.00821\n",
      "Epoch: 114, train_loss: 0.96867, time: 0.00794\n",
      "Epoch: 115, train_loss: 0.97141, time: 0.00798\n",
      "Epoch: 116, train_loss: 0.96870, time: 0.00799\n",
      "Epoch: 117, train_loss: 0.96611, time: 0.00795\n",
      "Epoch: 118, train_loss: 0.96214, time: 0.00798\n",
      "Epoch: 119, train_loss: 0.97602, time: 0.00793\n",
      "Epoch: 120, train_loss: 0.97223, time: 0.00826\n",
      "Epoch: 121, train_loss: 0.97059, time: 0.00803\n",
      "Epoch: 122, train_loss: 0.95685, time: 0.00802\n",
      "Epoch: 123, train_loss: 0.96387, time: 0.00806\n",
      "Epoch: 124, train_loss: 0.96256, time: 0.00823\n",
      "Epoch: 125, train_loss: 0.96397, time: 0.00802\n",
      "Epoch: 126, train_loss: 0.96084, time: 0.00801\n",
      "Epoch: 127, train_loss: 0.97143, time: 0.00803\n",
      "Epoch: 128, train_loss: 0.97073, time: 0.00807\n",
      "Epoch: 129, train_loss: 0.98370, time: 0.00806\n",
      "Epoch: 130, train_loss: 0.96334, time: 0.00804\n",
      "Epoch: 131, train_loss: 0.96547, time: 0.00805\n",
      "Epoch: 132, train_loss: 0.96230, time: 0.00808\n",
      "Epoch: 133, train_loss: 0.96733, time: 0.00800\n",
      "Epoch: 134, train_loss: 0.97125, time: 0.00800\n",
      "Epoch: 135, train_loss: 0.97183, time: 0.00806\n",
      "Epoch: 136, train_loss: 0.96986, time: 0.00819\n",
      "Epoch: 137, train_loss: 0.96546, time: 0.00827\n",
      "Epoch: 138, train_loss: 0.97033, time: 0.00815\n",
      "Epoch: 139, train_loss: 0.96707, time: 0.00799\n",
      "Epoch: 140, train_loss: 0.98581, time: 0.00791\n",
      "Epoch: 141, train_loss: 0.95746, time: 0.00798\n",
      "Epoch: 142, train_loss: 0.96703, time: 0.00817\n",
      "Epoch: 143, train_loss: 0.96222, time: 0.00819\n",
      "Epoch: 144, train_loss: 0.98290, time: 0.00831\n",
      "Epoch: 145, train_loss: 0.96418, time: 0.00802\n",
      "Epoch: 146, train_loss: 0.96600, time: 0.00813\n",
      "Epoch: 147, train_loss: 0.96651, time: 0.00801\n",
      "Epoch: 148, train_loss: 0.96596, time: 0.00799\n",
      "Epoch: 149, train_loss: 0.97192, time: 0.00795\n",
      "Epoch: 150, train_loss: 0.96430, time: 0.00794\n",
      "Epoch: 151, train_loss: 0.97166, time: 0.00795\n",
      "Epoch: 152, train_loss: 0.98503, time: 0.00800\n",
      "Epoch: 153, train_loss: 0.97608, time: 0.00807\n",
      "Epoch: 154, train_loss: 0.97103, time: 0.00799\n",
      "Epoch: 155, train_loss: 0.96632, time: 0.00799\n",
      "Epoch: 156, train_loss: 0.95633, time: 0.00799\n",
      "Epoch: 157, train_loss: 0.97605, time: 0.00803\n",
      "Epoch: 158, train_loss: 0.97267, time: 0.00798\n",
      "Epoch: 159, train_loss: 0.96992, time: 0.00796\n",
      "Epoch: 160, train_loss: 0.96651, time: 0.00807\n",
      "Epoch: 161, train_loss: 0.97459, time: 0.00798\n",
      "Epoch: 162, train_loss: 0.96339, time: 0.00846\n",
      "Epoch: 163, train_loss: 0.98542, time: 0.00842\n",
      "Epoch: 164, train_loss: 0.96879, time: 0.00793\n",
      "Epoch: 165, train_loss: 0.97201, time: 0.00793\n",
      "Epoch: 166, train_loss: 0.96294, time: 0.00789\n",
      "Epoch: 167, train_loss: 0.97325, time: 0.00789\n",
      "Epoch: 168, train_loss: 0.95938, time: 0.00784\n",
      "Epoch: 169, train_loss: 0.97758, time: 0.00784\n",
      "Epoch: 170, train_loss: 0.96999, time: 0.00782\n",
      "Epoch: 171, train_loss: 0.97336, time: 0.00783\n",
      "Epoch: 172, train_loss: 0.97760, time: 0.00783\n",
      "Epoch: 173, train_loss: 0.96373, time: 0.00777\n",
      "Epoch: 174, train_loss: 0.97544, time: 0.00783\n",
      "Epoch: 175, train_loss: 0.97127, time: 0.00792\n",
      "Epoch: 176, train_loss: 0.97678, time: 0.00784\n",
      "Epoch: 177, train_loss: 0.97198, time: 0.00785\n",
      "Epoch: 178, train_loss: 0.97440, time: 0.00784\n",
      "Epoch: 179, train_loss: 0.95901, time: 0.00779\n",
      "Epoch: 180, train_loss: 0.97149, time: 0.00782\n",
      "Epoch: 181, train_loss: 0.97170, time: 0.00786\n",
      "Epoch: 182, train_loss: 0.96885, time: 0.00777\n",
      "Epoch: 183, train_loss: 0.97203, time: 0.00779\n",
      "Epoch: 184, train_loss: 0.97029, time: 0.00779\n",
      "Epoch: 185, train_loss: 0.97606, time: 0.00774\n",
      "Epoch: 186, train_loss: 0.97439, time: 0.00780\n",
      "Epoch: 187, train_loss: 0.97764, time: 0.00780\n",
      "Epoch: 188, train_loss: 0.95776, time: 0.00823\n",
      "Epoch: 189, train_loss: 0.96945, time: 0.00801\n",
      "Epoch: 190, train_loss: 0.97531, time: 0.00788\n",
      "Epoch: 191, train_loss: 0.96807, time: 0.00785\n",
      "Epoch: 192, train_loss: 0.96462, time: 0.00778\n",
      "Epoch: 193, train_loss: 0.96516, time: 0.00780\n",
      "Epoch: 194, train_loss: 0.96697, time: 0.00784\n",
      "Epoch: 195, train_loss: 0.97005, time: 0.00782\n",
      "Epoch: 196, train_loss: 0.97098, time: 0.00783\n",
      "Epoch: 197, train_loss: 0.98624, time: 0.00801\n",
      "Epoch: 198, train_loss: 0.97280, time: 0.00793\n",
      "Epoch: 199, train_loss: 0.96775, time: 0.00806\n",
      "Epoch: 200, train_loss: 0.96579, time: 0.00783\n",
      "pairwise precision 0.11033 recall 0.71212 f1 0.19106\n",
      "average until now [0.3545268698019582, 0.6205929315441238, 0.4512612073672216]\n",
      "3 names 82.14356517791748 avg time 27.38118839263916\n",
      "Loading he_ping_li dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 234 nodes, 1837 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97209, time: 0.11228\n",
      "Epoch: 2, train_loss: 0.96932, time: 0.01321\n",
      "Epoch: 3, train_loss: 0.97391, time: 0.01139\n",
      "Epoch: 4, train_loss: 0.96574, time: 0.01127\n",
      "Epoch: 5, train_loss: 0.96489, time: 0.01114\n",
      "Epoch: 6, train_loss: 0.96840, time: 0.01161\n",
      "Epoch: 7, train_loss: 0.96702, time: 0.01135\n",
      "Epoch: 8, train_loss: 0.96685, time: 0.01188\n",
      "Epoch: 9, train_loss: 0.96370, time: 0.01165\n",
      "Epoch: 10, train_loss: 0.96757, time: 0.01186\n",
      "Epoch: 11, train_loss: 0.96602, time: 0.01216\n",
      "Epoch: 12, train_loss: 0.96567, time: 0.01161\n",
      "Epoch: 13, train_loss: 0.96840, time: 0.01151\n",
      "Epoch: 14, train_loss: 0.96320, time: 0.01136\n",
      "Epoch: 15, train_loss: 0.96467, time: 0.01139\n",
      "Epoch: 16, train_loss: 0.96496, time: 0.01118\n",
      "Epoch: 17, train_loss: 0.96700, time: 0.01140\n",
      "Epoch: 18, train_loss: 0.96829, time: 0.01091\n",
      "Epoch: 19, train_loss: 0.96736, time: 0.01157\n",
      "Epoch: 20, train_loss: 0.96623, time: 0.01127\n",
      "Epoch: 21, train_loss: 0.96457, time: 0.01155\n",
      "Epoch: 22, train_loss: 0.96662, time: 0.01116\n",
      "Epoch: 23, train_loss: 0.96581, time: 0.01132\n",
      "Epoch: 24, train_loss: 0.96488, time: 0.01126\n",
      "Epoch: 25, train_loss: 0.96799, time: 0.01120\n",
      "Epoch: 26, train_loss: 0.96495, time: 0.01134\n",
      "Epoch: 27, train_loss: 0.96902, time: 0.01129\n",
      "Epoch: 28, train_loss: 0.96534, time: 0.01121\n",
      "Epoch: 29, train_loss: 0.96592, time: 0.01129\n",
      "Epoch: 30, train_loss: 0.96836, time: 0.01098\n",
      "Epoch: 31, train_loss: 0.96440, time: 0.01110\n",
      "Epoch: 32, train_loss: 0.96589, time: 0.01113\n",
      "Epoch: 33, train_loss: 0.96949, time: 0.01138\n",
      "Epoch: 34, train_loss: 0.96415, time: 0.01111\n",
      "Epoch: 35, train_loss: 0.96557, time: 0.01120\n",
      "Epoch: 36, train_loss: 0.96933, time: 0.01144\n",
      "Epoch: 37, train_loss: 0.96409, time: 0.01132\n",
      "Epoch: 38, train_loss: 0.96602, time: 0.01123\n",
      "Epoch: 39, train_loss: 0.96343, time: 0.01132\n",
      "Epoch: 40, train_loss: 0.96701, time: 0.01113\n",
      "Epoch: 41, train_loss: 0.96248, time: 0.01153\n",
      "Epoch: 42, train_loss: 0.96383, time: 0.01138\n",
      "Epoch: 43, train_loss: 0.96261, time: 0.01110\n",
      "Epoch: 44, train_loss: 0.96640, time: 0.01127\n",
      "Epoch: 45, train_loss: 0.96468, time: 0.01149\n",
      "Epoch: 46, train_loss: 0.96439, time: 0.01129\n",
      "Epoch: 47, train_loss: 0.96614, time: 0.01145\n",
      "Epoch: 48, train_loss: 0.96397, time: 0.01139\n",
      "Epoch: 49, train_loss: 0.96665, time: 0.01181\n",
      "Epoch: 50, train_loss: 0.96678, time: 0.01159\n",
      "Epoch: 51, train_loss: 0.96793, time: 0.01166\n",
      "Epoch: 52, train_loss: 0.96345, time: 0.01126\n",
      "Epoch: 53, train_loss: 0.96561, time: 0.01159\n",
      "Epoch: 54, train_loss: 0.96576, time: 0.01147\n",
      "Epoch: 55, train_loss: 0.96636, time: 0.01175\n",
      "Epoch: 56, train_loss: 0.96407, time: 0.01126\n",
      "Epoch: 57, train_loss: 0.96673, time: 0.01162\n",
      "Epoch: 58, train_loss: 0.96361, time: 0.01110\n",
      "Epoch: 59, train_loss: 0.96572, time: 0.01132\n",
      "Epoch: 60, train_loss: 0.96471, time: 0.01101\n",
      "Epoch: 61, train_loss: 0.96494, time: 0.01133\n",
      "Epoch: 62, train_loss: 0.96599, time: 0.01116\n",
      "Epoch: 63, train_loss: 0.96548, time: 0.01140\n",
      "Epoch: 64, train_loss: 0.96784, time: 0.01121\n",
      "Epoch: 65, train_loss: 0.96751, time: 0.01134\n",
      "Epoch: 66, train_loss: 0.96412, time: 0.01118\n",
      "Epoch: 67, train_loss: 0.96734, time: 0.01135\n",
      "Epoch: 68, train_loss: 0.96216, time: 0.01104\n",
      "Epoch: 69, train_loss: 0.96440, time: 0.01124\n",
      "Epoch: 70, train_loss: 0.96359, time: 0.01118\n",
      "Epoch: 71, train_loss: 0.96563, time: 0.01134\n",
      "Epoch: 72, train_loss: 0.96763, time: 0.01125\n",
      "Epoch: 73, train_loss: 0.96514, time: 0.01184\n",
      "Epoch: 74, train_loss: 0.96397, time: 0.01133\n",
      "Epoch: 75, train_loss: 0.96525, time: 0.01110\n",
      "Epoch: 76, train_loss: 0.96550, time: 0.01116\n",
      "Epoch: 77, train_loss: 0.96660, time: 0.01147\n",
      "Epoch: 78, train_loss: 0.96528, time: 0.01125\n",
      "Epoch: 79, train_loss: 0.96536, time: 0.01131\n",
      "Epoch: 80, train_loss: 0.96459, time: 0.01115\n",
      "Epoch: 81, train_loss: 0.96886, time: 0.01133\n",
      "Epoch: 82, train_loss: 0.96821, time: 0.01093\n",
      "Epoch: 83, train_loss: 0.96504, time: 0.01133\n",
      "Epoch: 84, train_loss: 0.96483, time: 0.01123\n",
      "Epoch: 85, train_loss: 0.96563, time: 0.01130\n",
      "Epoch: 86, train_loss: 0.96487, time: 0.01101\n",
      "Epoch: 87, train_loss: 0.96262, time: 0.01128\n",
      "Epoch: 88, train_loss: 0.96439, time: 0.01121\n",
      "Epoch: 89, train_loss: 0.96657, time: 0.01134\n",
      "Epoch: 90, train_loss: 0.96344, time: 0.01136\n",
      "Epoch: 91, train_loss: 0.96756, time: 0.01170\n",
      "Epoch: 92, train_loss: 0.96486, time: 0.01104\n",
      "Epoch: 93, train_loss: 0.96625, time: 0.01124\n",
      "Epoch: 94, train_loss: 0.96755, time: 0.01121\n",
      "Epoch: 95, train_loss: 0.96555, time: 0.01123\n",
      "Epoch: 96, train_loss: 0.96554, time: 0.01107\n",
      "Epoch: 97, train_loss: 0.96610, time: 0.01123\n",
      "Epoch: 98, train_loss: 0.96370, time: 0.01137\n",
      "Epoch: 99, train_loss: 0.96570, time: 0.01133\n",
      "Epoch: 100, train_loss: 0.96347, time: 0.01141\n",
      "Epoch: 101, train_loss: 0.96337, time: 0.01100\n",
      "Epoch: 102, train_loss: 0.96712, time: 0.01123\n",
      "Epoch: 103, train_loss: 0.96607, time: 0.01129\n",
      "Epoch: 104, train_loss: 0.96898, time: 0.01094\n",
      "Epoch: 105, train_loss: 0.96661, time: 0.01126\n",
      "Epoch: 106, train_loss: 0.96963, time: 0.01104\n",
      "Epoch: 107, train_loss: 0.96379, time: 0.01118\n",
      "Epoch: 108, train_loss: 0.96972, time: 0.01107\n",
      "Epoch: 109, train_loss: 0.96367, time: 0.01178\n",
      "Epoch: 110, train_loss: 0.96645, time: 0.01143\n",
      "Epoch: 111, train_loss: 0.96272, time: 0.01125\n",
      "Epoch: 112, train_loss: 0.96537, time: 0.01112\n",
      "Epoch: 113, train_loss: 0.96442, time: 0.01120\n",
      "Epoch: 114, train_loss: 0.96610, time: 0.01100\n",
      "Epoch: 115, train_loss: 0.96518, time: 0.01094\n",
      "Epoch: 116, train_loss: 0.96444, time: 0.01088\n",
      "Epoch: 117, train_loss: 0.96566, time: 0.01101\n",
      "Epoch: 118, train_loss: 0.96600, time: 0.01095\n",
      "Epoch: 119, train_loss: 0.96642, time: 0.01137\n",
      "Epoch: 120, train_loss: 0.96444, time: 0.01116\n",
      "Epoch: 121, train_loss: 0.96707, time: 0.01114\n",
      "Epoch: 122, train_loss: 0.96404, time: 0.01114\n",
      "Epoch: 123, train_loss: 0.96394, time: 0.01136\n",
      "Epoch: 124, train_loss: 0.96415, time: 0.01128\n",
      "Epoch: 125, train_loss: 0.96518, time: 0.01146\n",
      "Epoch: 126, train_loss: 0.96570, time: 0.01105\n",
      "Epoch: 127, train_loss: 0.96528, time: 0.01148\n",
      "Epoch: 128, train_loss: 0.96487, time: 0.01112\n",
      "Epoch: 129, train_loss: 0.96373, time: 0.01114\n",
      "Epoch: 130, train_loss: 0.96328, time: 0.01113\n",
      "Epoch: 131, train_loss: 0.96398, time: 0.01139\n",
      "Epoch: 132, train_loss: 0.96641, time: 0.01107\n",
      "Epoch: 133, train_loss: 0.96348, time: 0.01125\n",
      "Epoch: 134, train_loss: 0.96441, time: 0.01097\n",
      "Epoch: 135, train_loss: 0.96712, time: 0.01152\n",
      "Epoch: 136, train_loss: 0.96347, time: 0.01114\n",
      "Epoch: 137, train_loss: 0.96666, time: 0.01132\n",
      "Epoch: 138, train_loss: 0.96337, time: 0.01094\n",
      "Epoch: 139, train_loss: 0.96358, time: 0.01136\n",
      "Epoch: 140, train_loss: 0.96717, time: 0.01144\n",
      "Epoch: 141, train_loss: 0.96882, time: 0.01120\n",
      "Epoch: 142, train_loss: 0.96686, time: 0.01096\n",
      "Epoch: 143, train_loss: 0.96405, time: 0.01120\n",
      "Epoch: 144, train_loss: 0.96563, time: 0.01118\n",
      "Epoch: 145, train_loss: 0.96349, time: 0.01158\n",
      "Epoch: 146, train_loss: 0.96761, time: 0.01150\n",
      "Epoch: 147, train_loss: 0.96445, time: 0.01156\n",
      "Epoch: 148, train_loss: 0.96639, time: 0.01113\n",
      "Epoch: 149, train_loss: 0.96349, time: 0.01160\n",
      "Epoch: 150, train_loss: 0.96624, time: 0.01093\n",
      "Epoch: 151, train_loss: 0.96327, time: 0.01142\n",
      "Epoch: 152, train_loss: 0.96468, time: 0.01156\n",
      "Epoch: 153, train_loss: 0.96348, time: 0.01142\n",
      "Epoch: 154, train_loss: 0.96447, time: 0.01134\n",
      "Epoch: 155, train_loss: 0.96674, time: 0.01134\n",
      "Epoch: 156, train_loss: 0.96290, time: 0.01123\n",
      "Epoch: 157, train_loss: 0.96641, time: 0.01144\n",
      "Epoch: 158, train_loss: 0.96554, time: 0.01137\n",
      "Epoch: 159, train_loss: 0.96348, time: 0.01135\n",
      "Epoch: 160, train_loss: 0.96573, time: 0.01123\n",
      "Epoch: 161, train_loss: 0.96477, time: 0.01140\n",
      "Epoch: 162, train_loss: 0.96533, time: 0.01113\n",
      "Epoch: 163, train_loss: 0.96875, time: 0.01166\n",
      "Epoch: 164, train_loss: 0.96385, time: 0.01142\n",
      "Epoch: 165, train_loss: 0.96447, time: 0.01121\n",
      "Epoch: 166, train_loss: 0.96595, time: 0.01100\n",
      "Epoch: 167, train_loss: 0.96266, time: 0.01136\n",
      "Epoch: 168, train_loss: 0.96500, time: 0.01131\n",
      "Epoch: 169, train_loss: 0.96280, time: 0.01130\n",
      "Epoch: 170, train_loss: 0.96493, time: 0.01106\n",
      "Epoch: 171, train_loss: 0.96548, time: 0.01118\n",
      "Epoch: 172, train_loss: 0.96563, time: 0.01081\n",
      "Epoch: 173, train_loss: 0.96382, time: 0.01124\n",
      "Epoch: 174, train_loss: 0.96384, time: 0.01101\n",
      "Epoch: 175, train_loss: 0.96654, time: 0.01153\n",
      "Epoch: 176, train_loss: 0.96397, time: 0.01088\n",
      "Epoch: 177, train_loss: 0.96435, time: 0.01136\n",
      "Epoch: 178, train_loss: 0.96518, time: 0.01117\n",
      "Epoch: 179, train_loss: 0.96675, time: 0.01138\n",
      "Epoch: 180, train_loss: 0.96478, time: 0.01103\n",
      "Epoch: 181, train_loss: 0.96267, time: 0.01173\n",
      "Epoch: 182, train_loss: 0.96607, time: 0.01142\n",
      "Epoch: 183, train_loss: 0.96588, time: 0.01133\n",
      "Epoch: 184, train_loss: 0.96607, time: 0.01099\n",
      "Epoch: 185, train_loss: 0.96700, time: 0.01150\n",
      "Epoch: 186, train_loss: 0.96818, time: 0.01094\n",
      "Epoch: 187, train_loss: 0.96566, time: 0.01106\n",
      "Epoch: 188, train_loss: 0.96574, time: 0.01107\n",
      "Epoch: 189, train_loss: 0.96450, time: 0.01129\n",
      "Epoch: 190, train_loss: 0.96319, time: 0.01110\n",
      "Epoch: 191, train_loss: 0.96820, time: 0.01117\n",
      "Epoch: 192, train_loss: 0.96645, time: 0.01111\n",
      "Epoch: 193, train_loss: 0.96612, time: 0.01394\n",
      "Epoch: 194, train_loss: 0.96796, time: 0.01161\n",
      "Epoch: 195, train_loss: 0.96460, time: 0.01117\n",
      "Epoch: 196, train_loss: 0.96833, time: 0.01100\n",
      "Epoch: 197, train_loss: 0.96631, time: 0.01082\n",
      "Epoch: 198, train_loss: 0.96840, time: 0.01118\n",
      "Epoch: 199, train_loss: 0.96234, time: 0.01127\n",
      "Epoch: 200, train_loss: 0.96386, time: 0.01167\n",
      "pairwise precision 0.61986 recall 0.95764 f1 0.75259\n",
      "average until now [0.42086002799225086, 0.704855704172583, 0.5270346374534534]\n",
      "4 names 84.5593774318695 avg time 21.139844357967377\n",
      "Loading song_chen dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 598 nodes, 2782 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99555, time: 0.12093\n",
      "Epoch: 2, train_loss: 0.99348, time: 0.02154\n",
      "Epoch: 3, train_loss: 0.99172, time: 0.02048\n",
      "Epoch: 4, train_loss: 0.99104, time: 0.02019\n",
      "Epoch: 5, train_loss: 0.99058, time: 0.01969\n",
      "Epoch: 6, train_loss: 0.99074, time: 0.02053\n",
      "Epoch: 7, train_loss: 0.99095, time: 0.02056\n",
      "Epoch: 8, train_loss: 0.99054, time: 0.02064\n",
      "Epoch: 9, train_loss: 0.99005, time: 0.02014\n",
      "Epoch: 10, train_loss: 0.98977, time: 0.02029\n",
      "Epoch: 11, train_loss: 0.98987, time: 0.02060\n",
      "Epoch: 12, train_loss: 0.99040, time: 0.02008\n",
      "Epoch: 13, train_loss: 0.99048, time: 0.02080\n",
      "Epoch: 14, train_loss: 0.99056, time: 0.02063\n",
      "Epoch: 15, train_loss: 0.99077, time: 0.01964\n",
      "Epoch: 16, train_loss: 0.98992, time: 0.01894\n",
      "Epoch: 17, train_loss: 0.99086, time: 0.02098\n",
      "Epoch: 18, train_loss: 0.98991, time: 0.01959\n",
      "Epoch: 19, train_loss: 0.99060, time: 0.01998\n",
      "Epoch: 20, train_loss: 0.99027, time: 0.01887\n",
      "Epoch: 21, train_loss: 0.98882, time: 0.01843\n",
      "Epoch: 22, train_loss: 0.99072, time: 0.01925\n",
      "Epoch: 23, train_loss: 0.98988, time: 0.02055\n",
      "Epoch: 24, train_loss: 0.98950, time: 0.01952\n",
      "Epoch: 25, train_loss: 0.99014, time: 0.01945\n",
      "Epoch: 26, train_loss: 0.99030, time: 0.01838\n",
      "Epoch: 27, train_loss: 0.99010, time: 0.01940\n",
      "Epoch: 28, train_loss: 0.99063, time: 0.02010\n",
      "Epoch: 29, train_loss: 0.99042, time: 0.01992\n",
      "Epoch: 30, train_loss: 0.99256, time: 0.01924\n",
      "Epoch: 31, train_loss: 0.98986, time: 0.02000\n",
      "Epoch: 32, train_loss: 0.99131, time: 0.02067\n",
      "Epoch: 33, train_loss: 0.98985, time: 0.02056\n",
      "Epoch: 34, train_loss: 0.98991, time: 0.02059\n",
      "Epoch: 35, train_loss: 0.99012, time: 0.02076\n",
      "Epoch: 36, train_loss: 0.98961, time: 0.02065\n",
      "Epoch: 37, train_loss: 0.99027, time: 0.02037\n",
      "Epoch: 38, train_loss: 0.98984, time: 0.02091\n",
      "Epoch: 39, train_loss: 0.98936, time: 0.02035\n",
      "Epoch: 40, train_loss: 0.98990, time: 0.02000\n",
      "Epoch: 41, train_loss: 0.98984, time: 0.02011\n",
      "Epoch: 42, train_loss: 0.98937, time: 0.02085\n",
      "Epoch: 43, train_loss: 0.99050, time: 0.02070\n",
      "Epoch: 44, train_loss: 0.99092, time: 0.02066\n",
      "Epoch: 45, train_loss: 0.98986, time: 0.01938\n",
      "Epoch: 46, train_loss: 0.98975, time: 0.01942\n",
      "Epoch: 47, train_loss: 0.99069, time: 0.01985\n",
      "Epoch: 48, train_loss: 0.99071, time: 0.02050\n",
      "Epoch: 49, train_loss: 0.99064, time: 0.02046\n",
      "Epoch: 50, train_loss: 0.99024, time: 0.02041\n",
      "Epoch: 51, train_loss: 0.99044, time: 0.01992\n",
      "Epoch: 52, train_loss: 0.99015, time: 0.01975\n",
      "Epoch: 53, train_loss: 0.99078, time: 0.01982\n",
      "Epoch: 54, train_loss: 0.98945, time: 0.02071\n",
      "Epoch: 55, train_loss: 0.99022, time: 0.02018\n",
      "Epoch: 56, train_loss: 0.99088, time: 0.01966\n",
      "Epoch: 57, train_loss: 0.99037, time: 0.01900\n",
      "Epoch: 58, train_loss: 0.98815, time: 0.01982\n",
      "Epoch: 59, train_loss: 0.98965, time: 0.01966\n",
      "Epoch: 60, train_loss: 0.99033, time: 0.01991\n",
      "Epoch: 61, train_loss: 0.98934, time: 0.02054\n",
      "Epoch: 62, train_loss: 0.99034, time: 0.02070\n",
      "Epoch: 63, train_loss: 0.98937, time: 0.02026\n",
      "Epoch: 64, train_loss: 0.99001, time: 0.01974\n",
      "Epoch: 65, train_loss: 0.99169, time: 0.02015\n",
      "Epoch: 66, train_loss: 0.99005, time: 0.01973\n",
      "Epoch: 67, train_loss: 0.99085, time: 0.02005\n",
      "Epoch: 68, train_loss: 0.98968, time: 0.01985\n",
      "Epoch: 69, train_loss: 0.98989, time: 0.02063\n",
      "Epoch: 70, train_loss: 0.99059, time: 0.02016\n",
      "Epoch: 71, train_loss: 0.98958, time: 0.01942\n",
      "Epoch: 72, train_loss: 0.99068, time: 0.02069\n",
      "Epoch: 73, train_loss: 0.99072, time: 0.01986\n",
      "Epoch: 74, train_loss: 0.99049, time: 0.02024\n",
      "Epoch: 75, train_loss: 0.99099, time: 0.02083\n",
      "Epoch: 76, train_loss: 0.99043, time: 0.01903\n",
      "Epoch: 77, train_loss: 0.99020, time: 0.01941\n",
      "Epoch: 78, train_loss: 0.98984, time: 0.01997\n",
      "Epoch: 79, train_loss: 0.99067, time: 0.01981\n",
      "Epoch: 80, train_loss: 0.99022, time: 0.02067\n",
      "Epoch: 81, train_loss: 0.98979, time: 0.01958\n",
      "Epoch: 82, train_loss: 0.99047, time: 0.02004\n",
      "Epoch: 83, train_loss: 0.99030, time: 0.01931\n",
      "Epoch: 84, train_loss: 0.98969, time: 0.02113\n",
      "Epoch: 85, train_loss: 0.99049, time: 0.02108\n",
      "Epoch: 86, train_loss: 0.98961, time: 0.02094\n",
      "Epoch: 87, train_loss: 0.99012, time: 0.01969\n",
      "Epoch: 88, train_loss: 0.98850, time: 0.02065\n",
      "Epoch: 89, train_loss: 0.98974, time: 0.02076\n",
      "Epoch: 90, train_loss: 0.99049, time: 0.02043\n",
      "Epoch: 91, train_loss: 0.99098, time: 0.02065\n",
      "Epoch: 92, train_loss: 0.99117, time: 0.02045\n",
      "Epoch: 93, train_loss: 0.98998, time: 0.02008\n",
      "Epoch: 94, train_loss: 0.99112, time: 0.02026\n",
      "Epoch: 95, train_loss: 0.98892, time: 0.02056\n",
      "Epoch: 96, train_loss: 0.99082, time: 0.02023\n",
      "Epoch: 97, train_loss: 0.99033, time: 0.02043\n",
      "Epoch: 98, train_loss: 0.98967, time: 0.02007\n",
      "Epoch: 99, train_loss: 0.98952, time: 0.02020\n",
      "Epoch: 100, train_loss: 0.98967, time: 0.02021\n",
      "Epoch: 101, train_loss: 0.99053, time: 0.02015\n",
      "Epoch: 102, train_loss: 0.98986, time: 0.02032\n",
      "Epoch: 103, train_loss: 0.99030, time: 0.02016\n",
      "Epoch: 104, train_loss: 0.99003, time: 0.02051\n",
      "Epoch: 105, train_loss: 0.98960, time: 0.02059\n",
      "Epoch: 106, train_loss: 0.99067, time: 0.02030\n",
      "Epoch: 107, train_loss: 0.99057, time: 0.02072\n",
      "Epoch: 108, train_loss: 0.98918, time: 0.01970\n",
      "Epoch: 109, train_loss: 0.98948, time: 0.02101\n",
      "Epoch: 110, train_loss: 0.99203, time: 0.02054\n",
      "Epoch: 111, train_loss: 0.99002, time: 0.02004\n",
      "Epoch: 112, train_loss: 0.99082, time: 0.01991\n",
      "Epoch: 113, train_loss: 0.98999, time: 0.01984\n",
      "Epoch: 114, train_loss: 0.99061, time: 0.02019\n",
      "Epoch: 115, train_loss: 0.98984, time: 0.01969\n",
      "Epoch: 116, train_loss: 0.99023, time: 0.02057\n",
      "Epoch: 117, train_loss: 0.99107, time: 0.02000\n",
      "Epoch: 118, train_loss: 0.99099, time: 0.02017\n",
      "Epoch: 119, train_loss: 0.98988, time: 0.02094\n",
      "Epoch: 120, train_loss: 0.99154, time: 0.02022\n",
      "Epoch: 121, train_loss: 0.98854, time: 0.02001\n",
      "Epoch: 122, train_loss: 0.98997, time: 0.01979\n",
      "Epoch: 123, train_loss: 0.99035, time: 0.02074\n",
      "Epoch: 124, train_loss: 0.98959, time: 0.02055\n",
      "Epoch: 125, train_loss: 0.99029, time: 0.01934\n",
      "Epoch: 126, train_loss: 0.98985, time: 0.01957\n",
      "Epoch: 127, train_loss: 0.98941, time: 0.01971\n",
      "Epoch: 128, train_loss: 0.98981, time: 0.01799\n",
      "Epoch: 129, train_loss: 0.99219, time: 0.01930\n",
      "Epoch: 130, train_loss: 0.98922, time: 0.01879\n",
      "Epoch: 131, train_loss: 0.99023, time: 0.01888\n",
      "Epoch: 132, train_loss: 0.98960, time: 0.02000\n",
      "Epoch: 133, train_loss: 0.98926, time: 0.01992\n",
      "Epoch: 134, train_loss: 0.98991, time: 0.02035\n",
      "Epoch: 135, train_loss: 0.99123, time: 0.02056\n",
      "Epoch: 136, train_loss: 0.98947, time: 0.02000\n",
      "Epoch: 137, train_loss: 0.98975, time: 0.02078\n",
      "Epoch: 138, train_loss: 0.98992, time: 0.02032\n",
      "Epoch: 139, train_loss: 0.99005, time: 0.02073\n",
      "Epoch: 140, train_loss: 0.98932, time: 0.02000\n",
      "Epoch: 141, train_loss: 0.98995, time: 0.02015\n",
      "Epoch: 142, train_loss: 0.99066, time: 0.01925\n",
      "Epoch: 143, train_loss: 0.99108, time: 0.01958\n",
      "Epoch: 144, train_loss: 0.98954, time: 0.02003\n",
      "Epoch: 145, train_loss: 0.99007, time: 0.01986\n",
      "Epoch: 146, train_loss: 0.99072, time: 0.01962\n",
      "Epoch: 147, train_loss: 0.98952, time: 0.02065\n",
      "Epoch: 148, train_loss: 0.99010, time: 0.01995\n",
      "Epoch: 149, train_loss: 0.98923, time: 0.01989\n",
      "Epoch: 150, train_loss: 0.99066, time: 0.02007\n",
      "Epoch: 151, train_loss: 0.99084, time: 0.01987\n",
      "Epoch: 152, train_loss: 0.99034, time: 0.02030\n",
      "Epoch: 153, train_loss: 0.99044, time: 0.02030\n",
      "Epoch: 154, train_loss: 0.99036, time: 0.02056\n",
      "Epoch: 155, train_loss: 0.99089, time: 0.02014\n",
      "Epoch: 156, train_loss: 0.98988, time: 0.01956\n",
      "Epoch: 157, train_loss: 0.99044, time: 0.01880\n",
      "Epoch: 158, train_loss: 0.98926, time: 0.01917\n",
      "Epoch: 159, train_loss: 0.98940, time: 0.01990\n",
      "Epoch: 160, train_loss: 0.99005, time: 0.02031\n",
      "Epoch: 161, train_loss: 0.99088, time: 0.02086\n",
      "Epoch: 162, train_loss: 0.98905, time: 0.02044\n",
      "Epoch: 163, train_loss: 0.99045, time: 0.02037\n",
      "Epoch: 164, train_loss: 0.98909, time: 0.01981\n",
      "Epoch: 165, train_loss: 0.99007, time: 0.01935\n",
      "Epoch: 166, train_loss: 0.98948, time: 0.02043\n",
      "Epoch: 167, train_loss: 0.98892, time: 0.01952\n",
      "Epoch: 168, train_loss: 0.98965, time: 0.02114\n",
      "Epoch: 169, train_loss: 0.98997, time: 0.02007\n",
      "Epoch: 170, train_loss: 0.99023, time: 0.02058\n",
      "Epoch: 171, train_loss: 0.98981, time: 0.02017\n",
      "Epoch: 172, train_loss: 0.99117, time: 0.02065\n",
      "Epoch: 173, train_loss: 0.98955, time: 0.02009\n",
      "Epoch: 174, train_loss: 0.99084, time: 0.02028\n",
      "Epoch: 175, train_loss: 0.98910, time: 0.02030\n",
      "Epoch: 176, train_loss: 0.99088, time: 0.02048\n",
      "Epoch: 177, train_loss: 0.98952, time: 0.02055\n",
      "Epoch: 178, train_loss: 0.99158, time: 0.01981\n",
      "Epoch: 179, train_loss: 0.99016, time: 0.01936\n",
      "Epoch: 180, train_loss: 0.99053, time: 0.02033\n",
      "Epoch: 181, train_loss: 0.99012, time: 0.01997\n",
      "Epoch: 182, train_loss: 0.99045, time: 0.02038\n",
      "Epoch: 183, train_loss: 0.99055, time: 0.02059\n",
      "Epoch: 184, train_loss: 0.98982, time: 0.02023\n",
      "Epoch: 185, train_loss: 0.99039, time: 0.01995\n",
      "Epoch: 186, train_loss: 0.99021, time: 0.01944\n",
      "Epoch: 187, train_loss: 0.98884, time: 0.02069\n",
      "Epoch: 188, train_loss: 0.99052, time: 0.02038\n",
      "Epoch: 189, train_loss: 0.99033, time: 0.01964\n",
      "Epoch: 190, train_loss: 0.98983, time: 0.02018\n",
      "Epoch: 191, train_loss: 0.98947, time: 0.02024\n",
      "Epoch: 192, train_loss: 0.99013, time: 0.01963\n",
      "Epoch: 193, train_loss: 0.99043, time: 0.02003\n",
      "Epoch: 194, train_loss: 0.99094, time: 0.01942\n",
      "Epoch: 195, train_loss: 0.99035, time: 0.02014\n",
      "Epoch: 196, train_loss: 0.99089, time: 0.02044\n",
      "Epoch: 197, train_loss: 0.99133, time: 0.02018\n",
      "Epoch: 198, train_loss: 0.98915, time: 0.01918\n",
      "Epoch: 199, train_loss: 0.99045, time: 0.01995\n",
      "Epoch: 200, train_loss: 0.99097, time: 0.02045\n",
      "pairwise precision 0.08173 recall 0.91078 f1 0.15001\n",
      "average until now [0.3530348439134492, 0.7460401371534594, 0.47927242073597176]\n",
      "5 names 88.80716729164124 avg time 17.76143345832825\n",
      "Loading jia_xu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 609 nodes, 7105 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98367, time: 0.12441\n",
      "Epoch: 2, train_loss: 0.98120, time: 0.02505\n",
      "Epoch: 3, train_loss: 0.97955, time: 0.02409\n",
      "Epoch: 4, train_loss: 0.97950, time: 0.02496\n",
      "Epoch: 5, train_loss: 0.97980, time: 0.02479\n",
      "Epoch: 6, train_loss: 0.98002, time: 0.02411\n",
      "Epoch: 7, train_loss: 0.98029, time: 0.02394\n",
      "Epoch: 8, train_loss: 0.97966, time: 0.02419\n",
      "Epoch: 9, train_loss: 0.98042, time: 0.02423\n",
      "Epoch: 10, train_loss: 0.97875, time: 0.02461\n",
      "Epoch: 11, train_loss: 0.98005, time: 0.02406\n",
      "Epoch: 12, train_loss: 0.97859, time: 0.02439\n",
      "Epoch: 13, train_loss: 0.98006, time: 0.02404\n",
      "Epoch: 14, train_loss: 0.97893, time: 0.02516\n",
      "Epoch: 15, train_loss: 0.97876, time: 0.02461\n",
      "Epoch: 16, train_loss: 0.97826, time: 0.02393\n",
      "Epoch: 17, train_loss: 0.97855, time: 0.02377\n",
      "Epoch: 18, train_loss: 0.97805, time: 0.02482\n",
      "Epoch: 19, train_loss: 0.97854, time: 0.02538\n",
      "Epoch: 20, train_loss: 0.97781, time: 0.02429\n",
      "Epoch: 21, train_loss: 0.97965, time: 0.02430\n",
      "Epoch: 22, train_loss: 0.98047, time: 0.02494\n",
      "Epoch: 23, train_loss: 0.97780, time: 0.02399\n",
      "Epoch: 24, train_loss: 0.97852, time: 0.02576\n",
      "Epoch: 25, train_loss: 0.97984, time: 0.02485\n",
      "Epoch: 26, train_loss: 0.97886, time: 0.02450\n",
      "Epoch: 27, train_loss: 0.97937, time: 0.02465\n",
      "Epoch: 28, train_loss: 0.98000, time: 0.02508\n",
      "Epoch: 29, train_loss: 0.97919, time: 0.02403\n",
      "Epoch: 30, train_loss: 0.97870, time: 0.02445\n",
      "Epoch: 31, train_loss: 0.97889, time: 0.02465\n",
      "Epoch: 32, train_loss: 0.97893, time: 0.02484\n",
      "Epoch: 33, train_loss: 0.97861, time: 0.02404\n",
      "Epoch: 34, train_loss: 0.97801, time: 0.02449\n",
      "Epoch: 35, train_loss: 0.97690, time: 0.02412\n",
      "Epoch: 36, train_loss: 0.97845, time: 0.02406\n",
      "Epoch: 37, train_loss: 0.97811, time: 0.02452\n",
      "Epoch: 38, train_loss: 0.97841, time: 0.02370\n",
      "Epoch: 39, train_loss: 0.97908, time: 0.02366\n",
      "Epoch: 40, train_loss: 0.97911, time: 0.02523\n",
      "Epoch: 41, train_loss: 0.97913, time: 0.02505\n",
      "Epoch: 42, train_loss: 0.98088, time: 0.02443\n",
      "Epoch: 43, train_loss: 0.97862, time: 0.02451\n",
      "Epoch: 44, train_loss: 0.98013, time: 0.02452\n",
      "Epoch: 45, train_loss: 0.97808, time: 0.02552\n",
      "Epoch: 46, train_loss: 0.97919, time: 0.02490\n",
      "Epoch: 47, train_loss: 0.97872, time: 0.02523\n",
      "Epoch: 48, train_loss: 0.97786, time: 0.02388\n",
      "Epoch: 49, train_loss: 0.97881, time: 0.02356\n",
      "Epoch: 50, train_loss: 0.97872, time: 0.02370\n",
      "Epoch: 51, train_loss: 0.98025, time: 0.02444\n",
      "Epoch: 52, train_loss: 0.97853, time: 0.02530\n",
      "Epoch: 53, train_loss: 0.97919, time: 0.02554\n",
      "Epoch: 54, train_loss: 0.97842, time: 0.02432\n",
      "Epoch: 55, train_loss: 0.97879, time: 0.02500\n",
      "Epoch: 56, train_loss: 0.97822, time: 0.02442\n",
      "Epoch: 57, train_loss: 0.97883, time: 0.02470\n",
      "Epoch: 58, train_loss: 0.97773, time: 0.02477\n",
      "Epoch: 59, train_loss: 0.97819, time: 0.02464\n",
      "Epoch: 60, train_loss: 0.97949, time: 0.02485\n",
      "Epoch: 61, train_loss: 0.97898, time: 0.02425\n",
      "Epoch: 62, train_loss: 0.97830, time: 0.02577\n",
      "Epoch: 63, train_loss: 0.97913, time: 0.02479\n",
      "Epoch: 64, train_loss: 0.97832, time: 0.02433\n",
      "Epoch: 65, train_loss: 0.97823, time: 0.02482\n",
      "Epoch: 66, train_loss: 0.97916, time: 0.02476\n",
      "Epoch: 67, train_loss: 0.98026, time: 0.02476\n",
      "Epoch: 68, train_loss: 0.97938, time: 0.02473\n",
      "Epoch: 69, train_loss: 0.97775, time: 0.02349\n",
      "Epoch: 70, train_loss: 0.97900, time: 0.02330\n",
      "Epoch: 71, train_loss: 0.97855, time: 0.02465\n",
      "Epoch: 72, train_loss: 0.97898, time: 0.02410\n",
      "Epoch: 73, train_loss: 0.97887, time: 0.02355\n",
      "Epoch: 74, train_loss: 0.97893, time: 0.02367\n",
      "Epoch: 75, train_loss: 0.97846, time: 0.02372\n",
      "Epoch: 76, train_loss: 0.97940, time: 0.02436\n",
      "Epoch: 77, train_loss: 0.97844, time: 0.02420\n",
      "Epoch: 78, train_loss: 0.97933, time: 0.02389\n",
      "Epoch: 79, train_loss: 0.97821, time: 0.02509\n",
      "Epoch: 80, train_loss: 0.97776, time: 0.02522\n",
      "Epoch: 81, train_loss: 0.97745, time: 0.02363\n",
      "Epoch: 82, train_loss: 0.97798, time: 0.02386\n",
      "Epoch: 83, train_loss: 0.97890, time: 0.02481\n",
      "Epoch: 84, train_loss: 0.97794, time: 0.02435\n",
      "Epoch: 85, train_loss: 0.97898, time: 0.02350\n",
      "Epoch: 86, train_loss: 0.97877, time: 0.02358\n",
      "Epoch: 87, train_loss: 0.97816, time: 0.02422\n",
      "Epoch: 88, train_loss: 0.97867, time: 0.02414\n",
      "Epoch: 89, train_loss: 0.97901, time: 0.02391\n",
      "Epoch: 90, train_loss: 0.97865, time: 0.02359\n",
      "Epoch: 91, train_loss: 0.97913, time: 0.02346\n",
      "Epoch: 92, train_loss: 0.97849, time: 0.02445\n",
      "Epoch: 93, train_loss: 0.97854, time: 0.02312\n",
      "Epoch: 94, train_loss: 0.97868, time: 0.02366\n",
      "Epoch: 95, train_loss: 0.97921, time: 0.02347\n",
      "Epoch: 96, train_loss: 0.97880, time: 0.02382\n",
      "Epoch: 97, train_loss: 0.97781, time: 0.02395\n",
      "Epoch: 98, train_loss: 0.97854, time: 0.02317\n",
      "Epoch: 99, train_loss: 0.97920, time: 0.02325\n",
      "Epoch: 100, train_loss: 0.97934, time: 0.02439\n",
      "Epoch: 101, train_loss: 0.97918, time: 0.02472\n",
      "Epoch: 102, train_loss: 0.97850, time: 0.02392\n",
      "Epoch: 103, train_loss: 0.97870, time: 0.02428\n",
      "Epoch: 104, train_loss: 0.97807, time: 0.02455\n",
      "Epoch: 105, train_loss: 0.97994, time: 0.02381\n",
      "Epoch: 106, train_loss: 0.97848, time: 0.02512\n",
      "Epoch: 107, train_loss: 0.97817, time: 0.02457\n",
      "Epoch: 108, train_loss: 0.97780, time: 0.02518\n",
      "Epoch: 109, train_loss: 0.97895, time: 0.02544\n",
      "Epoch: 110, train_loss: 0.97815, time: 0.02566\n",
      "Epoch: 111, train_loss: 0.97942, time: 0.02586\n",
      "Epoch: 112, train_loss: 0.97843, time: 0.02540\n",
      "Epoch: 113, train_loss: 0.97906, time: 0.02450\n",
      "Epoch: 114, train_loss: 0.97960, time: 0.02424\n",
      "Epoch: 115, train_loss: 0.97898, time: 0.02408\n",
      "Epoch: 116, train_loss: 0.97840, time: 0.02462\n",
      "Epoch: 117, train_loss: 0.97859, time: 0.02372\n",
      "Epoch: 118, train_loss: 0.97853, time: 0.02359\n",
      "Epoch: 119, train_loss: 0.97892, time: 0.02364\n",
      "Epoch: 120, train_loss: 0.97758, time: 0.02404\n",
      "Epoch: 121, train_loss: 0.97814, time: 0.02315\n",
      "Epoch: 122, train_loss: 0.97877, time: 0.02472\n",
      "Epoch: 123, train_loss: 0.97826, time: 0.02469\n",
      "Epoch: 124, train_loss: 0.97889, time: 0.02368\n",
      "Epoch: 125, train_loss: 0.97817, time: 0.02319\n",
      "Epoch: 126, train_loss: 0.97773, time: 0.02298\n",
      "Epoch: 127, train_loss: 0.97968, time: 0.02346\n",
      "Epoch: 128, train_loss: 0.97890, time: 0.02367\n",
      "Epoch: 129, train_loss: 0.97807, time: 0.02389\n",
      "Epoch: 130, train_loss: 0.97942, time: 0.02349\n",
      "Epoch: 131, train_loss: 0.97903, time: 0.02373\n",
      "Epoch: 132, train_loss: 0.97852, time: 0.02366\n",
      "Epoch: 133, train_loss: 0.97781, time: 0.02378\n",
      "Epoch: 134, train_loss: 0.97846, time: 0.02372\n",
      "Epoch: 135, train_loss: 0.97987, time: 0.02405\n",
      "Epoch: 136, train_loss: 0.97862, time: 0.02530\n",
      "Epoch: 137, train_loss: 0.97973, time: 0.02417\n",
      "Epoch: 138, train_loss: 0.97885, time: 0.02437\n",
      "Epoch: 139, train_loss: 0.97927, time: 0.02454\n",
      "Epoch: 140, train_loss: 0.97796, time: 0.02549\n",
      "Epoch: 141, train_loss: 0.97836, time: 0.02480\n",
      "Epoch: 142, train_loss: 0.97830, time: 0.02478\n",
      "Epoch: 143, train_loss: 0.97827, time: 0.02501\n",
      "Epoch: 144, train_loss: 0.97815, time: 0.02451\n",
      "Epoch: 145, train_loss: 0.97904, time: 0.02486\n",
      "Epoch: 146, train_loss: 0.97790, time: 0.02454\n",
      "Epoch: 147, train_loss: 0.97836, time: 0.02514\n",
      "Epoch: 148, train_loss: 0.97823, time: 0.02452\n",
      "Epoch: 149, train_loss: 0.97957, time: 0.02359\n",
      "Epoch: 150, train_loss: 0.97928, time: 0.02428\n",
      "Epoch: 151, train_loss: 0.97800, time: 0.02538\n",
      "Epoch: 152, train_loss: 0.97986, time: 0.02502\n",
      "Epoch: 153, train_loss: 0.97881, time: 0.02541\n",
      "Epoch: 154, train_loss: 0.97910, time: 0.02555\n",
      "Epoch: 155, train_loss: 0.97930, time: 0.02558\n",
      "Epoch: 156, train_loss: 0.97875, time: 0.02391\n",
      "Epoch: 157, train_loss: 0.97820, time: 0.02431\n",
      "Epoch: 158, train_loss: 0.97976, time: 0.02324\n",
      "Epoch: 159, train_loss: 0.97895, time: 0.02352\n",
      "Epoch: 160, train_loss: 0.97882, time: 0.02376\n",
      "Epoch: 161, train_loss: 0.97946, time: 0.02414\n",
      "Epoch: 162, train_loss: 0.97806, time: 0.02506\n",
      "Epoch: 163, train_loss: 0.97901, time: 0.02394\n",
      "Epoch: 164, train_loss: 0.97826, time: 0.02503\n",
      "Epoch: 165, train_loss: 0.97866, time: 0.02402\n",
      "Epoch: 166, train_loss: 0.97775, time: 0.02495\n",
      "Epoch: 167, train_loss: 0.97890, time: 0.02420\n",
      "Epoch: 168, train_loss: 0.97869, time: 0.02433\n",
      "Epoch: 169, train_loss: 0.97802, time: 0.02383\n",
      "Epoch: 170, train_loss: 0.97914, time: 0.02433\n",
      "Epoch: 171, train_loss: 0.97948, time: 0.02487\n",
      "Epoch: 172, train_loss: 0.97949, time: 0.02501\n",
      "Epoch: 173, train_loss: 0.98042, time: 0.02360\n",
      "Epoch: 174, train_loss: 0.97840, time: 0.02389\n",
      "Epoch: 175, train_loss: 0.97893, time: 0.02369\n",
      "Epoch: 176, train_loss: 0.97857, time: 0.02509\n",
      "Epoch: 177, train_loss: 0.97876, time: 0.02381\n",
      "Epoch: 178, train_loss: 0.97922, time: 0.02375\n",
      "Epoch: 179, train_loss: 0.97872, time: 0.02559\n",
      "Epoch: 180, train_loss: 0.97905, time: 0.02638\n",
      "Epoch: 181, train_loss: 0.97846, time: 0.02603\n",
      "Epoch: 182, train_loss: 0.97892, time: 0.02471\n",
      "Epoch: 183, train_loss: 0.97906, time: 0.02411\n",
      "Epoch: 184, train_loss: 0.97908, time: 0.02462\n",
      "Epoch: 185, train_loss: 0.97852, time: 0.02412\n",
      "Epoch: 186, train_loss: 0.97926, time: 0.02512\n",
      "Epoch: 187, train_loss: 0.97915, time: 0.02500\n",
      "Epoch: 188, train_loss: 0.97909, time: 0.02521\n",
      "Epoch: 189, train_loss: 0.97857, time: 0.02465\n",
      "Epoch: 190, train_loss: 0.97892, time: 0.02461\n",
      "Epoch: 191, train_loss: 0.97850, time: 0.02460\n",
      "Epoch: 192, train_loss: 0.97827, time: 0.02464\n",
      "Epoch: 193, train_loss: 0.97850, time: 0.02483\n",
      "Epoch: 194, train_loss: 0.97908, time: 0.02505\n",
      "Epoch: 195, train_loss: 0.97879, time: 0.02405\n",
      "Epoch: 196, train_loss: 0.97754, time: 0.02350\n",
      "Epoch: 197, train_loss: 0.97763, time: 0.02414\n",
      "Epoch: 198, train_loss: 0.97958, time: 0.02505\n",
      "Epoch: 199, train_loss: 0.97859, time: 0.02530\n",
      "Epoch: 200, train_loss: 0.97881, time: 0.02556\n",
      "pairwise precision 0.35532 recall 0.91794 f1 0.51233\n",
      "average until now [0.35341593508417923, 0.7746900420039352, 0.4853937682378537]\n",
      "6 names 93.92962861061096 avg time 15.654938101768494\n",
      "Loading lixin_tang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 195 nodes, 4831 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.87106, time: 0.11231\n",
      "Epoch: 2, train_loss: 0.87339, time: 0.01478\n",
      "Epoch: 3, train_loss: 0.87138, time: 0.01409\n",
      "Epoch: 4, train_loss: 0.87135, time: 0.01339\n",
      "Epoch: 5, train_loss: 0.87244, time: 0.01289\n",
      "Epoch: 6, train_loss: 0.87555, time: 0.01277\n",
      "Epoch: 7, train_loss: 0.87073, time: 0.01326\n",
      "Epoch: 8, train_loss: 0.87269, time: 0.01299\n",
      "Epoch: 9, train_loss: 0.87105, time: 0.01298\n",
      "Epoch: 10, train_loss: 0.87147, time: 0.01320\n",
      "Epoch: 11, train_loss: 0.87336, time: 0.01305\n",
      "Epoch: 12, train_loss: 0.87294, time: 0.01286\n",
      "Epoch: 13, train_loss: 0.87082, time: 0.01304\n",
      "Epoch: 14, train_loss: 0.87060, time: 0.01305\n",
      "Epoch: 15, train_loss: 0.87364, time: 0.01269\n",
      "Epoch: 16, train_loss: 0.86963, time: 0.01280\n",
      "Epoch: 17, train_loss: 0.87620, time: 0.01317\n",
      "Epoch: 18, train_loss: 0.87097, time: 0.01286\n",
      "Epoch: 19, train_loss: 0.86796, time: 0.01304\n",
      "Epoch: 20, train_loss: 0.87475, time: 0.01322\n",
      "Epoch: 21, train_loss: 0.87467, time: 0.01322\n",
      "Epoch: 22, train_loss: 0.87309, time: 0.01311\n",
      "Epoch: 23, train_loss: 0.87173, time: 0.01290\n",
      "Epoch: 24, train_loss: 0.87657, time: 0.01304\n",
      "Epoch: 25, train_loss: 0.87257, time: 0.01309\n",
      "Epoch: 26, train_loss: 0.87312, time: 0.01279\n",
      "Epoch: 27, train_loss: 0.86894, time: 0.01267\n",
      "Epoch: 28, train_loss: 0.86849, time: 0.01293\n",
      "Epoch: 29, train_loss: 0.87333, time: 0.01298\n",
      "Epoch: 30, train_loss: 0.87376, time: 0.01300\n",
      "Epoch: 31, train_loss: 0.87170, time: 0.01265\n",
      "Epoch: 32, train_loss: 0.86995, time: 0.01298\n",
      "Epoch: 33, train_loss: 0.87500, time: 0.01285\n",
      "Epoch: 34, train_loss: 0.87273, time: 0.01275\n",
      "Epoch: 35, train_loss: 0.86818, time: 0.01277\n",
      "Epoch: 36, train_loss: 0.87072, time: 0.01263\n",
      "Epoch: 37, train_loss: 0.87030, time: 0.01239\n",
      "Epoch: 38, train_loss: 0.87139, time: 0.01255\n",
      "Epoch: 39, train_loss: 0.87353, time: 0.01264\n",
      "Epoch: 40, train_loss: 0.87226, time: 0.01315\n",
      "Epoch: 41, train_loss: 0.86900, time: 0.01291\n",
      "Epoch: 42, train_loss: 0.87120, time: 0.01316\n",
      "Epoch: 43, train_loss: 0.87193, time: 0.01286\n",
      "Epoch: 44, train_loss: 0.87262, time: 0.01335\n",
      "Epoch: 45, train_loss: 0.87403, time: 0.01277\n",
      "Epoch: 46, train_loss: 0.86976, time: 0.01321\n",
      "Epoch: 47, train_loss: 0.86973, time: 0.01308\n",
      "Epoch: 48, train_loss: 0.87120, time: 0.01403\n",
      "Epoch: 49, train_loss: 0.87027, time: 0.01301\n",
      "Epoch: 50, train_loss: 0.87464, time: 0.01288\n",
      "Epoch: 51, train_loss: 0.87274, time: 0.01330\n",
      "Epoch: 52, train_loss: 0.87012, time: 0.01287\n",
      "Epoch: 53, train_loss: 0.87406, time: 0.01299\n",
      "Epoch: 54, train_loss: 0.87220, time: 0.01290\n",
      "Epoch: 55, train_loss: 0.87361, time: 0.01323\n",
      "Epoch: 56, train_loss: 0.86943, time: 0.01332\n",
      "Epoch: 57, train_loss: 0.86807, time: 0.01301\n",
      "Epoch: 58, train_loss: 0.87571, time: 0.01299\n",
      "Epoch: 59, train_loss: 0.87161, time: 0.01298\n",
      "Epoch: 60, train_loss: 0.87166, time: 0.01311\n",
      "Epoch: 61, train_loss: 0.86954, time: 0.01324\n",
      "Epoch: 62, train_loss: 0.87310, time: 0.01258\n",
      "Epoch: 63, train_loss: 0.87035, time: 0.01297\n",
      "Epoch: 64, train_loss: 0.87419, time: 0.01314\n",
      "Epoch: 65, train_loss: 0.86959, time: 0.01362\n",
      "Epoch: 66, train_loss: 0.87374, time: 0.01291\n",
      "Epoch: 67, train_loss: 0.87412, time: 0.01286\n",
      "Epoch: 68, train_loss: 0.87561, time: 0.01288\n",
      "Epoch: 69, train_loss: 0.87626, time: 0.01309\n",
      "Epoch: 70, train_loss: 0.87226, time: 0.01307\n",
      "Epoch: 71, train_loss: 0.87317, time: 0.01295\n",
      "Epoch: 72, train_loss: 0.87229, time: 0.01299\n",
      "Epoch: 73, train_loss: 0.87238, time: 0.01288\n",
      "Epoch: 74, train_loss: 0.86975, time: 0.01288\n",
      "Epoch: 75, train_loss: 0.87113, time: 0.01309\n",
      "Epoch: 76, train_loss: 0.87404, time: 0.01279\n",
      "Epoch: 77, train_loss: 0.87679, time: 0.01320\n",
      "Epoch: 78, train_loss: 0.87007, time: 0.01322\n",
      "Epoch: 79, train_loss: 0.87266, time: 0.01264\n",
      "Epoch: 80, train_loss: 0.87381, time: 0.01283\n",
      "Epoch: 81, train_loss: 0.87554, time: 0.01335\n",
      "Epoch: 82, train_loss: 0.87407, time: 0.01321\n",
      "Epoch: 83, train_loss: 0.87258, time: 0.01307\n",
      "Epoch: 84, train_loss: 0.87119, time: 0.01288\n",
      "Epoch: 85, train_loss: 0.87098, time: 0.01281\n",
      "Epoch: 86, train_loss: 0.87296, time: 0.01308\n",
      "Epoch: 87, train_loss: 0.87101, time: 0.01273\n",
      "Epoch: 88, train_loss: 0.87182, time: 0.01288\n",
      "Epoch: 89, train_loss: 0.87358, time: 0.01310\n",
      "Epoch: 90, train_loss: 0.87576, time: 0.01288\n",
      "Epoch: 91, train_loss: 0.87181, time: 0.01309\n",
      "Epoch: 92, train_loss: 0.87308, time: 0.01276\n",
      "Epoch: 93, train_loss: 0.87095, time: 0.01255\n",
      "Epoch: 94, train_loss: 0.86804, time: 0.01253\n",
      "Epoch: 95, train_loss: 0.86790, time: 0.01245\n",
      "Epoch: 96, train_loss: 0.87342, time: 0.01295\n",
      "Epoch: 97, train_loss: 0.87381, time: 0.01350\n",
      "Epoch: 98, train_loss: 0.86987, time: 0.01307\n",
      "Epoch: 99, train_loss: 0.87221, time: 0.01323\n",
      "Epoch: 100, train_loss: 0.87342, time: 0.01303\n",
      "Epoch: 101, train_loss: 0.87016, time: 0.01266\n",
      "Epoch: 102, train_loss: 0.87505, time: 0.01267\n",
      "Epoch: 103, train_loss: 0.87458, time: 0.01279\n",
      "Epoch: 104, train_loss: 0.87250, time: 0.01281\n",
      "Epoch: 105, train_loss: 0.87220, time: 0.01276\n",
      "Epoch: 106, train_loss: 0.86912, time: 0.01269\n",
      "Epoch: 107, train_loss: 0.87131, time: 0.01321\n",
      "Epoch: 108, train_loss: 0.87428, time: 0.01259\n",
      "Epoch: 109, train_loss: 0.87050, time: 0.01320\n",
      "Epoch: 110, train_loss: 0.87186, time: 0.01297\n",
      "Epoch: 111, train_loss: 0.87169, time: 0.01309\n",
      "Epoch: 112, train_loss: 0.87832, time: 0.01320\n",
      "Epoch: 113, train_loss: 0.87149, time: 0.01309\n",
      "Epoch: 114, train_loss: 0.87114, time: 0.01283\n",
      "Epoch: 115, train_loss: 0.87185, time: 0.01303\n",
      "Epoch: 116, train_loss: 0.87148, time: 0.01281\n",
      "Epoch: 117, train_loss: 0.87597, time: 0.01281\n",
      "Epoch: 118, train_loss: 0.87068, time: 0.01312\n",
      "Epoch: 119, train_loss: 0.87209, time: 0.01283\n",
      "Epoch: 120, train_loss: 0.86973, time: 0.01317\n",
      "Epoch: 121, train_loss: 0.87278, time: 0.01312\n",
      "Epoch: 122, train_loss: 0.87062, time: 0.01310\n",
      "Epoch: 123, train_loss: 0.86888, time: 0.01283\n",
      "Epoch: 124, train_loss: 0.87302, time: 0.01285\n",
      "Epoch: 125, train_loss: 0.87083, time: 0.01280\n",
      "Epoch: 126, train_loss: 0.87332, time: 0.01295\n",
      "Epoch: 127, train_loss: 0.87062, time: 0.01305\n",
      "Epoch: 128, train_loss: 0.87257, time: 0.01293\n",
      "Epoch: 129, train_loss: 0.87424, time: 0.01324\n",
      "Epoch: 130, train_loss: 0.87472, time: 0.01296\n",
      "Epoch: 131, train_loss: 0.86999, time: 0.01301\n",
      "Epoch: 132, train_loss: 0.86542, time: 0.01317\n",
      "Epoch: 133, train_loss: 0.87140, time: 0.01315\n",
      "Epoch: 134, train_loss: 0.87372, time: 0.01308\n",
      "Epoch: 135, train_loss: 0.87242, time: 0.01274\n",
      "Epoch: 136, train_loss: 0.87315, time: 0.01351\n",
      "Epoch: 137, train_loss: 0.87315, time: 0.01296\n",
      "Epoch: 138, train_loss: 0.87244, time: 0.01303\n",
      "Epoch: 139, train_loss: 0.87022, time: 0.01287\n",
      "Epoch: 140, train_loss: 0.87169, time: 0.01289\n",
      "Epoch: 141, train_loss: 0.87289, time: 0.01283\n",
      "Epoch: 142, train_loss: 0.87243, time: 0.01271\n",
      "Epoch: 143, train_loss: 0.86822, time: 0.01278\n",
      "Epoch: 144, train_loss: 0.87027, time: 0.01326\n",
      "Epoch: 145, train_loss: 0.87154, time: 0.01304\n",
      "Epoch: 146, train_loss: 0.87005, time: 0.01309\n",
      "Epoch: 147, train_loss: 0.87026, time: 0.01305\n",
      "Epoch: 148, train_loss: 0.87439, time: 0.01265\n",
      "Epoch: 149, train_loss: 0.87471, time: 0.01298\n",
      "Epoch: 150, train_loss: 0.87071, time: 0.01233\n",
      "Epoch: 151, train_loss: 0.87241, time: 0.01254\n",
      "Epoch: 152, train_loss: 0.87187, time: 0.01304\n",
      "Epoch: 153, train_loss: 0.87204, time: 0.01298\n",
      "Epoch: 154, train_loss: 0.87357, time: 0.01281\n",
      "Epoch: 155, train_loss: 0.87193, time: 0.01287\n",
      "Epoch: 156, train_loss: 0.87086, time: 0.01275\n",
      "Epoch: 157, train_loss: 0.87077, time: 0.01321\n",
      "Epoch: 158, train_loss: 0.86977, time: 0.01330\n",
      "Epoch: 159, train_loss: 0.87016, time: 0.01288\n",
      "Epoch: 160, train_loss: 0.87357, time: 0.01316\n",
      "Epoch: 161, train_loss: 0.87189, time: 0.01311\n",
      "Epoch: 162, train_loss: 0.87134, time: 0.01332\n",
      "Epoch: 163, train_loss: 0.87054, time: 0.01289\n",
      "Epoch: 164, train_loss: 0.86912, time: 0.01304\n",
      "Epoch: 165, train_loss: 0.87353, time: 0.01265\n",
      "Epoch: 166, train_loss: 0.87356, time: 0.01322\n",
      "Epoch: 167, train_loss: 0.86982, time: 0.01282\n",
      "Epoch: 168, train_loss: 0.87142, time: 0.01317\n",
      "Epoch: 169, train_loss: 0.87157, time: 0.01335\n",
      "Epoch: 170, train_loss: 0.87162, time: 0.01281\n",
      "Epoch: 171, train_loss: 0.87454, time: 0.01283\n",
      "Epoch: 172, train_loss: 0.86979, time: 0.01290\n",
      "Epoch: 173, train_loss: 0.87386, time: 0.01304\n",
      "Epoch: 174, train_loss: 0.87090, time: 0.01286\n",
      "Epoch: 175, train_loss: 0.86733, time: 0.01297\n",
      "Epoch: 176, train_loss: 0.86975, time: 0.01308\n",
      "Epoch: 177, train_loss: 0.87236, time: 0.01318\n",
      "Epoch: 178, train_loss: 0.87285, time: 0.01299\n",
      "Epoch: 179, train_loss: 0.87074, time: 0.01266\n",
      "Epoch: 180, train_loss: 0.87162, time: 0.01265\n",
      "Epoch: 181, train_loss: 0.87277, time: 0.01302\n",
      "Epoch: 182, train_loss: 0.87244, time: 0.01287\n",
      "Epoch: 183, train_loss: 0.87081, time: 0.01353\n",
      "Epoch: 184, train_loss: 0.87301, time: 0.01293\n",
      "Epoch: 185, train_loss: 0.87331, time: 0.01295\n",
      "Epoch: 186, train_loss: 0.87288, time: 0.01358\n",
      "Epoch: 187, train_loss: 0.87475, time: 0.01298\n",
      "Epoch: 188, train_loss: 0.87011, time: 0.01299\n",
      "Epoch: 189, train_loss: 0.87368, time: 0.01304\n",
      "Epoch: 190, train_loss: 0.87138, time: 0.01304\n",
      "Epoch: 191, train_loss: 0.87088, time: 0.01290\n",
      "Epoch: 192, train_loss: 0.87167, time: 0.01299\n",
      "Epoch: 193, train_loss: 0.87165, time: 0.01303\n",
      "Epoch: 194, train_loss: 0.87410, time: 0.01302\n",
      "Epoch: 195, train_loss: 0.87203, time: 0.01282\n",
      "Epoch: 196, train_loss: 0.87156, time: 0.01283\n",
      "Epoch: 197, train_loss: 0.87052, time: 0.01301\n",
      "Epoch: 198, train_loss: 0.87087, time: 0.01307\n",
      "Epoch: 199, train_loss: 0.87045, time: 0.01314\n",
      "Epoch: 200, train_loss: 0.87332, time: 0.01301\n",
      "pairwise precision 0.49779 recall 0.99816 f1 0.66429\n",
      "average until now [0.3740404783216344, 0.8066138173804244, 0.5110830819353572]\n",
      "7 names 96.68050837516785 avg time 13.81150119645255\n",
      "Loading guotong_du dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 220 nodes, 6991 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.85647, time: 0.11489\n",
      "Epoch: 2, train_loss: 0.85355, time: 0.01571\n",
      "Epoch: 3, train_loss: 0.85379, time: 0.01523\n",
      "Epoch: 4, train_loss: 0.85635, time: 0.01465\n",
      "Epoch: 5, train_loss: 0.85560, time: 0.01460\n",
      "Epoch: 6, train_loss: 0.85285, time: 0.01437\n",
      "Epoch: 7, train_loss: 0.85871, time: 0.01480\n",
      "Epoch: 8, train_loss: 0.85381, time: 0.01443\n",
      "Epoch: 9, train_loss: 0.85730, time: 0.01469\n",
      "Epoch: 10, train_loss: 0.85362, time: 0.01432\n",
      "Epoch: 11, train_loss: 0.85364, time: 0.01447\n",
      "Epoch: 12, train_loss: 0.85381, time: 0.01463\n",
      "Epoch: 13, train_loss: 0.85391, time: 0.01450\n",
      "Epoch: 14, train_loss: 0.85480, time: 0.01467\n",
      "Epoch: 15, train_loss: 0.85417, time: 0.01522\n",
      "Epoch: 16, train_loss: 0.85591, time: 0.01458\n",
      "Epoch: 17, train_loss: 0.85424, time: 0.01445\n",
      "Epoch: 18, train_loss: 0.85604, time: 0.01450\n",
      "Epoch: 19, train_loss: 0.85696, time: 0.01457\n",
      "Epoch: 20, train_loss: 0.85338, time: 0.01472\n",
      "Epoch: 21, train_loss: 0.85309, time: 0.01483\n",
      "Epoch: 22, train_loss: 0.85548, time: 0.01461\n",
      "Epoch: 23, train_loss: 0.85382, time: 0.01474\n",
      "Epoch: 24, train_loss: 0.85479, time: 0.01429\n",
      "Epoch: 25, train_loss: 0.85653, time: 0.01479\n",
      "Epoch: 26, train_loss: 0.85420, time: 0.01443\n",
      "Epoch: 27, train_loss: 0.85539, time: 0.01487\n",
      "Epoch: 28, train_loss: 0.85703, time: 0.01483\n",
      "Epoch: 29, train_loss: 0.85537, time: 0.01530\n",
      "Epoch: 30, train_loss: 0.85663, time: 0.01462\n",
      "Epoch: 31, train_loss: 0.85348, time: 0.01460\n",
      "Epoch: 32, train_loss: 0.85670, time: 0.01420\n",
      "Epoch: 33, train_loss: 0.85329, time: 0.01457\n",
      "Epoch: 34, train_loss: 0.85406, time: 0.01438\n",
      "Epoch: 35, train_loss: 0.85595, time: 0.01479\n",
      "Epoch: 36, train_loss: 0.85253, time: 0.01501\n",
      "Epoch: 37, train_loss: 0.85587, time: 0.01438\n",
      "Epoch: 38, train_loss: 0.85477, time: 0.01462\n",
      "Epoch: 39, train_loss: 0.85559, time: 0.01430\n",
      "Epoch: 40, train_loss: 0.85519, time: 0.01462\n",
      "Epoch: 41, train_loss: 0.85498, time: 0.01431\n",
      "Epoch: 42, train_loss: 0.85492, time: 0.01485\n",
      "Epoch: 43, train_loss: 0.85672, time: 0.01477\n",
      "Epoch: 44, train_loss: 0.85414, time: 0.01474\n",
      "Epoch: 45, train_loss: 0.85329, time: 0.01440\n",
      "Epoch: 46, train_loss: 0.85711, time: 0.01417\n",
      "Epoch: 47, train_loss: 0.85344, time: 0.01443\n",
      "Epoch: 48, train_loss: 0.85368, time: 0.01449\n",
      "Epoch: 49, train_loss: 0.85238, time: 0.01416\n",
      "Epoch: 50, train_loss: 0.85349, time: 0.01493\n",
      "Epoch: 51, train_loss: 0.85459, time: 0.01472\n",
      "Epoch: 52, train_loss: 0.85602, time: 0.01502\n",
      "Epoch: 53, train_loss: 0.85468, time: 0.01439\n",
      "Epoch: 54, train_loss: 0.85351, time: 0.01474\n",
      "Epoch: 55, train_loss: 0.85434, time: 0.01486\n",
      "Epoch: 56, train_loss: 0.85469, time: 0.01472\n",
      "Epoch: 57, train_loss: 0.85556, time: 0.01485\n",
      "Epoch: 58, train_loss: 0.85706, time: 0.01446\n",
      "Epoch: 59, train_loss: 0.85457, time: 0.01436\n",
      "Epoch: 60, train_loss: 0.85611, time: 0.01438\n",
      "Epoch: 61, train_loss: 0.85512, time: 0.01462\n",
      "Epoch: 62, train_loss: 0.85344, time: 0.01449\n",
      "Epoch: 63, train_loss: 0.85470, time: 0.01427\n",
      "Epoch: 64, train_loss: 0.85641, time: 0.01454\n",
      "Epoch: 65, train_loss: 0.85231, time: 0.01466\n",
      "Epoch: 66, train_loss: 0.85514, time: 0.01440\n",
      "Epoch: 67, train_loss: 0.85404, time: 0.01443\n",
      "Epoch: 68, train_loss: 0.85797, time: 0.01441\n",
      "Epoch: 69, train_loss: 0.85310, time: 0.01468\n",
      "Epoch: 70, train_loss: 0.85545, time: 0.01456\n",
      "Epoch: 71, train_loss: 0.85608, time: 0.01485\n",
      "Epoch: 72, train_loss: 0.85496, time: 0.01468\n",
      "Epoch: 73, train_loss: 0.85204, time: 0.01434\n",
      "Epoch: 74, train_loss: 0.85549, time: 0.01490\n",
      "Epoch: 75, train_loss: 0.85398, time: 0.01461\n",
      "Epoch: 76, train_loss: 0.85180, time: 0.01519\n",
      "Epoch: 77, train_loss: 0.85254, time: 0.01443\n",
      "Epoch: 78, train_loss: 0.85567, time: 0.01454\n",
      "Epoch: 79, train_loss: 0.85726, time: 0.01478\n",
      "Epoch: 80, train_loss: 0.85572, time: 0.01477\n",
      "Epoch: 81, train_loss: 0.85653, time: 0.01444\n",
      "Epoch: 82, train_loss: 0.85390, time: 0.01460\n",
      "Epoch: 83, train_loss: 0.85647, time: 0.01484\n",
      "Epoch: 84, train_loss: 0.85234, time: 0.01412\n",
      "Epoch: 85, train_loss: 0.85473, time: 0.01428\n",
      "Epoch: 86, train_loss: 0.85396, time: 0.01402\n",
      "Epoch: 87, train_loss: 0.85421, time: 0.01401\n",
      "Epoch: 88, train_loss: 0.85831, time: 0.01446\n",
      "Epoch: 89, train_loss: 0.85616, time: 0.01517\n",
      "Epoch: 90, train_loss: 0.85569, time: 0.01534\n",
      "Epoch: 91, train_loss: 0.85407, time: 0.01469\n",
      "Epoch: 92, train_loss: 0.85463, time: 0.01476\n",
      "Epoch: 93, train_loss: 0.85226, time: 0.01456\n",
      "Epoch: 94, train_loss: 0.85562, time: 0.01450\n",
      "Epoch: 95, train_loss: 0.85558, time: 0.01442\n",
      "Epoch: 96, train_loss: 0.85472, time: 0.01470\n",
      "Epoch: 97, train_loss: 0.85585, time: 0.01456\n",
      "Epoch: 98, train_loss: 0.85405, time: 0.01456\n",
      "Epoch: 99, train_loss: 0.85533, time: 0.01500\n",
      "Epoch: 100, train_loss: 0.85244, time: 0.01460\n",
      "Epoch: 101, train_loss: 0.85513, time: 0.01500\n",
      "Epoch: 102, train_loss: 0.85216, time: 0.01438\n",
      "Epoch: 103, train_loss: 0.85328, time: 0.01467\n",
      "Epoch: 104, train_loss: 0.85362, time: 0.01454\n",
      "Epoch: 105, train_loss: 0.85368, time: 0.01468\n",
      "Epoch: 106, train_loss: 0.85320, time: 0.01445\n",
      "Epoch: 107, train_loss: 0.85286, time: 0.01444\n",
      "Epoch: 108, train_loss: 0.85413, time: 0.01433\n",
      "Epoch: 109, train_loss: 0.85105, time: 0.01416\n",
      "Epoch: 110, train_loss: 0.85596, time: 0.01393\n",
      "Epoch: 111, train_loss: 0.85443, time: 0.01451\n",
      "Epoch: 112, train_loss: 0.85584, time: 0.01465\n",
      "Epoch: 113, train_loss: 0.85367, time: 0.01479\n",
      "Epoch: 114, train_loss: 0.85398, time: 0.01475\n",
      "Epoch: 115, train_loss: 0.85550, time: 0.01456\n",
      "Epoch: 116, train_loss: 0.85518, time: 0.01448\n",
      "Epoch: 117, train_loss: 0.85300, time: 0.01392\n",
      "Epoch: 118, train_loss: 0.85541, time: 0.01435\n",
      "Epoch: 119, train_loss: 0.85346, time: 0.01446\n",
      "Epoch: 120, train_loss: 0.85349, time: 0.01483\n",
      "Epoch: 121, train_loss: 0.85405, time: 0.01421\n",
      "Epoch: 122, train_loss: 0.85584, time: 0.01435\n",
      "Epoch: 123, train_loss: 0.85616, time: 0.01442\n",
      "Epoch: 124, train_loss: 0.85212, time: 0.01492\n",
      "Epoch: 125, train_loss: 0.85345, time: 0.01464\n",
      "Epoch: 126, train_loss: 0.85794, time: 0.01453\n",
      "Epoch: 127, train_loss: 0.85498, time: 0.01501\n",
      "Epoch: 128, train_loss: 0.85707, time: 0.01465\n",
      "Epoch: 129, train_loss: 0.85332, time: 0.01432\n",
      "Epoch: 130, train_loss: 0.85408, time: 0.01440\n",
      "Epoch: 131, train_loss: 0.85493, time: 0.01432\n",
      "Epoch: 132, train_loss: 0.85373, time: 0.01465\n",
      "Epoch: 133, train_loss: 0.85417, time: 0.01459\n",
      "Epoch: 134, train_loss: 0.85619, time: 0.01507\n",
      "Epoch: 135, train_loss: 0.85289, time: 0.01471\n",
      "Epoch: 136, train_loss: 0.85133, time: 0.01494\n",
      "Epoch: 137, train_loss: 0.85423, time: 0.01505\n",
      "Epoch: 138, train_loss: 0.85441, time: 0.01462\n",
      "Epoch: 139, train_loss: 0.85545, time: 0.01460\n",
      "Epoch: 140, train_loss: 0.85337, time: 0.01440\n",
      "Epoch: 141, train_loss: 0.85307, time: 0.01509\n",
      "Epoch: 142, train_loss: 0.85582, time: 0.01468\n",
      "Epoch: 143, train_loss: 0.85205, time: 0.01453\n",
      "Epoch: 144, train_loss: 0.85298, time: 0.01483\n",
      "Epoch: 145, train_loss: 0.85301, time: 0.01442\n",
      "Epoch: 146, train_loss: 0.85495, time: 0.01420\n",
      "Epoch: 147, train_loss: 0.85419, time: 0.01469\n",
      "Epoch: 148, train_loss: 0.85229, time: 0.01464\n",
      "Epoch: 149, train_loss: 0.85341, time: 0.01464\n",
      "Epoch: 150, train_loss: 0.85638, time: 0.01444\n",
      "Epoch: 151, train_loss: 0.85477, time: 0.01479\n",
      "Epoch: 152, train_loss: 0.85180, time: 0.01455\n",
      "Epoch: 153, train_loss: 0.85747, time: 0.01463\n",
      "Epoch: 154, train_loss: 0.85599, time: 0.01449\n",
      "Epoch: 155, train_loss: 0.85333, time: 0.01456\n",
      "Epoch: 156, train_loss: 0.85576, time: 0.01474\n",
      "Epoch: 157, train_loss: 0.85367, time: 0.01435\n",
      "Epoch: 158, train_loss: 0.85491, time: 0.01442\n",
      "Epoch: 159, train_loss: 0.85175, time: 0.01474\n",
      "Epoch: 160, train_loss: 0.85487, time: 0.01449\n",
      "Epoch: 161, train_loss: 0.85495, time: 0.01471\n",
      "Epoch: 162, train_loss: 0.85566, time: 0.01458\n",
      "Epoch: 163, train_loss: 0.85861, time: 0.01444\n",
      "Epoch: 164, train_loss: 0.85371, time: 0.01449\n",
      "Epoch: 165, train_loss: 0.85487, time: 0.01458\n",
      "Epoch: 166, train_loss: 0.85512, time: 0.01453\n",
      "Epoch: 167, train_loss: 0.85611, time: 0.01450\n",
      "Epoch: 168, train_loss: 0.85584, time: 0.01405\n",
      "Epoch: 169, train_loss: 0.85374, time: 0.01467\n",
      "Epoch: 170, train_loss: 0.85363, time: 0.01467\n",
      "Epoch: 171, train_loss: 0.85479, time: 0.01481\n",
      "Epoch: 172, train_loss: 0.85661, time: 0.01445\n",
      "Epoch: 173, train_loss: 0.85571, time: 0.01494\n",
      "Epoch: 174, train_loss: 0.85698, time: 0.01477\n",
      "Epoch: 175, train_loss: 0.85964, time: 0.01459\n",
      "Epoch: 176, train_loss: 0.85316, time: 0.01430\n",
      "Epoch: 177, train_loss: 0.85303, time: 0.01452\n",
      "Epoch: 178, train_loss: 0.85384, time: 0.01487\n",
      "Epoch: 179, train_loss: 0.85441, time: 0.01445\n",
      "Epoch: 180, train_loss: 0.85377, time: 0.01491\n",
      "Epoch: 181, train_loss: 0.85237, time: 0.01414\n",
      "Epoch: 182, train_loss: 0.85167, time: 0.01448\n",
      "Epoch: 183, train_loss: 0.85315, time: 0.01501\n",
      "Epoch: 184, train_loss: 0.85102, time: 0.01445\n",
      "Epoch: 185, train_loss: 0.85309, time: 0.01446\n",
      "Epoch: 186, train_loss: 0.85693, time: 0.01418\n",
      "Epoch: 187, train_loss: 0.85531, time: 0.01443\n",
      "Epoch: 188, train_loss: 0.85551, time: 0.01461\n",
      "Epoch: 189, train_loss: 0.85502, time: 0.01435\n",
      "Epoch: 190, train_loss: 0.85599, time: 0.01489\n",
      "Epoch: 191, train_loss: 0.85323, time: 0.01456\n",
      "Epoch: 192, train_loss: 0.85514, time: 0.01419\n",
      "Epoch: 193, train_loss: 0.85285, time: 0.01452\n",
      "Epoch: 194, train_loss: 0.85256, time: 0.01484\n",
      "Epoch: 195, train_loss: 0.85260, time: 0.01492\n",
      "Epoch: 196, train_loss: 0.85453, time: 0.01440\n",
      "Epoch: 197, train_loss: 0.85528, time: 0.01470\n",
      "Epoch: 198, train_loss: 0.85183, time: 0.01463\n",
      "Epoch: 199, train_loss: 0.85666, time: 0.01420\n",
      "Epoch: 200, train_loss: 0.85433, time: 0.01461\n",
      "pairwise precision 1.00000 recall 1.00000 f1 1.00000\n",
      "average until now [0.4522854185314301, 0.8307870902078713, 0.5857079537529436]\n",
      "8 names 99.75987815856934 avg time 12.469984769821167\n",
      "Loading chih_ming_lin dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 162 nodes, 1234 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.96461, time: 0.10874\n",
      "Epoch: 2, train_loss: 0.96126, time: 0.01149\n",
      "Epoch: 3, train_loss: 0.95628, time: 0.01067\n",
      "Epoch: 4, train_loss: 0.95544, time: 0.01034\n",
      "Epoch: 5, train_loss: 0.95366, time: 0.00987\n",
      "Epoch: 6, train_loss: 0.95192, time: 0.01012\n",
      "Epoch: 7, train_loss: 0.95339, time: 0.00988\n",
      "Epoch: 8, train_loss: 0.95300, time: 0.01022\n",
      "Epoch: 9, train_loss: 0.95597, time: 0.01008\n",
      "Epoch: 10, train_loss: 0.95287, time: 0.01033\n",
      "Epoch: 11, train_loss: 0.95068, time: 0.00989\n",
      "Epoch: 12, train_loss: 0.95831, time: 0.01011\n",
      "Epoch: 13, train_loss: 0.95670, time: 0.00976\n",
      "Epoch: 14, train_loss: 0.95163, time: 0.01011\n",
      "Epoch: 15, train_loss: 0.95440, time: 0.00998\n",
      "Epoch: 16, train_loss: 0.95171, time: 0.01016\n",
      "Epoch: 17, train_loss: 0.95381, time: 0.00983\n",
      "Epoch: 18, train_loss: 0.95658, time: 0.01023\n",
      "Epoch: 19, train_loss: 0.95328, time: 0.00983\n",
      "Epoch: 20, train_loss: 0.95357, time: 0.00990\n",
      "Epoch: 21, train_loss: 0.95189, time: 0.00965\n",
      "Epoch: 22, train_loss: 0.95258, time: 0.01022\n",
      "Epoch: 23, train_loss: 0.95542, time: 0.00968\n",
      "Epoch: 24, train_loss: 0.95416, time: 0.00996\n",
      "Epoch: 25, train_loss: 0.95536, time: 0.00978\n",
      "Epoch: 26, train_loss: 0.95334, time: 0.01026\n",
      "Epoch: 27, train_loss: 0.95469, time: 0.00986\n",
      "Epoch: 28, train_loss: 0.95696, time: 0.01032\n",
      "Epoch: 29, train_loss: 0.95708, time: 0.00997\n",
      "Epoch: 30, train_loss: 0.95283, time: 0.01013\n",
      "Epoch: 31, train_loss: 0.95657, time: 0.00975\n",
      "Epoch: 32, train_loss: 0.95636, time: 0.01018\n",
      "Epoch: 33, train_loss: 0.95626, time: 0.00969\n",
      "Epoch: 34, train_loss: 0.95377, time: 0.01012\n",
      "Epoch: 35, train_loss: 0.95248, time: 0.00977\n",
      "Epoch: 36, train_loss: 0.95180, time: 0.01023\n",
      "Epoch: 37, train_loss: 0.95304, time: 0.00980\n",
      "Epoch: 38, train_loss: 0.95320, time: 0.01025\n",
      "Epoch: 39, train_loss: 0.95673, time: 0.00983\n",
      "Epoch: 40, train_loss: 0.95024, time: 0.01024\n",
      "Epoch: 41, train_loss: 0.95093, time: 0.00975\n",
      "Epoch: 42, train_loss: 0.95297, time: 0.01015\n",
      "Epoch: 43, train_loss: 0.95611, time: 0.00991\n",
      "Epoch: 44, train_loss: 0.95484, time: 0.01052\n",
      "Epoch: 45, train_loss: 0.95337, time: 0.00995\n",
      "Epoch: 46, train_loss: 0.95288, time: 0.01025\n",
      "Epoch: 47, train_loss: 0.95081, time: 0.00989\n",
      "Epoch: 48, train_loss: 0.95017, time: 0.01020\n",
      "Epoch: 49, train_loss: 0.95419, time: 0.01002\n",
      "Epoch: 50, train_loss: 0.95123, time: 0.01021\n",
      "Epoch: 51, train_loss: 0.95173, time: 0.00987\n",
      "Epoch: 52, train_loss: 0.94893, time: 0.01014\n",
      "Epoch: 53, train_loss: 0.95346, time: 0.00982\n",
      "Epoch: 54, train_loss: 0.94858, time: 0.01019\n",
      "Epoch: 55, train_loss: 0.95584, time: 0.00970\n",
      "Epoch: 56, train_loss: 0.95071, time: 0.01019\n",
      "Epoch: 57, train_loss: 0.95202, time: 0.00972\n",
      "Epoch: 58, train_loss: 0.94999, time: 0.01033\n",
      "Epoch: 59, train_loss: 0.95549, time: 0.01004\n",
      "Epoch: 60, train_loss: 0.95049, time: 0.01025\n",
      "Epoch: 61, train_loss: 0.95087, time: 0.00976\n",
      "Epoch: 62, train_loss: 0.94825, time: 0.01012\n",
      "Epoch: 63, train_loss: 0.95344, time: 0.00970\n",
      "Epoch: 64, train_loss: 0.95586, time: 0.01025\n",
      "Epoch: 65, train_loss: 0.95508, time: 0.00975\n",
      "Epoch: 66, train_loss: 0.95622, time: 0.01031\n",
      "Epoch: 67, train_loss: 0.95513, time: 0.00981\n",
      "Epoch: 68, train_loss: 0.95219, time: 0.01022\n",
      "Epoch: 69, train_loss: 0.94975, time: 0.00972\n",
      "Epoch: 70, train_loss: 0.95093, time: 0.01020\n",
      "Epoch: 71, train_loss: 0.95370, time: 0.00979\n",
      "Epoch: 72, train_loss: 0.95512, time: 0.01031\n",
      "Epoch: 73, train_loss: 0.94911, time: 0.00980\n",
      "Epoch: 74, train_loss: 0.95181, time: 0.01020\n",
      "Epoch: 75, train_loss: 0.94823, time: 0.00987\n",
      "Epoch: 76, train_loss: 0.95277, time: 0.01017\n",
      "Epoch: 77, train_loss: 0.95302, time: 0.00990\n",
      "Epoch: 78, train_loss: 0.95279, time: 0.01020\n",
      "Epoch: 79, train_loss: 0.95566, time: 0.00980\n",
      "Epoch: 80, train_loss: 0.94900, time: 0.01014\n",
      "Epoch: 81, train_loss: 0.94955, time: 0.00984\n",
      "Epoch: 82, train_loss: 0.94918, time: 0.01026\n",
      "Epoch: 83, train_loss: 0.95279, time: 0.00991\n",
      "Epoch: 84, train_loss: 0.95245, time: 0.01025\n",
      "Epoch: 85, train_loss: 0.95151, time: 0.00969\n",
      "Epoch: 86, train_loss: 0.94934, time: 0.01015\n",
      "Epoch: 87, train_loss: 0.95508, time: 0.00978\n",
      "Epoch: 88, train_loss: 0.95196, time: 0.01026\n",
      "Epoch: 89, train_loss: 0.95066, time: 0.00974\n",
      "Epoch: 90, train_loss: 0.95189, time: 0.01017\n",
      "Epoch: 91, train_loss: 0.95333, time: 0.00980\n",
      "Epoch: 92, train_loss: 0.95276, time: 0.01010\n",
      "Epoch: 93, train_loss: 0.95352, time: 0.00976\n",
      "Epoch: 94, train_loss: 0.95409, time: 0.01026\n",
      "Epoch: 95, train_loss: 0.95076, time: 0.00971\n",
      "Epoch: 96, train_loss: 0.95249, time: 0.01031\n",
      "Epoch: 97, train_loss: 0.95196, time: 0.00965\n",
      "Epoch: 98, train_loss: 0.95151, time: 0.01013\n",
      "Epoch: 99, train_loss: 0.95445, time: 0.00958\n",
      "Epoch: 100, train_loss: 0.95537, time: 0.01026\n",
      "Epoch: 101, train_loss: 0.95348, time: 0.00977\n",
      "Epoch: 102, train_loss: 0.95230, time: 0.01021\n",
      "Epoch: 103, train_loss: 0.94864, time: 0.00969\n",
      "Epoch: 104, train_loss: 0.95238, time: 0.01036\n",
      "Epoch: 105, train_loss: 0.95140, time: 0.00987\n",
      "Epoch: 106, train_loss: 0.95475, time: 0.01020\n",
      "Epoch: 107, train_loss: 0.95205, time: 0.00984\n",
      "Epoch: 108, train_loss: 0.95063, time: 0.01038\n",
      "Epoch: 109, train_loss: 0.95220, time: 0.01008\n",
      "Epoch: 110, train_loss: 0.94696, time: 0.01028\n",
      "Epoch: 111, train_loss: 0.95355, time: 0.00978\n",
      "Epoch: 112, train_loss: 0.95403, time: 0.01021\n",
      "Epoch: 113, train_loss: 0.95322, time: 0.00991\n",
      "Epoch: 114, train_loss: 0.95466, time: 0.01041\n",
      "Epoch: 115, train_loss: 0.95326, time: 0.00998\n",
      "Epoch: 116, train_loss: 0.95402, time: 0.01019\n",
      "Epoch: 117, train_loss: 0.95044, time: 0.00986\n",
      "Epoch: 118, train_loss: 0.95652, time: 0.01030\n",
      "Epoch: 119, train_loss: 0.95685, time: 0.00982\n",
      "Epoch: 120, train_loss: 0.95185, time: 0.01025\n",
      "Epoch: 121, train_loss: 0.95217, time: 0.00986\n",
      "Epoch: 122, train_loss: 0.95105, time: 0.01025\n",
      "Epoch: 123, train_loss: 0.95214, time: 0.01014\n",
      "Epoch: 124, train_loss: 0.95270, time: 0.01001\n",
      "Epoch: 125, train_loss: 0.94906, time: 0.01026\n",
      "Epoch: 126, train_loss: 0.95525, time: 0.01000\n",
      "Epoch: 127, train_loss: 0.94936, time: 0.01020\n",
      "Epoch: 128, train_loss: 0.95481, time: 0.00981\n",
      "Epoch: 129, train_loss: 0.95392, time: 0.01012\n",
      "Epoch: 130, train_loss: 0.95272, time: 0.00984\n",
      "Epoch: 131, train_loss: 0.95377, time: 0.01027\n",
      "Epoch: 132, train_loss: 0.95210, time: 0.00970\n",
      "Epoch: 133, train_loss: 0.95352, time: 0.01015\n",
      "Epoch: 134, train_loss: 0.95069, time: 0.00972\n",
      "Epoch: 135, train_loss: 0.95090, time: 0.01007\n",
      "Epoch: 136, train_loss: 0.94903, time: 0.00972\n",
      "Epoch: 137, train_loss: 0.95225, time: 0.01020\n",
      "Epoch: 138, train_loss: 0.95458, time: 0.00968\n",
      "Epoch: 139, train_loss: 0.95472, time: 0.01017\n",
      "Epoch: 140, train_loss: 0.95236, time: 0.00973\n",
      "Epoch: 141, train_loss: 0.95162, time: 0.01023\n",
      "Epoch: 142, train_loss: 0.95305, time: 0.00977\n",
      "Epoch: 143, train_loss: 0.94977, time: 0.01015\n",
      "Epoch: 144, train_loss: 0.95425, time: 0.01007\n",
      "Epoch: 145, train_loss: 0.95170, time: 0.01033\n",
      "Epoch: 146, train_loss: 0.95023, time: 0.00973\n",
      "Epoch: 147, train_loss: 0.94913, time: 0.00982\n",
      "Epoch: 148, train_loss: 0.95459, time: 0.01037\n",
      "Epoch: 149, train_loss: 0.95269, time: 0.00978\n",
      "Epoch: 150, train_loss: 0.94914, time: 0.01019\n",
      "Epoch: 151, train_loss: 0.95172, time: 0.00981\n",
      "Epoch: 152, train_loss: 0.95523, time: 0.01001\n",
      "Epoch: 153, train_loss: 0.95195, time: 0.00960\n",
      "Epoch: 154, train_loss: 0.95717, time: 0.01016\n",
      "Epoch: 155, train_loss: 0.94667, time: 0.00987\n",
      "Epoch: 156, train_loss: 0.95328, time: 0.01008\n",
      "Epoch: 157, train_loss: 0.95217, time: 0.00975\n",
      "Epoch: 158, train_loss: 0.95205, time: 0.00968\n",
      "Epoch: 159, train_loss: 0.95107, time: 0.00993\n",
      "Epoch: 160, train_loss: 0.95087, time: 0.01023\n",
      "Epoch: 161, train_loss: 0.95114, time: 0.00990\n",
      "Epoch: 162, train_loss: 0.95232, time: 0.01015\n",
      "Epoch: 163, train_loss: 0.95566, time: 0.00979\n",
      "Epoch: 164, train_loss: 0.95096, time: 0.01026\n",
      "Epoch: 165, train_loss: 0.95241, time: 0.00967\n",
      "Epoch: 166, train_loss: 0.95617, time: 0.01035\n",
      "Epoch: 167, train_loss: 0.94890, time: 0.00987\n",
      "Epoch: 168, train_loss: 0.95093, time: 0.01018\n",
      "Epoch: 169, train_loss: 0.95202, time: 0.00986\n",
      "Epoch: 170, train_loss: 0.95373, time: 0.01023\n",
      "Epoch: 171, train_loss: 0.95327, time: 0.00970\n",
      "Epoch: 172, train_loss: 0.95296, time: 0.01008\n",
      "Epoch: 173, train_loss: 0.95685, time: 0.00972\n",
      "Epoch: 174, train_loss: 0.95059, time: 0.01020\n",
      "Epoch: 175, train_loss: 0.95112, time: 0.00991\n",
      "Epoch: 176, train_loss: 0.94949, time: 0.01014\n",
      "Epoch: 177, train_loss: 0.94936, time: 0.01014\n",
      "Epoch: 178, train_loss: 0.95788, time: 0.01003\n",
      "Epoch: 179, train_loss: 0.94796, time: 0.00998\n",
      "Epoch: 180, train_loss: 0.95568, time: 0.01018\n",
      "Epoch: 181, train_loss: 0.94976, time: 0.00976\n",
      "Epoch: 182, train_loss: 0.94862, time: 0.01009\n",
      "Epoch: 183, train_loss: 0.95158, time: 0.00978\n",
      "Epoch: 184, train_loss: 0.95190, time: 0.01003\n",
      "Epoch: 185, train_loss: 0.95489, time: 0.01016\n",
      "Epoch: 186, train_loss: 0.95889, time: 0.01017\n",
      "Epoch: 187, train_loss: 0.94955, time: 0.01012\n",
      "Epoch: 188, train_loss: 0.95153, time: 0.01023\n",
      "Epoch: 189, train_loss: 0.94753, time: 0.00984\n",
      "Epoch: 190, train_loss: 0.95599, time: 0.01017\n",
      "Epoch: 191, train_loss: 0.95271, time: 0.00981\n",
      "Epoch: 192, train_loss: 0.95027, time: 0.01019\n",
      "Epoch: 193, train_loss: 0.95283, time: 0.00995\n",
      "Epoch: 194, train_loss: 0.94947, time: 0.01019\n",
      "Epoch: 195, train_loss: 0.95554, time: 0.00965\n",
      "Epoch: 196, train_loss: 0.95029, time: 0.01012\n",
      "Epoch: 197, train_loss: 0.95327, time: 0.00968\n",
      "Epoch: 198, train_loss: 0.95175, time: 0.00996\n",
      "Epoch: 199, train_loss: 0.95017, time: 0.01009\n",
      "Epoch: 200, train_loss: 0.95030, time: 0.00975\n",
      "pairwise precision 0.32807 recall 0.90941 f1 0.48219\n",
      "average until now [0.4384837744933011, 0.8395231651520948, 0.5760802618686777]\n",
      "9 names 101.9026415348053 avg time 11.322515726089478\n",
      "Loading wensheng_yang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 276 nodes, 5532 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.92651, time: 0.11514\n",
      "Epoch: 2, train_loss: 0.92850, time: 0.01615\n",
      "Epoch: 3, train_loss: 0.92735, time: 0.01478\n",
      "Epoch: 4, train_loss: 0.92698, time: 0.01526\n",
      "Epoch: 5, train_loss: 0.92551, time: 0.01527\n",
      "Epoch: 6, train_loss: 0.92617, time: 0.01437\n",
      "Epoch: 7, train_loss: 0.92562, time: 0.01431\n",
      "Epoch: 8, train_loss: 0.92717, time: 0.01399\n",
      "Epoch: 9, train_loss: 0.92750, time: 0.01379\n",
      "Epoch: 10, train_loss: 0.92524, time: 0.01422\n",
      "Epoch: 11, train_loss: 0.92526, time: 0.01461\n",
      "Epoch: 12, train_loss: 0.92578, time: 0.01443\n",
      "Epoch: 13, train_loss: 0.92650, time: 0.01483\n",
      "Epoch: 14, train_loss: 0.92831, time: 0.01469\n",
      "Epoch: 15, train_loss: 0.92604, time: 0.01480\n",
      "Epoch: 16, train_loss: 0.92692, time: 0.01486\n",
      "Epoch: 17, train_loss: 0.92548, time: 0.01455\n",
      "Epoch: 18, train_loss: 0.92740, time: 0.01454\n",
      "Epoch: 19, train_loss: 0.92659, time: 0.01444\n",
      "Epoch: 20, train_loss: 0.92554, time: 0.01434\n",
      "Epoch: 21, train_loss: 0.92625, time: 0.01494\n",
      "Epoch: 22, train_loss: 0.92293, time: 0.01447\n",
      "Epoch: 23, train_loss: 0.92515, time: 0.01491\n",
      "Epoch: 24, train_loss: 0.92705, time: 0.01452\n",
      "Epoch: 25, train_loss: 0.92552, time: 0.01487\n",
      "Epoch: 26, train_loss: 0.92800, time: 0.01483\n",
      "Epoch: 27, train_loss: 0.92724, time: 0.01449\n",
      "Epoch: 28, train_loss: 0.92625, time: 0.01468\n",
      "Epoch: 29, train_loss: 0.92728, time: 0.01501\n",
      "Epoch: 30, train_loss: 0.92514, time: 0.01465\n",
      "Epoch: 31, train_loss: 0.92796, time: 0.01462\n",
      "Epoch: 32, train_loss: 0.92781, time: 0.01471\n",
      "Epoch: 33, train_loss: 0.92528, time: 0.01435\n",
      "Epoch: 34, train_loss: 0.92517, time: 0.01448\n",
      "Epoch: 35, train_loss: 0.92442, time: 0.01455\n",
      "Epoch: 36, train_loss: 0.92643, time: 0.01422\n",
      "Epoch: 37, train_loss: 0.92743, time: 0.01487\n",
      "Epoch: 38, train_loss: 0.92466, time: 0.01467\n",
      "Epoch: 39, train_loss: 0.92799, time: 0.01451\n",
      "Epoch: 40, train_loss: 0.92887, time: 0.01444\n",
      "Epoch: 41, train_loss: 0.92550, time: 0.01457\n",
      "Epoch: 42, train_loss: 0.92693, time: 0.01464\n",
      "Epoch: 43, train_loss: 0.92584, time: 0.01452\n",
      "Epoch: 44, train_loss: 0.92569, time: 0.01464\n",
      "Epoch: 45, train_loss: 0.92375, time: 0.01451\n",
      "Epoch: 46, train_loss: 0.92409, time: 0.01499\n",
      "Epoch: 47, train_loss: 0.92391, time: 0.01468\n",
      "Epoch: 48, train_loss: 0.92562, time: 0.01446\n",
      "Epoch: 49, train_loss: 0.92637, time: 0.01468\n",
      "Epoch: 50, train_loss: 0.92637, time: 0.01450\n",
      "Epoch: 51, train_loss: 0.92689, time: 0.01416\n",
      "Epoch: 52, train_loss: 0.92346, time: 0.01462\n",
      "Epoch: 53, train_loss: 0.92642, time: 0.01466\n",
      "Epoch: 54, train_loss: 0.92494, time: 0.01480\n",
      "Epoch: 55, train_loss: 0.92335, time: 0.01426\n",
      "Epoch: 56, train_loss: 0.92750, time: 0.01472\n",
      "Epoch: 57, train_loss: 0.92464, time: 0.01477\n",
      "Epoch: 58, train_loss: 0.92492, time: 0.01441\n",
      "Epoch: 59, train_loss: 0.91963, time: 0.01467\n",
      "Epoch: 60, train_loss: 0.92351, time: 0.01489\n",
      "Epoch: 61, train_loss: 0.92127, time: 0.01455\n",
      "Epoch: 62, train_loss: 0.91635, time: 0.01440\n",
      "Epoch: 63, train_loss: 0.92464, time: 0.01441\n",
      "Epoch: 64, train_loss: 0.92144, time: 0.01427\n",
      "Epoch: 65, train_loss: 0.92252, time: 0.01471\n",
      "Epoch: 66, train_loss: 0.92248, time: 0.01453\n",
      "Epoch: 67, train_loss: 0.92218, time: 0.01455\n",
      "Epoch: 68, train_loss: 0.92451, time: 0.01454\n",
      "Epoch: 69, train_loss: 0.92422, time: 0.01480\n",
      "Epoch: 70, train_loss: 0.92338, time: 0.01435\n",
      "Epoch: 71, train_loss: 0.92746, time: 0.01431\n",
      "Epoch: 72, train_loss: 0.92195, time: 0.01435\n",
      "Epoch: 73, train_loss: 0.92477, time: 0.01435\n",
      "Epoch: 74, train_loss: 0.91895, time: 0.01457\n",
      "Epoch: 75, train_loss: 0.92618, time: 0.01464\n",
      "Epoch: 76, train_loss: 0.92143, time: 0.01453\n",
      "Epoch: 77, train_loss: 0.92099, time: 0.01488\n",
      "Epoch: 78, train_loss: 0.92590, time: 0.01427\n",
      "Epoch: 79, train_loss: 0.92047, time: 0.01447\n",
      "Epoch: 80, train_loss: 0.92534, time: 0.01451\n",
      "Epoch: 81, train_loss: 0.92342, time: 0.01475\n",
      "Epoch: 82, train_loss: 0.92463, time: 0.01470\n",
      "Epoch: 83, train_loss: 0.92374, time: 0.01456\n",
      "Epoch: 84, train_loss: 0.92290, time: 0.01493\n",
      "Epoch: 85, train_loss: 0.92303, time: 0.01453\n",
      "Epoch: 86, train_loss: 0.92527, time: 0.01456\n",
      "Epoch: 87, train_loss: 0.92364, time: 0.01458\n",
      "Epoch: 88, train_loss: 0.92106, time: 0.01423\n",
      "Epoch: 89, train_loss: 0.92615, time: 0.01428\n",
      "Epoch: 90, train_loss: 0.92080, time: 0.01435\n",
      "Epoch: 91, train_loss: 0.91903, time: 0.01445\n",
      "Epoch: 92, train_loss: 0.92585, time: 0.01475\n",
      "Epoch: 93, train_loss: 0.92441, time: 0.01438\n",
      "Epoch: 94, train_loss: 0.92034, time: 0.01462\n",
      "Epoch: 95, train_loss: 0.92370, time: 0.01442\n",
      "Epoch: 96, train_loss: 0.91818, time: 0.01483\n",
      "Epoch: 97, train_loss: 0.92066, time: 0.01460\n",
      "Epoch: 98, train_loss: 0.91815, time: 0.01469\n",
      "Epoch: 99, train_loss: 0.92295, time: 0.01463\n",
      "Epoch: 100, train_loss: 0.92570, time: 0.01451\n",
      "Epoch: 101, train_loss: 0.92251, time: 0.01469\n",
      "Epoch: 102, train_loss: 0.92146, time: 0.01541\n",
      "Epoch: 103, train_loss: 0.92374, time: 0.01468\n",
      "Epoch: 104, train_loss: 0.91740, time: 0.01471\n",
      "Epoch: 105, train_loss: 0.92231, time: 0.01449\n",
      "Epoch: 106, train_loss: 0.92080, time: 0.01481\n",
      "Epoch: 107, train_loss: 0.92332, time: 0.01452\n",
      "Epoch: 108, train_loss: 0.91690, time: 0.01414\n",
      "Epoch: 109, train_loss: 0.91805, time: 0.01457\n",
      "Epoch: 110, train_loss: 0.92238, time: 0.01455\n",
      "Epoch: 111, train_loss: 0.91878, time: 0.01459\n",
      "Epoch: 112, train_loss: 0.92199, time: 0.01469\n",
      "Epoch: 113, train_loss: 0.92461, time: 0.01489\n",
      "Epoch: 114, train_loss: 0.91793, time: 0.01496\n",
      "Epoch: 115, train_loss: 0.92650, time: 0.01496\n",
      "Epoch: 116, train_loss: 0.92663, time: 0.01449\n",
      "Epoch: 117, train_loss: 0.92302, time: 0.01474\n",
      "Epoch: 118, train_loss: 0.92290, time: 0.01454\n",
      "Epoch: 119, train_loss: 0.92718, time: 0.01431\n",
      "Epoch: 120, train_loss: 0.92095, time: 0.01457\n",
      "Epoch: 121, train_loss: 0.92417, time: 0.01439\n",
      "Epoch: 122, train_loss: 0.92693, time: 0.01455\n",
      "Epoch: 123, train_loss: 0.92029, time: 0.01452\n",
      "Epoch: 124, train_loss: 0.92133, time: 0.01431\n",
      "Epoch: 125, train_loss: 0.92432, time: 0.01446\n",
      "Epoch: 126, train_loss: 0.92940, time: 0.01475\n",
      "Epoch: 127, train_loss: 0.92596, time: 0.01502\n",
      "Epoch: 128, train_loss: 0.91452, time: 0.01437\n",
      "Epoch: 129, train_loss: 0.92298, time: 0.01475\n",
      "Epoch: 130, train_loss: 0.92007, time: 0.01448\n",
      "Epoch: 131, train_loss: 0.92591, time: 0.01479\n",
      "Epoch: 132, train_loss: 0.92626, time: 0.01439\n",
      "Epoch: 133, train_loss: 0.92172, time: 0.01439\n",
      "Epoch: 134, train_loss: 0.92080, time: 0.01448\n",
      "Epoch: 135, train_loss: 0.92313, time: 0.01495\n",
      "Epoch: 136, train_loss: 0.92302, time: 0.01428\n",
      "Epoch: 137, train_loss: 0.92064, time: 0.01452\n",
      "Epoch: 138, train_loss: 0.92120, time: 0.01454\n",
      "Epoch: 139, train_loss: 0.92127, time: 0.01497\n",
      "Epoch: 140, train_loss: 0.91962, time: 0.01459\n",
      "Epoch: 141, train_loss: 0.92030, time: 0.01484\n",
      "Epoch: 142, train_loss: 0.91748, time: 0.01492\n",
      "Epoch: 143, train_loss: 0.92331, time: 0.01466\n",
      "Epoch: 144, train_loss: 0.91836, time: 0.01441\n",
      "Epoch: 145, train_loss: 0.92491, time: 0.01425\n",
      "Epoch: 146, train_loss: 0.91810, time: 0.01371\n",
      "Epoch: 147, train_loss: 0.92095, time: 0.01407\n",
      "Epoch: 148, train_loss: 0.92488, time: 0.01457\n",
      "Epoch: 149, train_loss: 0.91853, time: 0.01442\n",
      "Epoch: 150, train_loss: 0.91792, time: 0.01409\n",
      "Epoch: 151, train_loss: 0.92468, time: 0.01456\n",
      "Epoch: 152, train_loss: 0.92530, time: 0.01490\n",
      "Epoch: 153, train_loss: 0.92680, time: 0.01438\n",
      "Epoch: 154, train_loss: 0.91953, time: 0.01434\n",
      "Epoch: 155, train_loss: 0.91283, time: 0.01438\n",
      "Epoch: 156, train_loss: 0.92374, time: 0.01468\n",
      "Epoch: 157, train_loss: 0.91909, time: 0.01467\n",
      "Epoch: 158, train_loss: 0.91855, time: 0.01454\n",
      "Epoch: 159, train_loss: 0.92191, time: 0.01451\n",
      "Epoch: 160, train_loss: 0.92487, time: 0.01450\n",
      "Epoch: 161, train_loss: 0.92308, time: 0.01424\n",
      "Epoch: 162, train_loss: 0.92013, time: 0.01470\n",
      "Epoch: 163, train_loss: 0.92176, time: 0.01424\n",
      "Epoch: 164, train_loss: 0.92303, time: 0.01428\n",
      "Epoch: 165, train_loss: 0.91798, time: 0.01452\n",
      "Epoch: 166, train_loss: 0.91770, time: 0.01426\n",
      "Epoch: 167, train_loss: 0.91906, time: 0.01476\n",
      "Epoch: 168, train_loss: 0.92790, time: 0.01471\n",
      "Epoch: 169, train_loss: 0.91743, time: 0.01450\n",
      "Epoch: 170, train_loss: 0.92598, time: 0.01461\n",
      "Epoch: 171, train_loss: 0.92598, time: 0.01454\n",
      "Epoch: 172, train_loss: 0.91825, time: 0.01475\n",
      "Epoch: 173, train_loss: 0.91788, time: 0.01431\n",
      "Epoch: 174, train_loss: 0.91831, time: 0.01421\n",
      "Epoch: 175, train_loss: 0.92215, time: 0.01481\n",
      "Epoch: 176, train_loss: 0.92058, time: 0.01419\n",
      "Epoch: 177, train_loss: 0.92402, time: 0.01455\n",
      "Epoch: 178, train_loss: 0.91731, time: 0.01400\n",
      "Epoch: 179, train_loss: 0.92319, time: 0.01361\n",
      "Epoch: 180, train_loss: 0.92149, time: 0.01425\n",
      "Epoch: 181, train_loss: 0.91922, time: 0.01468\n",
      "Epoch: 182, train_loss: 0.91806, time: 0.01428\n",
      "Epoch: 183, train_loss: 0.92077, time: 0.01452\n",
      "Epoch: 184, train_loss: 0.91525, time: 0.01465\n",
      "Epoch: 185, train_loss: 0.92592, time: 0.01459\n",
      "Epoch: 186, train_loss: 0.91917, time: 0.01455\n",
      "Epoch: 187, train_loss: 0.92074, time: 0.01473\n",
      "Epoch: 188, train_loss: 0.92749, time: 0.01464\n",
      "Epoch: 189, train_loss: 0.91951, time: 0.01468\n",
      "Epoch: 190, train_loss: 0.92737, time: 0.01450\n",
      "Epoch: 191, train_loss: 0.92424, time: 0.01494\n",
      "Epoch: 192, train_loss: 0.92306, time: 0.01424\n",
      "Epoch: 193, train_loss: 0.92817, time: 0.01417\n",
      "Epoch: 194, train_loss: 0.92545, time: 0.01427\n",
      "Epoch: 195, train_loss: 0.91977, time: 0.01445\n",
      "Epoch: 196, train_loss: 0.92510, time: 0.01474\n",
      "Epoch: 197, train_loss: 0.93077, time: 0.01492\n",
      "Epoch: 198, train_loss: 0.92275, time: 0.01457\n",
      "Epoch: 199, train_loss: 0.92136, time: 0.01464\n",
      "Epoch: 200, train_loss: 0.92176, time: 0.01456\n",
      "pairwise precision 0.88652 recall 0.94977 f1 0.91706\n",
      "average until now [0.48328766199468853, 0.8505481425034936, 0.6163568586451135]\n",
      "10 names 104.98451042175293 avg time 10.498451042175294\n",
      "Loading fosong_wang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 356 nodes, 21104 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.83274, time: 0.13002\n",
      "Epoch: 2, train_loss: 0.83152, time: 0.03014\n",
      "Epoch: 3, train_loss: 0.83193, time: 0.03105\n",
      "Epoch: 4, train_loss: 0.83099, time: 0.03052\n",
      "Epoch: 5, train_loss: 0.83301, time: 0.03041\n",
      "Epoch: 6, train_loss: 0.83056, time: 0.02919\n",
      "Epoch: 7, train_loss: 0.83086, time: 0.02850\n",
      "Epoch: 8, train_loss: 0.83209, time: 0.02893\n",
      "Epoch: 9, train_loss: 0.83246, time: 0.02885\n",
      "Epoch: 10, train_loss: 0.82936, time: 0.02909\n",
      "Epoch: 11, train_loss: 0.83318, time: 0.03041\n",
      "Epoch: 12, train_loss: 0.83266, time: 0.03000\n",
      "Epoch: 13, train_loss: 0.83096, time: 0.02940\n",
      "Epoch: 14, train_loss: 0.82993, time: 0.02852\n",
      "Epoch: 15, train_loss: 0.83118, time: 0.02917\n",
      "Epoch: 16, train_loss: 0.82971, time: 0.02944\n",
      "Epoch: 17, train_loss: 0.83044, time: 0.02908\n",
      "Epoch: 18, train_loss: 0.83294, time: 0.02970\n",
      "Epoch: 19, train_loss: 0.82718, time: 0.03062\n",
      "Epoch: 20, train_loss: 0.83336, time: 0.03041\n",
      "Epoch: 21, train_loss: 0.82908, time: 0.03033\n",
      "Epoch: 22, train_loss: 0.82579, time: 0.03037\n",
      "Epoch: 23, train_loss: 0.82749, time: 0.03004\n",
      "Epoch: 24, train_loss: 0.82668, time: 0.02948\n",
      "Epoch: 25, train_loss: 0.82877, time: 0.02967\n",
      "Epoch: 26, train_loss: 0.82743, time: 0.03093\n",
      "Epoch: 27, train_loss: 0.82377, time: 0.02915\n",
      "Epoch: 28, train_loss: 0.81827, time: 0.02947\n",
      "Epoch: 29, train_loss: 0.81598, time: 0.03018\n",
      "Epoch: 30, train_loss: 0.82315, time: 0.02938\n",
      "Epoch: 31, train_loss: 0.81230, time: 0.02803\n",
      "Epoch: 32, train_loss: 0.82256, time: 0.02987\n",
      "Epoch: 33, train_loss: 0.82111, time: 0.02971\n",
      "Epoch: 34, train_loss: 0.81840, time: 0.02789\n",
      "Epoch: 35, train_loss: 0.81627, time: 0.02962\n",
      "Epoch: 36, train_loss: 0.82030, time: 0.02977\n",
      "Epoch: 37, train_loss: 0.80977, time: 0.02937\n",
      "Epoch: 38, train_loss: 0.81535, time: 0.02815\n",
      "Epoch: 39, train_loss: 0.81897, time: 0.02887\n",
      "Epoch: 40, train_loss: 0.81435, time: 0.02923\n",
      "Epoch: 41, train_loss: 0.81621, time: 0.02948\n",
      "Epoch: 42, train_loss: 0.81436, time: 0.02908\n",
      "Epoch: 43, train_loss: 0.81356, time: 0.03072\n",
      "Epoch: 44, train_loss: 0.81622, time: 0.02911\n",
      "Epoch: 45, train_loss: 0.82056, time: 0.02877\n",
      "Epoch: 46, train_loss: 0.82309, time: 0.02879\n",
      "Epoch: 47, train_loss: 0.81874, time: 0.03121\n",
      "Epoch: 48, train_loss: 0.80563, time: 0.03010\n",
      "Epoch: 49, train_loss: 0.82211, time: 0.03031\n",
      "Epoch: 50, train_loss: 0.81201, time: 0.03040\n",
      "Epoch: 51, train_loss: 0.81482, time: 0.03025\n",
      "Epoch: 52, train_loss: 0.81956, time: 0.03003\n",
      "Epoch: 53, train_loss: 0.81412, time: 0.02992\n",
      "Epoch: 54, train_loss: 0.81323, time: 0.03001\n",
      "Epoch: 55, train_loss: 0.83085, time: 0.03028\n",
      "Epoch: 56, train_loss: 0.81070, time: 0.03162\n",
      "Epoch: 57, train_loss: 0.82263, time: 0.03074\n",
      "Epoch: 58, train_loss: 0.82474, time: 0.03121\n",
      "Epoch: 59, train_loss: 0.81038, time: 0.03046\n",
      "Epoch: 60, train_loss: 0.81056, time: 0.03092\n",
      "Epoch: 61, train_loss: 0.81669, time: 0.02989\n",
      "Epoch: 62, train_loss: 0.81306, time: 0.03061\n",
      "Epoch: 63, train_loss: 0.81012, time: 0.03054\n",
      "Epoch: 64, train_loss: 0.81076, time: 0.02825\n",
      "Epoch: 65, train_loss: 0.81756, time: 0.03045\n",
      "Epoch: 66, train_loss: 0.80839, time: 0.02956\n",
      "Epoch: 67, train_loss: 0.81818, time: 0.02864\n",
      "Epoch: 68, train_loss: 0.81310, time: 0.03028\n",
      "Epoch: 69, train_loss: 0.81574, time: 0.02944\n",
      "Epoch: 70, train_loss: 0.81645, time: 0.03134\n",
      "Epoch: 71, train_loss: 0.81142, time: 0.03152\n",
      "Epoch: 72, train_loss: 0.81282, time: 0.03080\n",
      "Epoch: 73, train_loss: 0.81207, time: 0.02926\n",
      "Epoch: 74, train_loss: 0.80733, time: 0.03038\n",
      "Epoch: 75, train_loss: 0.81061, time: 0.02982\n",
      "Epoch: 76, train_loss: 0.82175, time: 0.02989\n",
      "Epoch: 77, train_loss: 0.80489, time: 0.02973\n",
      "Epoch: 78, train_loss: 0.81789, time: 0.03007\n",
      "Epoch: 79, train_loss: 0.80949, time: 0.03020\n",
      "Epoch: 80, train_loss: 0.81729, time: 0.03160\n",
      "Epoch: 81, train_loss: 0.81015, time: 0.02990\n",
      "Epoch: 82, train_loss: 0.81685, time: 0.03018\n",
      "Epoch: 83, train_loss: 0.80500, time: 0.03031\n",
      "Epoch: 84, train_loss: 0.81297, time: 0.02969\n",
      "Epoch: 85, train_loss: 0.82157, time: 0.02917\n",
      "Epoch: 86, train_loss: 0.82656, time: 0.03081\n",
      "Epoch: 87, train_loss: 0.80936, time: 0.03028\n",
      "Epoch: 88, train_loss: 0.81211, time: 0.02840\n",
      "Epoch: 89, train_loss: 0.82057, time: 0.03006\n",
      "Epoch: 90, train_loss: 0.81245, time: 0.03008\n",
      "Epoch: 91, train_loss: 0.80940, time: 0.02852\n",
      "Epoch: 92, train_loss: 0.80049, time: 0.02916\n",
      "Epoch: 93, train_loss: 0.81280, time: 0.03017\n",
      "Epoch: 94, train_loss: 0.81055, time: 0.03118\n",
      "Epoch: 95, train_loss: 0.80713, time: 0.03038\n",
      "Epoch: 96, train_loss: 0.80877, time: 0.03011\n",
      "Epoch: 97, train_loss: 0.80935, time: 0.03130\n",
      "Epoch: 98, train_loss: 0.81800, time: 0.02996\n",
      "Epoch: 99, train_loss: 0.81730, time: 0.02956\n",
      "Epoch: 100, train_loss: 0.81456, time: 0.03090\n",
      "Epoch: 101, train_loss: 0.81706, time: 0.03086\n",
      "Epoch: 102, train_loss: 0.80956, time: 0.02836\n",
      "Epoch: 103, train_loss: 0.81365, time: 0.02812\n",
      "Epoch: 104, train_loss: 0.80472, time: 0.02867\n",
      "Epoch: 105, train_loss: 0.81031, time: 0.02853\n",
      "Epoch: 106, train_loss: 0.81672, time: 0.03055\n",
      "Epoch: 107, train_loss: 0.81985, time: 0.03101\n",
      "Epoch: 108, train_loss: 0.81196, time: 0.03035\n",
      "Epoch: 109, train_loss: 0.81658, time: 0.02901\n",
      "Epoch: 110, train_loss: 0.80655, time: 0.03003\n",
      "Epoch: 111, train_loss: 0.80620, time: 0.02976\n",
      "Epoch: 112, train_loss: 0.80525, time: 0.02982\n",
      "Epoch: 113, train_loss: 0.81144, time: 0.02979\n",
      "Epoch: 114, train_loss: 0.81055, time: 0.02843\n",
      "Epoch: 115, train_loss: 0.81634, time: 0.02965\n",
      "Epoch: 116, train_loss: 0.81029, time: 0.03047\n",
      "Epoch: 117, train_loss: 0.81591, time: 0.03109\n",
      "Epoch: 118, train_loss: 0.81740, time: 0.02992\n",
      "Epoch: 119, train_loss: 0.81419, time: 0.02945\n",
      "Epoch: 120, train_loss: 0.80575, time: 0.03011\n",
      "Epoch: 121, train_loss: 0.81426, time: 0.02876\n",
      "Epoch: 122, train_loss: 0.80617, time: 0.02873\n",
      "Epoch: 123, train_loss: 0.81194, time: 0.02839\n",
      "Epoch: 124, train_loss: 0.82070, time: 0.02952\n",
      "Epoch: 125, train_loss: 0.81542, time: 0.03039\n",
      "Epoch: 126, train_loss: 0.81881, time: 0.03037\n",
      "Epoch: 127, train_loss: 0.80981, time: 0.03049\n",
      "Epoch: 128, train_loss: 0.80551, time: 0.03011\n",
      "Epoch: 129, train_loss: 0.80954, time: 0.03075\n",
      "Epoch: 130, train_loss: 0.81366, time: 0.03050\n",
      "Epoch: 131, train_loss: 0.81422, time: 0.02786\n",
      "Epoch: 132, train_loss: 0.81124, time: 0.02836\n",
      "Epoch: 133, train_loss: 0.81084, time: 0.03045\n",
      "Epoch: 134, train_loss: 0.82378, time: 0.03022\n",
      "Epoch: 135, train_loss: 0.80932, time: 0.02856\n",
      "Epoch: 136, train_loss: 0.81640, time: 0.02951\n",
      "Epoch: 137, train_loss: 0.81927, time: 0.03037\n",
      "Epoch: 138, train_loss: 0.81994, time: 0.03118\n",
      "Epoch: 139, train_loss: 0.80679, time: 0.03178\n",
      "Epoch: 140, train_loss: 0.81237, time: 0.03055\n",
      "Epoch: 141, train_loss: 0.81506, time: 0.03004\n",
      "Epoch: 142, train_loss: 0.81122, time: 0.03027\n",
      "Epoch: 143, train_loss: 0.81426, time: 0.03049\n",
      "Epoch: 144, train_loss: 0.80312, time: 0.02985\n",
      "Epoch: 145, train_loss: 0.80157, time: 0.03073\n",
      "Epoch: 146, train_loss: 0.81432, time: 0.03067\n",
      "Epoch: 147, train_loss: 0.81322, time: 0.02874\n",
      "Epoch: 148, train_loss: 0.81419, time: 0.03074\n",
      "Epoch: 149, train_loss: 0.81391, time: 0.02798\n",
      "Epoch: 150, train_loss: 0.81186, time: 0.02961\n",
      "Epoch: 151, train_loss: 0.81717, time: 0.03059\n",
      "Epoch: 152, train_loss: 0.81804, time: 0.02921\n",
      "Epoch: 153, train_loss: 0.80624, time: 0.02943\n",
      "Epoch: 154, train_loss: 0.82857, time: 0.03042\n",
      "Epoch: 155, train_loss: 0.81521, time: 0.02913\n",
      "Epoch: 156, train_loss: 0.81375, time: 0.02906\n",
      "Epoch: 157, train_loss: 0.81988, time: 0.02837\n",
      "Epoch: 158, train_loss: 0.81596, time: 0.02875\n",
      "Epoch: 159, train_loss: 0.81810, time: 0.02878\n",
      "Epoch: 160, train_loss: 0.81621, time: 0.03040\n",
      "Epoch: 161, train_loss: 0.81442, time: 0.03092\n",
      "Epoch: 162, train_loss: 0.80457, time: 0.02987\n",
      "Epoch: 163, train_loss: 0.81466, time: 0.02918\n",
      "Epoch: 164, train_loss: 0.81711, time: 0.02889\n",
      "Epoch: 165, train_loss: 0.80872, time: 0.02885\n",
      "Epoch: 166, train_loss: 0.81477, time: 0.02868\n",
      "Epoch: 167, train_loss: 0.81200, time: 0.03075\n",
      "Epoch: 168, train_loss: 0.81102, time: 0.03187\n",
      "Epoch: 169, train_loss: 0.80935, time: 0.03010\n",
      "Epoch: 170, train_loss: 0.81880, time: 0.02990\n",
      "Epoch: 171, train_loss: 0.80852, time: 0.03013\n",
      "Epoch: 172, train_loss: 0.81388, time: 0.03022\n",
      "Epoch: 173, train_loss: 0.80249, time: 0.02969\n",
      "Epoch: 174, train_loss: 0.81231, time: 0.03004\n",
      "Epoch: 175, train_loss: 0.81171, time: 0.03062\n",
      "Epoch: 176, train_loss: 0.81482, time: 0.02867\n",
      "Epoch: 177, train_loss: 0.80480, time: 0.02935\n",
      "Epoch: 178, train_loss: 0.81470, time: 0.02985\n",
      "Epoch: 179, train_loss: 0.81742, time: 0.02960\n",
      "Epoch: 180, train_loss: 0.80754, time: 0.02942\n",
      "Epoch: 181, train_loss: 0.81718, time: 0.02846\n",
      "Epoch: 182, train_loss: 0.81286, time: 0.03001\n",
      "Epoch: 183, train_loss: 0.82525, time: 0.03099\n",
      "Epoch: 184, train_loss: 0.81052, time: 0.03120\n",
      "Epoch: 185, train_loss: 0.80531, time: 0.03065\n",
      "Epoch: 186, train_loss: 0.81074, time: 0.03068\n",
      "Epoch: 187, train_loss: 0.80465, time: 0.03148\n",
      "Epoch: 188, train_loss: 0.80825, time: 0.02998\n",
      "Epoch: 189, train_loss: 0.80496, time: 0.02884\n",
      "Epoch: 190, train_loss: 0.81049, time: 0.02924\n",
      "Epoch: 191, train_loss: 0.82008, time: 0.02857\n",
      "Epoch: 192, train_loss: 0.82221, time: 0.02872\n",
      "Epoch: 193, train_loss: 0.81691, time: 0.02907\n",
      "Epoch: 194, train_loss: 0.80798, time: 0.02827\n",
      "Epoch: 195, train_loss: 0.80890, time: 0.03049\n",
      "Epoch: 196, train_loss: 0.81694, time: 0.02925\n",
      "Epoch: 197, train_loss: 0.81083, time: 0.02931\n",
      "Epoch: 198, train_loss: 0.81399, time: 0.03067\n",
      "Epoch: 199, train_loss: 0.81902, time: 0.02910\n",
      "Epoch: 200, train_loss: 0.80592, time: 0.02785\n",
      "pairwise precision 1.00000 recall 1.00000 f1 1.00000\n",
      "average until now [0.5302615109042623, 0.8641346750031761, 0.6572269244895437]\n",
      "11 names 111.16321063041687 avg time 10.105746420946987\n",
      "Loading guorong_chen dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 335 nodes, 12151 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.89025, time: 0.12138\n",
      "Epoch: 2, train_loss: 0.89237, time: 0.02355\n",
      "Epoch: 3, train_loss: 0.89165, time: 0.02200\n",
      "Epoch: 4, train_loss: 0.89192, time: 0.02189\n",
      "Epoch: 5, train_loss: 0.88913, time: 0.02240\n",
      "Epoch: 6, train_loss: 0.89355, time: 0.02320\n",
      "Epoch: 7, train_loss: 0.89045, time: 0.02220\n",
      "Epoch: 8, train_loss: 0.89056, time: 0.02223\n",
      "Epoch: 9, train_loss: 0.89148, time: 0.02335\n",
      "Epoch: 10, train_loss: 0.89197, time: 0.02335\n",
      "Epoch: 11, train_loss: 0.89039, time: 0.02116\n",
      "Epoch: 12, train_loss: 0.89103, time: 0.02131\n",
      "Epoch: 13, train_loss: 0.89058, time: 0.02227\n",
      "Epoch: 14, train_loss: 0.88936, time: 0.02235\n",
      "Epoch: 15, train_loss: 0.89049, time: 0.02291\n",
      "Epoch: 16, train_loss: 0.88933, time: 0.02201\n",
      "Epoch: 17, train_loss: 0.88959, time: 0.02238\n",
      "Epoch: 18, train_loss: 0.88953, time: 0.02229\n",
      "Epoch: 19, train_loss: 0.88845, time: 0.02221\n",
      "Epoch: 20, train_loss: 0.89142, time: 0.02142\n",
      "Epoch: 21, train_loss: 0.89051, time: 0.02145\n",
      "Epoch: 22, train_loss: 0.89117, time: 0.02193\n",
      "Epoch: 23, train_loss: 0.89009, time: 0.02221\n",
      "Epoch: 24, train_loss: 0.89046, time: 0.02157\n",
      "Epoch: 25, train_loss: 0.89129, time: 0.02184\n",
      "Epoch: 26, train_loss: 0.88924, time: 0.02142\n",
      "Epoch: 27, train_loss: 0.88911, time: 0.02223\n",
      "Epoch: 28, train_loss: 0.88993, time: 0.02332\n",
      "Epoch: 29, train_loss: 0.89078, time: 0.02234\n",
      "Epoch: 30, train_loss: 0.89081, time: 0.02204\n",
      "Epoch: 31, train_loss: 0.89047, time: 0.02212\n",
      "Epoch: 32, train_loss: 0.89197, time: 0.02235\n",
      "Epoch: 33, train_loss: 0.89375, time: 0.02166\n",
      "Epoch: 34, train_loss: 0.89154, time: 0.02317\n",
      "Epoch: 35, train_loss: 0.88975, time: 0.02255\n",
      "Epoch: 36, train_loss: 0.88943, time: 0.02301\n",
      "Epoch: 37, train_loss: 0.89027, time: 0.02264\n",
      "Epoch: 38, train_loss: 0.88993, time: 0.02154\n",
      "Epoch: 39, train_loss: 0.89081, time: 0.02235\n",
      "Epoch: 40, train_loss: 0.88987, time: 0.02116\n",
      "Epoch: 41, train_loss: 0.89049, time: 0.02306\n",
      "Epoch: 42, train_loss: 0.88946, time: 0.02238\n",
      "Epoch: 43, train_loss: 0.88963, time: 0.02141\n",
      "Epoch: 44, train_loss: 0.89082, time: 0.02193\n",
      "Epoch: 45, train_loss: 0.89260, time: 0.02134\n",
      "Epoch: 46, train_loss: 0.89054, time: 0.02101\n",
      "Epoch: 47, train_loss: 0.89122, time: 0.02052\n",
      "Epoch: 48, train_loss: 0.89290, time: 0.02126\n",
      "Epoch: 49, train_loss: 0.88897, time: 0.02210\n",
      "Epoch: 50, train_loss: 0.88899, time: 0.02233\n",
      "Epoch: 51, train_loss: 0.89144, time: 0.02261\n",
      "Epoch: 52, train_loss: 0.89216, time: 0.02283\n",
      "Epoch: 53, train_loss: 0.89181, time: 0.02223\n",
      "Epoch: 54, train_loss: 0.89069, time: 0.02246\n",
      "Epoch: 55, train_loss: 0.89009, time: 0.02253\n",
      "Epoch: 56, train_loss: 0.89269, time: 0.02261\n",
      "Epoch: 57, train_loss: 0.89043, time: 0.02262\n",
      "Epoch: 58, train_loss: 0.89068, time: 0.02236\n",
      "Epoch: 59, train_loss: 0.89176, time: 0.02196\n",
      "Epoch: 60, train_loss: 0.89115, time: 0.02245\n",
      "Epoch: 61, train_loss: 0.89108, time: 0.02239\n",
      "Epoch: 62, train_loss: 0.89069, time: 0.02131\n",
      "Epoch: 63, train_loss: 0.89101, time: 0.02219\n",
      "Epoch: 64, train_loss: 0.89130, time: 0.02192\n",
      "Epoch: 65, train_loss: 0.89108, time: 0.02199\n",
      "Epoch: 66, train_loss: 0.89270, time: 0.02230\n",
      "Epoch: 67, train_loss: 0.88778, time: 0.02198\n",
      "Epoch: 68, train_loss: 0.89394, time: 0.02374\n",
      "Epoch: 69, train_loss: 0.89002, time: 0.02261\n",
      "Epoch: 70, train_loss: 0.89111, time: 0.02298\n",
      "Epoch: 71, train_loss: 0.89221, time: 0.02302\n",
      "Epoch: 72, train_loss: 0.89133, time: 0.02319\n",
      "Epoch: 73, train_loss: 0.88950, time: 0.02321\n",
      "Epoch: 74, train_loss: 0.89125, time: 0.02135\n",
      "Epoch: 75, train_loss: 0.89109, time: 0.02348\n",
      "Epoch: 76, train_loss: 0.88886, time: 0.02251\n",
      "Epoch: 77, train_loss: 0.89112, time: 0.02302\n",
      "Epoch: 78, train_loss: 0.89202, time: 0.02233\n",
      "Epoch: 79, train_loss: 0.89119, time: 0.02108\n",
      "Epoch: 80, train_loss: 0.88991, time: 0.02256\n",
      "Epoch: 81, train_loss: 0.89146, time: 0.02282\n",
      "Epoch: 82, train_loss: 0.88907, time: 0.02270\n",
      "Epoch: 83, train_loss: 0.89039, time: 0.02273\n",
      "Epoch: 84, train_loss: 0.89017, time: 0.02220\n",
      "Epoch: 85, train_loss: 0.89023, time: 0.02200\n",
      "Epoch: 86, train_loss: 0.88981, time: 0.02300\n",
      "Epoch: 87, train_loss: 0.89209, time: 0.02182\n",
      "Epoch: 88, train_loss: 0.89030, time: 0.02236\n",
      "Epoch: 89, train_loss: 0.88997, time: 0.02290\n",
      "Epoch: 90, train_loss: 0.89039, time: 0.02262\n",
      "Epoch: 91, train_loss: 0.89081, time: 0.02261\n",
      "Epoch: 92, train_loss: 0.88884, time: 0.02269\n",
      "Epoch: 93, train_loss: 0.89079, time: 0.02235\n",
      "Epoch: 94, train_loss: 0.89230, time: 0.02317\n",
      "Epoch: 95, train_loss: 0.89164, time: 0.02274\n",
      "Epoch: 96, train_loss: 0.89178, time: 0.02184\n",
      "Epoch: 97, train_loss: 0.88968, time: 0.02124\n",
      "Epoch: 98, train_loss: 0.89085, time: 0.02259\n",
      "Epoch: 99, train_loss: 0.89127, time: 0.02262\n",
      "Epoch: 100, train_loss: 0.89097, time: 0.02222\n",
      "Epoch: 101, train_loss: 0.89112, time: 0.02269\n",
      "Epoch: 102, train_loss: 0.89007, time: 0.02335\n",
      "Epoch: 103, train_loss: 0.89086, time: 0.02270\n",
      "Epoch: 104, train_loss: 0.88983, time: 0.02284\n",
      "Epoch: 105, train_loss: 0.89070, time: 0.02148\n",
      "Epoch: 106, train_loss: 0.88999, time: 0.02292\n",
      "Epoch: 107, train_loss: 0.89088, time: 0.02246\n",
      "Epoch: 108, train_loss: 0.89140, time: 0.02150\n",
      "Epoch: 109, train_loss: 0.89102, time: 0.02182\n",
      "Epoch: 110, train_loss: 0.88973, time: 0.02280\n",
      "Epoch: 111, train_loss: 0.89087, time: 0.02189\n",
      "Epoch: 112, train_loss: 0.89016, time: 0.02292\n",
      "Epoch: 113, train_loss: 0.89000, time: 0.02209\n",
      "Epoch: 114, train_loss: 0.89098, time: 0.02292\n",
      "Epoch: 115, train_loss: 0.89029, time: 0.02348\n",
      "Epoch: 116, train_loss: 0.89218, time: 0.02215\n",
      "Epoch: 117, train_loss: 0.89021, time: 0.02231\n",
      "Epoch: 118, train_loss: 0.89163, time: 0.02414\n",
      "Epoch: 119, train_loss: 0.89028, time: 0.02281\n",
      "Epoch: 120, train_loss: 0.89013, time: 0.02312\n",
      "Epoch: 121, train_loss: 0.89141, time: 0.02272\n",
      "Epoch: 122, train_loss: 0.89104, time: 0.02244\n",
      "Epoch: 123, train_loss: 0.89122, time: 0.02278\n",
      "Epoch: 124, train_loss: 0.89145, time: 0.02413\n",
      "Epoch: 125, train_loss: 0.88938, time: 0.02279\n",
      "Epoch: 126, train_loss: 0.89238, time: 0.02180\n",
      "Epoch: 127, train_loss: 0.88999, time: 0.02162\n",
      "Epoch: 128, train_loss: 0.89024, time: 0.02209\n",
      "Epoch: 129, train_loss: 0.89199, time: 0.02267\n",
      "Epoch: 130, train_loss: 0.89079, time: 0.02260\n",
      "Epoch: 131, train_loss: 0.89232, time: 0.02243\n",
      "Epoch: 132, train_loss: 0.88970, time: 0.02219\n",
      "Epoch: 133, train_loss: 0.88999, time: 0.02248\n",
      "Epoch: 134, train_loss: 0.89117, time: 0.02208\n",
      "Epoch: 135, train_loss: 0.89050, time: 0.02154\n",
      "Epoch: 136, train_loss: 0.89027, time: 0.02110\n",
      "Epoch: 137, train_loss: 0.88933, time: 0.02233\n",
      "Epoch: 138, train_loss: 0.89006, time: 0.02218\n",
      "Epoch: 139, train_loss: 0.88918, time: 0.02209\n",
      "Epoch: 140, train_loss: 0.89001, time: 0.02182\n",
      "Epoch: 141, train_loss: 0.89155, time: 0.02256\n",
      "Epoch: 142, train_loss: 0.89165, time: 0.02273\n",
      "Epoch: 143, train_loss: 0.89005, time: 0.02387\n",
      "Epoch: 144, train_loss: 0.89185, time: 0.02290\n",
      "Epoch: 145, train_loss: 0.88939, time: 0.02285\n",
      "Epoch: 146, train_loss: 0.89157, time: 0.02142\n",
      "Epoch: 147, train_loss: 0.88986, time: 0.02232\n",
      "Epoch: 148, train_loss: 0.89175, time: 0.02213\n",
      "Epoch: 149, train_loss: 0.89058, time: 0.02090\n",
      "Epoch: 150, train_loss: 0.89006, time: 0.02023\n",
      "Epoch: 151, train_loss: 0.88861, time: 0.02142\n",
      "Epoch: 152, train_loss: 0.89051, time: 0.02205\n",
      "Epoch: 153, train_loss: 0.89063, time: 0.02228\n",
      "Epoch: 154, train_loss: 0.89117, time: 0.02267\n",
      "Epoch: 155, train_loss: 0.88947, time: 0.02231\n",
      "Epoch: 156, train_loss: 0.88915, time: 0.02218\n",
      "Epoch: 157, train_loss: 0.88941, time: 0.02150\n",
      "Epoch: 158, train_loss: 0.89033, time: 0.02130\n",
      "Epoch: 159, train_loss: 0.89045, time: 0.02275\n",
      "Epoch: 160, train_loss: 0.88938, time: 0.02235\n",
      "Epoch: 161, train_loss: 0.89165, time: 0.02213\n",
      "Epoch: 162, train_loss: 0.88988, time: 0.02235\n",
      "Epoch: 163, train_loss: 0.88857, time: 0.02097\n",
      "Epoch: 164, train_loss: 0.89016, time: 0.02183\n",
      "Epoch: 165, train_loss: 0.89075, time: 0.02334\n",
      "Epoch: 166, train_loss: 0.89121, time: 0.02224\n",
      "Epoch: 167, train_loss: 0.89079, time: 0.02243\n",
      "Epoch: 168, train_loss: 0.89040, time: 0.02251\n",
      "Epoch: 169, train_loss: 0.89021, time: 0.02249\n",
      "Epoch: 170, train_loss: 0.89233, time: 0.02302\n",
      "Epoch: 171, train_loss: 0.88811, time: 0.02252\n",
      "Epoch: 172, train_loss: 0.88867, time: 0.02241\n",
      "Epoch: 173, train_loss: 0.89177, time: 0.02265\n",
      "Epoch: 174, train_loss: 0.89109, time: 0.02196\n",
      "Epoch: 175, train_loss: 0.89065, time: 0.02211\n",
      "Epoch: 176, train_loss: 0.89092, time: 0.02204\n",
      "Epoch: 177, train_loss: 0.89085, time: 0.02252\n",
      "Epoch: 178, train_loss: 0.89029, time: 0.02267\n",
      "Epoch: 179, train_loss: 0.89162, time: 0.02236\n",
      "Epoch: 180, train_loss: 0.89108, time: 0.02340\n",
      "Epoch: 181, train_loss: 0.89227, time: 0.02237\n",
      "Epoch: 182, train_loss: 0.89072, time: 0.02304\n",
      "Epoch: 183, train_loss: 0.88899, time: 0.02124\n",
      "Epoch: 184, train_loss: 0.88977, time: 0.02259\n",
      "Epoch: 185, train_loss: 0.89102, time: 0.02198\n",
      "Epoch: 186, train_loss: 0.89168, time: 0.02221\n",
      "Epoch: 187, train_loss: 0.88973, time: 0.02160\n",
      "Epoch: 188, train_loss: 0.89083, time: 0.02318\n",
      "Epoch: 189, train_loss: 0.88921, time: 0.02216\n",
      "Epoch: 190, train_loss: 0.89252, time: 0.02266\n",
      "Epoch: 191, train_loss: 0.89149, time: 0.02190\n",
      "Epoch: 192, train_loss: 0.88979, time: 0.02223\n",
      "Epoch: 193, train_loss: 0.89109, time: 0.02208\n",
      "Epoch: 194, train_loss: 0.89094, time: 0.02252\n",
      "Epoch: 195, train_loss: 0.89039, time: 0.02153\n",
      "Epoch: 196, train_loss: 0.89091, time: 0.02351\n",
      "Epoch: 197, train_loss: 0.88844, time: 0.02244\n",
      "Epoch: 198, train_loss: 0.88995, time: 0.02212\n",
      "Epoch: 199, train_loss: 0.88938, time: 0.02260\n",
      "Epoch: 200, train_loss: 0.88991, time: 0.02167\n",
      "pairwise precision 0.91746 recall 0.97853 f1 0.94701\n",
      "average until now [0.5625279514775998, 0.8736677860436005, 0.6843949429947698]\n",
      "12 names 115.82148861885071 avg time 9.651790718237558\n",
      "Loading bo_ai dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 225 nodes, 8121 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.83999, time: 0.11614\n",
      "Epoch: 2, train_loss: 0.84029, time: 0.01681\n",
      "Epoch: 3, train_loss: 0.83818, time: 0.01584\n",
      "Epoch: 4, train_loss: 0.83666, time: 0.01573\n",
      "Epoch: 5, train_loss: 0.83821, time: 0.01533\n",
      "Epoch: 6, train_loss: 0.83847, time: 0.01568\n",
      "Epoch: 7, train_loss: 0.83960, time: 0.01550\n",
      "Epoch: 8, train_loss: 0.83896, time: 0.01516\n",
      "Epoch: 9, train_loss: 0.84132, time: 0.01548\n",
      "Epoch: 10, train_loss: 0.83884, time: 0.01546\n",
      "Epoch: 11, train_loss: 0.83810, time: 0.01525\n",
      "Epoch: 12, train_loss: 0.83956, time: 0.01514\n",
      "Epoch: 13, train_loss: 0.83881, time: 0.01554\n",
      "Epoch: 14, train_loss: 0.84083, time: 0.01582\n",
      "Epoch: 15, train_loss: 0.84201, time: 0.01529\n",
      "Epoch: 16, train_loss: 0.83624, time: 0.01536\n",
      "Epoch: 17, train_loss: 0.83516, time: 0.01567\n",
      "Epoch: 18, train_loss: 0.83875, time: 0.01505\n",
      "Epoch: 19, train_loss: 0.84035, time: 0.01579\n",
      "Epoch: 20, train_loss: 0.83930, time: 0.01499\n",
      "Epoch: 21, train_loss: 0.83977, time: 0.01528\n",
      "Epoch: 22, train_loss: 0.83950, time: 0.01556\n",
      "Epoch: 23, train_loss: 0.83912, time: 0.01546\n",
      "Epoch: 24, train_loss: 0.83922, time: 0.01521\n",
      "Epoch: 25, train_loss: 0.83662, time: 0.01534\n",
      "Epoch: 26, train_loss: 0.83674, time: 0.01460\n",
      "Epoch: 27, train_loss: 0.84022, time: 0.01487\n",
      "Epoch: 28, train_loss: 0.83745, time: 0.01511\n",
      "Epoch: 29, train_loss: 0.83702, time: 0.01548\n",
      "Epoch: 30, train_loss: 0.83673, time: 0.01543\n",
      "Epoch: 31, train_loss: 0.83919, time: 0.01590\n",
      "Epoch: 32, train_loss: 0.83964, time: 0.01537\n",
      "Epoch: 33, train_loss: 0.83919, time: 0.01596\n",
      "Epoch: 34, train_loss: 0.84064, time: 0.01633\n",
      "Epoch: 35, train_loss: 0.83975, time: 0.01591\n",
      "Epoch: 36, train_loss: 0.83764, time: 0.01545\n",
      "Epoch: 37, train_loss: 0.83982, time: 0.01541\n",
      "Epoch: 38, train_loss: 0.83960, time: 0.01588\n",
      "Epoch: 39, train_loss: 0.83978, time: 0.01535\n",
      "Epoch: 40, train_loss: 0.83964, time: 0.01553\n",
      "Epoch: 41, train_loss: 0.84242, time: 0.01561\n",
      "Epoch: 42, train_loss: 0.84100, time: 0.01563\n",
      "Epoch: 43, train_loss: 0.83907, time: 0.01534\n",
      "Epoch: 44, train_loss: 0.83712, time: 0.01567\n",
      "Epoch: 45, train_loss: 0.83956, time: 0.01517\n",
      "Epoch: 46, train_loss: 0.83870, time: 0.01557\n",
      "Epoch: 47, train_loss: 0.84034, time: 0.01556\n",
      "Epoch: 48, train_loss: 0.83880, time: 0.01534\n",
      "Epoch: 49, train_loss: 0.84101, time: 0.01576\n",
      "Epoch: 50, train_loss: 0.83909, time: 0.01566\n",
      "Epoch: 51, train_loss: 0.83982, time: 0.01590\n",
      "Epoch: 52, train_loss: 0.83726, time: 0.01551\n",
      "Epoch: 53, train_loss: 0.83909, time: 0.01581\n",
      "Epoch: 54, train_loss: 0.83794, time: 0.01585\n",
      "Epoch: 55, train_loss: 0.83908, time: 0.01587\n",
      "Epoch: 56, train_loss: 0.84137, time: 0.01513\n",
      "Epoch: 57, train_loss: 0.83910, time: 0.01574\n",
      "Epoch: 58, train_loss: 0.84018, time: 0.01515\n",
      "Epoch: 59, train_loss: 0.83832, time: 0.01536\n",
      "Epoch: 60, train_loss: 0.84082, time: 0.01534\n",
      "Epoch: 61, train_loss: 0.84165, time: 0.01562\n",
      "Epoch: 62, train_loss: 0.83615, time: 0.01537\n",
      "Epoch: 63, train_loss: 0.84024, time: 0.01560\n",
      "Epoch: 64, train_loss: 0.83875, time: 0.01555\n",
      "Epoch: 65, train_loss: 0.83871, time: 0.01578\n",
      "Epoch: 66, train_loss: 0.83935, time: 0.01538\n",
      "Epoch: 67, train_loss: 0.84026, time: 0.01600\n",
      "Epoch: 68, train_loss: 0.83876, time: 0.01588\n",
      "Epoch: 69, train_loss: 0.83810, time: 0.01540\n",
      "Epoch: 70, train_loss: 0.83792, time: 0.01534\n",
      "Epoch: 71, train_loss: 0.84019, time: 0.01514\n",
      "Epoch: 72, train_loss: 0.83790, time: 0.01514\n",
      "Epoch: 73, train_loss: 0.83904, time: 0.01629\n",
      "Epoch: 74, train_loss: 0.83985, time: 0.01526\n",
      "Epoch: 75, train_loss: 0.83627, time: 0.01623\n",
      "Epoch: 76, train_loss: 0.83819, time: 0.01560\n",
      "Epoch: 77, train_loss: 0.83619, time: 0.01517\n",
      "Epoch: 78, train_loss: 0.83796, time: 0.01537\n",
      "Epoch: 79, train_loss: 0.83785, time: 0.01568\n",
      "Epoch: 80, train_loss: 0.83991, time: 0.01616\n",
      "Epoch: 81, train_loss: 0.84083, time: 0.01523\n",
      "Epoch: 82, train_loss: 0.83756, time: 0.01546\n",
      "Epoch: 83, train_loss: 0.83956, time: 0.01561\n",
      "Epoch: 84, train_loss: 0.83642, time: 0.01604\n",
      "Epoch: 85, train_loss: 0.83838, time: 0.01540\n",
      "Epoch: 86, train_loss: 0.83769, time: 0.01578\n",
      "Epoch: 87, train_loss: 0.84060, time: 0.01549\n",
      "Epoch: 88, train_loss: 0.83733, time: 0.01692\n",
      "Epoch: 89, train_loss: 0.83919, time: 0.01548\n",
      "Epoch: 90, train_loss: 0.83723, time: 0.01503\n",
      "Epoch: 91, train_loss: 0.83793, time: 0.01511\n",
      "Epoch: 92, train_loss: 0.83796, time: 0.01570\n",
      "Epoch: 93, train_loss: 0.83895, time: 0.01553\n",
      "Epoch: 94, train_loss: 0.83909, time: 0.01554\n",
      "Epoch: 95, train_loss: 0.83759, time: 0.01548\n",
      "Epoch: 96, train_loss: 0.83832, time: 0.01600\n",
      "Epoch: 97, train_loss: 0.84035, time: 0.01513\n",
      "Epoch: 98, train_loss: 0.84023, time: 0.01513\n",
      "Epoch: 99, train_loss: 0.83946, time: 0.01514\n",
      "Epoch: 100, train_loss: 0.83883, time: 0.01501\n",
      "Epoch: 101, train_loss: 0.83776, time: 0.01596\n",
      "Epoch: 102, train_loss: 0.83855, time: 0.01555\n",
      "Epoch: 103, train_loss: 0.83924, time: 0.01609\n",
      "Epoch: 104, train_loss: 0.83679, time: 0.01613\n",
      "Epoch: 105, train_loss: 0.84026, time: 0.01554\n",
      "Epoch: 106, train_loss: 0.83737, time: 0.01552\n",
      "Epoch: 107, train_loss: 0.83888, time: 0.01579\n",
      "Epoch: 108, train_loss: 0.83864, time: 0.01586\n",
      "Epoch: 109, train_loss: 0.83930, time: 0.01574\n",
      "Epoch: 110, train_loss: 0.83698, time: 0.01562\n",
      "Epoch: 111, train_loss: 0.83836, time: 0.01522\n",
      "Epoch: 112, train_loss: 0.84116, time: 0.01555\n",
      "Epoch: 113, train_loss: 0.84092, time: 0.01560\n",
      "Epoch: 114, train_loss: 0.83998, time: 0.01563\n",
      "Epoch: 115, train_loss: 0.83944, time: 0.01606\n",
      "Epoch: 116, train_loss: 0.83856, time: 0.01479\n",
      "Epoch: 117, train_loss: 0.83768, time: 0.01521\n",
      "Epoch: 118, train_loss: 0.83826, time: 0.01544\n",
      "Epoch: 119, train_loss: 0.83859, time: 0.01551\n",
      "Epoch: 120, train_loss: 0.83948, time: 0.01604\n",
      "Epoch: 121, train_loss: 0.84007, time: 0.01559\n",
      "Epoch: 122, train_loss: 0.83985, time: 0.01632\n",
      "Epoch: 123, train_loss: 0.83800, time: 0.01551\n",
      "Epoch: 124, train_loss: 0.83658, time: 0.01559\n",
      "Epoch: 125, train_loss: 0.83801, time: 0.01600\n",
      "Epoch: 126, train_loss: 0.83890, time: 0.01565\n",
      "Epoch: 127, train_loss: 0.83902, time: 0.01559\n",
      "Epoch: 128, train_loss: 0.83810, time: 0.01513\n",
      "Epoch: 129, train_loss: 0.83972, time: 0.01531\n",
      "Epoch: 130, train_loss: 0.83944, time: 0.01566\n",
      "Epoch: 131, train_loss: 0.84071, time: 0.01597\n",
      "Epoch: 132, train_loss: 0.83885, time: 0.01620\n",
      "Epoch: 133, train_loss: 0.83986, time: 0.01593\n",
      "Epoch: 134, train_loss: 0.83927, time: 0.01583\n",
      "Epoch: 135, train_loss: 0.83728, time: 0.01575\n",
      "Epoch: 136, train_loss: 0.83942, time: 0.01543\n",
      "Epoch: 137, train_loss: 0.83967, time: 0.01639\n",
      "Epoch: 138, train_loss: 0.83548, time: 0.01576\n",
      "Epoch: 139, train_loss: 0.83635, time: 0.01573\n",
      "Epoch: 140, train_loss: 0.83981, time: 0.01548\n",
      "Epoch: 141, train_loss: 0.83812, time: 0.01533\n",
      "Epoch: 142, train_loss: 0.84108, time: 0.01547\n",
      "Epoch: 143, train_loss: 0.83998, time: 0.01511\n",
      "Epoch: 144, train_loss: 0.83923, time: 0.01533\n",
      "Epoch: 145, train_loss: 0.83985, time: 0.01578\n",
      "Epoch: 146, train_loss: 0.83617, time: 0.01611\n",
      "Epoch: 147, train_loss: 0.83823, time: 0.01524\n",
      "Epoch: 148, train_loss: 0.83731, time: 0.01608\n",
      "Epoch: 149, train_loss: 0.83911, time: 0.01569\n",
      "Epoch: 150, train_loss: 0.83762, time: 0.01597\n",
      "Epoch: 151, train_loss: 0.83738, time: 0.01518\n",
      "Epoch: 152, train_loss: 0.83681, time: 0.01583\n",
      "Epoch: 153, train_loss: 0.83740, time: 0.01569\n",
      "Epoch: 154, train_loss: 0.83777, time: 0.01543\n",
      "Epoch: 155, train_loss: 0.83809, time: 0.01499\n",
      "Epoch: 156, train_loss: 0.83736, time: 0.01463\n",
      "Epoch: 157, train_loss: 0.84128, time: 0.01506\n",
      "Epoch: 158, train_loss: 0.83497, time: 0.01521\n",
      "Epoch: 159, train_loss: 0.83731, time: 0.01548\n",
      "Epoch: 160, train_loss: 0.83759, time: 0.01602\n",
      "Epoch: 161, train_loss: 0.83827, time: 0.01580\n",
      "Epoch: 162, train_loss: 0.83834, time: 0.01608\n",
      "Epoch: 163, train_loss: 0.83506, time: 0.01637\n",
      "Epoch: 164, train_loss: 0.83620, time: 0.01522\n",
      "Epoch: 165, train_loss: 0.83959, time: 0.01543\n",
      "Epoch: 166, train_loss: 0.83773, time: 0.01601\n",
      "Epoch: 167, train_loss: 0.83846, time: 0.01500\n",
      "Epoch: 168, train_loss: 0.83588, time: 0.01482\n",
      "Epoch: 169, train_loss: 0.83321, time: 0.01475\n",
      "Epoch: 170, train_loss: 0.83764, time: 0.01510\n",
      "Epoch: 171, train_loss: 0.84059, time: 0.01528\n",
      "Epoch: 172, train_loss: 0.83174, time: 0.01501\n",
      "Epoch: 173, train_loss: 0.83567, time: 0.01520\n",
      "Epoch: 174, train_loss: 0.83730, time: 0.01538\n",
      "Epoch: 175, train_loss: 0.83076, time: 0.01480\n",
      "Epoch: 176, train_loss: 0.83861, time: 0.01508\n",
      "Epoch: 177, train_loss: 0.83999, time: 0.01503\n",
      "Epoch: 178, train_loss: 0.82963, time: 0.01524\n",
      "Epoch: 179, train_loss: 0.84015, time: 0.01565\n",
      "Epoch: 180, train_loss: 0.83491, time: 0.01538\n",
      "Epoch: 181, train_loss: 0.83815, time: 0.01504\n",
      "Epoch: 182, train_loss: 0.83691, time: 0.01550\n",
      "Epoch: 183, train_loss: 0.83436, time: 0.01556\n",
      "Epoch: 184, train_loss: 0.83723, time: 0.01569\n",
      "Epoch: 185, train_loss: 0.83640, time: 0.01613\n",
      "Epoch: 186, train_loss: 0.82621, time: 0.01670\n",
      "Epoch: 187, train_loss: 0.83404, time: 0.01595\n",
      "Epoch: 188, train_loss: 0.83177, time: 0.01578\n",
      "Epoch: 189, train_loss: 0.82547, time: 0.01536\n",
      "Epoch: 190, train_loss: 0.82888, time: 0.01517\n",
      "Epoch: 191, train_loss: 0.83090, time: 0.01580\n",
      "Epoch: 192, train_loss: 0.83161, time: 0.01562\n",
      "Epoch: 193, train_loss: 0.82145, time: 0.01545\n",
      "Epoch: 194, train_loss: 0.83216, time: 0.01566\n",
      "Epoch: 195, train_loss: 0.83343, time: 0.01530\n",
      "Epoch: 196, train_loss: 0.82732, time: 0.01540\n",
      "Epoch: 197, train_loss: 0.82265, time: 0.01551\n",
      "Epoch: 198, train_loss: 0.81818, time: 0.01585\n",
      "Epoch: 199, train_loss: 0.83714, time: 0.01631\n",
      "Epoch: 200, train_loss: 0.82437, time: 0.01766\n",
      "pairwise precision 0.94659 recall 0.53697 f1 0.68523\n",
      "average until now [0.5920709538955975, 0.847768283713126, 0.697215304750385]\n",
      "13 names 119.10088419914246 avg time 9.161606476857113\n",
      "Loading yongjian_tang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 198 nodes, 7977 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.79732, time: 0.11540\n",
      "Epoch: 2, train_loss: 0.79248, time: 0.01615\n",
      "Epoch: 3, train_loss: 0.79462, time: 0.01545\n",
      "Epoch: 4, train_loss: 0.79561, time: 0.01551\n",
      "Epoch: 5, train_loss: 0.79757, time: 0.01500\n",
      "Epoch: 6, train_loss: 0.79599, time: 0.01518\n",
      "Epoch: 7, train_loss: 0.79581, time: 0.01505\n",
      "Epoch: 8, train_loss: 0.79571, time: 0.01500\n",
      "Epoch: 9, train_loss: 0.79222, time: 0.01528\n",
      "Epoch: 10, train_loss: 0.79507, time: 0.01508\n",
      "Epoch: 11, train_loss: 0.79889, time: 0.01454\n",
      "Epoch: 12, train_loss: 0.79439, time: 0.01460\n",
      "Epoch: 13, train_loss: 0.79637, time: 0.01523\n",
      "Epoch: 14, train_loss: 0.79634, time: 0.01525\n",
      "Epoch: 15, train_loss: 0.79495, time: 0.01535\n",
      "Epoch: 16, train_loss: 0.79398, time: 0.01510\n",
      "Epoch: 17, train_loss: 0.79483, time: 0.01464\n",
      "Epoch: 18, train_loss: 0.79340, time: 0.01527\n",
      "Epoch: 19, train_loss: 0.79577, time: 0.01520\n",
      "Epoch: 20, train_loss: 0.79446, time: 0.01462\n",
      "Epoch: 21, train_loss: 0.79426, time: 0.01524\n",
      "Epoch: 22, train_loss: 0.79624, time: 0.01542\n",
      "Epoch: 23, train_loss: 0.79495, time: 0.01521\n",
      "Epoch: 24, train_loss: 0.79467, time: 0.01514\n",
      "Epoch: 25, train_loss: 0.79473, time: 0.01519\n",
      "Epoch: 26, train_loss: 0.79493, time: 0.01471\n",
      "Epoch: 27, train_loss: 0.79577, time: 0.01513\n",
      "Epoch: 28, train_loss: 0.79754, time: 0.01525\n",
      "Epoch: 29, train_loss: 0.79458, time: 0.01555\n",
      "Epoch: 30, train_loss: 0.79643, time: 0.01521\n",
      "Epoch: 31, train_loss: 0.79424, time: 0.01517\n",
      "Epoch: 32, train_loss: 0.79774, time: 0.01518\n",
      "Epoch: 33, train_loss: 0.79654, time: 0.01444\n",
      "Epoch: 34, train_loss: 0.79391, time: 0.01492\n",
      "Epoch: 35, train_loss: 0.79518, time: 0.01537\n",
      "Epoch: 36, train_loss: 0.79736, time: 0.01505\n",
      "Epoch: 37, train_loss: 0.79838, time: 0.01500\n",
      "Epoch: 38, train_loss: 0.79668, time: 0.01542\n",
      "Epoch: 39, train_loss: 0.79460, time: 0.01532\n",
      "Epoch: 40, train_loss: 0.79537, time: 0.01513\n",
      "Epoch: 41, train_loss: 0.79330, time: 0.01511\n",
      "Epoch: 42, train_loss: 0.79482, time: 0.01527\n",
      "Epoch: 43, train_loss: 0.79608, time: 0.01534\n",
      "Epoch: 44, train_loss: 0.79360, time: 0.01581\n",
      "Epoch: 45, train_loss: 0.79433, time: 0.01512\n",
      "Epoch: 46, train_loss: 0.79524, time: 0.01523\n",
      "Epoch: 47, train_loss: 0.79694, time: 0.01487\n",
      "Epoch: 48, train_loss: 0.79399, time: 0.01495\n",
      "Epoch: 49, train_loss: 0.79738, time: 0.01530\n",
      "Epoch: 50, train_loss: 0.79394, time: 0.01476\n",
      "Epoch: 51, train_loss: 0.79199, time: 0.01515\n",
      "Epoch: 52, train_loss: 0.79280, time: 0.01487\n",
      "Epoch: 53, train_loss: 0.79636, time: 0.01522\n",
      "Epoch: 54, train_loss: 0.79735, time: 0.01495\n",
      "Epoch: 55, train_loss: 0.79480, time: 0.01490\n",
      "Epoch: 56, train_loss: 0.79560, time: 0.01498\n",
      "Epoch: 57, train_loss: 0.79524, time: 0.01540\n",
      "Epoch: 58, train_loss: 0.79580, time: 0.01556\n",
      "Epoch: 59, train_loss: 0.79753, time: 0.01535\n",
      "Epoch: 60, train_loss: 0.79549, time: 0.01474\n",
      "Epoch: 61, train_loss: 0.79552, time: 0.01498\n",
      "Epoch: 62, train_loss: 0.79677, time: 0.01503\n",
      "Epoch: 63, train_loss: 0.79570, time: 0.01527\n",
      "Epoch: 64, train_loss: 0.79512, time: 0.01532\n",
      "Epoch: 65, train_loss: 0.79777, time: 0.01530\n",
      "Epoch: 66, train_loss: 0.79473, time: 0.01543\n",
      "Epoch: 67, train_loss: 0.79718, time: 0.01481\n",
      "Epoch: 68, train_loss: 0.79273, time: 0.01505\n",
      "Epoch: 69, train_loss: 0.79636, time: 0.01515\n",
      "Epoch: 70, train_loss: 0.79831, time: 0.01523\n",
      "Epoch: 71, train_loss: 0.79430, time: 0.01575\n",
      "Epoch: 72, train_loss: 0.79692, time: 0.01476\n",
      "Epoch: 73, train_loss: 0.79398, time: 0.01596\n",
      "Epoch: 74, train_loss: 0.79657, time: 0.01547\n",
      "Epoch: 75, train_loss: 0.79524, time: 0.01530\n",
      "Epoch: 76, train_loss: 0.79310, time: 0.01505\n",
      "Epoch: 77, train_loss: 0.79649, time: 0.01462\n",
      "Epoch: 78, train_loss: 0.79594, time: 0.01452\n",
      "Epoch: 79, train_loss: 0.79765, time: 0.01449\n",
      "Epoch: 80, train_loss: 0.79794, time: 0.01516\n",
      "Epoch: 81, train_loss: 0.79463, time: 0.01557\n",
      "Epoch: 82, train_loss: 0.79590, time: 0.01554\n",
      "Epoch: 83, train_loss: 0.79817, time: 0.01487\n",
      "Epoch: 84, train_loss: 0.79458, time: 0.01546\n",
      "Epoch: 85, train_loss: 0.79410, time: 0.01553\n",
      "Epoch: 86, train_loss: 0.79688, time: 0.01549\n",
      "Epoch: 87, train_loss: 0.79674, time: 0.01517\n",
      "Epoch: 88, train_loss: 0.79485, time: 0.01552\n",
      "Epoch: 89, train_loss: 0.79511, time: 0.01550\n",
      "Epoch: 90, train_loss: 0.79844, time: 0.01505\n",
      "Epoch: 91, train_loss: 0.79757, time: 0.01490\n",
      "Epoch: 92, train_loss: 0.79590, time: 0.01500\n",
      "Epoch: 93, train_loss: 0.79478, time: 0.01527\n",
      "Epoch: 94, train_loss: 0.79694, time: 0.01516\n",
      "Epoch: 95, train_loss: 0.79565, time: 0.01608\n",
      "Epoch: 96, train_loss: 0.79531, time: 0.01496\n",
      "Epoch: 97, train_loss: 0.79643, time: 0.01536\n",
      "Epoch: 98, train_loss: 0.79351, time: 0.01502\n",
      "Epoch: 99, train_loss: 0.79483, time: 0.01536\n",
      "Epoch: 100, train_loss: 0.79693, time: 0.01515\n",
      "Epoch: 101, train_loss: 0.79621, time: 0.01558\n",
      "Epoch: 102, train_loss: 0.79790, time: 0.01621\n",
      "Epoch: 103, train_loss: 0.79494, time: 0.01524\n",
      "Epoch: 104, train_loss: 0.79555, time: 0.01525\n",
      "Epoch: 105, train_loss: 0.79981, time: 0.01484\n",
      "Epoch: 106, train_loss: 0.79500, time: 0.01482\n",
      "Epoch: 107, train_loss: 0.79735, time: 0.01513\n",
      "Epoch: 108, train_loss: 0.79632, time: 0.01509\n",
      "Epoch: 109, train_loss: 0.79589, time: 0.01515\n",
      "Epoch: 110, train_loss: 0.79263, time: 0.01524\n",
      "Epoch: 111, train_loss: 0.79361, time: 0.01551\n",
      "Epoch: 112, train_loss: 0.79863, time: 0.01497\n",
      "Epoch: 113, train_loss: 0.79457, time: 0.01560\n",
      "Epoch: 114, train_loss: 0.79540, time: 0.01572\n",
      "Epoch: 115, train_loss: 0.79432, time: 0.01574\n",
      "Epoch: 116, train_loss: 0.79579, time: 0.01515\n",
      "Epoch: 117, train_loss: 0.79780, time: 0.01516\n",
      "Epoch: 118, train_loss: 0.79661, time: 0.01542\n",
      "Epoch: 119, train_loss: 0.79520, time: 0.01527\n",
      "Epoch: 120, train_loss: 0.79510, time: 0.01644\n",
      "Epoch: 121, train_loss: 0.79659, time: 0.01594\n",
      "Epoch: 122, train_loss: 0.79509, time: 0.01648\n",
      "Epoch: 123, train_loss: 0.79397, time: 0.01630\n",
      "Epoch: 124, train_loss: 0.79240, time: 0.01525\n",
      "Epoch: 125, train_loss: 0.79583, time: 0.01581\n",
      "Epoch: 126, train_loss: 0.79688, time: 0.01733\n",
      "Epoch: 127, train_loss: 0.79474, time: 0.01543\n",
      "Epoch: 128, train_loss: 0.79682, time: 0.01603\n",
      "Epoch: 129, train_loss: 0.79891, time: 0.01741\n",
      "Epoch: 130, train_loss: 0.79584, time: 0.01727\n",
      "Epoch: 131, train_loss: 0.79365, time: 0.01578\n",
      "Epoch: 132, train_loss: 0.79557, time: 0.01602\n",
      "Epoch: 133, train_loss: 0.79621, time: 0.01579\n",
      "Epoch: 134, train_loss: 0.79664, time: 0.01526\n",
      "Epoch: 135, train_loss: 0.79590, time: 0.01573\n",
      "Epoch: 136, train_loss: 0.79287, time: 0.01551\n",
      "Epoch: 137, train_loss: 0.79694, time: 0.01602\n",
      "Epoch: 138, train_loss: 0.79440, time: 0.01645\n",
      "Epoch: 139, train_loss: 0.79633, time: 0.01671\n",
      "Epoch: 140, train_loss: 0.79648, time: 0.01539\n",
      "Epoch: 141, train_loss: 0.79587, time: 0.01492\n",
      "Epoch: 142, train_loss: 0.79526, time: 0.01516\n",
      "Epoch: 143, train_loss: 0.79654, time: 0.01548\n",
      "Epoch: 144, train_loss: 0.79375, time: 0.01542\n",
      "Epoch: 145, train_loss: 0.79531, time: 0.01512\n",
      "Epoch: 146, train_loss: 0.79761, time: 0.01566\n",
      "Epoch: 147, train_loss: 0.79336, time: 0.01644\n",
      "Epoch: 148, train_loss: 0.79423, time: 0.01619\n",
      "Epoch: 149, train_loss: 0.79583, time: 0.01541\n",
      "Epoch: 150, train_loss: 0.79963, time: 0.01493\n",
      "Epoch: 151, train_loss: 0.79405, time: 0.01504\n",
      "Epoch: 152, train_loss: 0.79432, time: 0.01630\n",
      "Epoch: 153, train_loss: 0.79542, time: 0.01502\n",
      "Epoch: 154, train_loss: 0.79587, time: 0.01517\n",
      "Epoch: 155, train_loss: 0.79823, time: 0.01510\n",
      "Epoch: 156, train_loss: 0.79499, time: 0.01493\n",
      "Epoch: 157, train_loss: 0.79357, time: 0.01511\n",
      "Epoch: 158, train_loss: 0.79674, time: 0.01495\n",
      "Epoch: 159, train_loss: 0.79593, time: 0.01555\n",
      "Epoch: 160, train_loss: 0.79569, time: 0.01585\n",
      "Epoch: 161, train_loss: 0.79589, time: 0.01496\n",
      "Epoch: 162, train_loss: 0.79422, time: 0.01521\n",
      "Epoch: 163, train_loss: 0.79434, time: 0.01526\n",
      "Epoch: 164, train_loss: 0.79834, time: 0.01471\n",
      "Epoch: 165, train_loss: 0.79614, time: 0.01521\n",
      "Epoch: 166, train_loss: 0.79673, time: 0.01537\n",
      "Epoch: 167, train_loss: 0.79719, time: 0.01506\n",
      "Epoch: 168, train_loss: 0.79789, time: 0.01553\n",
      "Epoch: 169, train_loss: 0.79270, time: 0.01531\n",
      "Epoch: 170, train_loss: 0.79524, time: 0.01710\n",
      "Epoch: 171, train_loss: 0.79769, time: 0.01641\n",
      "Epoch: 172, train_loss: 0.79775, time: 0.01623\n",
      "Epoch: 173, train_loss: 0.79542, time: 0.01514\n",
      "Epoch: 174, train_loss: 0.79484, time: 0.01527\n",
      "Epoch: 175, train_loss: 0.79811, time: 0.01529\n",
      "Epoch: 176, train_loss: 0.79502, time: 0.01526\n",
      "Epoch: 177, train_loss: 0.79461, time: 0.01531\n",
      "Epoch: 178, train_loss: 0.79250, time: 0.01521\n",
      "Epoch: 179, train_loss: 0.79431, time: 0.01510\n",
      "Epoch: 180, train_loss: 0.79400, time: 0.01549\n",
      "Epoch: 181, train_loss: 0.79624, time: 0.01567\n",
      "Epoch: 182, train_loss: 0.79527, time: 0.01506\n",
      "Epoch: 183, train_loss: 0.79440, time: 0.01588\n",
      "Epoch: 184, train_loss: 0.79564, time: 0.01529\n",
      "Epoch: 185, train_loss: 0.79650, time: 0.01628\n",
      "Epoch: 186, train_loss: 0.79320, time: 0.01521\n",
      "Epoch: 187, train_loss: 0.79404, time: 0.01517\n",
      "Epoch: 188, train_loss: 0.79543, time: 0.01501\n",
      "Epoch: 189, train_loss: 0.79608, time: 0.01542\n",
      "Epoch: 190, train_loss: 0.79443, time: 0.01511\n",
      "Epoch: 191, train_loss: 0.79722, time: 0.01571\n",
      "Epoch: 192, train_loss: 0.79414, time: 0.01570\n",
      "Epoch: 193, train_loss: 0.79615, time: 0.01583\n",
      "Epoch: 194, train_loss: 0.79375, time: 0.01570\n",
      "Epoch: 195, train_loss: 0.79400, time: 0.01593\n",
      "Epoch: 196, train_loss: 0.79299, time: 0.01579\n",
      "Epoch: 197, train_loss: 0.79571, time: 0.01480\n",
      "Epoch: 198, train_loss: 0.79914, time: 0.01593\n",
      "Epoch: 199, train_loss: 0.79469, time: 0.01559\n",
      "Epoch: 200, train_loss: 0.79589, time: 0.01521\n",
      "pairwise precision 1.00000 recall 1.00000 f1 1.00000\n",
      "average until now [0.6212087429030548, 0.858641977733617, 0.720877986074449]\n",
      "14 names 122.3346164226532 avg time 8.738186887332372\n",
      "Loading qi_hu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 173 nodes, 590 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99787, time: 0.10869\n",
      "Epoch: 2, train_loss: 0.98770, time: 0.01152\n",
      "Epoch: 3, train_loss: 0.97937, time: 0.00996\n",
      "Epoch: 4, train_loss: 0.98559, time: 0.00956\n",
      "Epoch: 5, train_loss: 0.98051, time: 0.00954\n",
      "Epoch: 6, train_loss: 0.97997, time: 0.00959\n",
      "Epoch: 7, train_loss: 0.97601, time: 0.00945\n",
      "Epoch: 8, train_loss: 0.98096, time: 0.00951\n",
      "Epoch: 9, train_loss: 0.98085, time: 0.00961\n",
      "Epoch: 10, train_loss: 0.98204, time: 0.00946\n",
      "Epoch: 11, train_loss: 0.97769, time: 0.00948\n",
      "Epoch: 12, train_loss: 0.97845, time: 0.00934\n",
      "Epoch: 13, train_loss: 0.98039, time: 0.00951\n",
      "Epoch: 14, train_loss: 0.97933, time: 0.00948\n",
      "Epoch: 15, train_loss: 0.97787, time: 0.00944\n",
      "Epoch: 16, train_loss: 0.98109, time: 0.00955\n",
      "Epoch: 17, train_loss: 0.98056, time: 0.00956\n",
      "Epoch: 18, train_loss: 0.98272, time: 0.00952\n",
      "Epoch: 19, train_loss: 0.97665, time: 0.00951\n",
      "Epoch: 20, train_loss: 0.98325, time: 0.00941\n",
      "Epoch: 21, train_loss: 0.98005, time: 0.00947\n",
      "Epoch: 22, train_loss: 0.98130, time: 0.01002\n",
      "Epoch: 23, train_loss: 0.98057, time: 0.00957\n",
      "Epoch: 24, train_loss: 0.98085, time: 0.00953\n",
      "Epoch: 25, train_loss: 0.97990, time: 0.00956\n",
      "Epoch: 26, train_loss: 0.98205, time: 0.00949\n",
      "Epoch: 27, train_loss: 0.98115, time: 0.00936\n",
      "Epoch: 28, train_loss: 0.98139, time: 0.00946\n",
      "Epoch: 29, train_loss: 0.98036, time: 0.00954\n",
      "Epoch: 30, train_loss: 0.98047, time: 0.00959\n",
      "Epoch: 31, train_loss: 0.97611, time: 0.00965\n",
      "Epoch: 32, train_loss: 0.97928, time: 0.00967\n",
      "Epoch: 33, train_loss: 0.98216, time: 0.00935\n",
      "Epoch: 34, train_loss: 0.97985, time: 0.00958\n",
      "Epoch: 35, train_loss: 0.97922, time: 0.00963\n",
      "Epoch: 36, train_loss: 0.98165, time: 0.00953\n",
      "Epoch: 37, train_loss: 0.97946, time: 0.00939\n",
      "Epoch: 38, train_loss: 0.98021, time: 0.00955\n",
      "Epoch: 39, train_loss: 0.97996, time: 0.00940\n",
      "Epoch: 40, train_loss: 0.98100, time: 0.00946\n",
      "Epoch: 41, train_loss: 0.97992, time: 0.00944\n",
      "Epoch: 42, train_loss: 0.98175, time: 0.00951\n",
      "Epoch: 43, train_loss: 0.97818, time: 0.00952\n",
      "Epoch: 44, train_loss: 0.97788, time: 0.01021\n",
      "Epoch: 45, train_loss: 0.97899, time: 0.00986\n",
      "Epoch: 46, train_loss: 0.97902, time: 0.00963\n",
      "Epoch: 47, train_loss: 0.98009, time: 0.00960\n",
      "Epoch: 48, train_loss: 0.98099, time: 0.00950\n",
      "Epoch: 49, train_loss: 0.98229, time: 0.00933\n",
      "Epoch: 50, train_loss: 0.97816, time: 0.00946\n",
      "Epoch: 51, train_loss: 0.98295, time: 0.00936\n",
      "Epoch: 52, train_loss: 0.98007, time: 0.00951\n",
      "Epoch: 53, train_loss: 0.97723, time: 0.00942\n",
      "Epoch: 54, train_loss: 0.97788, time: 0.00944\n",
      "Epoch: 55, train_loss: 0.98460, time: 0.00929\n",
      "Epoch: 56, train_loss: 0.97663, time: 0.00960\n",
      "Epoch: 57, train_loss: 0.98177, time: 0.00944\n",
      "Epoch: 58, train_loss: 0.98025, time: 0.00954\n",
      "Epoch: 59, train_loss: 0.97809, time: 0.00951\n",
      "Epoch: 60, train_loss: 0.98373, time: 0.00944\n",
      "Epoch: 61, train_loss: 0.98143, time: 0.00957\n",
      "Epoch: 62, train_loss: 0.97916, time: 0.00933\n",
      "Epoch: 63, train_loss: 0.98411, time: 0.00948\n",
      "Epoch: 64, train_loss: 0.97902, time: 0.00944\n",
      "Epoch: 65, train_loss: 0.98274, time: 0.00924\n",
      "Epoch: 66, train_loss: 0.98229, time: 0.00991\n",
      "Epoch: 67, train_loss: 0.98545, time: 0.00948\n",
      "Epoch: 68, train_loss: 0.97817, time: 0.00940\n",
      "Epoch: 69, train_loss: 0.97899, time: 0.00961\n",
      "Epoch: 70, train_loss: 0.97761, time: 0.00950\n",
      "Epoch: 71, train_loss: 0.98204, time: 0.00936\n",
      "Epoch: 72, train_loss: 0.98210, time: 0.00964\n",
      "Epoch: 73, train_loss: 0.98257, time: 0.00941\n",
      "Epoch: 74, train_loss: 0.98300, time: 0.00957\n",
      "Epoch: 75, train_loss: 0.97837, time: 0.00943\n",
      "Epoch: 76, train_loss: 0.98108, time: 0.00941\n",
      "Epoch: 77, train_loss: 0.97477, time: 0.00944\n",
      "Epoch: 78, train_loss: 0.97855, time: 0.00938\n",
      "Epoch: 79, train_loss: 0.97940, time: 0.00948\n",
      "Epoch: 80, train_loss: 0.97787, time: 0.00944\n",
      "Epoch: 81, train_loss: 0.98009, time: 0.00944\n",
      "Epoch: 82, train_loss: 0.98167, time: 0.00940\n",
      "Epoch: 83, train_loss: 0.98179, time: 0.00948\n",
      "Epoch: 84, train_loss: 0.97839, time: 0.00924\n",
      "Epoch: 85, train_loss: 0.97831, time: 0.00952\n",
      "Epoch: 86, train_loss: 0.97916, time: 0.00956\n",
      "Epoch: 87, train_loss: 0.97885, time: 0.00952\n",
      "Epoch: 88, train_loss: 0.98391, time: 0.00991\n",
      "Epoch: 89, train_loss: 0.98711, time: 0.00961\n",
      "Epoch: 90, train_loss: 0.97551, time: 0.00965\n",
      "Epoch: 91, train_loss: 0.97435, time: 0.00950\n",
      "Epoch: 92, train_loss: 0.97926, time: 0.00964\n",
      "Epoch: 93, train_loss: 0.97902, time: 0.00947\n",
      "Epoch: 94, train_loss: 0.98111, time: 0.00943\n",
      "Epoch: 95, train_loss: 0.97995, time: 0.00934\n",
      "Epoch: 96, train_loss: 0.97707, time: 0.00938\n",
      "Epoch: 97, train_loss: 0.98173, time: 0.00948\n",
      "Epoch: 98, train_loss: 0.98106, time: 0.00941\n",
      "Epoch: 99, train_loss: 0.97667, time: 0.00942\n",
      "Epoch: 100, train_loss: 0.97750, time: 0.00957\n",
      "Epoch: 101, train_loss: 0.98269, time: 0.00934\n",
      "Epoch: 102, train_loss: 0.98121, time: 0.00938\n",
      "Epoch: 103, train_loss: 0.98029, time: 0.00955\n",
      "Epoch: 104, train_loss: 0.97898, time: 0.00940\n",
      "Epoch: 105, train_loss: 0.97980, time: 0.00959\n",
      "Epoch: 106, train_loss: 0.97940, time: 0.00945\n",
      "Epoch: 107, train_loss: 0.98341, time: 0.00934\n",
      "Epoch: 108, train_loss: 0.97927, time: 0.00935\n",
      "Epoch: 109, train_loss: 0.98116, time: 0.00937\n",
      "Epoch: 110, train_loss: 0.97834, time: 0.01025\n",
      "Epoch: 111, train_loss: 0.97611, time: 0.00984\n",
      "Epoch: 112, train_loss: 0.98178, time: 0.00977\n",
      "Epoch: 113, train_loss: 0.97644, time: 0.00949\n",
      "Epoch: 114, train_loss: 0.97661, time: 0.00937\n",
      "Epoch: 115, train_loss: 0.97743, time: 0.00987\n",
      "Epoch: 116, train_loss: 0.97963, time: 0.00944\n",
      "Epoch: 117, train_loss: 0.98200, time: 0.00964\n",
      "Epoch: 118, train_loss: 0.97907, time: 0.00952\n",
      "Epoch: 119, train_loss: 0.97726, time: 0.00956\n",
      "Epoch: 120, train_loss: 0.98250, time: 0.00954\n",
      "Epoch: 121, train_loss: 0.98247, time: 0.00975\n",
      "Epoch: 122, train_loss: 0.97739, time: 0.00967\n",
      "Epoch: 123, train_loss: 0.98087, time: 0.00961\n",
      "Epoch: 124, train_loss: 0.98292, time: 0.00961\n",
      "Epoch: 125, train_loss: 0.97867, time: 0.00951\n",
      "Epoch: 126, train_loss: 0.97948, time: 0.00923\n",
      "Epoch: 127, train_loss: 0.97766, time: 0.00922\n",
      "Epoch: 128, train_loss: 0.97660, time: 0.00937\n",
      "Epoch: 129, train_loss: 0.98131, time: 0.00951\n",
      "Epoch: 130, train_loss: 0.97897, time: 0.00961\n",
      "Epoch: 131, train_loss: 0.97606, time: 0.00977\n",
      "Epoch: 132, train_loss: 0.97867, time: 0.00956\n",
      "Epoch: 133, train_loss: 0.98069, time: 0.00953\n",
      "Epoch: 134, train_loss: 0.97853, time: 0.00952\n",
      "Epoch: 135, train_loss: 0.98023, time: 0.00954\n",
      "Epoch: 136, train_loss: 0.98039, time: 0.00950\n",
      "Epoch: 137, train_loss: 0.98366, time: 0.00955\n",
      "Epoch: 138, train_loss: 0.97745, time: 0.00962\n",
      "Epoch: 139, train_loss: 0.98089, time: 0.00960\n",
      "Epoch: 140, train_loss: 0.97690, time: 0.00960\n",
      "Epoch: 141, train_loss: 0.98117, time: 0.00953\n",
      "Epoch: 142, train_loss: 0.98131, time: 0.00960\n",
      "Epoch: 143, train_loss: 0.97977, time: 0.00940\n",
      "Epoch: 144, train_loss: 0.97935, time: 0.00946\n",
      "Epoch: 145, train_loss: 0.98207, time: 0.00951\n",
      "Epoch: 146, train_loss: 0.97852, time: 0.00952\n",
      "Epoch: 147, train_loss: 0.97753, time: 0.00954\n",
      "Epoch: 148, train_loss: 0.97854, time: 0.00946\n",
      "Epoch: 149, train_loss: 0.98011, time: 0.00944\n",
      "Epoch: 150, train_loss: 0.98081, time: 0.00949\n",
      "Epoch: 151, train_loss: 0.97673, time: 0.00944\n",
      "Epoch: 152, train_loss: 0.97798, time: 0.00957\n",
      "Epoch: 153, train_loss: 0.97456, time: 0.00990\n",
      "Epoch: 154, train_loss: 0.98313, time: 0.00959\n",
      "Epoch: 155, train_loss: 0.98120, time: 0.00950\n",
      "Epoch: 156, train_loss: 0.97791, time: 0.00934\n",
      "Epoch: 157, train_loss: 0.97881, time: 0.00949\n",
      "Epoch: 158, train_loss: 0.98176, time: 0.00939\n",
      "Epoch: 159, train_loss: 0.97905, time: 0.00937\n",
      "Epoch: 160, train_loss: 0.98183, time: 0.00944\n",
      "Epoch: 161, train_loss: 0.97996, time: 0.00944\n",
      "Epoch: 162, train_loss: 0.97958, time: 0.00933\n",
      "Epoch: 163, train_loss: 0.97716, time: 0.00944\n",
      "Epoch: 164, train_loss: 0.97997, time: 0.00944\n",
      "Epoch: 165, train_loss: 0.97733, time: 0.00937\n",
      "Epoch: 166, train_loss: 0.97887, time: 0.00954\n",
      "Epoch: 167, train_loss: 0.97651, time: 0.00947\n",
      "Epoch: 168, train_loss: 0.97875, time: 0.00948\n",
      "Epoch: 169, train_loss: 0.98064, time: 0.00937\n",
      "Epoch: 170, train_loss: 0.97803, time: 0.00933\n",
      "Epoch: 171, train_loss: 0.97505, time: 0.00952\n",
      "Epoch: 172, train_loss: 0.97959, time: 0.00942\n",
      "Epoch: 173, train_loss: 0.98083, time: 0.00945\n",
      "Epoch: 174, train_loss: 0.97536, time: 0.00954\n",
      "Epoch: 175, train_loss: 0.97994, time: 0.00998\n",
      "Epoch: 176, train_loss: 0.98063, time: 0.00958\n",
      "Epoch: 177, train_loss: 0.98106, time: 0.00949\n",
      "Epoch: 178, train_loss: 0.97841, time: 0.00947\n",
      "Epoch: 179, train_loss: 0.98495, time: 0.00943\n",
      "Epoch: 180, train_loss: 0.98146, time: 0.00949\n",
      "Epoch: 181, train_loss: 0.98006, time: 0.00941\n",
      "Epoch: 182, train_loss: 0.98037, time: 0.00964\n",
      "Epoch: 183, train_loss: 0.98004, time: 0.00956\n",
      "Epoch: 184, train_loss: 0.97928, time: 0.00953\n",
      "Epoch: 185, train_loss: 0.98232, time: 0.00953\n",
      "Epoch: 186, train_loss: 0.97939, time: 0.00949\n",
      "Epoch: 187, train_loss: 0.97987, time: 0.00954\n",
      "Epoch: 188, train_loss: 0.97481, time: 0.00965\n",
      "Epoch: 189, train_loss: 0.98024, time: 0.00936\n",
      "Epoch: 190, train_loss: 0.97749, time: 0.00930\n",
      "Epoch: 191, train_loss: 0.98118, time: 0.00947\n",
      "Epoch: 192, train_loss: 0.98138, time: 0.00965\n",
      "Epoch: 193, train_loss: 0.98316, time: 0.00939\n",
      "Epoch: 194, train_loss: 0.98008, time: 0.00941\n",
      "Epoch: 195, train_loss: 0.97982, time: 0.00954\n",
      "Epoch: 196, train_loss: 0.97757, time: 0.00959\n",
      "Epoch: 197, train_loss: 0.97811, time: 0.00986\n",
      "Epoch: 198, train_loss: 0.97923, time: 0.00979\n",
      "Epoch: 199, train_loss: 0.97732, time: 0.00960\n",
      "Epoch: 200, train_loss: 0.98060, time: 0.00966\n",
      "pairwise precision 0.11055 recall 0.81706 f1 0.19476\n",
      "average until now [0.5871650909565729, 0.8558697674533365, 0.6964999451329517]\n",
      "15 names 124.38119387626648 avg time 8.292079591751099\n",
      "Loading ping_fu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 287 nodes, 3514 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.96600, time: 0.11295\n",
      "Epoch: 2, train_loss: 0.95925, time: 0.01478\n",
      "Epoch: 3, train_loss: 0.95879, time: 0.01351\n",
      "Epoch: 4, train_loss: 0.95881, time: 0.01325\n",
      "Epoch: 5, train_loss: 0.95754, time: 0.01398\n",
      "Epoch: 6, train_loss: 0.95715, time: 0.01338\n",
      "Epoch: 7, train_loss: 0.95555, time: 0.01340\n",
      "Epoch: 8, train_loss: 0.95905, time: 0.01334\n",
      "Epoch: 9, train_loss: 0.95460, time: 0.01359\n",
      "Epoch: 10, train_loss: 0.95574, time: 0.01336\n",
      "Epoch: 11, train_loss: 0.95663, time: 0.01320\n",
      "Epoch: 12, train_loss: 0.95464, time: 0.01361\n",
      "Epoch: 13, train_loss: 0.95437, time: 0.01378\n",
      "Epoch: 14, train_loss: 0.95870, time: 0.01305\n",
      "Epoch: 15, train_loss: 0.95848, time: 0.01360\n",
      "Epoch: 16, train_loss: 0.96058, time: 0.01400\n",
      "Epoch: 17, train_loss: 0.95917, time: 0.01382\n",
      "Epoch: 18, train_loss: 0.95663, time: 0.01391\n",
      "Epoch: 19, train_loss: 0.95726, time: 0.01339\n",
      "Epoch: 20, train_loss: 0.95773, time: 0.01313\n",
      "Epoch: 21, train_loss: 0.95465, time: 0.01304\n",
      "Epoch: 22, train_loss: 0.95478, time: 0.01297\n",
      "Epoch: 23, train_loss: 0.95752, time: 0.01286\n",
      "Epoch: 24, train_loss: 0.95592, time: 0.01334\n",
      "Epoch: 25, train_loss: 0.95597, time: 0.01313\n",
      "Epoch: 26, train_loss: 0.95589, time: 0.01315\n",
      "Epoch: 27, train_loss: 0.95767, time: 0.01377\n",
      "Epoch: 28, train_loss: 0.95748, time: 0.01347\n",
      "Epoch: 29, train_loss: 0.95588, time: 0.01354\n",
      "Epoch: 30, train_loss: 0.95474, time: 0.01379\n",
      "Epoch: 31, train_loss: 0.95825, time: 0.01357\n",
      "Epoch: 32, train_loss: 0.95357, time: 0.01374\n",
      "Epoch: 33, train_loss: 0.95434, time: 0.01351\n",
      "Epoch: 34, train_loss: 0.95588, time: 0.01382\n",
      "Epoch: 35, train_loss: 0.95694, time: 0.01337\n",
      "Epoch: 36, train_loss: 0.95809, time: 0.01363\n",
      "Epoch: 37, train_loss: 0.95454, time: 0.01321\n",
      "Epoch: 38, train_loss: 0.95693, time: 0.01327\n",
      "Epoch: 39, train_loss: 0.95329, time: 0.01380\n",
      "Epoch: 40, train_loss: 0.95450, time: 0.01310\n",
      "Epoch: 41, train_loss: 0.95424, time: 0.01261\n",
      "Epoch: 42, train_loss: 0.95447, time: 0.01310\n",
      "Epoch: 43, train_loss: 0.95606, time: 0.01326\n",
      "Epoch: 44, train_loss: 0.95737, time: 0.01336\n",
      "Epoch: 45, train_loss: 0.95431, time: 0.01367\n",
      "Epoch: 46, train_loss: 0.95822, time: 0.01356\n",
      "Epoch: 47, train_loss: 0.95478, time: 0.01323\n",
      "Epoch: 48, train_loss: 0.95727, time: 0.01395\n",
      "Epoch: 49, train_loss: 0.95665, time: 0.01354\n",
      "Epoch: 50, train_loss: 0.95770, time: 0.01344\n",
      "Epoch: 51, train_loss: 0.95475, time: 0.01328\n",
      "Epoch: 52, train_loss: 0.95604, time: 0.01353\n",
      "Epoch: 53, train_loss: 0.95605, time: 0.01344\n",
      "Epoch: 54, train_loss: 0.95703, time: 0.01346\n",
      "Epoch: 55, train_loss: 0.95545, time: 0.01368\n",
      "Epoch: 56, train_loss: 0.95608, time: 0.01324\n",
      "Epoch: 57, train_loss: 0.95457, time: 0.01347\n",
      "Epoch: 58, train_loss: 0.95597, time: 0.01342\n",
      "Epoch: 59, train_loss: 0.95628, time: 0.01350\n",
      "Epoch: 60, train_loss: 0.95517, time: 0.01370\n",
      "Epoch: 61, train_loss: 0.95668, time: 0.01360\n",
      "Epoch: 62, train_loss: 0.95541, time: 0.01354\n",
      "Epoch: 63, train_loss: 0.95508, time: 0.01296\n",
      "Epoch: 64, train_loss: 0.95626, time: 0.01349\n",
      "Epoch: 65, train_loss: 0.95418, time: 0.01369\n",
      "Epoch: 66, train_loss: 0.95514, time: 0.01330\n",
      "Epoch: 67, train_loss: 0.95510, time: 0.01373\n",
      "Epoch: 68, train_loss: 0.95720, time: 0.01343\n",
      "Epoch: 69, train_loss: 0.95641, time: 0.01303\n",
      "Epoch: 70, train_loss: 0.95653, time: 0.01337\n",
      "Epoch: 71, train_loss: 0.95386, time: 0.01305\n",
      "Epoch: 72, train_loss: 0.95519, time: 0.01318\n",
      "Epoch: 73, train_loss: 0.95499, time: 0.01372\n",
      "Epoch: 74, train_loss: 0.95664, time: 0.01364\n",
      "Epoch: 75, train_loss: 0.95719, time: 0.01356\n",
      "Epoch: 76, train_loss: 0.95655, time: 0.01376\n",
      "Epoch: 77, train_loss: 0.95783, time: 0.01378\n",
      "Epoch: 78, train_loss: 0.95510, time: 0.01315\n",
      "Epoch: 79, train_loss: 0.95643, time: 0.01349\n",
      "Epoch: 80, train_loss: 0.95604, time: 0.01371\n",
      "Epoch: 81, train_loss: 0.95489, time: 0.01349\n",
      "Epoch: 82, train_loss: 0.95605, time: 0.01341\n",
      "Epoch: 83, train_loss: 0.95682, time: 0.01351\n",
      "Epoch: 84, train_loss: 0.95468, time: 0.01338\n",
      "Epoch: 85, train_loss: 0.95580, time: 0.01320\n",
      "Epoch: 86, train_loss: 0.95775, time: 0.01327\n",
      "Epoch: 87, train_loss: 0.95570, time: 0.01329\n",
      "Epoch: 88, train_loss: 0.95390, time: 0.01349\n",
      "Epoch: 89, train_loss: 0.95701, time: 0.01348\n",
      "Epoch: 90, train_loss: 0.95402, time: 0.01351\n",
      "Epoch: 91, train_loss: 0.95906, time: 0.01354\n",
      "Epoch: 92, train_loss: 0.95641, time: 0.01322\n",
      "Epoch: 93, train_loss: 0.95427, time: 0.01328\n",
      "Epoch: 94, train_loss: 0.95521, time: 0.01344\n",
      "Epoch: 95, train_loss: 0.95614, time: 0.01348\n",
      "Epoch: 96, train_loss: 0.95678, time: 0.01344\n",
      "Epoch: 97, train_loss: 0.95687, time: 0.01361\n",
      "Epoch: 98, train_loss: 0.95919, time: 0.01312\n",
      "Epoch: 99, train_loss: 0.95624, time: 0.01343\n",
      "Epoch: 100, train_loss: 0.95806, time: 0.01338\n",
      "Epoch: 101, train_loss: 0.95647, time: 0.01345\n",
      "Epoch: 102, train_loss: 0.95564, time: 0.01354\n",
      "Epoch: 103, train_loss: 0.95668, time: 0.01338\n",
      "Epoch: 104, train_loss: 0.95407, time: 0.01344\n",
      "Epoch: 105, train_loss: 0.95734, time: 0.01387\n",
      "Epoch: 106, train_loss: 0.95787, time: 0.01374\n",
      "Epoch: 107, train_loss: 0.95663, time: 0.01360\n",
      "Epoch: 108, train_loss: 0.95783, time: 0.01329\n",
      "Epoch: 109, train_loss: 0.95451, time: 0.01355\n",
      "Epoch: 110, train_loss: 0.95640, time: 0.01333\n",
      "Epoch: 111, train_loss: 0.95613, time: 0.01346\n",
      "Epoch: 112, train_loss: 0.95576, time: 0.01310\n",
      "Epoch: 113, train_loss: 0.95643, time: 0.01340\n",
      "Epoch: 114, train_loss: 0.95696, time: 0.01297\n",
      "Epoch: 115, train_loss: 0.95455, time: 0.01370\n",
      "Epoch: 116, train_loss: 0.95640, time: 0.01377\n",
      "Epoch: 117, train_loss: 0.95564, time: 0.01337\n",
      "Epoch: 118, train_loss: 0.95509, time: 0.01369\n",
      "Epoch: 119, train_loss: 0.95590, time: 0.01345\n",
      "Epoch: 120, train_loss: 0.95492, time: 0.01339\n",
      "Epoch: 121, train_loss: 0.95604, time: 0.01353\n",
      "Epoch: 122, train_loss: 0.95744, time: 0.01453\n",
      "Epoch: 123, train_loss: 0.95534, time: 0.01356\n",
      "Epoch: 124, train_loss: 0.95815, time: 0.01343\n",
      "Epoch: 125, train_loss: 0.95657, time: 0.01331\n",
      "Epoch: 126, train_loss: 0.95550, time: 0.01354\n",
      "Epoch: 127, train_loss: 0.95799, time: 0.01354\n",
      "Epoch: 128, train_loss: 0.95772, time: 0.01344\n",
      "Epoch: 129, train_loss: 0.95805, time: 0.01331\n",
      "Epoch: 130, train_loss: 0.95595, time: 0.01360\n",
      "Epoch: 131, train_loss: 0.95533, time: 0.01338\n",
      "Epoch: 132, train_loss: 0.95403, time: 0.01353\n",
      "Epoch: 133, train_loss: 0.95659, time: 0.01349\n",
      "Epoch: 134, train_loss: 0.95524, time: 0.01367\n",
      "Epoch: 135, train_loss: 0.95518, time: 0.01310\n",
      "Epoch: 136, train_loss: 0.95513, time: 0.01344\n",
      "Epoch: 137, train_loss: 0.95526, time: 0.01356\n",
      "Epoch: 138, train_loss: 0.95651, time: 0.01309\n",
      "Epoch: 139, train_loss: 0.95572, time: 0.01324\n",
      "Epoch: 140, train_loss: 0.95870, time: 0.01323\n",
      "Epoch: 141, train_loss: 0.95485, time: 0.01390\n",
      "Epoch: 142, train_loss: 0.95641, time: 0.01358\n",
      "Epoch: 143, train_loss: 0.95789, time: 0.01346\n",
      "Epoch: 144, train_loss: 0.95392, time: 0.01377\n",
      "Epoch: 145, train_loss: 0.95620, time: 0.01306\n",
      "Epoch: 146, train_loss: 0.95637, time: 0.01288\n",
      "Epoch: 147, train_loss: 0.95440, time: 0.01311\n",
      "Epoch: 148, train_loss: 0.95480, time: 0.01345\n",
      "Epoch: 149, train_loss: 0.95698, time: 0.01343\n",
      "Epoch: 150, train_loss: 0.95590, time: 0.01361\n",
      "Epoch: 151, train_loss: 0.95641, time: 0.01363\n",
      "Epoch: 152, train_loss: 0.95587, time: 0.01372\n",
      "Epoch: 153, train_loss: 0.95602, time: 0.01352\n",
      "Epoch: 154, train_loss: 0.95682, time: 0.01356\n",
      "Epoch: 155, train_loss: 0.95627, time: 0.01311\n",
      "Epoch: 156, train_loss: 0.95628, time: 0.01311\n",
      "Epoch: 157, train_loss: 0.95510, time: 0.01339\n",
      "Epoch: 158, train_loss: 0.95804, time: 0.01317\n",
      "Epoch: 159, train_loss: 0.95800, time: 0.01334\n",
      "Epoch: 160, train_loss: 0.95512, time: 0.01346\n",
      "Epoch: 161, train_loss: 0.95558, time: 0.01366\n",
      "Epoch: 162, train_loss: 0.95528, time: 0.01312\n",
      "Epoch: 163, train_loss: 0.95514, time: 0.01363\n",
      "Epoch: 164, train_loss: 0.95798, time: 0.01336\n",
      "Epoch: 165, train_loss: 0.95662, time: 0.01340\n",
      "Epoch: 166, train_loss: 0.95780, time: 0.01402\n",
      "Epoch: 167, train_loss: 0.95669, time: 0.01347\n",
      "Epoch: 168, train_loss: 0.95496, time: 0.01323\n",
      "Epoch: 169, train_loss: 0.95549, time: 0.01312\n",
      "Epoch: 170, train_loss: 0.95593, time: 0.01322\n",
      "Epoch: 171, train_loss: 0.95359, time: 0.01329\n",
      "Epoch: 172, train_loss: 0.95555, time: 0.01336\n",
      "Epoch: 173, train_loss: 0.95561, time: 0.01327\n",
      "Epoch: 174, train_loss: 0.95563, time: 0.01374\n",
      "Epoch: 175, train_loss: 0.95802, time: 0.01351\n",
      "Epoch: 176, train_loss: 0.95534, time: 0.01332\n",
      "Epoch: 177, train_loss: 0.95396, time: 0.01363\n",
      "Epoch: 178, train_loss: 0.95524, time: 0.01373\n",
      "Epoch: 179, train_loss: 0.95419, time: 0.01323\n",
      "Epoch: 180, train_loss: 0.95787, time: 0.01312\n",
      "Epoch: 181, train_loss: 0.95423, time: 0.01340\n",
      "Epoch: 182, train_loss: 0.95644, time: 0.01353\n",
      "Epoch: 183, train_loss: 0.95505, time: 0.01328\n",
      "Epoch: 184, train_loss: 0.95890, time: 0.01343\n",
      "Epoch: 185, train_loss: 0.95467, time: 0.01417\n",
      "Epoch: 186, train_loss: 0.95734, time: 0.01344\n",
      "Epoch: 187, train_loss: 0.95560, time: 0.01366\n",
      "Epoch: 188, train_loss: 0.95453, time: 0.01343\n",
      "Epoch: 189, train_loss: 0.95598, time: 0.01368\n",
      "Epoch: 190, train_loss: 0.95468, time: 0.01328\n",
      "Epoch: 191, train_loss: 0.95623, time: 0.01348\n",
      "Epoch: 192, train_loss: 0.95671, time: 0.01320\n",
      "Epoch: 193, train_loss: 0.95572, time: 0.01354\n",
      "Epoch: 194, train_loss: 0.95676, time: 0.01333\n",
      "Epoch: 195, train_loss: 0.95959, time: 0.01350\n",
      "Epoch: 196, train_loss: 0.95497, time: 0.01366\n",
      "Epoch: 197, train_loss: 0.95539, time: 0.01332\n",
      "Epoch: 198, train_loss: 0.95596, time: 0.01290\n",
      "Epoch: 199, train_loss: 0.95351, time: 0.01345\n",
      "Epoch: 200, train_loss: 0.95585, time: 0.01345\n",
      "pairwise precision 0.29613 recall 0.91811 f1 0.44782\n",
      "average until now [0.5689755708264841, 0.8597597412736774, 0.6847766488612228]\n",
      "16 names 127.2398374080658 avg time 7.952489838004112\n",
      "Loading geng_yang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 292 nodes, 4474 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.94630, time: 0.11367\n",
      "Epoch: 2, train_loss: 0.94921, time: 0.01558\n",
      "Epoch: 3, train_loss: 0.94622, time: 0.01503\n",
      "Epoch: 4, train_loss: 0.94708, time: 0.01417\n",
      "Epoch: 5, train_loss: 0.94815, time: 0.01422\n",
      "Epoch: 6, train_loss: 0.94736, time: 0.01454\n",
      "Epoch: 7, train_loss: 0.94567, time: 0.01351\n",
      "Epoch: 8, train_loss: 0.94820, time: 0.01426\n",
      "Epoch: 9, train_loss: 0.94342, time: 0.01418\n",
      "Epoch: 10, train_loss: 0.94500, time: 0.01370\n",
      "Epoch: 11, train_loss: 0.94687, time: 0.01425\n",
      "Epoch: 12, train_loss: 0.94668, time: 0.01436\n",
      "Epoch: 13, train_loss: 0.94430, time: 0.01410\n",
      "Epoch: 14, train_loss: 0.94812, time: 0.01443\n",
      "Epoch: 15, train_loss: 0.94551, time: 0.01469\n",
      "Epoch: 16, train_loss: 0.94696, time: 0.01436\n",
      "Epoch: 17, train_loss: 0.94414, time: 0.01386\n",
      "Epoch: 18, train_loss: 0.94538, time: 0.01448\n",
      "Epoch: 19, train_loss: 0.94630, time: 0.01429\n",
      "Epoch: 20, train_loss: 0.94657, time: 0.01444\n",
      "Epoch: 21, train_loss: 0.94710, time: 0.01412\n",
      "Epoch: 22, train_loss: 0.94702, time: 0.01396\n",
      "Epoch: 23, train_loss: 0.94539, time: 0.01392\n",
      "Epoch: 24, train_loss: 0.94708, time: 0.01447\n",
      "Epoch: 25, train_loss: 0.94842, time: 0.01437\n",
      "Epoch: 26, train_loss: 0.94873, time: 0.01377\n",
      "Epoch: 27, train_loss: 0.94552, time: 0.01406\n",
      "Epoch: 28, train_loss: 0.94624, time: 0.01433\n",
      "Epoch: 29, train_loss: 0.94377, time: 0.01412\n",
      "Epoch: 30, train_loss: 0.94696, time: 0.01449\n",
      "Epoch: 31, train_loss: 0.94583, time: 0.01412\n",
      "Epoch: 32, train_loss: 0.94709, time: 0.01435\n",
      "Epoch: 33, train_loss: 0.94711, time: 0.01422\n",
      "Epoch: 34, train_loss: 0.94552, time: 0.01446\n",
      "Epoch: 35, train_loss: 0.94451, time: 0.01417\n",
      "Epoch: 36, train_loss: 0.94677, time: 0.01392\n",
      "Epoch: 37, train_loss: 0.94727, time: 0.01423\n",
      "Epoch: 38, train_loss: 0.94739, time: 0.01412\n",
      "Epoch: 39, train_loss: 0.94641, time: 0.01415\n",
      "Epoch: 40, train_loss: 0.94806, time: 0.01433\n",
      "Epoch: 41, train_loss: 0.94555, time: 0.01419\n",
      "Epoch: 42, train_loss: 0.94453, time: 0.01435\n",
      "Epoch: 43, train_loss: 0.94766, time: 0.01420\n",
      "Epoch: 44, train_loss: 0.94813, time: 0.01448\n",
      "Epoch: 45, train_loss: 0.94559, time: 0.01398\n",
      "Epoch: 46, train_loss: 0.94784, time: 0.01412\n",
      "Epoch: 47, train_loss: 0.94699, time: 0.01392\n",
      "Epoch: 48, train_loss: 0.94541, time: 0.01413\n",
      "Epoch: 49, train_loss: 0.94606, time: 0.01426\n",
      "Epoch: 50, train_loss: 0.94777, time: 0.01383\n",
      "Epoch: 51, train_loss: 0.94814, time: 0.01424\n",
      "Epoch: 52, train_loss: 0.94625, time: 0.01432\n",
      "Epoch: 53, train_loss: 0.94773, time: 0.01404\n",
      "Epoch: 54, train_loss: 0.94502, time: 0.01417\n",
      "Epoch: 55, train_loss: 0.94711, time: 0.01461\n",
      "Epoch: 56, train_loss: 0.94859, time: 0.01400\n",
      "Epoch: 57, train_loss: 0.94762, time: 0.01454\n",
      "Epoch: 58, train_loss: 0.94753, time: 0.01396\n",
      "Epoch: 59, train_loss: 0.94700, time: 0.01406\n",
      "Epoch: 60, train_loss: 0.94722, time: 0.01409\n",
      "Epoch: 61, train_loss: 0.94661, time: 0.01436\n",
      "Epoch: 62, train_loss: 0.94519, time: 0.01427\n",
      "Epoch: 63, train_loss: 0.94768, time: 0.01432\n",
      "Epoch: 64, train_loss: 0.94618, time: 0.01371\n",
      "Epoch: 65, train_loss: 0.94660, time: 0.01345\n",
      "Epoch: 66, train_loss: 0.94702, time: 0.01407\n",
      "Epoch: 67, train_loss: 0.94779, time: 0.01417\n",
      "Epoch: 68, train_loss: 0.94485, time: 0.01413\n",
      "Epoch: 69, train_loss: 0.94792, time: 0.01437\n",
      "Epoch: 70, train_loss: 0.94550, time: 0.01408\n",
      "Epoch: 71, train_loss: 0.94726, time: 0.01428\n",
      "Epoch: 72, train_loss: 0.94653, time: 0.01404\n",
      "Epoch: 73, train_loss: 0.94452, time: 0.01372\n",
      "Epoch: 74, train_loss: 0.94402, time: 0.01426\n",
      "Epoch: 75, train_loss: 0.94724, time: 0.01440\n",
      "Epoch: 76, train_loss: 0.94741, time: 0.01427\n",
      "Epoch: 77, train_loss: 0.94577, time: 0.01391\n",
      "Epoch: 78, train_loss: 0.94734, time: 0.01391\n",
      "Epoch: 79, train_loss: 0.94510, time: 0.01396\n",
      "Epoch: 80, train_loss: 0.94638, time: 0.01367\n",
      "Epoch: 81, train_loss: 0.94487, time: 0.01362\n",
      "Epoch: 82, train_loss: 0.94555, time: 0.01369\n",
      "Epoch: 83, train_loss: 0.94807, time: 0.01403\n",
      "Epoch: 84, train_loss: 0.94566, time: 0.01422\n",
      "Epoch: 85, train_loss: 0.94606, time: 0.01377\n",
      "Epoch: 86, train_loss: 0.94898, time: 0.01381\n",
      "Epoch: 87, train_loss: 0.94868, time: 0.01412\n",
      "Epoch: 88, train_loss: 0.94611, time: 0.01422\n",
      "Epoch: 89, train_loss: 0.94458, time: 0.01380\n",
      "Epoch: 90, train_loss: 0.94552, time: 0.01446\n",
      "Epoch: 91, train_loss: 0.94618, time: 0.01440\n",
      "Epoch: 92, train_loss: 0.94482, time: 0.01424\n",
      "Epoch: 93, train_loss: 0.94856, time: 0.01407\n",
      "Epoch: 94, train_loss: 0.94699, time: 0.01395\n",
      "Epoch: 95, train_loss: 0.94773, time: 0.01427\n",
      "Epoch: 96, train_loss: 0.94623, time: 0.01458\n",
      "Epoch: 97, train_loss: 0.94706, time: 0.01380\n",
      "Epoch: 98, train_loss: 0.94625, time: 0.01398\n",
      "Epoch: 99, train_loss: 0.94806, time: 0.01396\n",
      "Epoch: 100, train_loss: 0.94871, time: 0.01392\n",
      "Epoch: 101, train_loss: 0.94592, time: 0.01410\n",
      "Epoch: 102, train_loss: 0.94621, time: 0.01401\n",
      "Epoch: 103, train_loss: 0.94608, time: 0.01425\n",
      "Epoch: 104, train_loss: 0.94607, time: 0.01431\n",
      "Epoch: 105, train_loss: 0.94771, time: 0.01442\n",
      "Epoch: 106, train_loss: 0.94616, time: 0.01429\n",
      "Epoch: 107, train_loss: 0.94536, time: 0.01371\n",
      "Epoch: 108, train_loss: 0.94550, time: 0.01445\n",
      "Epoch: 109, train_loss: 0.94741, time: 0.01402\n",
      "Epoch: 110, train_loss: 0.94628, time: 0.01419\n",
      "Epoch: 111, train_loss: 0.94462, time: 0.01443\n",
      "Epoch: 112, train_loss: 0.94645, time: 0.01396\n",
      "Epoch: 113, train_loss: 0.94565, time: 0.01423\n",
      "Epoch: 114, train_loss: 0.94545, time: 0.01399\n",
      "Epoch: 115, train_loss: 0.94773, time: 0.01391\n",
      "Epoch: 116, train_loss: 0.94582, time: 0.01410\n",
      "Epoch: 117, train_loss: 0.94527, time: 0.01391\n",
      "Epoch: 118, train_loss: 0.94576, time: 0.01434\n",
      "Epoch: 119, train_loss: 0.94621, time: 0.01420\n",
      "Epoch: 120, train_loss: 0.94554, time: 0.01523\n",
      "Epoch: 121, train_loss: 0.94691, time: 0.01457\n",
      "Epoch: 122, train_loss: 0.94440, time: 0.01404\n",
      "Epoch: 123, train_loss: 0.94512, time: 0.01481\n",
      "Epoch: 124, train_loss: 0.94740, time: 0.01411\n",
      "Epoch: 125, train_loss: 0.95020, time: 0.01406\n",
      "Epoch: 126, train_loss: 0.94736, time: 0.01422\n",
      "Epoch: 127, train_loss: 0.94608, time: 0.01440\n",
      "Epoch: 128, train_loss: 0.94442, time: 0.01465\n",
      "Epoch: 129, train_loss: 0.94568, time: 0.01461\n",
      "Epoch: 130, train_loss: 0.94450, time: 0.01410\n",
      "Epoch: 131, train_loss: 0.94655, time: 0.01410\n",
      "Epoch: 132, train_loss: 0.94694, time: 0.01429\n",
      "Epoch: 133, train_loss: 0.94423, time: 0.01440\n",
      "Epoch: 134, train_loss: 0.94468, time: 0.01457\n",
      "Epoch: 135, train_loss: 0.94711, time: 0.01426\n",
      "Epoch: 136, train_loss: 0.94640, time: 0.01424\n",
      "Epoch: 137, train_loss: 0.94558, time: 0.01381\n",
      "Epoch: 138, train_loss: 0.94695, time: 0.01477\n",
      "Epoch: 139, train_loss: 0.94671, time: 0.01408\n",
      "Epoch: 140, train_loss: 0.94804, time: 0.01442\n",
      "Epoch: 141, train_loss: 0.94730, time: 0.01412\n",
      "Epoch: 142, train_loss: 0.94575, time: 0.01359\n",
      "Epoch: 143, train_loss: 0.94462, time: 0.01393\n",
      "Epoch: 144, train_loss: 0.94909, time: 0.01370\n",
      "Epoch: 145, train_loss: 0.94397, time: 0.01420\n",
      "Epoch: 146, train_loss: 0.94455, time: 0.01431\n",
      "Epoch: 147, train_loss: 0.94766, time: 0.01409\n",
      "Epoch: 148, train_loss: 0.94643, time: 0.01399\n",
      "Epoch: 149, train_loss: 0.94642, time: 0.01438\n",
      "Epoch: 150, train_loss: 0.94801, time: 0.01446\n",
      "Epoch: 151, train_loss: 0.94656, time: 0.01414\n",
      "Epoch: 152, train_loss: 0.94604, time: 0.01479\n",
      "Epoch: 153, train_loss: 0.94438, time: 0.01449\n",
      "Epoch: 154, train_loss: 0.94768, time: 0.01426\n",
      "Epoch: 155, train_loss: 0.94559, time: 0.01442\n",
      "Epoch: 156, train_loss: 0.94678, time: 0.01428\n",
      "Epoch: 157, train_loss: 0.94639, time: 0.01399\n",
      "Epoch: 158, train_loss: 0.94616, time: 0.01439\n",
      "Epoch: 159, train_loss: 0.94561, time: 0.01405\n",
      "Epoch: 160, train_loss: 0.94449, time: 0.01454\n",
      "Epoch: 161, train_loss: 0.94790, time: 0.01421\n",
      "Epoch: 162, train_loss: 0.94914, time: 0.01415\n",
      "Epoch: 163, train_loss: 0.94766, time: 0.01407\n",
      "Epoch: 164, train_loss: 0.94876, time: 0.01449\n",
      "Epoch: 165, train_loss: 0.94654, time: 0.01448\n",
      "Epoch: 166, train_loss: 0.94857, time: 0.01422\n",
      "Epoch: 167, train_loss: 0.94704, time: 0.01420\n",
      "Epoch: 168, train_loss: 0.94645, time: 0.01468\n",
      "Epoch: 169, train_loss: 0.94561, time: 0.01441\n",
      "Epoch: 170, train_loss: 0.94786, time: 0.01415\n",
      "Epoch: 171, train_loss: 0.94676, time: 0.01393\n",
      "Epoch: 172, train_loss: 0.94822, time: 0.01409\n",
      "Epoch: 173, train_loss: 0.94736, time: 0.01439\n",
      "Epoch: 174, train_loss: 0.94590, time: 0.01401\n",
      "Epoch: 175, train_loss: 0.94610, time: 0.01460\n",
      "Epoch: 176, train_loss: 0.94790, time: 0.01405\n",
      "Epoch: 177, train_loss: 0.94707, time: 0.01447\n",
      "Epoch: 178, train_loss: 0.94558, time: 0.01416\n",
      "Epoch: 179, train_loss: 0.94823, time: 0.01432\n",
      "Epoch: 180, train_loss: 0.94553, time: 0.01457\n",
      "Epoch: 181, train_loss: 0.94693, time: 0.01427\n",
      "Epoch: 182, train_loss: 0.94429, time: 0.01412\n",
      "Epoch: 183, train_loss: 0.94522, time: 0.01434\n",
      "Epoch: 184, train_loss: 0.94890, time: 0.01396\n",
      "Epoch: 185, train_loss: 0.94827, time: 0.01469\n",
      "Epoch: 186, train_loss: 0.94607, time: 0.01451\n",
      "Epoch: 187, train_loss: 0.94681, time: 0.01385\n",
      "Epoch: 188, train_loss: 0.94511, time: 0.01397\n",
      "Epoch: 189, train_loss: 0.94581, time: 0.01432\n",
      "Epoch: 190, train_loss: 0.94473, time: 0.01416\n",
      "Epoch: 191, train_loss: 0.94422, time: 0.01423\n",
      "Epoch: 192, train_loss: 0.94451, time: 0.01455\n",
      "Epoch: 193, train_loss: 0.94600, time: 0.01413\n",
      "Epoch: 194, train_loss: 0.94810, time: 0.01433\n",
      "Epoch: 195, train_loss: 0.94665, time: 0.01451\n",
      "Epoch: 196, train_loss: 0.94471, time: 0.01415\n",
      "Epoch: 197, train_loss: 0.94640, time: 0.01433\n",
      "Epoch: 198, train_loss: 0.94835, time: 0.01449\n",
      "Epoch: 199, train_loss: 0.94592, time: 0.01406\n",
      "Epoch: 200, train_loss: 0.94669, time: 0.01445\n",
      "pairwise precision 0.43194 recall 0.99786 f1 0.60290\n",
      "average until now [0.5609146236216114, 0.8678831932278724, 0.681423738105021]\n",
      "17 names 130.25218677520752 avg time 7.661893339718089\n",
      "Loading li_zhu_wu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 224 nodes, 9549 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.80912, time: 0.11743\n",
      "Epoch: 2, train_loss: 0.80866, time: 0.01842\n",
      "Epoch: 3, train_loss: 0.80988, time: 0.01735\n",
      "Epoch: 4, train_loss: 0.81054, time: 0.01627\n",
      "Epoch: 5, train_loss: 0.81111, time: 0.01644\n",
      "Epoch: 6, train_loss: 0.80552, time: 0.01604\n",
      "Epoch: 7, train_loss: 0.80944, time: 0.01655\n",
      "Epoch: 8, train_loss: 0.80748, time: 0.01633\n",
      "Epoch: 9, train_loss: 0.80838, time: 0.01632\n",
      "Epoch: 10, train_loss: 0.80910, time: 0.01561\n",
      "Epoch: 11, train_loss: 0.80805, time: 0.01557\n",
      "Epoch: 12, train_loss: 0.80796, time: 0.01657\n",
      "Epoch: 13, train_loss: 0.81155, time: 0.01625\n",
      "Epoch: 14, train_loss: 0.80669, time: 0.01595\n",
      "Epoch: 15, train_loss: 0.80778, time: 0.01610\n",
      "Epoch: 16, train_loss: 0.80899, time: 0.01632\n",
      "Epoch: 17, train_loss: 0.80932, time: 0.01655\n",
      "Epoch: 18, train_loss: 0.80976, time: 0.01682\n",
      "Epoch: 19, train_loss: 0.81009, time: 0.01620\n",
      "Epoch: 20, train_loss: 0.80741, time: 0.01640\n",
      "Epoch: 21, train_loss: 0.80666, time: 0.01641\n",
      "Epoch: 22, train_loss: 0.80893, time: 0.01704\n",
      "Epoch: 23, train_loss: 0.80847, time: 0.01697\n",
      "Epoch: 24, train_loss: 0.80774, time: 0.01612\n",
      "Epoch: 25, train_loss: 0.80859, time: 0.01627\n",
      "Epoch: 26, train_loss: 0.80629, time: 0.01605\n",
      "Epoch: 27, train_loss: 0.80766, time: 0.01678\n",
      "Epoch: 28, train_loss: 0.80882, time: 0.01643\n",
      "Epoch: 29, train_loss: 0.80854, time: 0.01637\n",
      "Epoch: 30, train_loss: 0.80769, time: 0.01641\n",
      "Epoch: 31, train_loss: 0.80945, time: 0.01604\n",
      "Epoch: 32, train_loss: 0.81051, time: 0.01640\n",
      "Epoch: 33, train_loss: 0.81037, time: 0.01614\n",
      "Epoch: 34, train_loss: 0.80998, time: 0.01655\n",
      "Epoch: 35, train_loss: 0.80658, time: 0.01667\n",
      "Epoch: 36, train_loss: 0.80846, time: 0.01647\n",
      "Epoch: 37, train_loss: 0.80771, time: 0.01730\n",
      "Epoch: 38, train_loss: 0.80848, time: 0.01652\n",
      "Epoch: 39, train_loss: 0.80578, time: 0.01589\n",
      "Epoch: 40, train_loss: 0.80710, time: 0.01602\n",
      "Epoch: 41, train_loss: 0.80868, time: 0.01594\n",
      "Epoch: 42, train_loss: 0.80711, time: 0.01593\n",
      "Epoch: 43, train_loss: 0.81133, time: 0.01604\n",
      "Epoch: 44, train_loss: 0.80997, time: 0.01613\n",
      "Epoch: 45, train_loss: 0.80777, time: 0.01599\n",
      "Epoch: 46, train_loss: 0.80908, time: 0.01674\n",
      "Epoch: 47, train_loss: 0.80754, time: 0.01626\n",
      "Epoch: 48, train_loss: 0.80800, time: 0.01637\n",
      "Epoch: 49, train_loss: 0.80661, time: 0.01566\n",
      "Epoch: 50, train_loss: 0.80977, time: 0.01574\n",
      "Epoch: 51, train_loss: 0.80825, time: 0.01615\n",
      "Epoch: 52, train_loss: 0.80584, time: 0.01592\n",
      "Epoch: 53, train_loss: 0.80979, time: 0.01694\n",
      "Epoch: 54, train_loss: 0.80818, time: 0.01682\n",
      "Epoch: 55, train_loss: 0.81307, time: 0.01626\n",
      "Epoch: 56, train_loss: 0.80700, time: 0.01580\n",
      "Epoch: 57, train_loss: 0.80763, time: 0.01628\n",
      "Epoch: 58, train_loss: 0.80904, time: 0.01602\n",
      "Epoch: 59, train_loss: 0.80734, time: 0.01601\n",
      "Epoch: 60, train_loss: 0.80501, time: 0.01562\n",
      "Epoch: 61, train_loss: 0.80666, time: 0.01641\n",
      "Epoch: 62, train_loss: 0.81086, time: 0.01733\n",
      "Epoch: 63, train_loss: 0.80994, time: 0.01675\n",
      "Epoch: 64, train_loss: 0.81551, time: 0.01629\n",
      "Epoch: 65, train_loss: 0.80965, time: 0.01661\n",
      "Epoch: 66, train_loss: 0.80852, time: 0.01724\n",
      "Epoch: 67, train_loss: 0.80106, time: 0.01624\n",
      "Epoch: 68, train_loss: 0.80803, time: 0.01676\n",
      "Epoch: 69, train_loss: 0.80725, time: 0.01686\n",
      "Epoch: 70, train_loss: 0.80722, time: 0.01701\n",
      "Epoch: 71, train_loss: 0.80438, time: 0.01703\n",
      "Epoch: 72, train_loss: 0.80806, time: 0.01746\n",
      "Epoch: 73, train_loss: 0.80556, time: 0.01642\n",
      "Epoch: 74, train_loss: 0.80396, time: 0.01619\n",
      "Epoch: 75, train_loss: 0.79879, time: 0.01650\n",
      "Epoch: 76, train_loss: 0.80575, time: 0.01682\n",
      "Epoch: 77, train_loss: 0.79681, time: 0.01682\n",
      "Epoch: 78, train_loss: 0.79777, time: 0.01685\n",
      "Epoch: 79, train_loss: 0.80547, time: 0.01634\n",
      "Epoch: 80, train_loss: 0.79510, time: 0.01684\n",
      "Epoch: 81, train_loss: 0.79579, time: 0.01677\n",
      "Epoch: 82, train_loss: 0.79621, time: 0.01740\n",
      "Epoch: 83, train_loss: 0.82436, time: 0.01684\n",
      "Epoch: 84, train_loss: 0.79572, time: 0.01610\n",
      "Epoch: 85, train_loss: 0.80182, time: 0.01558\n",
      "Epoch: 86, train_loss: 0.80399, time: 0.01661\n",
      "Epoch: 87, train_loss: 0.80557, time: 0.01562\n",
      "Epoch: 88, train_loss: 0.79172, time: 0.01602\n",
      "Epoch: 89, train_loss: 0.79514, time: 0.01684\n",
      "Epoch: 90, train_loss: 0.80488, time: 0.01620\n",
      "Epoch: 91, train_loss: 0.79788, time: 0.01677\n",
      "Epoch: 92, train_loss: 0.82468, time: 0.01744\n",
      "Epoch: 93, train_loss: 0.79984, time: 0.01668\n",
      "Epoch: 94, train_loss: 0.80204, time: 0.01721\n",
      "Epoch: 95, train_loss: 0.79719, time: 0.01713\n",
      "Epoch: 96, train_loss: 0.78726, time: 0.01632\n",
      "Epoch: 97, train_loss: 0.79158, time: 0.01667\n",
      "Epoch: 98, train_loss: 0.79406, time: 0.01708\n",
      "Epoch: 99, train_loss: 0.79379, time: 0.01676\n",
      "Epoch: 100, train_loss: 0.79819, time: 0.01575\n",
      "Epoch: 101, train_loss: 0.79111, time: 0.01693\n",
      "Epoch: 102, train_loss: 0.79476, time: 0.01671\n",
      "Epoch: 103, train_loss: 0.79139, time: 0.01775\n",
      "Epoch: 104, train_loss: 0.79645, time: 0.01675\n",
      "Epoch: 105, train_loss: 0.79620, time: 0.01630\n",
      "Epoch: 106, train_loss: 0.78794, time: 0.01684\n",
      "Epoch: 107, train_loss: 0.79116, time: 0.01685\n",
      "Epoch: 108, train_loss: 0.78844, time: 0.01612\n",
      "Epoch: 109, train_loss: 0.78397, time: 0.01672\n",
      "Epoch: 110, train_loss: 0.82307, time: 0.01695\n",
      "Epoch: 111, train_loss: 0.80106, time: 0.01669\n",
      "Epoch: 112, train_loss: 0.78223, time: 0.01645\n",
      "Epoch: 113, train_loss: 0.80826, time: 0.01660\n",
      "Epoch: 114, train_loss: 0.78653, time: 0.01633\n",
      "Epoch: 115, train_loss: 0.79070, time: 0.01623\n",
      "Epoch: 116, train_loss: 0.79228, time: 0.01607\n",
      "Epoch: 117, train_loss: 0.79727, time: 0.01671\n",
      "Epoch: 118, train_loss: 0.79329, time: 0.01594\n",
      "Epoch: 119, train_loss: 0.78957, time: 0.01619\n",
      "Epoch: 120, train_loss: 0.78022, time: 0.01645\n",
      "Epoch: 121, train_loss: 0.80535, time: 0.01673\n",
      "Epoch: 122, train_loss: 0.78963, time: 0.01621\n",
      "Epoch: 123, train_loss: 0.79634, time: 0.01667\n",
      "Epoch: 124, train_loss: 0.79009, time: 0.01651\n",
      "Epoch: 125, train_loss: 0.78791, time: 0.01621\n",
      "Epoch: 126, train_loss: 0.83027, time: 0.01660\n",
      "Epoch: 127, train_loss: 0.78710, time: 0.01641\n",
      "Epoch: 128, train_loss: 0.79492, time: 0.01613\n",
      "Epoch: 129, train_loss: 0.80024, time: 0.01675\n",
      "Epoch: 130, train_loss: 0.82742, time: 0.01638\n",
      "Epoch: 131, train_loss: 0.78300, time: 0.01639\n",
      "Epoch: 132, train_loss: 0.78894, time: 0.01623\n",
      "Epoch: 133, train_loss: 0.78889, time: 0.01666\n",
      "Epoch: 134, train_loss: 0.78922, time: 0.01647\n",
      "Epoch: 135, train_loss: 0.78808, time: 0.01573\n",
      "Epoch: 136, train_loss: 0.80543, time: 0.01596\n",
      "Epoch: 137, train_loss: 0.79792, time: 0.01559\n",
      "Epoch: 138, train_loss: 0.79435, time: 0.01671\n",
      "Epoch: 139, train_loss: 0.79029, time: 0.01585\n",
      "Epoch: 140, train_loss: 0.78923, time: 0.01646\n",
      "Epoch: 141, train_loss: 0.78377, time: 0.01601\n",
      "Epoch: 142, train_loss: 0.79886, time: 0.01573\n",
      "Epoch: 143, train_loss: 0.78292, time: 0.01573\n",
      "Epoch: 144, train_loss: 0.80983, time: 0.01630\n",
      "Epoch: 145, train_loss: 0.83279, time: 0.01612\n",
      "Epoch: 146, train_loss: 0.79657, time: 0.01621\n",
      "Epoch: 147, train_loss: 0.78671, time: 0.01592\n",
      "Epoch: 148, train_loss: 0.79337, time: 0.01602\n",
      "Epoch: 149, train_loss: 0.79361, time: 0.01586\n",
      "Epoch: 150, train_loss: 0.79332, time: 0.01599\n",
      "Epoch: 151, train_loss: 0.79607, time: 0.01569\n",
      "Epoch: 152, train_loss: 0.81363, time: 0.01582\n",
      "Epoch: 153, train_loss: 0.80072, time: 0.01606\n",
      "Epoch: 154, train_loss: 0.79213, time: 0.01582\n",
      "Epoch: 155, train_loss: 0.80244, time: 0.01620\n",
      "Epoch: 156, train_loss: 0.80100, time: 0.01725\n",
      "Epoch: 157, train_loss: 0.78418, time: 0.01646\n",
      "Epoch: 158, train_loss: 0.80070, time: 0.01608\n",
      "Epoch: 159, train_loss: 0.79390, time: 0.01634\n",
      "Epoch: 160, train_loss: 0.80893, time: 0.01601\n",
      "Epoch: 161, train_loss: 0.80387, time: 0.01754\n",
      "Epoch: 162, train_loss: 0.79960, time: 0.01682\n",
      "Epoch: 163, train_loss: 0.79672, time: 0.01628\n",
      "Epoch: 164, train_loss: 0.78678, time: 0.01617\n",
      "Epoch: 165, train_loss: 0.77840, time: 0.01668\n",
      "Epoch: 166, train_loss: 0.80203, time: 0.01679\n",
      "Epoch: 167, train_loss: 0.80561, time: 0.01641\n",
      "Epoch: 168, train_loss: 0.78911, time: 0.01777\n",
      "Epoch: 169, train_loss: 0.79223, time: 0.01684\n",
      "Epoch: 170, train_loss: 0.85930, time: 0.01652\n",
      "Epoch: 171, train_loss: 0.79230, time: 0.01658\n",
      "Epoch: 172, train_loss: 0.78530, time: 0.01627\n",
      "Epoch: 173, train_loss: 0.81150, time: 0.01722\n",
      "Epoch: 174, train_loss: 0.78441, time: 0.01672\n",
      "Epoch: 175, train_loss: 0.80647, time: 0.01726\n",
      "Epoch: 176, train_loss: 0.78882, time: 0.01714\n",
      "Epoch: 177, train_loss: 0.77914, time: 0.01619\n",
      "Epoch: 178, train_loss: 0.79884, time: 0.01552\n",
      "Epoch: 179, train_loss: 0.79824, time: 0.01574\n",
      "Epoch: 180, train_loss: 0.79437, time: 0.01593\n",
      "Epoch: 181, train_loss: 0.79210, time: 0.01682\n",
      "Epoch: 182, train_loss: 0.78167, time: 0.01673\n",
      "Epoch: 183, train_loss: 0.79809, time: 0.01625\n",
      "Epoch: 184, train_loss: 0.81119, time: 0.01692\n",
      "Epoch: 185, train_loss: 0.78796, time: 0.01685\n",
      "Epoch: 186, train_loss: 0.79458, time: 0.01629\n",
      "Epoch: 187, train_loss: 0.80798, time: 0.01608\n",
      "Epoch: 188, train_loss: 0.79331, time: 0.01604\n",
      "Epoch: 189, train_loss: 0.78889, time: 0.01631\n",
      "Epoch: 190, train_loss: 0.78943, time: 0.01656\n",
      "Epoch: 191, train_loss: 0.79465, time: 0.01689\n",
      "Epoch: 192, train_loss: 0.78565, time: 0.01655\n",
      "Epoch: 193, train_loss: 0.77877, time: 0.01684\n",
      "Epoch: 194, train_loss: 0.78502, time: 0.01662\n",
      "Epoch: 195, train_loss: 0.80660, time: 0.01629\n",
      "Epoch: 196, train_loss: 0.78343, time: 0.01628\n",
      "Epoch: 197, train_loss: 0.79458, time: 0.01602\n",
      "Epoch: 198, train_loss: 0.78366, time: 0.01563\n",
      "Epoch: 199, train_loss: 0.81670, time: 0.01610\n",
      "Epoch: 200, train_loss: 0.82469, time: 0.01640\n",
      "pairwise precision 0.93250 recall 0.50710 f1 0.65695\n",
      "average until now [0.5815583863001014, 0.8478395766255168, 0.689896346311436]\n",
      "18 names 133.70820355415344 avg time 7.4282335307863026\n",
      "Loading da_xing dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 596 nodes, 44234 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.87399, time: 0.15576\n",
      "Epoch: 2, train_loss: 0.87454, time: 0.05684\n",
      "Epoch: 3, train_loss: 0.87398, time: 0.05812\n",
      "Epoch: 4, train_loss: 0.87285, time: 0.05683\n",
      "Epoch: 5, train_loss: 0.87388, time: 0.05447\n",
      "Epoch: 6, train_loss: 0.87378, time: 0.05849\n",
      "Epoch: 7, train_loss: 0.87290, time: 0.05755\n",
      "Epoch: 8, train_loss: 0.87455, time: 0.05649\n",
      "Epoch: 9, train_loss: 0.87382, time: 0.05660\n",
      "Epoch: 10, train_loss: 0.87455, time: 0.05556\n",
      "Epoch: 11, train_loss: 0.87430, time: 0.05697\n",
      "Epoch: 12, train_loss: 0.87584, time: 0.05549\n",
      "Epoch: 13, train_loss: 0.87477, time: 0.05610\n",
      "Epoch: 14, train_loss: 0.87374, time: 0.05576\n",
      "Epoch: 15, train_loss: 0.87422, time: 0.05633\n",
      "Epoch: 16, train_loss: 0.87255, time: 0.05675\n",
      "Epoch: 17, train_loss: 0.87403, time: 0.05653\n",
      "Epoch: 18, train_loss: 0.87378, time: 0.05529\n",
      "Epoch: 19, train_loss: 0.87387, time: 0.05609\n",
      "Epoch: 20, train_loss: 0.87420, time: 0.05676\n",
      "Epoch: 21, train_loss: 0.87418, time: 0.05595\n",
      "Epoch: 22, train_loss: 0.87386, time: 0.05599\n",
      "Epoch: 23, train_loss: 0.87329, time: 0.05640\n",
      "Epoch: 24, train_loss: 0.87249, time: 0.05656\n",
      "Epoch: 25, train_loss: 0.87366, time: 0.05434\n",
      "Epoch: 26, train_loss: 0.87330, time: 0.05575\n",
      "Epoch: 27, train_loss: 0.87321, time: 0.05561\n",
      "Epoch: 28, train_loss: 0.87320, time: 0.05610\n",
      "Epoch: 29, train_loss: 0.87406, time: 0.05607\n",
      "Epoch: 30, train_loss: 0.87380, time: 0.05589\n",
      "Epoch: 31, train_loss: 0.87363, time: 0.05596\n",
      "Epoch: 32, train_loss: 0.87378, time: 0.05602\n",
      "Epoch: 33, train_loss: 0.87397, time: 0.05682\n",
      "Epoch: 34, train_loss: 0.87409, time: 0.05689\n",
      "Epoch: 35, train_loss: 0.87380, time: 0.05582\n",
      "Epoch: 36, train_loss: 0.87322, time: 0.05448\n",
      "Epoch: 37, train_loss: 0.87280, time: 0.05541\n",
      "Epoch: 38, train_loss: 0.87199, time: 0.05510\n",
      "Epoch: 39, train_loss: 0.87241, time: 0.05520\n",
      "Epoch: 40, train_loss: 0.87252, time: 0.05618\n",
      "Epoch: 41, train_loss: 0.87399, time: 0.05651\n",
      "Epoch: 42, train_loss: 0.87303, time: 0.05568\n",
      "Epoch: 43, train_loss: 0.87399, time: 0.05510\n",
      "Epoch: 44, train_loss: 0.87343, time: 0.05511\n",
      "Epoch: 45, train_loss: 0.87390, time: 0.05653\n",
      "Epoch: 46, train_loss: 0.87384, time: 0.05543\n",
      "Epoch: 47, train_loss: 0.87407, time: 0.05577\n",
      "Epoch: 48, train_loss: 0.87333, time: 0.05830\n",
      "Epoch: 49, train_loss: 0.87253, time: 0.05351\n",
      "Epoch: 50, train_loss: 0.87340, time: 0.05482\n",
      "Epoch: 51, train_loss: 0.87314, time: 0.05688\n",
      "Epoch: 52, train_loss: 0.87302, time: 0.05579\n",
      "Epoch: 53, train_loss: 0.87378, time: 0.05576\n",
      "Epoch: 54, train_loss: 0.87377, time: 0.05779\n",
      "Epoch: 55, train_loss: 0.87257, time: 0.05664\n",
      "Epoch: 56, train_loss: 0.87357, time: 0.05786\n",
      "Epoch: 57, train_loss: 0.87326, time: 0.05688\n",
      "Epoch: 58, train_loss: 0.87345, time: 0.05669\n",
      "Epoch: 59, train_loss: 0.87484, time: 0.05681\n",
      "Epoch: 60, train_loss: 0.87371, time: 0.05518\n",
      "Epoch: 61, train_loss: 0.87399, time: 0.05407\n",
      "Epoch: 62, train_loss: 0.87210, time: 0.05457\n",
      "Epoch: 63, train_loss: 0.87172, time: 0.05746\n",
      "Epoch: 64, train_loss: 0.87282, time: 0.05734\n",
      "Epoch: 65, train_loss: 0.87338, time: 0.05712\n",
      "Epoch: 66, train_loss: 0.87291, time: 0.05636\n",
      "Epoch: 67, train_loss: 0.87391, time: 0.05464\n",
      "Epoch: 68, train_loss: 0.87371, time: 0.05558\n",
      "Epoch: 69, train_loss: 0.87376, time: 0.05510\n",
      "Epoch: 70, train_loss: 0.87229, time: 0.05581\n",
      "Epoch: 71, train_loss: 0.87387, time: 0.05538\n",
      "Epoch: 72, train_loss: 0.87293, time: 0.05510\n",
      "Epoch: 73, train_loss: 0.87302, time: 0.05607\n",
      "Epoch: 74, train_loss: 0.87340, time: 0.05567\n",
      "Epoch: 75, train_loss: 0.87398, time: 0.05636\n",
      "Epoch: 76, train_loss: 0.87296, time: 0.05570\n",
      "Epoch: 77, train_loss: 0.87324, time: 0.05641\n",
      "Epoch: 78, train_loss: 0.87332, time: 0.05539\n",
      "Epoch: 79, train_loss: 0.87398, time: 0.05617\n",
      "Epoch: 80, train_loss: 0.87242, time: 0.05480\n",
      "Epoch: 81, train_loss: 0.87387, time: 0.05682\n",
      "Epoch: 82, train_loss: 0.87249, time: 0.05458\n",
      "Epoch: 83, train_loss: 0.87393, time: 0.05476\n",
      "Epoch: 84, train_loss: 0.87335, time: 0.05537\n",
      "Epoch: 85, train_loss: 0.87367, time: 0.05705\n",
      "Epoch: 86, train_loss: 0.87438, time: 0.05536\n",
      "Epoch: 87, train_loss: 0.87432, time: 0.05668\n",
      "Epoch: 88, train_loss: 0.87379, time: 0.05657\n",
      "Epoch: 89, train_loss: 0.87336, time: 0.05625\n",
      "Epoch: 90, train_loss: 0.87363, time: 0.05660\n",
      "Epoch: 91, train_loss: 0.87302, time: 0.05638\n",
      "Epoch: 92, train_loss: 0.87354, time: 0.05596\n",
      "Epoch: 93, train_loss: 0.87344, time: 0.05555\n",
      "Epoch: 94, train_loss: 0.87479, time: 0.05692\n",
      "Epoch: 95, train_loss: 0.87414, time: 0.05694\n",
      "Epoch: 96, train_loss: 0.87305, time: 0.05550\n",
      "Epoch: 97, train_loss: 0.87276, time: 0.05576\n",
      "Epoch: 98, train_loss: 0.87361, time: 0.05625\n",
      "Epoch: 99, train_loss: 0.87451, time: 0.05489\n",
      "Epoch: 100, train_loss: 0.87314, time: 0.05547\n",
      "Epoch: 101, train_loss: 0.87283, time: 0.05621\n",
      "Epoch: 102, train_loss: 0.87363, time: 0.05557\n",
      "Epoch: 103, train_loss: 0.87435, time: 0.05594\n",
      "Epoch: 104, train_loss: 0.87362, time: 0.05529\n",
      "Epoch: 105, train_loss: 0.87365, time: 0.05645\n",
      "Epoch: 106, train_loss: 0.87307, time: 0.05515\n",
      "Epoch: 107, train_loss: 0.87439, time: 0.05648\n",
      "Epoch: 108, train_loss: 0.87416, time: 0.05526\n",
      "Epoch: 109, train_loss: 0.87378, time: 0.05622\n",
      "Epoch: 110, train_loss: 0.87280, time: 0.05624\n",
      "Epoch: 111, train_loss: 0.87304, time: 0.05722\n",
      "Epoch: 112, train_loss: 0.87420, time: 0.05556\n",
      "Epoch: 113, train_loss: 0.87333, time: 0.05611\n",
      "Epoch: 114, train_loss: 0.87405, time: 0.05670\n",
      "Epoch: 115, train_loss: 0.87347, time: 0.05535\n",
      "Epoch: 116, train_loss: 0.87378, time: 0.05646\n",
      "Epoch: 117, train_loss: 0.87264, time: 0.05639\n",
      "Epoch: 118, train_loss: 0.87434, time: 0.05618\n",
      "Epoch: 119, train_loss: 0.87332, time: 0.05409\n",
      "Epoch: 120, train_loss: 0.87479, time: 0.05558\n",
      "Epoch: 121, train_loss: 0.87385, time: 0.05541\n",
      "Epoch: 122, train_loss: 0.87364, time: 0.05596\n",
      "Epoch: 123, train_loss: 0.87452, time: 0.05606\n",
      "Epoch: 124, train_loss: 0.87296, time: 0.05642\n",
      "Epoch: 125, train_loss: 0.87450, time: 0.05641\n",
      "Epoch: 126, train_loss: 0.87340, time: 0.05599\n",
      "Epoch: 127, train_loss: 0.87302, time: 0.05658\n",
      "Epoch: 128, train_loss: 0.87342, time: 0.05484\n",
      "Epoch: 129, train_loss: 0.87278, time: 0.05459\n",
      "Epoch: 130, train_loss: 0.87356, time: 0.05585\n",
      "Epoch: 131, train_loss: 0.87351, time: 0.05634\n",
      "Epoch: 132, train_loss: 0.87489, time: 0.05621\n",
      "Epoch: 133, train_loss: 0.87330, time: 0.05619\n",
      "Epoch: 134, train_loss: 0.87464, time: 0.05602\n",
      "Epoch: 135, train_loss: 0.87298, time: 0.05641\n",
      "Epoch: 136, train_loss: 0.87432, time: 0.05526\n",
      "Epoch: 137, train_loss: 0.87392, time: 0.05627\n",
      "Epoch: 138, train_loss: 0.87377, time: 0.05707\n",
      "Epoch: 139, train_loss: 0.87411, time: 0.05724\n",
      "Epoch: 140, train_loss: 0.87394, time: 0.05669\n",
      "Epoch: 141, train_loss: 0.87258, time: 0.05617\n",
      "Epoch: 142, train_loss: 0.87279, time: 0.05628\n",
      "Epoch: 143, train_loss: 0.87350, time: 0.05606\n",
      "Epoch: 144, train_loss: 0.87427, time: 0.05593\n",
      "Epoch: 145, train_loss: 0.87448, time: 0.05552\n",
      "Epoch: 146, train_loss: 0.87399, time: 0.05621\n",
      "Epoch: 147, train_loss: 0.87359, time: 0.05622\n",
      "Epoch: 148, train_loss: 0.87341, time: 0.05594\n",
      "Epoch: 149, train_loss: 0.87394, time: 0.05577\n",
      "Epoch: 150, train_loss: 0.87379, time: 0.05710\n",
      "Epoch: 151, train_loss: 0.87387, time: 0.05648\n",
      "Epoch: 152, train_loss: 0.87427, time: 0.05671\n",
      "Epoch: 153, train_loss: 0.87335, time: 0.05722\n",
      "Epoch: 154, train_loss: 0.87352, time: 0.05574\n",
      "Epoch: 155, train_loss: 0.87315, time: 0.05634\n",
      "Epoch: 156, train_loss: 0.87348, time: 0.05428\n",
      "Epoch: 157, train_loss: 0.87348, time: 0.05511\n",
      "Epoch: 158, train_loss: 0.87307, time: 0.05538\n",
      "Epoch: 159, train_loss: 0.87281, time: 0.05622\n",
      "Epoch: 160, train_loss: 0.87338, time: 0.05667\n",
      "Epoch: 161, train_loss: 0.87333, time: 0.05595\n",
      "Epoch: 162, train_loss: 0.87439, time: 0.05575\n",
      "Epoch: 163, train_loss: 0.87411, time: 0.05642\n",
      "Epoch: 164, train_loss: 0.87364, time: 0.05579\n",
      "Epoch: 165, train_loss: 0.87349, time: 0.05592\n",
      "Epoch: 166, train_loss: 0.87334, time: 0.05560\n",
      "Epoch: 167, train_loss: 0.87436, time: 0.05689\n",
      "Epoch: 168, train_loss: 0.87473, time: 0.05581\n",
      "Epoch: 169, train_loss: 0.87269, time: 0.05558\n",
      "Epoch: 170, train_loss: 0.87301, time: 0.05708\n",
      "Epoch: 171, train_loss: 0.87276, time: 0.05573\n",
      "Epoch: 172, train_loss: 0.87453, time: 0.05530\n",
      "Epoch: 173, train_loss: 0.87238, time: 0.05532\n",
      "Epoch: 174, train_loss: 0.87328, time: 0.05579\n",
      "Epoch: 175, train_loss: 0.87414, time: 0.05515\n",
      "Epoch: 176, train_loss: 0.87256, time: 0.05621\n",
      "Epoch: 177, train_loss: 0.87241, time: 0.05638\n",
      "Epoch: 178, train_loss: 0.87353, time: 0.05583\n",
      "Epoch: 179, train_loss: 0.87291, time: 0.05664\n",
      "Epoch: 180, train_loss: 0.87159, time: 0.05661\n",
      "Epoch: 181, train_loss: 0.87259, time: 0.05579\n",
      "Epoch: 182, train_loss: 0.87302, time: 0.05592\n",
      "Epoch: 183, train_loss: 0.87303, time: 0.05554\n",
      "Epoch: 184, train_loss: 0.87523, time: 0.05580\n",
      "Epoch: 185, train_loss: 0.87318, time: 0.05587\n",
      "Epoch: 186, train_loss: 0.87422, time: 0.05530\n",
      "Epoch: 187, train_loss: 0.87265, time: 0.05524\n",
      "Epoch: 188, train_loss: 0.87340, time: 0.05627\n",
      "Epoch: 189, train_loss: 0.87390, time: 0.05629\n",
      "Epoch: 190, train_loss: 0.87327, time: 0.05608\n",
      "Epoch: 191, train_loss: 0.87347, time: 0.05684\n",
      "Epoch: 192, train_loss: 0.87334, time: 0.05496\n",
      "Epoch: 193, train_loss: 0.87364, time: 0.05678\n",
      "Epoch: 194, train_loss: 0.87423, time: 0.05637\n",
      "Epoch: 195, train_loss: 0.87367, time: 0.05558\n",
      "Epoch: 196, train_loss: 0.87363, time: 0.05581\n",
      "Epoch: 197, train_loss: 0.87380, time: 0.05674\n",
      "Epoch: 198, train_loss: 0.87304, time: 0.05580\n",
      "Epoch: 199, train_loss: 0.87388, time: 0.05546\n",
      "Epoch: 200, train_loss: 0.87253, time: 0.05598\n",
      "pairwise precision 0.52780 recall 0.99144 f1 0.68887\n",
      "average until now [0.5787287991790581, 0.8553972562167914, 0.6903758913643924]\n",
      "19 names 145.22264075279236 avg time 7.643296881725914\n",
      "Loading jian_guo_he dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 185 nodes, 3526 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.89846, time: 0.11065\n",
      "Epoch: 2, train_loss: 0.89829, time: 0.01283\n",
      "Epoch: 3, train_loss: 0.89887, time: 0.01271\n",
      "Epoch: 4, train_loss: 0.90094, time: 0.01247\n",
      "Epoch: 5, train_loss: 0.89973, time: 0.01230\n",
      "Epoch: 6, train_loss: 0.89724, time: 0.01225\n",
      "Epoch: 7, train_loss: 0.89338, time: 0.01209\n",
      "Epoch: 8, train_loss: 0.89794, time: 0.01205\n",
      "Epoch: 9, train_loss: 0.89858, time: 0.01214\n",
      "Epoch: 10, train_loss: 0.89825, time: 0.01190\n",
      "Epoch: 11, train_loss: 0.89517, time: 0.01189\n",
      "Epoch: 12, train_loss: 0.89532, time: 0.01197\n",
      "Epoch: 13, train_loss: 0.90015, time: 0.01214\n",
      "Epoch: 14, train_loss: 0.89727, time: 0.01189\n",
      "Epoch: 15, train_loss: 0.89583, time: 0.01206\n",
      "Epoch: 16, train_loss: 0.89116, time: 0.01167\n",
      "Epoch: 17, train_loss: 0.90381, time: 0.01185\n",
      "Epoch: 18, train_loss: 0.89999, time: 0.01221\n",
      "Epoch: 19, train_loss: 0.89678, time: 0.01204\n",
      "Epoch: 20, train_loss: 0.89767, time: 0.01176\n",
      "Epoch: 21, train_loss: 0.89815, time: 0.01154\n",
      "Epoch: 22, train_loss: 0.90087, time: 0.01208\n",
      "Epoch: 23, train_loss: 0.89526, time: 0.01207\n",
      "Epoch: 24, train_loss: 0.89250, time: 0.01219\n",
      "Epoch: 25, train_loss: 0.89304, time: 0.01202\n",
      "Epoch: 26, train_loss: 0.89969, time: 0.01190\n",
      "Epoch: 27, train_loss: 0.89739, time: 0.01187\n",
      "Epoch: 28, train_loss: 0.89559, time: 0.01204\n",
      "Epoch: 29, train_loss: 0.90019, time: 0.01198\n",
      "Epoch: 30, train_loss: 0.89528, time: 0.01202\n",
      "Epoch: 31, train_loss: 0.89413, time: 0.01195\n",
      "Epoch: 32, train_loss: 0.89535, time: 0.01206\n",
      "Epoch: 33, train_loss: 0.90060, time: 0.01204\n",
      "Epoch: 34, train_loss: 0.89615, time: 0.01202\n",
      "Epoch: 35, train_loss: 0.89045, time: 0.01210\n",
      "Epoch: 36, train_loss: 0.89507, time: 0.01208\n",
      "Epoch: 37, train_loss: 0.89451, time: 0.01175\n",
      "Epoch: 38, train_loss: 0.89421, time: 0.01186\n",
      "Epoch: 39, train_loss: 0.89508, time: 0.01196\n",
      "Epoch: 40, train_loss: 0.89326, time: 0.01220\n",
      "Epoch: 41, train_loss: 0.89124, time: 0.01186\n",
      "Epoch: 42, train_loss: 0.89214, time: 0.01188\n",
      "Epoch: 43, train_loss: 0.89873, time: 0.01147\n",
      "Epoch: 44, train_loss: 0.89608, time: 0.01186\n",
      "Epoch: 45, train_loss: 0.89114, time: 0.01215\n",
      "Epoch: 46, train_loss: 0.88561, time: 0.01180\n",
      "Epoch: 47, train_loss: 0.89074, time: 0.01207\n",
      "Epoch: 48, train_loss: 0.89293, time: 0.01206\n",
      "Epoch: 49, train_loss: 0.88735, time: 0.01171\n",
      "Epoch: 50, train_loss: 0.88751, time: 0.01210\n",
      "Epoch: 51, train_loss: 0.88735, time: 0.01173\n",
      "Epoch: 52, train_loss: 0.88341, time: 0.01226\n",
      "Epoch: 53, train_loss: 0.89221, time: 0.01205\n",
      "Epoch: 54, train_loss: 0.88656, time: 0.01192\n",
      "Epoch: 55, train_loss: 0.88334, time: 0.01196\n",
      "Epoch: 56, train_loss: 0.89205, time: 0.01221\n",
      "Epoch: 57, train_loss: 0.90566, time: 0.01213\n",
      "Epoch: 58, train_loss: 0.86890, time: 0.01205\n",
      "Epoch: 59, train_loss: 0.89723, time: 0.01188\n",
      "Epoch: 60, train_loss: 0.89417, time: 0.01194\n",
      "Epoch: 61, train_loss: 0.88373, time: 0.01202\n",
      "Epoch: 62, train_loss: 0.88061, time: 0.01190\n",
      "Epoch: 63, train_loss: 0.89357, time: 0.01210\n",
      "Epoch: 64, train_loss: 0.88499, time: 0.01195\n",
      "Epoch: 65, train_loss: 0.89127, time: 0.01216\n",
      "Epoch: 66, train_loss: 0.89688, time: 0.01190\n",
      "Epoch: 67, train_loss: 0.89177, time: 0.01214\n",
      "Epoch: 68, train_loss: 0.89101, time: 0.01190\n",
      "Epoch: 69, train_loss: 0.87991, time: 0.01210\n",
      "Epoch: 70, train_loss: 0.88319, time: 0.01231\n",
      "Epoch: 71, train_loss: 0.89055, time: 0.01178\n",
      "Epoch: 72, train_loss: 0.88346, time: 0.01213\n",
      "Epoch: 73, train_loss: 0.88740, time: 0.01229\n",
      "Epoch: 74, train_loss: 0.87919, time: 0.01192\n",
      "Epoch: 75, train_loss: 0.88118, time: 0.01194\n",
      "Epoch: 76, train_loss: 0.88478, time: 0.01209\n",
      "Epoch: 77, train_loss: 0.88066, time: 0.01185\n",
      "Epoch: 78, train_loss: 0.89330, time: 0.01180\n",
      "Epoch: 79, train_loss: 0.88846, time: 0.01164\n",
      "Epoch: 80, train_loss: 0.88747, time: 0.01253\n",
      "Epoch: 81, train_loss: 0.87514, time: 0.01152\n",
      "Epoch: 82, train_loss: 0.88359, time: 0.01191\n",
      "Epoch: 83, train_loss: 0.88613, time: 0.01197\n",
      "Epoch: 84, train_loss: 0.88155, time: 0.01228\n",
      "Epoch: 85, train_loss: 0.89555, time: 0.01171\n",
      "Epoch: 86, train_loss: 0.88779, time: 0.01260\n",
      "Epoch: 87, train_loss: 0.88637, time: 0.01181\n",
      "Epoch: 88, train_loss: 0.88368, time: 0.01195\n",
      "Epoch: 89, train_loss: 0.89133, time: 0.01189\n",
      "Epoch: 90, train_loss: 0.88519, time: 0.01199\n",
      "Epoch: 91, train_loss: 0.88364, time: 0.01193\n",
      "Epoch: 92, train_loss: 0.88165, time: 0.01173\n",
      "Epoch: 93, train_loss: 0.88547, time: 0.01190\n",
      "Epoch: 94, train_loss: 0.88061, time: 0.01176\n",
      "Epoch: 95, train_loss: 0.87895, time: 0.01207\n",
      "Epoch: 96, train_loss: 0.90378, time: 0.01186\n",
      "Epoch: 97, train_loss: 0.88912, time: 0.01216\n",
      "Epoch: 98, train_loss: 0.88832, time: 0.01199\n",
      "Epoch: 99, train_loss: 0.89638, time: 0.01266\n",
      "Epoch: 100, train_loss: 0.88698, time: 0.01187\n",
      "Epoch: 101, train_loss: 0.87128, time: 0.01183\n",
      "Epoch: 102, train_loss: 0.88231, time: 0.01337\n",
      "Epoch: 103, train_loss: 0.87936, time: 0.01238\n",
      "Epoch: 104, train_loss: 0.88022, time: 0.01172\n",
      "Epoch: 105, train_loss: 0.88879, time: 0.01185\n",
      "Epoch: 106, train_loss: 0.88899, time: 0.01178\n",
      "Epoch: 107, train_loss: 0.88905, time: 0.01157\n",
      "Epoch: 108, train_loss: 0.88887, time: 0.01213\n",
      "Epoch: 109, train_loss: 0.87841, time: 0.01218\n",
      "Epoch: 110, train_loss: 0.87040, time: 0.01222\n",
      "Epoch: 111, train_loss: 0.88661, time: 0.01240\n",
      "Epoch: 112, train_loss: 0.86902, time: 0.01218\n",
      "Epoch: 113, train_loss: 0.88287, time: 0.01192\n",
      "Epoch: 114, train_loss: 0.88830, time: 0.01208\n",
      "Epoch: 115, train_loss: 0.89521, time: 0.01168\n",
      "Epoch: 116, train_loss: 0.88834, time: 0.01204\n",
      "Epoch: 117, train_loss: 0.89369, time: 0.01199\n",
      "Epoch: 118, train_loss: 0.88131, time: 0.01152\n",
      "Epoch: 119, train_loss: 0.88767, time: 0.01195\n",
      "Epoch: 120, train_loss: 0.88188, time: 0.01228\n",
      "Epoch: 121, train_loss: 0.88988, time: 0.01220\n",
      "Epoch: 122, train_loss: 0.88111, time: 0.01218\n",
      "Epoch: 123, train_loss: 0.88606, time: 0.01187\n",
      "Epoch: 124, train_loss: 0.87436, time: 0.01193\n",
      "Epoch: 125, train_loss: 0.88791, time: 0.01218\n",
      "Epoch: 126, train_loss: 0.87938, time: 0.01220\n",
      "Epoch: 127, train_loss: 0.88525, time: 0.01159\n",
      "Epoch: 128, train_loss: 0.88485, time: 0.01200\n",
      "Epoch: 129, train_loss: 0.88198, time: 0.01153\n",
      "Epoch: 130, train_loss: 0.88695, time: 0.01124\n",
      "Epoch: 131, train_loss: 0.89127, time: 0.01208\n",
      "Epoch: 132, train_loss: 0.87984, time: 0.01174\n",
      "Epoch: 133, train_loss: 0.88712, time: 0.01208\n",
      "Epoch: 134, train_loss: 0.87995, time: 0.01185\n",
      "Epoch: 135, train_loss: 0.88125, time: 0.01169\n",
      "Epoch: 136, train_loss: 0.88480, time: 0.01178\n",
      "Epoch: 137, train_loss: 0.87996, time: 0.01208\n",
      "Epoch: 138, train_loss: 0.89137, time: 0.01199\n",
      "Epoch: 139, train_loss: 0.87662, time: 0.01201\n",
      "Epoch: 140, train_loss: 0.87832, time: 0.01187\n",
      "Epoch: 141, train_loss: 0.88042, time: 0.01203\n",
      "Epoch: 142, train_loss: 0.87556, time: 0.01203\n",
      "Epoch: 143, train_loss: 0.87735, time: 0.01195\n",
      "Epoch: 144, train_loss: 0.88501, time: 0.01207\n",
      "Epoch: 145, train_loss: 0.88033, time: 0.01201\n",
      "Epoch: 146, train_loss: 0.88353, time: 0.01186\n",
      "Epoch: 147, train_loss: 0.87909, time: 0.01229\n",
      "Epoch: 148, train_loss: 0.88981, time: 0.01207\n",
      "Epoch: 149, train_loss: 0.88695, time: 0.01201\n",
      "Epoch: 150, train_loss: 0.88266, time: 0.01169\n",
      "Epoch: 151, train_loss: 0.88194, time: 0.01206\n",
      "Epoch: 152, train_loss: 0.87565, time: 0.01198\n",
      "Epoch: 153, train_loss: 0.87474, time: 0.01224\n",
      "Epoch: 154, train_loss: 0.88099, time: 0.01209\n",
      "Epoch: 155, train_loss: 0.87352, time: 0.01167\n",
      "Epoch: 156, train_loss: 0.87267, time: 0.01130\n",
      "Epoch: 157, train_loss: 0.87875, time: 0.01161\n",
      "Epoch: 158, train_loss: 0.87609, time: 0.01162\n",
      "Epoch: 159, train_loss: 0.87895, time: 0.01182\n",
      "Epoch: 160, train_loss: 0.87900, time: 0.01216\n",
      "Epoch: 161, train_loss: 0.89220, time: 0.01217\n",
      "Epoch: 162, train_loss: 0.88757, time: 0.01180\n",
      "Epoch: 163, train_loss: 0.87856, time: 0.01222\n",
      "Epoch: 164, train_loss: 0.88287, time: 0.01175\n",
      "Epoch: 165, train_loss: 0.89206, time: 0.01166\n",
      "Epoch: 166, train_loss: 0.89195, time: 0.01190\n",
      "Epoch: 167, train_loss: 0.89070, time: 0.01210\n",
      "Epoch: 168, train_loss: 0.88407, time: 0.01195\n",
      "Epoch: 169, train_loss: 0.88733, time: 0.01183\n",
      "Epoch: 170, train_loss: 0.87109, time: 0.01218\n",
      "Epoch: 171, train_loss: 0.88432, time: 0.01239\n",
      "Epoch: 172, train_loss: 0.88692, time: 0.01230\n",
      "Epoch: 173, train_loss: 0.88099, time: 0.01210\n",
      "Epoch: 174, train_loss: 0.87537, time: 0.01218\n",
      "Epoch: 175, train_loss: 0.88419, time: 0.01166\n",
      "Epoch: 176, train_loss: 0.88650, time: 0.01207\n",
      "Epoch: 177, train_loss: 0.88472, time: 0.01195\n",
      "Epoch: 178, train_loss: 0.87241, time: 0.01214\n",
      "Epoch: 179, train_loss: 0.87885, time: 0.01213\n",
      "Epoch: 180, train_loss: 0.88020, time: 0.01215\n",
      "Epoch: 181, train_loss: 0.88230, time: 0.01185\n",
      "Epoch: 182, train_loss: 0.88482, time: 0.01220\n",
      "Epoch: 183, train_loss: 0.87852, time: 0.01143\n",
      "Epoch: 184, train_loss: 0.87019, time: 0.01186\n",
      "Epoch: 185, train_loss: 0.90521, time: 0.01202\n",
      "Epoch: 186, train_loss: 0.88053, time: 0.01198\n",
      "Epoch: 187, train_loss: 0.87385, time: 0.01142\n",
      "Epoch: 188, train_loss: 0.87723, time: 0.01209\n",
      "Epoch: 189, train_loss: 0.87516, time: 0.01213\n",
      "Epoch: 190, train_loss: 0.87495, time: 0.01162\n",
      "Epoch: 191, train_loss: 0.88346, time: 0.01180\n",
      "Epoch: 192, train_loss: 0.88650, time: 0.01182\n",
      "Epoch: 193, train_loss: 0.87408, time: 0.01230\n",
      "Epoch: 194, train_loss: 0.89100, time: 0.01204\n",
      "Epoch: 195, train_loss: 0.88366, time: 0.01222\n",
      "Epoch: 196, train_loss: 0.87812, time: 0.01189\n",
      "Epoch: 197, train_loss: 0.88010, time: 0.01179\n",
      "Epoch: 198, train_loss: 0.87377, time: 0.01217\n",
      "Epoch: 199, train_loss: 0.87669, time: 0.01198\n",
      "Epoch: 200, train_loss: 0.87104, time: 0.01185\n",
      "pairwise precision 0.67191 recall 0.55855 f1 0.61001\n",
      "average until now [0.5833876440924037, 0.8405548799321645, 0.6887487702074386]\n",
      "20 names 147.76800179481506 avg time 7.388400089740753\n",
      "Loading xiaobing_luo dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 196 nodes, 8114 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.78804, time: 0.11561\n",
      "Epoch: 2, train_loss: 0.78978, time: 0.01613\n",
      "Epoch: 3, train_loss: 0.78816, time: 0.01544\n",
      "Epoch: 4, train_loss: 0.78785, time: 0.01505\n",
      "Epoch: 5, train_loss: 0.78687, time: 0.01486\n",
      "Epoch: 6, train_loss: 0.78939, time: 0.01480\n",
      "Epoch: 7, train_loss: 0.78856, time: 0.01460\n",
      "Epoch: 8, train_loss: 0.79023, time: 0.01517\n",
      "Epoch: 9, train_loss: 0.78968, time: 0.01519\n",
      "Epoch: 10, train_loss: 0.78820, time: 0.01541\n",
      "Epoch: 11, train_loss: 0.79109, time: 0.01497\n",
      "Epoch: 12, train_loss: 0.78781, time: 0.01479\n",
      "Epoch: 13, train_loss: 0.78462, time: 0.01413\n",
      "Epoch: 14, train_loss: 0.79049, time: 0.01515\n",
      "Epoch: 15, train_loss: 0.78786, time: 0.01514\n",
      "Epoch: 16, train_loss: 0.78165, time: 0.01509\n",
      "Epoch: 17, train_loss: 0.78787, time: 0.01520\n",
      "Epoch: 18, train_loss: 0.78894, time: 0.01440\n",
      "Epoch: 19, train_loss: 0.79073, time: 0.01450\n",
      "Epoch: 20, train_loss: 0.78792, time: 0.01499\n",
      "Epoch: 21, train_loss: 0.79273, time: 0.01462\n",
      "Epoch: 22, train_loss: 0.79073, time: 0.01532\n",
      "Epoch: 23, train_loss: 0.78837, time: 0.01527\n",
      "Epoch: 24, train_loss: 0.78767, time: 0.01467\n",
      "Epoch: 25, train_loss: 0.78735, time: 0.01510\n",
      "Epoch: 26, train_loss: 0.78563, time: 0.01553\n",
      "Epoch: 27, train_loss: 0.78828, time: 0.01501\n",
      "Epoch: 28, train_loss: 0.78838, time: 0.01476\n",
      "Epoch: 29, train_loss: 0.78745, time: 0.01536\n",
      "Epoch: 30, train_loss: 0.79082, time: 0.01498\n",
      "Epoch: 31, train_loss: 0.78976, time: 0.01498\n",
      "Epoch: 32, train_loss: 0.78739, time: 0.01497\n",
      "Epoch: 33, train_loss: 0.78913, time: 0.01486\n",
      "Epoch: 34, train_loss: 0.78650, time: 0.01499\n",
      "Epoch: 35, train_loss: 0.78809, time: 0.01496\n",
      "Epoch: 36, train_loss: 0.78315, time: 0.01507\n",
      "Epoch: 37, train_loss: 0.78652, time: 0.01503\n",
      "Epoch: 38, train_loss: 0.78807, time: 0.01465\n",
      "Epoch: 39, train_loss: 0.78457, time: 0.01468\n",
      "Epoch: 40, train_loss: 0.78996, time: 0.01487\n",
      "Epoch: 41, train_loss: 0.78705, time: 0.01490\n",
      "Epoch: 42, train_loss: 0.78858, time: 0.01521\n",
      "Epoch: 43, train_loss: 0.78115, time: 0.01500\n",
      "Epoch: 44, train_loss: 0.78899, time: 0.01554\n",
      "Epoch: 45, train_loss: 0.78264, time: 0.01480\n",
      "Epoch: 46, train_loss: 0.78526, time: 0.01491\n",
      "Epoch: 47, train_loss: 0.78219, time: 0.01506\n",
      "Epoch: 48, train_loss: 0.78518, time: 0.01496\n",
      "Epoch: 49, train_loss: 0.78601, time: 0.01470\n",
      "Epoch: 50, train_loss: 0.79548, time: 0.01454\n",
      "Epoch: 51, train_loss: 0.79021, time: 0.01508\n",
      "Epoch: 52, train_loss: 0.78412, time: 0.01497\n",
      "Epoch: 53, train_loss: 0.77978, time: 0.01490\n",
      "Epoch: 54, train_loss: 0.78046, time: 0.01479\n",
      "Epoch: 55, train_loss: 0.77823, time: 0.01503\n",
      "Epoch: 56, train_loss: 0.77616, time: 0.01492\n",
      "Epoch: 57, train_loss: 0.78908, time: 0.01502\n",
      "Epoch: 58, train_loss: 0.78147, time: 0.01527\n",
      "Epoch: 59, train_loss: 0.78030, time: 0.01499\n",
      "Epoch: 60, train_loss: 0.77622, time: 0.01518\n",
      "Epoch: 61, train_loss: 0.78299, time: 0.01476\n",
      "Epoch: 62, train_loss: 0.78164, time: 0.01523\n",
      "Epoch: 63, train_loss: 0.78306, time: 0.01574\n",
      "Epoch: 64, train_loss: 0.78537, time: 0.01496\n",
      "Epoch: 65, train_loss: 0.78663, time: 0.01486\n",
      "Epoch: 66, train_loss: 0.78922, time: 0.01499\n",
      "Epoch: 67, train_loss: 0.78258, time: 0.01494\n",
      "Epoch: 68, train_loss: 0.77967, time: 0.01472\n",
      "Epoch: 69, train_loss: 0.78079, time: 0.01478\n",
      "Epoch: 70, train_loss: 0.78211, time: 0.01484\n",
      "Epoch: 71, train_loss: 0.77598, time: 0.01539\n",
      "Epoch: 72, train_loss: 0.78116, time: 0.01538\n",
      "Epoch: 73, train_loss: 0.77878, time: 0.01426\n",
      "Epoch: 74, train_loss: 0.78201, time: 0.01421\n",
      "Epoch: 75, train_loss: 0.77595, time: 0.01406\n",
      "Epoch: 76, train_loss: 0.77189, time: 0.01504\n",
      "Epoch: 77, train_loss: 0.77560, time: 0.01459\n",
      "Epoch: 78, train_loss: 0.78687, time: 0.01463\n",
      "Epoch: 79, train_loss: 0.77679, time: 0.01475\n",
      "Epoch: 80, train_loss: 0.77974, time: 0.01487\n",
      "Epoch: 81, train_loss: 0.78356, time: 0.01456\n",
      "Epoch: 82, train_loss: 0.77514, time: 0.01464\n",
      "Epoch: 83, train_loss: 0.78721, time: 0.01455\n",
      "Epoch: 84, train_loss: 0.78366, time: 0.01433\n",
      "Epoch: 85, train_loss: 0.78037, time: 0.01565\n",
      "Epoch: 86, train_loss: 0.78048, time: 0.01475\n",
      "Epoch: 87, train_loss: 0.77442, time: 0.01511\n",
      "Epoch: 88, train_loss: 0.78789, time: 0.01489\n",
      "Epoch: 89, train_loss: 0.77319, time: 0.01493\n",
      "Epoch: 90, train_loss: 0.77926, time: 0.01519\n",
      "Epoch: 91, train_loss: 0.77823, time: 0.01526\n",
      "Epoch: 92, train_loss: 0.78040, time: 0.01492\n",
      "Epoch: 93, train_loss: 0.78745, time: 0.01526\n",
      "Epoch: 94, train_loss: 0.78299, time: 0.01504\n",
      "Epoch: 95, train_loss: 0.78202, time: 0.01453\n",
      "Epoch: 96, train_loss: 0.77611, time: 0.01511\n",
      "Epoch: 97, train_loss: 0.78235, time: 0.01495\n",
      "Epoch: 98, train_loss: 0.78776, time: 0.01481\n",
      "Epoch: 99, train_loss: 0.77986, time: 0.01474\n",
      "Epoch: 100, train_loss: 0.78215, time: 0.01477\n",
      "Epoch: 101, train_loss: 0.77553, time: 0.01466\n",
      "Epoch: 102, train_loss: 0.78159, time: 0.01498\n",
      "Epoch: 103, train_loss: 0.78666, time: 0.01452\n",
      "Epoch: 104, train_loss: 0.78665, time: 0.01555\n",
      "Epoch: 105, train_loss: 0.78123, time: 0.01512\n",
      "Epoch: 106, train_loss: 0.78538, time: 0.01465\n",
      "Epoch: 107, train_loss: 0.76415, time: 0.01475\n",
      "Epoch: 108, train_loss: 0.78484, time: 0.01481\n",
      "Epoch: 109, train_loss: 0.77504, time: 0.01508\n",
      "Epoch: 110, train_loss: 0.78445, time: 0.01471\n",
      "Epoch: 111, train_loss: 0.77639, time: 0.01500\n",
      "Epoch: 112, train_loss: 0.78111, time: 0.01518\n",
      "Epoch: 113, train_loss: 0.77417, time: 0.01545\n",
      "Epoch: 114, train_loss: 0.78702, time: 0.01509\n",
      "Epoch: 115, train_loss: 0.77297, time: 0.01471\n",
      "Epoch: 116, train_loss: 0.78564, time: 0.01484\n",
      "Epoch: 117, train_loss: 0.77712, time: 0.01491\n",
      "Epoch: 118, train_loss: 0.77911, time: 0.01498\n",
      "Epoch: 119, train_loss: 0.77163, time: 0.01517\n",
      "Epoch: 120, train_loss: 0.77866, time: 0.01482\n",
      "Epoch: 121, train_loss: 0.77110, time: 0.01450\n",
      "Epoch: 122, train_loss: 0.77607, time: 0.01406\n",
      "Epoch: 123, train_loss: 0.76321, time: 0.01441\n",
      "Epoch: 124, train_loss: 0.76880, time: 0.01496\n",
      "Epoch: 125, train_loss: 0.77970, time: 0.01509\n",
      "Epoch: 126, train_loss: 0.77441, time: 0.01504\n",
      "Epoch: 127, train_loss: 0.76267, time: 0.01508\n",
      "Epoch: 128, train_loss: 0.77895, time: 0.01474\n",
      "Epoch: 129, train_loss: 0.78035, time: 0.01492\n",
      "Epoch: 130, train_loss: 0.77218, time: 0.01540\n",
      "Epoch: 131, train_loss: 0.76801, time: 0.01475\n",
      "Epoch: 132, train_loss: 0.77521, time: 0.01482\n",
      "Epoch: 133, train_loss: 0.77873, time: 0.01457\n",
      "Epoch: 134, train_loss: 0.79022, time: 0.01425\n",
      "Epoch: 135, train_loss: 0.79833, time: 0.01475\n",
      "Epoch: 136, train_loss: 0.78034, time: 0.01541\n",
      "Epoch: 137, train_loss: 0.77715, time: 0.01500\n",
      "Epoch: 138, train_loss: 0.77822, time: 0.01487\n",
      "Epoch: 139, train_loss: 0.78545, time: 0.01530\n",
      "Epoch: 140, train_loss: 0.77625, time: 0.01547\n",
      "Epoch: 141, train_loss: 0.78092, time: 0.01572\n",
      "Epoch: 142, train_loss: 0.77496, time: 0.01529\n",
      "Epoch: 143, train_loss: 0.77111, time: 0.01479\n",
      "Epoch: 144, train_loss: 0.77126, time: 0.01498\n",
      "Epoch: 145, train_loss: 0.77432, time: 0.01490\n",
      "Epoch: 146, train_loss: 0.78201, time: 0.01482\n",
      "Epoch: 147, train_loss: 0.78900, time: 0.01482\n",
      "Epoch: 148, train_loss: 0.77064, time: 0.01500\n",
      "Epoch: 149, train_loss: 0.77700, time: 0.01488\n",
      "Epoch: 150, train_loss: 0.78255, time: 0.01526\n",
      "Epoch: 151, train_loss: 0.76804, time: 0.01502\n",
      "Epoch: 152, train_loss: 0.76480, time: 0.01460\n",
      "Epoch: 153, train_loss: 0.77774, time: 0.01493\n",
      "Epoch: 154, train_loss: 0.77165, time: 0.01441\n",
      "Epoch: 155, train_loss: 0.77857, time: 0.01517\n",
      "Epoch: 156, train_loss: 0.78678, time: 0.01506\n",
      "Epoch: 157, train_loss: 0.76976, time: 0.01469\n",
      "Epoch: 158, train_loss: 0.76825, time: 0.01415\n",
      "Epoch: 159, train_loss: 0.75638, time: 0.01448\n",
      "Epoch: 160, train_loss: 0.78282, time: 0.01456\n",
      "Epoch: 161, train_loss: 0.78382, time: 0.01422\n",
      "Epoch: 162, train_loss: 0.76591, time: 0.01444\n",
      "Epoch: 163, train_loss: 0.78143, time: 0.01503\n",
      "Epoch: 164, train_loss: 0.78581, time: 0.01499\n",
      "Epoch: 165, train_loss: 0.77160, time: 0.01511\n",
      "Epoch: 166, train_loss: 0.78940, time: 0.01559\n",
      "Epoch: 167, train_loss: 0.77806, time: 0.01496\n",
      "Epoch: 168, train_loss: 0.77045, time: 0.01496\n",
      "Epoch: 169, train_loss: 0.76404, time: 0.01551\n",
      "Epoch: 170, train_loss: 0.76736, time: 0.01512\n",
      "Epoch: 171, train_loss: 0.78511, time: 0.01477\n",
      "Epoch: 172, train_loss: 0.75844, time: 0.01497\n",
      "Epoch: 173, train_loss: 0.78050, time: 0.01487\n",
      "Epoch: 174, train_loss: 0.78570, time: 0.01501\n",
      "Epoch: 175, train_loss: 0.75653, time: 0.01504\n",
      "Epoch: 176, train_loss: 0.78111, time: 0.01490\n",
      "Epoch: 177, train_loss: 0.75732, time: 0.01476\n",
      "Epoch: 178, train_loss: 0.76942, time: 0.01508\n",
      "Epoch: 179, train_loss: 0.79341, time: 0.01504\n",
      "Epoch: 180, train_loss: 0.78359, time: 0.01487\n",
      "Epoch: 181, train_loss: 0.78450, time: 0.01505\n",
      "Epoch: 182, train_loss: 0.76257, time: 0.01499\n",
      "Epoch: 183, train_loss: 0.76700, time: 0.01519\n",
      "Epoch: 184, train_loss: 0.78646, time: 0.01505\n",
      "Epoch: 185, train_loss: 0.78042, time: 0.01485\n",
      "Epoch: 186, train_loss: 0.78005, time: 0.01483\n",
      "Epoch: 187, train_loss: 0.76382, time: 0.01528\n",
      "Epoch: 188, train_loss: 0.78116, time: 0.01486\n",
      "Epoch: 189, train_loss: 0.79230, time: 0.01488\n",
      "Epoch: 190, train_loss: 0.77001, time: 0.01490\n",
      "Epoch: 191, train_loss: 0.77570, time: 0.01488\n",
      "Epoch: 192, train_loss: 0.76497, time: 0.01491\n",
      "Epoch: 193, train_loss: 0.78431, time: 0.01528\n",
      "Epoch: 194, train_loss: 0.78738, time: 0.01533\n",
      "Epoch: 195, train_loss: 0.76939, time: 0.01490\n",
      "Epoch: 196, train_loss: 0.77563, time: 0.01429\n",
      "Epoch: 197, train_loss: 0.78688, time: 0.01493\n",
      "Epoch: 198, train_loss: 0.77201, time: 0.01450\n",
      "Epoch: 199, train_loss: 0.78216, time: 0.01427\n",
      "Epoch: 200, train_loss: 0.77859, time: 0.01470\n",
      "pairwise precision 0.91409 recall 0.61792 f1 0.73738\n",
      "average until now [0.5991353274183323, 0.8299533565969339, 0.6959041543167603]\n",
      "21 names 150.912495136261 avg time 7.186309292202904\n",
      "Loading yang_shen dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 485 nodes, 3280 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99272, time: 0.11802\n",
      "Epoch: 2, train_loss: 0.98730, time: 0.01828\n",
      "Epoch: 3, train_loss: 0.98749, time: 0.01765\n",
      "Epoch: 4, train_loss: 0.98460, time: 0.01666\n",
      "Epoch: 5, train_loss: 0.98525, time: 0.01674\n",
      "Epoch: 6, train_loss: 0.98400, time: 0.01809\n",
      "Epoch: 7, train_loss: 0.98578, time: 0.01682\n",
      "Epoch: 8, train_loss: 0.98448, time: 0.01749\n",
      "Epoch: 9, train_loss: 0.98481, time: 0.01666\n",
      "Epoch: 10, train_loss: 0.98388, time: 0.01740\n",
      "Epoch: 11, train_loss: 0.98519, time: 0.01692\n",
      "Epoch: 12, train_loss: 0.98365, time: 0.01704\n",
      "Epoch: 13, train_loss: 0.98482, time: 0.01799\n",
      "Epoch: 14, train_loss: 0.98376, time: 0.01692\n",
      "Epoch: 15, train_loss: 0.98430, time: 0.01741\n",
      "Epoch: 16, train_loss: 0.98364, time: 0.01738\n",
      "Epoch: 17, train_loss: 0.98520, time: 0.01770\n",
      "Epoch: 18, train_loss: 0.98498, time: 0.01735\n",
      "Epoch: 19, train_loss: 0.98487, time: 0.01749\n",
      "Epoch: 20, train_loss: 0.98543, time: 0.01698\n",
      "Epoch: 21, train_loss: 0.98394, time: 0.01651\n",
      "Epoch: 22, train_loss: 0.98260, time: 0.01709\n",
      "Epoch: 23, train_loss: 0.98314, time: 0.01738\n",
      "Epoch: 24, train_loss: 0.98505, time: 0.01683\n",
      "Epoch: 25, train_loss: 0.98460, time: 0.01703\n",
      "Epoch: 26, train_loss: 0.98561, time: 0.01698\n",
      "Epoch: 27, train_loss: 0.98501, time: 0.01638\n",
      "Epoch: 28, train_loss: 0.98391, time: 0.01765\n",
      "Epoch: 29, train_loss: 0.98224, time: 0.01634\n",
      "Epoch: 30, train_loss: 0.98468, time: 0.01777\n",
      "Epoch: 31, train_loss: 0.98440, time: 0.01688\n",
      "Epoch: 32, train_loss: 0.98451, time: 0.01683\n",
      "Epoch: 33, train_loss: 0.98448, time: 0.01821\n",
      "Epoch: 34, train_loss: 0.98415, time: 0.01769\n",
      "Epoch: 35, train_loss: 0.98516, time: 0.01762\n",
      "Epoch: 36, train_loss: 0.98336, time: 0.01760\n",
      "Epoch: 37, train_loss: 0.98500, time: 0.01757\n",
      "Epoch: 38, train_loss: 0.98379, time: 0.01758\n",
      "Epoch: 39, train_loss: 0.98394, time: 0.01572\n",
      "Epoch: 40, train_loss: 0.98404, time: 0.01663\n",
      "Epoch: 41, train_loss: 0.98376, time: 0.01721\n",
      "Epoch: 42, train_loss: 0.98435, time: 0.01749\n",
      "Epoch: 43, train_loss: 0.98428, time: 0.01682\n",
      "Epoch: 44, train_loss: 0.98463, time: 0.01712\n",
      "Epoch: 45, train_loss: 0.98553, time: 0.01755\n",
      "Epoch: 46, train_loss: 0.98559, time: 0.01764\n",
      "Epoch: 47, train_loss: 0.98496, time: 0.01761\n",
      "Epoch: 48, train_loss: 0.98557, time: 0.01814\n",
      "Epoch: 49, train_loss: 0.98373, time: 0.01793\n",
      "Epoch: 50, train_loss: 0.98505, time: 0.01804\n",
      "Epoch: 51, train_loss: 0.98577, time: 0.01776\n",
      "Epoch: 52, train_loss: 0.98444, time: 0.01680\n",
      "Epoch: 53, train_loss: 0.98432, time: 0.01768\n",
      "Epoch: 54, train_loss: 0.98371, time: 0.01771\n",
      "Epoch: 55, train_loss: 0.98526, time: 0.01765\n",
      "Epoch: 56, train_loss: 0.98431, time: 0.01759\n",
      "Epoch: 57, train_loss: 0.98461, time: 0.01716\n",
      "Epoch: 58, train_loss: 0.98531, time: 0.01795\n",
      "Epoch: 59, train_loss: 0.98421, time: 0.01806\n",
      "Epoch: 60, train_loss: 0.98367, time: 0.01752\n",
      "Epoch: 61, train_loss: 0.98275, time: 0.01848\n",
      "Epoch: 62, train_loss: 0.98514, time: 0.01793\n",
      "Epoch: 63, train_loss: 0.98424, time: 0.01790\n",
      "Epoch: 64, train_loss: 0.98383, time: 0.01713\n",
      "Epoch: 65, train_loss: 0.98437, time: 0.01757\n",
      "Epoch: 66, train_loss: 0.98622, time: 0.01781\n",
      "Epoch: 67, train_loss: 0.98277, time: 0.01731\n",
      "Epoch: 68, train_loss: 0.98451, time: 0.01696\n",
      "Epoch: 69, train_loss: 0.98290, time: 0.01649\n",
      "Epoch: 70, train_loss: 0.98329, time: 0.01702\n",
      "Epoch: 71, train_loss: 0.98393, time: 0.01761\n",
      "Epoch: 72, train_loss: 0.98483, time: 0.01681\n",
      "Epoch: 73, train_loss: 0.98438, time: 0.01694\n",
      "Epoch: 74, train_loss: 0.98350, time: 0.01752\n",
      "Epoch: 75, train_loss: 0.98355, time: 0.01628\n",
      "Epoch: 76, train_loss: 0.98374, time: 0.01659\n",
      "Epoch: 77, train_loss: 0.98400, time: 0.01716\n",
      "Epoch: 78, train_loss: 0.98392, time: 0.01763\n",
      "Epoch: 79, train_loss: 0.98360, time: 0.01736\n",
      "Epoch: 80, train_loss: 0.98371, time: 0.01764\n",
      "Epoch: 81, train_loss: 0.98355, time: 0.01790\n",
      "Epoch: 82, train_loss: 0.98317, time: 0.01784\n",
      "Epoch: 83, train_loss: 0.98344, time: 0.01739\n",
      "Epoch: 84, train_loss: 0.98509, time: 0.01755\n",
      "Epoch: 85, train_loss: 0.98421, time: 0.01724\n",
      "Epoch: 86, train_loss: 0.98351, time: 0.01773\n",
      "Epoch: 87, train_loss: 0.98431, time: 0.01705\n",
      "Epoch: 88, train_loss: 0.98447, time: 0.01687\n",
      "Epoch: 89, train_loss: 0.98348, time: 0.01780\n",
      "Epoch: 90, train_loss: 0.98331, time: 0.01745\n",
      "Epoch: 91, train_loss: 0.98469, time: 0.01773\n",
      "Epoch: 92, train_loss: 0.98378, time: 0.01746\n",
      "Epoch: 93, train_loss: 0.98391, time: 0.01769\n",
      "Epoch: 94, train_loss: 0.98461, time: 0.01791\n",
      "Epoch: 95, train_loss: 0.98452, time: 0.01811\n",
      "Epoch: 96, train_loss: 0.98558, time: 0.01788\n",
      "Epoch: 97, train_loss: 0.98514, time: 0.01798\n",
      "Epoch: 98, train_loss: 0.98562, time: 0.01722\n",
      "Epoch: 99, train_loss: 0.98487, time: 0.01706\n",
      "Epoch: 100, train_loss: 0.98561, time: 0.01820\n",
      "Epoch: 101, train_loss: 0.98245, time: 0.01731\n",
      "Epoch: 102, train_loss: 0.98448, time: 0.01646\n",
      "Epoch: 103, train_loss: 0.98464, time: 0.01666\n",
      "Epoch: 104, train_loss: 0.98258, time: 0.01660\n",
      "Epoch: 105, train_loss: 0.98468, time: 0.01715\n",
      "Epoch: 106, train_loss: 0.98446, time: 0.01714\n",
      "Epoch: 107, train_loss: 0.98462, time: 0.01750\n",
      "Epoch: 108, train_loss: 0.98291, time: 0.01765\n",
      "Epoch: 109, train_loss: 0.98210, time: 0.01746\n",
      "Epoch: 110, train_loss: 0.98294, time: 0.01759\n",
      "Epoch: 111, train_loss: 0.98559, time: 0.01770\n",
      "Epoch: 112, train_loss: 0.98663, time: 0.01718\n",
      "Epoch: 113, train_loss: 0.98494, time: 0.01676\n",
      "Epoch: 114, train_loss: 0.98327, time: 0.01731\n",
      "Epoch: 115, train_loss: 0.98284, time: 0.01832\n",
      "Epoch: 116, train_loss: 0.98310, time: 0.01731\n",
      "Epoch: 117, train_loss: 0.98567, time: 0.01799\n",
      "Epoch: 118, train_loss: 0.98393, time: 0.01738\n",
      "Epoch: 119, train_loss: 0.98481, time: 0.01695\n",
      "Epoch: 120, train_loss: 0.98433, time: 0.01736\n",
      "Epoch: 121, train_loss: 0.98326, time: 0.01734\n",
      "Epoch: 122, train_loss: 0.98488, time: 0.01753\n",
      "Epoch: 123, train_loss: 0.98360, time: 0.01780\n",
      "Epoch: 124, train_loss: 0.98444, time: 0.01754\n",
      "Epoch: 125, train_loss: 0.98250, time: 0.01709\n",
      "Epoch: 126, train_loss: 0.98443, time: 0.01695\n",
      "Epoch: 127, train_loss: 0.98486, time: 0.01763\n",
      "Epoch: 128, train_loss: 0.98378, time: 0.01662\n",
      "Epoch: 129, train_loss: 0.98339, time: 0.01809\n",
      "Epoch: 130, train_loss: 0.98402, time: 0.01624\n",
      "Epoch: 131, train_loss: 0.98444, time: 0.01711\n",
      "Epoch: 132, train_loss: 0.98376, time: 0.01728\n",
      "Epoch: 133, train_loss: 0.98441, time: 0.01738\n",
      "Epoch: 134, train_loss: 0.98451, time: 0.01760\n",
      "Epoch: 135, train_loss: 0.98340, time: 0.01755\n",
      "Epoch: 136, train_loss: 0.98512, time: 0.01793\n",
      "Epoch: 137, train_loss: 0.98475, time: 0.01719\n",
      "Epoch: 138, train_loss: 0.98378, time: 0.01661\n",
      "Epoch: 139, train_loss: 0.98462, time: 0.01739\n",
      "Epoch: 140, train_loss: 0.98448, time: 0.01768\n",
      "Epoch: 141, train_loss: 0.98311, time: 0.01693\n",
      "Epoch: 142, train_loss: 0.98243, time: 0.01693\n",
      "Epoch: 143, train_loss: 0.98497, time: 0.01674\n",
      "Epoch: 144, train_loss: 0.98563, time: 0.01753\n",
      "Epoch: 145, train_loss: 0.98286, time: 0.01748\n",
      "Epoch: 146, train_loss: 0.98439, time: 0.01748\n",
      "Epoch: 147, train_loss: 0.98304, time: 0.01757\n",
      "Epoch: 148, train_loss: 0.98367, time: 0.01779\n",
      "Epoch: 149, train_loss: 0.98316, time: 0.01728\n",
      "Epoch: 150, train_loss: 0.98342, time: 0.01773\n",
      "Epoch: 151, train_loss: 0.98564, time: 0.01792\n",
      "Epoch: 152, train_loss: 0.98657, time: 0.01742\n",
      "Epoch: 153, train_loss: 0.98349, time: 0.01798\n",
      "Epoch: 154, train_loss: 0.98399, time: 0.01656\n",
      "Epoch: 155, train_loss: 0.98462, time: 0.01696\n",
      "Epoch: 156, train_loss: 0.98451, time: 0.01777\n",
      "Epoch: 157, train_loss: 0.98432, time: 0.01787\n",
      "Epoch: 158, train_loss: 0.98452, time: 0.01587\n",
      "Epoch: 159, train_loss: 0.98297, time: 0.01675\n",
      "Epoch: 160, train_loss: 0.98489, time: 0.01758\n",
      "Epoch: 161, train_loss: 0.98375, time: 0.01823\n",
      "Epoch: 162, train_loss: 0.98581, time: 0.01723\n",
      "Epoch: 163, train_loss: 0.98449, time: 0.01736\n",
      "Epoch: 164, train_loss: 0.98394, time: 0.01784\n",
      "Epoch: 165, train_loss: 0.98543, time: 0.01656\n",
      "Epoch: 166, train_loss: 0.98436, time: 0.01774\n",
      "Epoch: 167, train_loss: 0.98559, time: 0.01747\n",
      "Epoch: 168, train_loss: 0.98334, time: 0.01749\n",
      "Epoch: 169, train_loss: 0.98378, time: 0.01758\n",
      "Epoch: 170, train_loss: 0.98455, time: 0.01675\n",
      "Epoch: 171, train_loss: 0.98411, time: 0.01775\n",
      "Epoch: 172, train_loss: 0.98447, time: 0.01765\n",
      "Epoch: 173, train_loss: 0.98360, time: 0.01778\n",
      "Epoch: 174, train_loss: 0.98329, time: 0.01649\n",
      "Epoch: 175, train_loss: 0.98310, time: 0.01646\n",
      "Epoch: 176, train_loss: 0.98410, time: 0.01739\n",
      "Epoch: 177, train_loss: 0.98462, time: 0.01776\n",
      "Epoch: 178, train_loss: 0.98506, time: 0.01750\n",
      "Epoch: 179, train_loss: 0.98643, time: 0.01744\n",
      "Epoch: 180, train_loss: 0.98607, time: 0.01774\n",
      "Epoch: 181, train_loss: 0.98474, time: 0.01702\n",
      "Epoch: 182, train_loss: 0.98290, time: 0.01807\n",
      "Epoch: 183, train_loss: 0.98411, time: 0.01696\n",
      "Epoch: 184, train_loss: 0.98355, time: 0.01787\n",
      "Epoch: 185, train_loss: 0.98556, time: 0.01614\n",
      "Epoch: 186, train_loss: 0.98574, time: 0.01616\n",
      "Epoch: 187, train_loss: 0.98380, time: 0.01530\n",
      "Epoch: 188, train_loss: 0.98370, time: 0.01688\n",
      "Epoch: 189, train_loss: 0.98551, time: 0.01672\n",
      "Epoch: 190, train_loss: 0.98306, time: 0.01736\n",
      "Epoch: 191, train_loss: 0.98354, time: 0.01753\n",
      "Epoch: 192, train_loss: 0.98309, time: 0.01732\n",
      "Epoch: 193, train_loss: 0.98408, time: 0.01749\n",
      "Epoch: 194, train_loss: 0.98466, time: 0.01655\n",
      "Epoch: 195, train_loss: 0.98368, time: 0.01716\n",
      "Epoch: 196, train_loss: 0.98389, time: 0.01785\n",
      "Epoch: 197, train_loss: 0.98290, time: 0.01740\n",
      "Epoch: 198, train_loss: 0.98383, time: 0.01811\n",
      "Epoch: 199, train_loss: 0.98478, time: 0.01672\n",
      "Epoch: 200, train_loss: 0.98384, time: 0.01763\n",
      "pairwise precision 0.07558 recall 0.89102 f1 0.13935\n",
      "average until now [0.5753375029330028, 0.8327292370821973, 0.6805080274490455]\n",
      "22 names 154.59564352035522 avg time 7.027074705470692\n",
      "Loading lan_sun dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 181 nodes, 1571 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.95959, time: 0.10913\n",
      "Epoch: 2, train_loss: 0.96097, time: 0.01209\n",
      "Epoch: 3, train_loss: 0.95321, time: 0.01044\n",
      "Epoch: 4, train_loss: 0.95758, time: 0.00999\n",
      "Epoch: 5, train_loss: 0.94844, time: 0.01012\n",
      "Epoch: 6, train_loss: 0.95397, time: 0.01001\n",
      "Epoch: 7, train_loss: 0.95107, time: 0.01036\n",
      "Epoch: 8, train_loss: 0.95347, time: 0.01042\n",
      "Epoch: 9, train_loss: 0.95628, time: 0.01051\n",
      "Epoch: 10, train_loss: 0.95705, time: 0.01013\n",
      "Epoch: 11, train_loss: 0.94966, time: 0.00996\n",
      "Epoch: 12, train_loss: 0.95242, time: 0.01016\n",
      "Epoch: 13, train_loss: 0.95101, time: 0.01028\n",
      "Epoch: 14, train_loss: 0.95390, time: 0.01028\n",
      "Epoch: 15, train_loss: 0.95232, time: 0.01011\n",
      "Epoch: 16, train_loss: 0.95174, time: 0.01035\n",
      "Epoch: 17, train_loss: 0.95735, time: 0.00995\n",
      "Epoch: 18, train_loss: 0.95447, time: 0.01027\n",
      "Epoch: 19, train_loss: 0.95180, time: 0.01017\n",
      "Epoch: 20, train_loss: 0.95290, time: 0.01008\n",
      "Epoch: 21, train_loss: 0.94936, time: 0.01058\n",
      "Epoch: 22, train_loss: 0.95126, time: 0.01023\n",
      "Epoch: 23, train_loss: 0.95830, time: 0.01001\n",
      "Epoch: 24, train_loss: 0.95117, time: 0.01021\n",
      "Epoch: 25, train_loss: 0.95289, time: 0.01013\n",
      "Epoch: 26, train_loss: 0.95311, time: 0.01038\n",
      "Epoch: 27, train_loss: 0.95432, time: 0.01032\n",
      "Epoch: 28, train_loss: 0.95212, time: 0.00998\n",
      "Epoch: 29, train_loss: 0.95238, time: 0.01029\n",
      "Epoch: 30, train_loss: 0.95059, time: 0.01028\n",
      "Epoch: 31, train_loss: 0.95212, time: 0.01024\n",
      "Epoch: 32, train_loss: 0.95294, time: 0.01038\n",
      "Epoch: 33, train_loss: 0.95221, time: 0.01035\n",
      "Epoch: 34, train_loss: 0.95327, time: 0.01020\n",
      "Epoch: 35, train_loss: 0.94785, time: 0.01019\n",
      "Epoch: 36, train_loss: 0.95442, time: 0.01009\n",
      "Epoch: 37, train_loss: 0.94875, time: 0.01029\n",
      "Epoch: 38, train_loss: 0.95305, time: 0.01016\n",
      "Epoch: 39, train_loss: 0.95259, time: 0.01036\n",
      "Epoch: 40, train_loss: 0.95390, time: 0.01035\n",
      "Epoch: 41, train_loss: 0.95527, time: 0.01042\n",
      "Epoch: 42, train_loss: 0.94873, time: 0.01027\n",
      "Epoch: 43, train_loss: 0.95224, time: 0.00997\n",
      "Epoch: 44, train_loss: 0.95015, time: 0.01020\n",
      "Epoch: 45, train_loss: 0.95539, time: 0.01019\n",
      "Epoch: 46, train_loss: 0.94766, time: 0.01027\n",
      "Epoch: 47, train_loss: 0.95325, time: 0.01032\n",
      "Epoch: 48, train_loss: 0.95271, time: 0.01021\n",
      "Epoch: 49, train_loss: 0.95266, time: 0.01025\n",
      "Epoch: 50, train_loss: 0.95261, time: 0.01023\n",
      "Epoch: 51, train_loss: 0.95333, time: 0.01006\n",
      "Epoch: 52, train_loss: 0.95361, time: 0.01016\n",
      "Epoch: 53, train_loss: 0.95475, time: 0.00985\n",
      "Epoch: 54, train_loss: 0.94928, time: 0.01035\n",
      "Epoch: 55, train_loss: 0.95043, time: 0.01016\n",
      "Epoch: 56, train_loss: 0.95053, time: 0.01030\n",
      "Epoch: 57, train_loss: 0.94972, time: 0.01005\n",
      "Epoch: 58, train_loss: 0.95240, time: 0.01035\n",
      "Epoch: 59, train_loss: 0.95179, time: 0.00996\n",
      "Epoch: 60, train_loss: 0.95185, time: 0.01023\n",
      "Epoch: 61, train_loss: 0.94940, time: 0.01026\n",
      "Epoch: 62, train_loss: 0.95457, time: 0.01035\n",
      "Epoch: 63, train_loss: 0.95006, time: 0.01012\n",
      "Epoch: 64, train_loss: 0.95314, time: 0.01002\n",
      "Epoch: 65, train_loss: 0.95088, time: 0.01016\n",
      "Epoch: 66, train_loss: 0.95151, time: 0.01043\n",
      "Epoch: 67, train_loss: 0.95370, time: 0.01030\n",
      "Epoch: 68, train_loss: 0.94661, time: 0.01040\n",
      "Epoch: 69, train_loss: 0.94829, time: 0.01035\n",
      "Epoch: 70, train_loss: 0.95160, time: 0.01021\n",
      "Epoch: 71, train_loss: 0.95082, time: 0.01019\n",
      "Epoch: 72, train_loss: 0.94905, time: 0.01026\n",
      "Epoch: 73, train_loss: 0.94824, time: 0.01040\n",
      "Epoch: 74, train_loss: 0.95295, time: 0.01039\n",
      "Epoch: 75, train_loss: 0.94830, time: 0.01032\n",
      "Epoch: 76, train_loss: 0.94907, time: 0.01031\n",
      "Epoch: 77, train_loss: 0.95432, time: 0.01007\n",
      "Epoch: 78, train_loss: 0.94952, time: 0.01043\n",
      "Epoch: 79, train_loss: 0.95092, time: 0.01011\n",
      "Epoch: 80, train_loss: 0.94685, time: 0.00996\n",
      "Epoch: 81, train_loss: 0.95661, time: 0.01051\n",
      "Epoch: 82, train_loss: 0.95014, time: 0.01034\n",
      "Epoch: 83, train_loss: 0.95216, time: 0.01030\n",
      "Epoch: 84, train_loss: 0.95439, time: 0.01025\n",
      "Epoch: 85, train_loss: 0.95446, time: 0.01036\n",
      "Epoch: 86, train_loss: 0.95403, time: 0.01027\n",
      "Epoch: 87, train_loss: 0.95448, time: 0.00999\n",
      "Epoch: 88, train_loss: 0.94898, time: 0.01035\n",
      "Epoch: 89, train_loss: 0.94804, time: 0.01047\n",
      "Epoch: 90, train_loss: 0.95141, time: 0.01029\n",
      "Epoch: 91, train_loss: 0.95365, time: 0.01010\n",
      "Epoch: 92, train_loss: 0.95176, time: 0.01029\n",
      "Epoch: 93, train_loss: 0.95137, time: 0.01011\n",
      "Epoch: 94, train_loss: 0.94740, time: 0.00998\n",
      "Epoch: 95, train_loss: 0.94909, time: 0.01019\n",
      "Epoch: 96, train_loss: 0.94883, time: 0.01044\n",
      "Epoch: 97, train_loss: 0.95241, time: 0.01029\n",
      "Epoch: 98, train_loss: 0.95031, time: 0.00981\n",
      "Epoch: 99, train_loss: 0.95190, time: 0.01005\n",
      "Epoch: 100, train_loss: 0.95201, time: 0.01000\n",
      "Epoch: 101, train_loss: 0.95054, time: 0.01047\n",
      "Epoch: 102, train_loss: 0.94945, time: 0.01007\n",
      "Epoch: 103, train_loss: 0.94826, time: 0.01014\n",
      "Epoch: 104, train_loss: 0.95356, time: 0.01019\n",
      "Epoch: 105, train_loss: 0.95073, time: 0.01051\n",
      "Epoch: 106, train_loss: 0.94946, time: 0.01008\n",
      "Epoch: 107, train_loss: 0.95117, time: 0.01011\n",
      "Epoch: 108, train_loss: 0.95143, time: 0.01092\n",
      "Epoch: 109, train_loss: 0.94870, time: 0.01028\n",
      "Epoch: 110, train_loss: 0.94959, time: 0.01007\n",
      "Epoch: 111, train_loss: 0.95018, time: 0.01043\n",
      "Epoch: 112, train_loss: 0.95530, time: 0.01032\n",
      "Epoch: 113, train_loss: 0.95516, time: 0.01028\n",
      "Epoch: 114, train_loss: 0.95173, time: 0.01041\n",
      "Epoch: 115, train_loss: 0.95116, time: 0.00985\n",
      "Epoch: 116, train_loss: 0.95598, time: 0.01030\n",
      "Epoch: 117, train_loss: 0.95067, time: 0.01026\n",
      "Epoch: 118, train_loss: 0.95139, time: 0.01035\n",
      "Epoch: 119, train_loss: 0.95114, time: 0.00998\n",
      "Epoch: 120, train_loss: 0.95096, time: 0.01018\n",
      "Epoch: 121, train_loss: 0.95077, time: 0.01055\n",
      "Epoch: 122, train_loss: 0.94893, time: 0.01015\n",
      "Epoch: 123, train_loss: 0.95216, time: 0.01024\n",
      "Epoch: 124, train_loss: 0.95535, time: 0.01028\n",
      "Epoch: 125, train_loss: 0.94725, time: 0.01007\n",
      "Epoch: 126, train_loss: 0.95224, time: 0.01015\n",
      "Epoch: 127, train_loss: 0.95090, time: 0.01025\n",
      "Epoch: 128, train_loss: 0.95101, time: 0.01024\n",
      "Epoch: 129, train_loss: 0.95072, time: 0.01017\n",
      "Epoch: 130, train_loss: 0.95210, time: 0.01031\n",
      "Epoch: 131, train_loss: 0.95609, time: 0.00999\n",
      "Epoch: 132, train_loss: 0.95083, time: 0.01042\n",
      "Epoch: 133, train_loss: 0.95083, time: 0.00993\n",
      "Epoch: 134, train_loss: 0.95317, time: 0.01043\n",
      "Epoch: 135, train_loss: 0.95497, time: 0.01041\n",
      "Epoch: 136, train_loss: 0.95143, time: 0.01026\n",
      "Epoch: 137, train_loss: 0.95148, time: 0.01026\n",
      "Epoch: 138, train_loss: 0.95077, time: 0.00997\n",
      "Epoch: 139, train_loss: 0.95253, time: 0.01040\n",
      "Epoch: 140, train_loss: 0.95121, time: 0.00993\n",
      "Epoch: 141, train_loss: 0.95102, time: 0.01042\n",
      "Epoch: 142, train_loss: 0.95421, time: 0.01019\n",
      "Epoch: 143, train_loss: 0.95310, time: 0.01022\n",
      "Epoch: 144, train_loss: 0.94594, time: 0.01049\n",
      "Epoch: 145, train_loss: 0.95167, time: 0.01007\n",
      "Epoch: 146, train_loss: 0.95139, time: 0.01015\n",
      "Epoch: 147, train_loss: 0.95187, time: 0.00992\n",
      "Epoch: 148, train_loss: 0.95156, time: 0.01009\n",
      "Epoch: 149, train_loss: 0.95413, time: 0.01019\n",
      "Epoch: 150, train_loss: 0.94905, time: 0.01026\n",
      "Epoch: 151, train_loss: 0.95231, time: 0.01014\n",
      "Epoch: 152, train_loss: 0.94944, time: 0.01033\n",
      "Epoch: 153, train_loss: 0.94951, time: 0.01003\n",
      "Epoch: 154, train_loss: 0.95311, time: 0.01019\n",
      "Epoch: 155, train_loss: 0.95298, time: 0.01017\n",
      "Epoch: 156, train_loss: 0.95108, time: 0.00989\n",
      "Epoch: 157, train_loss: 0.94869, time: 0.01006\n",
      "Epoch: 158, train_loss: 0.95123, time: 0.01027\n",
      "Epoch: 159, train_loss: 0.95156, time: 0.01034\n",
      "Epoch: 160, train_loss: 0.94900, time: 0.01015\n",
      "Epoch: 161, train_loss: 0.95138, time: 0.01025\n",
      "Epoch: 162, train_loss: 0.94678, time: 0.01032\n",
      "Epoch: 163, train_loss: 0.95144, time: 0.01033\n",
      "Epoch: 164, train_loss: 0.95095, time: 0.01008\n",
      "Epoch: 165, train_loss: 0.95350, time: 0.01037\n",
      "Epoch: 166, train_loss: 0.95283, time: 0.01002\n",
      "Epoch: 167, train_loss: 0.95176, time: 0.01017\n",
      "Epoch: 168, train_loss: 0.94875, time: 0.01003\n",
      "Epoch: 169, train_loss: 0.95188, time: 0.01018\n",
      "Epoch: 170, train_loss: 0.95285, time: 0.01016\n",
      "Epoch: 171, train_loss: 0.95106, time: 0.01044\n",
      "Epoch: 172, train_loss: 0.95336, time: 0.00984\n",
      "Epoch: 173, train_loss: 0.94904, time: 0.01013\n",
      "Epoch: 174, train_loss: 0.94946, time: 0.01035\n",
      "Epoch: 175, train_loss: 0.95325, time: 0.01030\n",
      "Epoch: 176, train_loss: 0.95382, time: 0.01009\n",
      "Epoch: 177, train_loss: 0.95001, time: 0.01057\n",
      "Epoch: 178, train_loss: 0.94917, time: 0.01047\n",
      "Epoch: 179, train_loss: 0.95089, time: 0.00996\n",
      "Epoch: 180, train_loss: 0.95041, time: 0.01029\n",
      "Epoch: 181, train_loss: 0.95129, time: 0.01037\n",
      "Epoch: 182, train_loss: 0.95090, time: 0.01021\n",
      "Epoch: 183, train_loss: 0.95047, time: 0.01019\n",
      "Epoch: 184, train_loss: 0.95187, time: 0.01023\n",
      "Epoch: 185, train_loss: 0.94879, time: 0.01026\n",
      "Epoch: 186, train_loss: 0.95170, time: 0.01019\n",
      "Epoch: 187, train_loss: 0.95007, time: 0.01011\n",
      "Epoch: 188, train_loss: 0.94913, time: 0.01017\n",
      "Epoch: 189, train_loss: 0.95081, time: 0.01017\n",
      "Epoch: 190, train_loss: 0.95066, time: 0.01006\n",
      "Epoch: 191, train_loss: 0.95174, time: 0.01036\n",
      "Epoch: 192, train_loss: 0.94997, time: 0.01014\n",
      "Epoch: 193, train_loss: 0.95055, time: 0.01022\n",
      "Epoch: 194, train_loss: 0.95242, time: 0.01035\n",
      "Epoch: 195, train_loss: 0.95108, time: 0.01009\n",
      "Epoch: 196, train_loss: 0.94678, time: 0.01020\n",
      "Epoch: 197, train_loss: 0.95138, time: 0.01001\n",
      "Epoch: 198, train_loss: 0.94702, time: 0.01017\n",
      "Epoch: 199, train_loss: 0.95044, time: 0.01000\n",
      "Epoch: 200, train_loss: 0.95106, time: 0.01045\n",
      "pairwise precision 0.32135 recall 0.58538 f1 0.41492\n",
      "average until now [0.5642945145913308, 0.8219750921565706, 0.6691858977169466]\n",
      "23 names 156.78446054458618 avg time 6.816715675851573\n",
      "Loading jian_du dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 162 nodes, 679 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98568, time: 0.10933\n",
      "Epoch: 2, train_loss: 0.97558, time: 0.01082\n",
      "Epoch: 3, train_loss: 0.97593, time: 0.00971\n",
      "Epoch: 4, train_loss: 0.97887, time: 0.00953\n",
      "Epoch: 5, train_loss: 0.97223, time: 0.00947\n",
      "Epoch: 6, train_loss: 0.97962, time: 0.00957\n",
      "Epoch: 7, train_loss: 0.97928, time: 0.00948\n",
      "Epoch: 8, train_loss: 0.97438, time: 0.00937\n",
      "Epoch: 9, train_loss: 0.97424, time: 0.00953\n",
      "Epoch: 10, train_loss: 0.97185, time: 0.00958\n",
      "Epoch: 11, train_loss: 0.97607, time: 0.00947\n",
      "Epoch: 12, train_loss: 0.97496, time: 0.00957\n",
      "Epoch: 13, train_loss: 0.97537, time: 0.00942\n",
      "Epoch: 14, train_loss: 0.97443, time: 0.00945\n",
      "Epoch: 15, train_loss: 0.97527, time: 0.00953\n",
      "Epoch: 16, train_loss: 0.97456, time: 0.00934\n",
      "Epoch: 17, train_loss: 0.97367, time: 0.00957\n",
      "Epoch: 18, train_loss: 0.98047, time: 0.00982\n",
      "Epoch: 19, train_loss: 0.97483, time: 0.00999\n",
      "Epoch: 20, train_loss: 0.96996, time: 0.00999\n",
      "Epoch: 21, train_loss: 0.97461, time: 0.00994\n",
      "Epoch: 22, train_loss: 0.97911, time: 0.01000\n",
      "Epoch: 23, train_loss: 0.97353, time: 0.01001\n",
      "Epoch: 24, train_loss: 0.97507, time: 0.01004\n",
      "Epoch: 25, train_loss: 0.97796, time: 0.00994\n",
      "Epoch: 26, train_loss: 0.98051, time: 0.01001\n",
      "Epoch: 27, train_loss: 0.97384, time: 0.00979\n",
      "Epoch: 28, train_loss: 0.97176, time: 0.00978\n",
      "Epoch: 29, train_loss: 0.97506, time: 0.00982\n",
      "Epoch: 30, train_loss: 0.97484, time: 0.00997\n",
      "Epoch: 31, train_loss: 0.97551, time: 0.01021\n",
      "Epoch: 32, train_loss: 0.97959, time: 0.01019\n",
      "Epoch: 33, train_loss: 0.97621, time: 0.00998\n",
      "Epoch: 34, train_loss: 0.97460, time: 0.00987\n",
      "Epoch: 35, train_loss: 0.97665, time: 0.01009\n",
      "Epoch: 36, train_loss: 0.97450, time: 0.01011\n",
      "Epoch: 37, train_loss: 0.97043, time: 0.01001\n",
      "Epoch: 38, train_loss: 0.97255, time: 0.01020\n",
      "Epoch: 39, train_loss: 0.97285, time: 0.00980\n",
      "Epoch: 40, train_loss: 0.97429, time: 0.00980\n",
      "Epoch: 41, train_loss: 0.97299, time: 0.00981\n",
      "Epoch: 42, train_loss: 0.97399, time: 0.00997\n",
      "Epoch: 43, train_loss: 0.97711, time: 0.00999\n",
      "Epoch: 44, train_loss: 0.97433, time: 0.01003\n",
      "Epoch: 45, train_loss: 0.97107, time: 0.00997\n",
      "Epoch: 46, train_loss: 0.97452, time: 0.01002\n",
      "Epoch: 47, train_loss: 0.97165, time: 0.01008\n",
      "Epoch: 48, train_loss: 0.97070, time: 0.01008\n",
      "Epoch: 49, train_loss: 0.97757, time: 0.01005\n",
      "Epoch: 50, train_loss: 0.97643, time: 0.01003\n",
      "Epoch: 51, train_loss: 0.97408, time: 0.01005\n",
      "Epoch: 52, train_loss: 0.97313, time: 0.01016\n",
      "Epoch: 53, train_loss: 0.97339, time: 0.01018\n",
      "Epoch: 54, train_loss: 0.97779, time: 0.01002\n",
      "Epoch: 55, train_loss: 0.97642, time: 0.01008\n",
      "Epoch: 56, train_loss: 0.97396, time: 0.00997\n",
      "Epoch: 57, train_loss: 0.97276, time: 0.00999\n",
      "Epoch: 58, train_loss: 0.97301, time: 0.01004\n",
      "Epoch: 59, train_loss: 0.97454, time: 0.01000\n",
      "Epoch: 60, train_loss: 0.97758, time: 0.00989\n",
      "Epoch: 61, train_loss: 0.97396, time: 0.01002\n",
      "Epoch: 62, train_loss: 0.97887, time: 0.00995\n",
      "Epoch: 63, train_loss: 0.96997, time: 0.01000\n",
      "Epoch: 64, train_loss: 0.97299, time: 0.01002\n",
      "Epoch: 65, train_loss: 0.97516, time: 0.01021\n",
      "Epoch: 66, train_loss: 0.97074, time: 0.01012\n",
      "Epoch: 67, train_loss: 0.97703, time: 0.01011\n",
      "Epoch: 68, train_loss: 0.97148, time: 0.01002\n",
      "Epoch: 69, train_loss: 0.97155, time: 0.01000\n",
      "Epoch: 70, train_loss: 0.97565, time: 0.00990\n",
      "Epoch: 71, train_loss: 0.97150, time: 0.01012\n",
      "Epoch: 72, train_loss: 0.97356, time: 0.01015\n",
      "Epoch: 73, train_loss: 0.97644, time: 0.01006\n",
      "Epoch: 74, train_loss: 0.97258, time: 0.01006\n",
      "Epoch: 75, train_loss: 0.96870, time: 0.00992\n",
      "Epoch: 76, train_loss: 0.96965, time: 0.01005\n",
      "Epoch: 77, train_loss: 0.97409, time: 0.01013\n",
      "Epoch: 78, train_loss: 0.97283, time: 0.01008\n",
      "Epoch: 79, train_loss: 0.97249, time: 0.00992\n",
      "Epoch: 80, train_loss: 0.97258, time: 0.00986\n",
      "Epoch: 81, train_loss: 0.97156, time: 0.01001\n",
      "Epoch: 82, train_loss: 0.97253, time: 0.00993\n",
      "Epoch: 83, train_loss: 0.97395, time: 0.00993\n",
      "Epoch: 84, train_loss: 0.97619, time: 0.01006\n",
      "Epoch: 85, train_loss: 0.97291, time: 0.01002\n",
      "Epoch: 86, train_loss: 0.97163, time: 0.01001\n",
      "Epoch: 87, train_loss: 0.97701, time: 0.01013\n",
      "Epoch: 88, train_loss: 0.97200, time: 0.00997\n",
      "Epoch: 89, train_loss: 0.97287, time: 0.01000\n",
      "Epoch: 90, train_loss: 0.97284, time: 0.01012\n",
      "Epoch: 91, train_loss: 0.96968, time: 0.01012\n",
      "Epoch: 92, train_loss: 0.97358, time: 0.01017\n",
      "Epoch: 93, train_loss: 0.97504, time: 0.01019\n",
      "Epoch: 94, train_loss: 0.97650, time: 0.01021\n",
      "Epoch: 95, train_loss: 0.97107, time: 0.01015\n",
      "Epoch: 96, train_loss: 0.97632, time: 0.00991\n",
      "Epoch: 97, train_loss: 0.96862, time: 0.00989\n",
      "Epoch: 98, train_loss: 0.97755, time: 0.00998\n",
      "Epoch: 99, train_loss: 0.97190, time: 0.01009\n",
      "Epoch: 100, train_loss: 0.97256, time: 0.01017\n",
      "Epoch: 101, train_loss: 0.97292, time: 0.01004\n",
      "Epoch: 102, train_loss: 0.97414, time: 0.01002\n",
      "Epoch: 103, train_loss: 0.97445, time: 0.00974\n",
      "Epoch: 104, train_loss: 0.97096, time: 0.01011\n",
      "Epoch: 105, train_loss: 0.97111, time: 0.00999\n",
      "Epoch: 106, train_loss: 0.97639, time: 0.01005\n",
      "Epoch: 107, train_loss: 0.97620, time: 0.01024\n",
      "Epoch: 108, train_loss: 0.97449, time: 0.01005\n",
      "Epoch: 109, train_loss: 0.97522, time: 0.00990\n",
      "Epoch: 110, train_loss: 0.97333, time: 0.00995\n",
      "Epoch: 111, train_loss: 0.97352, time: 0.00998\n",
      "Epoch: 112, train_loss: 0.97673, time: 0.00998\n",
      "Epoch: 113, train_loss: 0.97618, time: 0.01005\n",
      "Epoch: 114, train_loss: 0.97420, time: 0.01002\n",
      "Epoch: 115, train_loss: 0.97320, time: 0.01002\n",
      "Epoch: 116, train_loss: 0.97327, time: 0.00992\n",
      "Epoch: 117, train_loss: 0.97146, time: 0.00994\n",
      "Epoch: 118, train_loss: 0.97277, time: 0.00961\n",
      "Epoch: 119, train_loss: 0.97598, time: 0.00990\n",
      "Epoch: 120, train_loss: 0.97550, time: 0.01006\n",
      "Epoch: 121, train_loss: 0.97532, time: 0.01001\n",
      "Epoch: 122, train_loss: 0.97513, time: 0.00990\n",
      "Epoch: 123, train_loss: 0.97719, time: 0.00986\n",
      "Epoch: 124, train_loss: 0.97388, time: 0.00999\n",
      "Epoch: 125, train_loss: 0.97185, time: 0.01008\n",
      "Epoch: 126, train_loss: 0.97457, time: 0.00988\n",
      "Epoch: 127, train_loss: 0.97562, time: 0.00990\n",
      "Epoch: 128, train_loss: 0.97164, time: 0.00992\n",
      "Epoch: 129, train_loss: 0.98137, time: 0.00994\n",
      "Epoch: 130, train_loss: 0.97504, time: 0.00995\n",
      "Epoch: 131, train_loss: 0.97300, time: 0.01006\n",
      "Epoch: 132, train_loss: 0.97037, time: 0.01008\n",
      "Epoch: 133, train_loss: 0.97453, time: 0.01014\n",
      "Epoch: 134, train_loss: 0.97584, time: 0.01002\n",
      "Epoch: 135, train_loss: 0.97261, time: 0.00999\n",
      "Epoch: 136, train_loss: 0.97297, time: 0.00988\n",
      "Epoch: 137, train_loss: 0.97675, time: 0.00991\n",
      "Epoch: 138, train_loss: 0.97437, time: 0.01001\n",
      "Epoch: 139, train_loss: 0.97237, time: 0.01013\n",
      "Epoch: 140, train_loss: 0.97234, time: 0.01012\n",
      "Epoch: 141, train_loss: 0.97519, time: 0.01012\n",
      "Epoch: 142, train_loss: 0.97401, time: 0.00988\n",
      "Epoch: 143, train_loss: 0.97110, time: 0.00993\n",
      "Epoch: 144, train_loss: 0.97671, time: 0.00985\n",
      "Epoch: 145, train_loss: 0.97595, time: 0.00993\n",
      "Epoch: 146, train_loss: 0.97118, time: 0.01016\n",
      "Epoch: 147, train_loss: 0.97525, time: 0.00997\n",
      "Epoch: 148, train_loss: 0.97144, time: 0.00997\n",
      "Epoch: 149, train_loss: 0.97174, time: 0.00984\n",
      "Epoch: 150, train_loss: 0.97224, time: 0.01000\n",
      "Epoch: 151, train_loss: 0.97318, time: 0.01001\n",
      "Epoch: 152, train_loss: 0.97436, time: 0.00999\n",
      "Epoch: 153, train_loss: 0.96892, time: 0.01007\n",
      "Epoch: 154, train_loss: 0.97104, time: 0.01015\n",
      "Epoch: 155, train_loss: 0.96747, time: 0.01000\n",
      "Epoch: 156, train_loss: 0.97214, time: 0.00996\n",
      "Epoch: 157, train_loss: 0.97352, time: 0.00989\n",
      "Epoch: 158, train_loss: 0.97336, time: 0.01011\n",
      "Epoch: 159, train_loss: 0.97279, time: 0.00996\n",
      "Epoch: 160, train_loss: 0.97194, time: 0.00995\n",
      "Epoch: 161, train_loss: 0.97031, time: 0.00983\n",
      "Epoch: 162, train_loss: 0.97048, time: 0.00990\n",
      "Epoch: 163, train_loss: 0.97222, time: 0.00994\n",
      "Epoch: 164, train_loss: 0.97366, time: 0.00984\n",
      "Epoch: 165, train_loss: 0.97394, time: 0.00988\n",
      "Epoch: 166, train_loss: 0.97661, time: 0.00982\n",
      "Epoch: 167, train_loss: 0.97681, time: 0.01009\n",
      "Epoch: 168, train_loss: 0.97790, time: 0.01004\n",
      "Epoch: 169, train_loss: 0.97296, time: 0.01015\n",
      "Epoch: 170, train_loss: 0.97448, time: 0.01007\n",
      "Epoch: 171, train_loss: 0.97807, time: 0.01004\n",
      "Epoch: 172, train_loss: 0.97108, time: 0.01000\n",
      "Epoch: 173, train_loss: 0.97850, time: 0.00984\n",
      "Epoch: 174, train_loss: 0.97307, time: 0.01010\n",
      "Epoch: 175, train_loss: 0.97309, time: 0.01016\n",
      "Epoch: 176, train_loss: 0.97310, time: 0.01027\n",
      "Epoch: 177, train_loss: 0.96939, time: 0.01007\n",
      "Epoch: 178, train_loss: 0.97408, time: 0.01019\n",
      "Epoch: 179, train_loss: 0.97140, time: 0.01004\n",
      "Epoch: 180, train_loss: 0.97115, time: 0.01003\n",
      "Epoch: 181, train_loss: 0.97455, time: 0.01007\n",
      "Epoch: 182, train_loss: 0.97489, time: 0.01004\n",
      "Epoch: 183, train_loss: 0.97076, time: 0.00992\n",
      "Epoch: 184, train_loss: 0.97242, time: 0.01011\n",
      "Epoch: 185, train_loss: 0.97381, time: 0.01011\n",
      "Epoch: 186, train_loss: 0.97205, time: 0.01005\n",
      "Epoch: 187, train_loss: 0.97328, time: 0.00992\n",
      "Epoch: 188, train_loss: 0.97111, time: 0.00996\n",
      "Epoch: 189, train_loss: 0.96961, time: 0.00983\n",
      "Epoch: 190, train_loss: 0.97222, time: 0.00990\n",
      "Epoch: 191, train_loss: 0.96945, time: 0.01015\n",
      "Epoch: 192, train_loss: 0.97501, time: 0.00994\n",
      "Epoch: 193, train_loss: 0.97149, time: 0.01001\n",
      "Epoch: 194, train_loss: 0.97221, time: 0.00997\n",
      "Epoch: 195, train_loss: 0.97234, time: 0.00987\n",
      "Epoch: 196, train_loss: 0.97570, time: 0.01010\n",
      "Epoch: 197, train_loss: 0.97143, time: 0.00991\n",
      "Epoch: 198, train_loss: 0.97406, time: 0.00986\n",
      "Epoch: 199, train_loss: 0.97600, time: 0.00985\n",
      "Epoch: 200, train_loss: 0.97557, time: 0.00991\n",
      "pairwise precision 0.11998 recall 0.80210 f1 0.20874\n",
      "average until now [0.5457815874983075, 0.8211471249428367, 0.655728389259757]\n",
      "24 names 158.91747403144836 avg time 6.6215614179770155\n",
      "Loading jianhua_lu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 361 nodes, 10377 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.92134, time: 0.12143\n",
      "Epoch: 2, train_loss: 0.91983, time: 0.02143\n",
      "Epoch: 3, train_loss: 0.92074, time: 0.02094\n",
      "Epoch: 4, train_loss: 0.91812, time: 0.02018\n",
      "Epoch: 5, train_loss: 0.91922, time: 0.01930\n",
      "Epoch: 6, train_loss: 0.92026, time: 0.01986\n",
      "Epoch: 7, train_loss: 0.91897, time: 0.02050\n",
      "Epoch: 8, train_loss: 0.92038, time: 0.02066\n",
      "Epoch: 9, train_loss: 0.91789, time: 0.02096\n",
      "Epoch: 10, train_loss: 0.92015, time: 0.02155\n",
      "Epoch: 11, train_loss: 0.91737, time: 0.02138\n",
      "Epoch: 12, train_loss: 0.91814, time: 0.02053\n",
      "Epoch: 13, train_loss: 0.91916, time: 0.02055\n",
      "Epoch: 14, train_loss: 0.91985, time: 0.02025\n",
      "Epoch: 15, train_loss: 0.91891, time: 0.02054\n",
      "Epoch: 16, train_loss: 0.92041, time: 0.01984\n",
      "Epoch: 17, train_loss: 0.92100, time: 0.02055\n",
      "Epoch: 18, train_loss: 0.91949, time: 0.01999\n",
      "Epoch: 19, train_loss: 0.91890, time: 0.02075\n",
      "Epoch: 20, train_loss: 0.91953, time: 0.02013\n",
      "Epoch: 21, train_loss: 0.92090, time: 0.02047\n",
      "Epoch: 22, train_loss: 0.91647, time: 0.01995\n",
      "Epoch: 23, train_loss: 0.91845, time: 0.02175\n",
      "Epoch: 24, train_loss: 0.92017, time: 0.02081\n",
      "Epoch: 25, train_loss: 0.91887, time: 0.02170\n",
      "Epoch: 26, train_loss: 0.91856, time: 0.02096\n",
      "Epoch: 27, train_loss: 0.91726, time: 0.02111\n",
      "Epoch: 28, train_loss: 0.91923, time: 0.02140\n",
      "Epoch: 29, train_loss: 0.91977, time: 0.02157\n",
      "Epoch: 30, train_loss: 0.91829, time: 0.02149\n",
      "Epoch: 31, train_loss: 0.92042, time: 0.02123\n",
      "Epoch: 32, train_loss: 0.92096, time: 0.02008\n",
      "Epoch: 33, train_loss: 0.91935, time: 0.02174\n",
      "Epoch: 34, train_loss: 0.91978, time: 0.02176\n",
      "Epoch: 35, train_loss: 0.91998, time: 0.02211\n",
      "Epoch: 36, train_loss: 0.91827, time: 0.02162\n",
      "Epoch: 37, train_loss: 0.92044, time: 0.01984\n",
      "Epoch: 38, train_loss: 0.91870, time: 0.02116\n",
      "Epoch: 39, train_loss: 0.92021, time: 0.02066\n",
      "Epoch: 40, train_loss: 0.92117, time: 0.02208\n",
      "Epoch: 41, train_loss: 0.91966, time: 0.02174\n",
      "Epoch: 42, train_loss: 0.91802, time: 0.02144\n",
      "Epoch: 43, train_loss: 0.91898, time: 0.02090\n",
      "Epoch: 44, train_loss: 0.92134, time: 0.01958\n",
      "Epoch: 45, train_loss: 0.92081, time: 0.01982\n",
      "Epoch: 46, train_loss: 0.91929, time: 0.02120\n",
      "Epoch: 47, train_loss: 0.91794, time: 0.02058\n",
      "Epoch: 48, train_loss: 0.91954, time: 0.01974\n",
      "Epoch: 49, train_loss: 0.91845, time: 0.01984\n",
      "Epoch: 50, train_loss: 0.92009, time: 0.01849\n",
      "Epoch: 51, train_loss: 0.92046, time: 0.01908\n",
      "Epoch: 52, train_loss: 0.91946, time: 0.01863\n",
      "Epoch: 53, train_loss: 0.91968, time: 0.01856\n",
      "Epoch: 54, train_loss: 0.91971, time: 0.01885\n",
      "Epoch: 55, train_loss: 0.92083, time: 0.01815\n",
      "Epoch: 56, train_loss: 0.92030, time: 0.01890\n",
      "Epoch: 57, train_loss: 0.91933, time: 0.01974\n",
      "Epoch: 58, train_loss: 0.91943, time: 0.01925\n",
      "Epoch: 59, train_loss: 0.92098, time: 0.01924\n",
      "Epoch: 60, train_loss: 0.91988, time: 0.02049\n",
      "Epoch: 61, train_loss: 0.91956, time: 0.02030\n",
      "Epoch: 62, train_loss: 0.91916, time: 0.01940\n",
      "Epoch: 63, train_loss: 0.91785, time: 0.01972\n",
      "Epoch: 64, train_loss: 0.91972, time: 0.01950\n",
      "Epoch: 65, train_loss: 0.92069, time: 0.01946\n",
      "Epoch: 66, train_loss: 0.91966, time: 0.01967\n",
      "Epoch: 67, train_loss: 0.91853, time: 0.01978\n",
      "Epoch: 68, train_loss: 0.92036, time: 0.02055\n",
      "Epoch: 69, train_loss: 0.91772, time: 0.02126\n",
      "Epoch: 70, train_loss: 0.91953, time: 0.02116\n",
      "Epoch: 71, train_loss: 0.91776, time: 0.02035\n",
      "Epoch: 72, train_loss: 0.91809, time: 0.02045\n",
      "Epoch: 73, train_loss: 0.91898, time: 0.01987\n",
      "Epoch: 74, train_loss: 0.91805, time: 0.02021\n",
      "Epoch: 75, train_loss: 0.91785, time: 0.01944\n",
      "Epoch: 76, train_loss: 0.91761, time: 0.02032\n",
      "Epoch: 77, train_loss: 0.91967, time: 0.01945\n",
      "Epoch: 78, train_loss: 0.92027, time: 0.01968\n",
      "Epoch: 79, train_loss: 0.92097, time: 0.02128\n",
      "Epoch: 80, train_loss: 0.92002, time: 0.02066\n",
      "Epoch: 81, train_loss: 0.92020, time: 0.02072\n",
      "Epoch: 82, train_loss: 0.91880, time: 0.02222\n",
      "Epoch: 83, train_loss: 0.92153, time: 0.02071\n",
      "Epoch: 84, train_loss: 0.91872, time: 0.02121\n",
      "Epoch: 85, train_loss: 0.91829, time: 0.02186\n",
      "Epoch: 86, train_loss: 0.91975, time: 0.02016\n",
      "Epoch: 87, train_loss: 0.91813, time: 0.02074\n",
      "Epoch: 88, train_loss: 0.92048, time: 0.02099\n",
      "Epoch: 89, train_loss: 0.91919, time: 0.02155\n",
      "Epoch: 90, train_loss: 0.91783, time: 0.02102\n",
      "Epoch: 91, train_loss: 0.91967, time: 0.02027\n",
      "Epoch: 92, train_loss: 0.91928, time: 0.02111\n",
      "Epoch: 93, train_loss: 0.91852, time: 0.01968\n",
      "Epoch: 94, train_loss: 0.91941, time: 0.02096\n",
      "Epoch: 95, train_loss: 0.92009, time: 0.02109\n",
      "Epoch: 96, train_loss: 0.92062, time: 0.02102\n",
      "Epoch: 97, train_loss: 0.91931, time: 0.02084\n",
      "Epoch: 98, train_loss: 0.91893, time: 0.02067\n",
      "Epoch: 99, train_loss: 0.91872, time: 0.02100\n",
      "Epoch: 100, train_loss: 0.91904, time: 0.02099\n",
      "Epoch: 101, train_loss: 0.91892, time: 0.02022\n",
      "Epoch: 102, train_loss: 0.92160, time: 0.02176\n",
      "Epoch: 103, train_loss: 0.91778, time: 0.02167\n",
      "Epoch: 104, train_loss: 0.92145, time: 0.02215\n",
      "Epoch: 105, train_loss: 0.92068, time: 0.02221\n",
      "Epoch: 106, train_loss: 0.91871, time: 0.02048\n",
      "Epoch: 107, train_loss: 0.91829, time: 0.02003\n",
      "Epoch: 108, train_loss: 0.92069, time: 0.02002\n",
      "Epoch: 109, train_loss: 0.91947, time: 0.01924\n",
      "Epoch: 110, train_loss: 0.91877, time: 0.01955\n",
      "Epoch: 111, train_loss: 0.91948, time: 0.02023\n",
      "Epoch: 112, train_loss: 0.91862, time: 0.01989\n",
      "Epoch: 113, train_loss: 0.91800, time: 0.01861\n",
      "Epoch: 114, train_loss: 0.91762, time: 0.01944\n",
      "Epoch: 115, train_loss: 0.91931, time: 0.02061\n",
      "Epoch: 116, train_loss: 0.92008, time: 0.02025\n",
      "Epoch: 117, train_loss: 0.91923, time: 0.02034\n",
      "Epoch: 118, train_loss: 0.91851, time: 0.02060\n",
      "Epoch: 119, train_loss: 0.91945, time: 0.02246\n",
      "Epoch: 120, train_loss: 0.92017, time: 0.02055\n",
      "Epoch: 121, train_loss: 0.91851, time: 0.01958\n",
      "Epoch: 122, train_loss: 0.91829, time: 0.02154\n",
      "Epoch: 123, train_loss: 0.92029, time: 0.02155\n",
      "Epoch: 124, train_loss: 0.91955, time: 0.02152\n",
      "Epoch: 125, train_loss: 0.91791, time: 0.02135\n",
      "Epoch: 126, train_loss: 0.91866, time: 0.02071\n",
      "Epoch: 127, train_loss: 0.91904, time: 0.02104\n",
      "Epoch: 128, train_loss: 0.91903, time: 0.02083\n",
      "Epoch: 129, train_loss: 0.92033, time: 0.01981\n",
      "Epoch: 130, train_loss: 0.92097, time: 0.02165\n",
      "Epoch: 131, train_loss: 0.91800, time: 0.02206\n",
      "Epoch: 132, train_loss: 0.91912, time: 0.02179\n",
      "Epoch: 133, train_loss: 0.92007, time: 0.01976\n",
      "Epoch: 134, train_loss: 0.91968, time: 0.02083\n",
      "Epoch: 135, train_loss: 0.91664, time: 0.02150\n",
      "Epoch: 136, train_loss: 0.91803, time: 0.01993\n",
      "Epoch: 137, train_loss: 0.91847, time: 0.02103\n",
      "Epoch: 138, train_loss: 0.91911, time: 0.01991\n",
      "Epoch: 139, train_loss: 0.91917, time: 0.02021\n",
      "Epoch: 140, train_loss: 0.91958, time: 0.01962\n",
      "Epoch: 141, train_loss: 0.91910, time: 0.02029\n",
      "Epoch: 142, train_loss: 0.91806, time: 0.02048\n",
      "Epoch: 143, train_loss: 0.91733, time: 0.02142\n",
      "Epoch: 144, train_loss: 0.92031, time: 0.02027\n",
      "Epoch: 145, train_loss: 0.91845, time: 0.01949\n",
      "Epoch: 146, train_loss: 0.91881, time: 0.01934\n",
      "Epoch: 147, train_loss: 0.91894, time: 0.02033\n",
      "Epoch: 148, train_loss: 0.91829, time: 0.02036\n",
      "Epoch: 149, train_loss: 0.91772, time: 0.02214\n",
      "Epoch: 150, train_loss: 0.91879, time: 0.02091\n",
      "Epoch: 151, train_loss: 0.91937, time: 0.02018\n",
      "Epoch: 152, train_loss: 0.92043, time: 0.02092\n",
      "Epoch: 153, train_loss: 0.91913, time: 0.01962\n",
      "Epoch: 154, train_loss: 0.92186, time: 0.02003\n",
      "Epoch: 155, train_loss: 0.91898, time: 0.01949\n",
      "Epoch: 156, train_loss: 0.91863, time: 0.01957\n",
      "Epoch: 157, train_loss: 0.91766, time: 0.01954\n",
      "Epoch: 158, train_loss: 0.91742, time: 0.01963\n",
      "Epoch: 159, train_loss: 0.91914, time: 0.01880\n",
      "Epoch: 160, train_loss: 0.91835, time: 0.01935\n",
      "Epoch: 161, train_loss: 0.91922, time: 0.01912\n",
      "Epoch: 162, train_loss: 0.91835, time: 0.01949\n",
      "Epoch: 163, train_loss: 0.91905, time: 0.02063\n",
      "Epoch: 164, train_loss: 0.91829, time: 0.02060\n",
      "Epoch: 165, train_loss: 0.91735, time: 0.01993\n",
      "Epoch: 166, train_loss: 0.91787, time: 0.01957\n",
      "Epoch: 167, train_loss: 0.91954, time: 0.02086\n",
      "Epoch: 168, train_loss: 0.91855, time: 0.02012\n",
      "Epoch: 169, train_loss: 0.92003, time: 0.02068\n",
      "Epoch: 170, train_loss: 0.91805, time: 0.02028\n",
      "Epoch: 171, train_loss: 0.91826, time: 0.01959\n",
      "Epoch: 172, train_loss: 0.91924, time: 0.02008\n",
      "Epoch: 173, train_loss: 0.91850, time: 0.01999\n",
      "Epoch: 174, train_loss: 0.91962, time: 0.02081\n",
      "Epoch: 175, train_loss: 0.91989, time: 0.02063\n",
      "Epoch: 176, train_loss: 0.91733, time: 0.02065\n",
      "Epoch: 177, train_loss: 0.91965, time: 0.02002\n",
      "Epoch: 178, train_loss: 0.91833, time: 0.01991\n",
      "Epoch: 179, train_loss: 0.91943, time: 0.02049\n",
      "Epoch: 180, train_loss: 0.91987, time: 0.02024\n",
      "Epoch: 181, train_loss: 0.91684, time: 0.02039\n",
      "Epoch: 182, train_loss: 0.91813, time: 0.02032\n",
      "Epoch: 183, train_loss: 0.91890, time: 0.02190\n",
      "Epoch: 184, train_loss: 0.91811, time: 0.02144\n",
      "Epoch: 185, train_loss: 0.91753, time: 0.02115\n",
      "Epoch: 186, train_loss: 0.91935, time: 0.02101\n",
      "Epoch: 187, train_loss: 0.91798, time: 0.02182\n",
      "Epoch: 188, train_loss: 0.91748, time: 0.02082\n",
      "Epoch: 189, train_loss: 0.91769, time: 0.02186\n",
      "Epoch: 190, train_loss: 0.91769, time: 0.02065\n",
      "Epoch: 191, train_loss: 0.92068, time: 0.02093\n",
      "Epoch: 192, train_loss: 0.91883, time: 0.02056\n",
      "Epoch: 193, train_loss: 0.91834, time: 0.02180\n",
      "Epoch: 194, train_loss: 0.91972, time: 0.02010\n",
      "Epoch: 195, train_loss: 0.91803, time: 0.01958\n",
      "Epoch: 196, train_loss: 0.92048, time: 0.02147\n",
      "Epoch: 197, train_loss: 0.91829, time: 0.02147\n",
      "Epoch: 198, train_loss: 0.91856, time: 0.02142\n",
      "Epoch: 199, train_loss: 0.91756, time: 0.02147\n",
      "Epoch: 200, train_loss: 0.92011, time: 0.02107\n",
      "pairwise precision 0.74908 recall 0.99042 f1 0.85301\n",
      "average until now [0.5539134057212283, 0.8279180700853743, 0.6637494164640377]\n",
      "25 names 163.21526050567627 avg time 6.528610420227051\n",
      "Loading jing_luo dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 469 nodes, 2702 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99247, time: 0.11514\n",
      "Epoch: 2, train_loss: 0.98780, time: 0.01813\n",
      "Epoch: 3, train_loss: 0.98646, time: 0.01605\n",
      "Epoch: 4, train_loss: 0.98681, time: 0.01584\n",
      "Epoch: 5, train_loss: 0.98819, time: 0.01529\n",
      "Epoch: 6, train_loss: 0.98721, time: 0.01517\n",
      "Epoch: 7, train_loss: 0.98654, time: 0.01498\n",
      "Epoch: 8, train_loss: 0.98697, time: 0.01598\n",
      "Epoch: 9, train_loss: 0.98561, time: 0.01595\n",
      "Epoch: 10, train_loss: 0.98576, time: 0.01669\n",
      "Epoch: 11, train_loss: 0.98534, time: 0.01657\n",
      "Epoch: 12, train_loss: 0.98696, time: 0.01701\n",
      "Epoch: 13, train_loss: 0.98548, time: 0.01700\n",
      "Epoch: 14, train_loss: 0.98636, time: 0.01674\n",
      "Epoch: 15, train_loss: 0.98705, time: 0.01709\n",
      "Epoch: 16, train_loss: 0.98642, time: 0.01692\n",
      "Epoch: 17, train_loss: 0.98618, time: 0.01693\n",
      "Epoch: 18, train_loss: 0.98673, time: 0.01589\n",
      "Epoch: 19, train_loss: 0.98527, time: 0.01736\n",
      "Epoch: 20, train_loss: 0.98516, time: 0.01651\n",
      "Epoch: 21, train_loss: 0.98587, time: 0.01696\n",
      "Epoch: 22, train_loss: 0.98752, time: 0.01642\n",
      "Epoch: 23, train_loss: 0.98556, time: 0.01593\n",
      "Epoch: 24, train_loss: 0.98547, time: 0.01610\n",
      "Epoch: 25, train_loss: 0.98721, time: 0.01574\n",
      "Epoch: 26, train_loss: 0.98676, time: 0.01684\n",
      "Epoch: 27, train_loss: 0.98567, time: 0.01716\n",
      "Epoch: 28, train_loss: 0.98536, time: 0.01576\n",
      "Epoch: 29, train_loss: 0.98751, time: 0.01670\n",
      "Epoch: 30, train_loss: 0.98563, time: 0.01602\n",
      "Epoch: 31, train_loss: 0.98693, time: 0.01686\n",
      "Epoch: 32, train_loss: 0.98588, time: 0.01781\n",
      "Epoch: 33, train_loss: 0.98724, time: 0.01633\n",
      "Epoch: 34, train_loss: 0.98519, time: 0.01723\n",
      "Epoch: 35, train_loss: 0.98608, time: 0.01719\n",
      "Epoch: 36, train_loss: 0.98620, time: 0.01726\n",
      "Epoch: 37, train_loss: 0.98539, time: 0.01729\n",
      "Epoch: 38, train_loss: 0.98584, time: 0.01628\n",
      "Epoch: 39, train_loss: 0.98454, time: 0.01704\n",
      "Epoch: 40, train_loss: 0.98408, time: 0.01692\n",
      "Epoch: 41, train_loss: 0.98687, time: 0.01714\n",
      "Epoch: 42, train_loss: 0.98583, time: 0.01724\n",
      "Epoch: 43, train_loss: 0.98693, time: 0.01634\n",
      "Epoch: 44, train_loss: 0.98539, time: 0.01698\n",
      "Epoch: 45, train_loss: 0.98648, time: 0.01666\n",
      "Epoch: 46, train_loss: 0.98556, time: 0.01681\n",
      "Epoch: 47, train_loss: 0.98670, time: 0.01764\n",
      "Epoch: 48, train_loss: 0.98560, time: 0.01656\n",
      "Epoch: 49, train_loss: 0.98654, time: 0.01676\n",
      "Epoch: 50, train_loss: 0.98682, time: 0.01659\n",
      "Epoch: 51, train_loss: 0.98754, time: 0.01629\n",
      "Epoch: 52, train_loss: 0.98662, time: 0.01649\n",
      "Epoch: 53, train_loss: 0.98535, time: 0.01671\n",
      "Epoch: 54, train_loss: 0.98490, time: 0.01704\n",
      "Epoch: 55, train_loss: 0.98520, time: 0.01574\n",
      "Epoch: 56, train_loss: 0.98521, time: 0.01675\n",
      "Epoch: 57, train_loss: 0.98740, time: 0.01578\n",
      "Epoch: 58, train_loss: 0.98554, time: 0.01677\n",
      "Epoch: 59, train_loss: 0.98518, time: 0.01649\n",
      "Epoch: 60, train_loss: 0.98561, time: 0.01570\n",
      "Epoch: 61, train_loss: 0.98530, time: 0.01709\n",
      "Epoch: 62, train_loss: 0.98469, time: 0.01672\n",
      "Epoch: 63, train_loss: 0.98600, time: 0.01654\n",
      "Epoch: 64, train_loss: 0.98500, time: 0.01520\n",
      "Epoch: 65, train_loss: 0.98546, time: 0.01672\n",
      "Epoch: 66, train_loss: 0.98532, time: 0.01696\n",
      "Epoch: 67, train_loss: 0.98681, time: 0.01660\n",
      "Epoch: 68, train_loss: 0.98502, time: 0.01631\n",
      "Epoch: 69, train_loss: 0.98600, time: 0.01537\n",
      "Epoch: 70, train_loss: 0.98579, time: 0.01723\n",
      "Epoch: 71, train_loss: 0.98528, time: 0.01684\n",
      "Epoch: 72, train_loss: 0.98577, time: 0.01678\n",
      "Epoch: 73, train_loss: 0.98680, time: 0.01629\n",
      "Epoch: 74, train_loss: 0.98499, time: 0.01649\n",
      "Epoch: 75, train_loss: 0.98597, time: 0.01638\n",
      "Epoch: 76, train_loss: 0.98716, time: 0.01758\n",
      "Epoch: 77, train_loss: 0.98645, time: 0.01586\n",
      "Epoch: 78, train_loss: 0.98624, time: 0.01664\n",
      "Epoch: 79, train_loss: 0.98606, time: 0.01704\n",
      "Epoch: 80, train_loss: 0.98557, time: 0.01619\n",
      "Epoch: 81, train_loss: 0.98587, time: 0.01738\n",
      "Epoch: 82, train_loss: 0.98611, time: 0.01685\n",
      "Epoch: 83, train_loss: 0.98586, time: 0.01669\n",
      "Epoch: 84, train_loss: 0.98428, time: 0.01674\n",
      "Epoch: 85, train_loss: 0.98509, time: 0.01574\n",
      "Epoch: 86, train_loss: 0.98547, time: 0.01652\n",
      "Epoch: 87, train_loss: 0.98579, time: 0.01720\n",
      "Epoch: 88, train_loss: 0.98512, time: 0.01662\n",
      "Epoch: 89, train_loss: 0.98555, time: 0.01587\n",
      "Epoch: 90, train_loss: 0.98548, time: 0.01629\n",
      "Epoch: 91, train_loss: 0.98550, time: 0.01576\n",
      "Epoch: 92, train_loss: 0.98599, time: 0.01686\n",
      "Epoch: 93, train_loss: 0.98640, time: 0.01664\n",
      "Epoch: 94, train_loss: 0.98621, time: 0.01714\n",
      "Epoch: 95, train_loss: 0.98478, time: 0.01564\n",
      "Epoch: 96, train_loss: 0.98597, time: 0.01635\n",
      "Epoch: 97, train_loss: 0.98459, time: 0.01626\n",
      "Epoch: 98, train_loss: 0.98448, time: 0.01585\n",
      "Epoch: 99, train_loss: 0.98518, time: 0.01658\n",
      "Epoch: 100, train_loss: 0.98568, time: 0.01721\n",
      "Epoch: 101, train_loss: 0.98526, time: 0.01589\n",
      "Epoch: 102, train_loss: 0.98619, time: 0.01688\n",
      "Epoch: 103, train_loss: 0.98651, time: 0.01630\n",
      "Epoch: 104, train_loss: 0.98451, time: 0.01607\n",
      "Epoch: 105, train_loss: 0.98540, time: 0.01662\n",
      "Epoch: 106, train_loss: 0.98653, time: 0.01722\n",
      "Epoch: 107, train_loss: 0.98656, time: 0.01554\n",
      "Epoch: 108, train_loss: 0.98602, time: 0.01625\n",
      "Epoch: 109, train_loss: 0.98616, time: 0.01726\n",
      "Epoch: 110, train_loss: 0.98568, time: 0.01737\n",
      "Epoch: 111, train_loss: 0.98663, time: 0.01643\n",
      "Epoch: 112, train_loss: 0.98668, time: 0.01672\n",
      "Epoch: 113, train_loss: 0.98573, time: 0.01601\n",
      "Epoch: 114, train_loss: 0.98664, time: 0.01737\n",
      "Epoch: 115, train_loss: 0.98667, time: 0.01739\n",
      "Epoch: 116, train_loss: 0.98620, time: 0.01652\n",
      "Epoch: 117, train_loss: 0.98589, time: 0.01669\n",
      "Epoch: 118, train_loss: 0.98588, time: 0.01584\n",
      "Epoch: 119, train_loss: 0.98543, time: 0.01696\n",
      "Epoch: 120, train_loss: 0.98477, time: 0.01619\n",
      "Epoch: 121, train_loss: 0.98551, time: 0.01657\n",
      "Epoch: 122, train_loss: 0.98551, time: 0.01652\n",
      "Epoch: 123, train_loss: 0.98564, time: 0.01658\n",
      "Epoch: 124, train_loss: 0.98562, time: 0.01689\n",
      "Epoch: 125, train_loss: 0.98595, time: 0.01656\n",
      "Epoch: 126, train_loss: 0.98594, time: 0.01646\n",
      "Epoch: 127, train_loss: 0.98625, time: 0.01720\n",
      "Epoch: 128, train_loss: 0.98722, time: 0.01725\n",
      "Epoch: 129, train_loss: 0.98463, time: 0.01670\n",
      "Epoch: 130, train_loss: 0.98649, time: 0.01676\n",
      "Epoch: 131, train_loss: 0.98518, time: 0.01514\n",
      "Epoch: 132, train_loss: 0.98730, time: 0.01529\n",
      "Epoch: 133, train_loss: 0.98620, time: 0.01645\n",
      "Epoch: 134, train_loss: 0.98669, time: 0.01722\n",
      "Epoch: 135, train_loss: 0.98630, time: 0.01670\n",
      "Epoch: 136, train_loss: 0.98605, time: 0.01698\n",
      "Epoch: 137, train_loss: 0.98621, time: 0.01696\n",
      "Epoch: 138, train_loss: 0.98606, time: 0.01607\n",
      "Epoch: 139, train_loss: 0.98669, time: 0.01635\n",
      "Epoch: 140, train_loss: 0.98448, time: 0.01724\n",
      "Epoch: 141, train_loss: 0.98631, time: 0.01652\n",
      "Epoch: 142, train_loss: 0.98572, time: 0.01720\n",
      "Epoch: 143, train_loss: 0.98712, time: 0.01692\n",
      "Epoch: 144, train_loss: 0.98669, time: 0.01641\n",
      "Epoch: 145, train_loss: 0.98593, time: 0.01564\n",
      "Epoch: 146, train_loss: 0.98748, time: 0.01452\n",
      "Epoch: 147, train_loss: 0.98586, time: 0.01616\n",
      "Epoch: 148, train_loss: 0.98637, time: 0.01643\n",
      "Epoch: 149, train_loss: 0.98543, time: 0.01679\n",
      "Epoch: 150, train_loss: 0.98474, time: 0.01681\n",
      "Epoch: 151, train_loss: 0.98456, time: 0.01708\n",
      "Epoch: 152, train_loss: 0.98591, time: 0.01678\n",
      "Epoch: 153, train_loss: 0.98634, time: 0.01683\n",
      "Epoch: 154, train_loss: 0.98642, time: 0.01587\n",
      "Epoch: 155, train_loss: 0.98544, time: 0.01554\n",
      "Epoch: 156, train_loss: 0.98651, time: 0.01639\n",
      "Epoch: 157, train_loss: 0.98609, time: 0.01605\n",
      "Epoch: 158, train_loss: 0.98487, time: 0.01594\n",
      "Epoch: 159, train_loss: 0.98483, time: 0.01576\n",
      "Epoch: 160, train_loss: 0.98521, time: 0.01614\n",
      "Epoch: 161, train_loss: 0.98594, time: 0.01737\n",
      "Epoch: 162, train_loss: 0.98702, time: 0.01727\n",
      "Epoch: 163, train_loss: 0.98540, time: 0.01586\n",
      "Epoch: 164, train_loss: 0.98656, time: 0.01625\n",
      "Epoch: 165, train_loss: 0.98614, time: 0.01670\n",
      "Epoch: 166, train_loss: 0.98682, time: 0.01658\n",
      "Epoch: 167, train_loss: 0.98602, time: 0.01663\n",
      "Epoch: 168, train_loss: 0.98538, time: 0.01687\n",
      "Epoch: 169, train_loss: 0.98614, time: 0.01610\n",
      "Epoch: 170, train_loss: 0.98713, time: 0.01683\n",
      "Epoch: 171, train_loss: 0.98734, time: 0.01653\n",
      "Epoch: 172, train_loss: 0.98699, time: 0.01575\n",
      "Epoch: 173, train_loss: 0.98552, time: 0.01638\n",
      "Epoch: 174, train_loss: 0.98465, time: 0.01625\n",
      "Epoch: 175, train_loss: 0.98713, time: 0.01693\n",
      "Epoch: 176, train_loss: 0.98563, time: 0.01693\n",
      "Epoch: 177, train_loss: 0.98485, time: 0.01696\n",
      "Epoch: 178, train_loss: 0.98497, time: 0.01635\n",
      "Epoch: 179, train_loss: 0.98589, time: 0.01653\n",
      "Epoch: 180, train_loss: 0.98506, time: 0.01636\n",
      "Epoch: 181, train_loss: 0.98705, time: 0.01642\n",
      "Epoch: 182, train_loss: 0.98574, time: 0.01692\n",
      "Epoch: 183, train_loss: 0.98378, time: 0.01693\n",
      "Epoch: 184, train_loss: 0.98395, time: 0.01632\n",
      "Epoch: 185, train_loss: 0.98736, time: 0.01612\n",
      "Epoch: 186, train_loss: 0.98574, time: 0.01616\n",
      "Epoch: 187, train_loss: 0.98545, time: 0.01684\n",
      "Epoch: 188, train_loss: 0.98510, time: 0.01645\n",
      "Epoch: 189, train_loss: 0.98526, time: 0.01587\n",
      "Epoch: 190, train_loss: 0.98665, time: 0.01608\n",
      "Epoch: 191, train_loss: 0.98637, time: 0.01683\n",
      "Epoch: 192, train_loss: 0.98537, time: 0.01687\n",
      "Epoch: 193, train_loss: 0.98563, time: 0.01687\n",
      "Epoch: 194, train_loss: 0.98591, time: 0.01637\n",
      "Epoch: 195, train_loss: 0.98512, time: 0.01706\n",
      "Epoch: 196, train_loss: 0.98612, time: 0.01666\n",
      "Epoch: 197, train_loss: 0.98504, time: 0.01734\n",
      "Epoch: 198, train_loss: 0.98525, time: 0.01673\n",
      "Epoch: 199, train_loss: 0.98751, time: 0.01561\n",
      "Epoch: 200, train_loss: 0.98531, time: 0.01621\n",
      "pairwise precision 0.06576 recall 0.90484 f1 0.12261\n",
      "average until now [0.5351382607776505, 0.8308767051139111, 0.650994207234212]\n",
      "26 names 166.72456669807434 avg time 6.412483334541321\n",
      "Loading zhigang_zeng dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 235 nodes, 3753 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.93832, time: 0.11191\n",
      "Epoch: 2, train_loss: 0.93173, time: 0.01421\n",
      "Epoch: 3, train_loss: 0.93490, time: 0.01353\n",
      "Epoch: 4, train_loss: 0.93306, time: 0.01290\n",
      "Epoch: 5, train_loss: 0.93417, time: 0.01287\n",
      "Epoch: 6, train_loss: 0.93333, time: 0.01286\n",
      "Epoch: 7, train_loss: 0.93191, time: 0.01255\n",
      "Epoch: 8, train_loss: 0.93132, time: 0.01277\n",
      "Epoch: 9, train_loss: 0.93488, time: 0.01266\n",
      "Epoch: 10, train_loss: 0.93191, time: 0.01283\n",
      "Epoch: 11, train_loss: 0.93349, time: 0.01256\n",
      "Epoch: 12, train_loss: 0.92840, time: 0.01251\n",
      "Epoch: 13, train_loss: 0.92721, time: 0.01255\n",
      "Epoch: 14, train_loss: 0.93480, time: 0.01290\n",
      "Epoch: 15, train_loss: 0.93095, time: 0.01236\n",
      "Epoch: 16, train_loss: 0.93034, time: 0.01267\n",
      "Epoch: 17, train_loss: 0.93000, time: 0.01253\n",
      "Epoch: 18, train_loss: 0.93227, time: 0.01273\n",
      "Epoch: 19, train_loss: 0.93191, time: 0.01259\n",
      "Epoch: 20, train_loss: 0.92950, time: 0.01282\n",
      "Epoch: 21, train_loss: 0.93041, time: 0.01323\n",
      "Epoch: 22, train_loss: 0.93377, time: 0.01297\n",
      "Epoch: 23, train_loss: 0.93284, time: 0.01279\n",
      "Epoch: 24, train_loss: 0.93079, time: 0.01255\n",
      "Epoch: 25, train_loss: 0.92840, time: 0.01336\n",
      "Epoch: 26, train_loss: 0.93048, time: 0.01273\n",
      "Epoch: 27, train_loss: 0.93226, time: 0.01265\n",
      "Epoch: 28, train_loss: 0.93111, time: 0.01277\n",
      "Epoch: 29, train_loss: 0.93275, time: 0.01308\n",
      "Epoch: 30, train_loss: 0.93025, time: 0.01284\n",
      "Epoch: 31, train_loss: 0.93152, time: 0.01246\n",
      "Epoch: 32, train_loss: 0.93178, time: 0.01287\n",
      "Epoch: 33, train_loss: 0.93069, time: 0.01298\n",
      "Epoch: 34, train_loss: 0.93445, time: 0.01271\n",
      "Epoch: 35, train_loss: 0.93271, time: 0.01288\n",
      "Epoch: 36, train_loss: 0.93508, time: 0.01297\n",
      "Epoch: 37, train_loss: 0.93140, time: 0.01304\n",
      "Epoch: 38, train_loss: 0.93255, time: 0.01285\n",
      "Epoch: 39, train_loss: 0.93247, time: 0.01280\n",
      "Epoch: 40, train_loss: 0.93264, time: 0.01263\n",
      "Epoch: 41, train_loss: 0.93012, time: 0.01249\n",
      "Epoch: 42, train_loss: 0.92948, time: 0.01252\n",
      "Epoch: 43, train_loss: 0.93052, time: 0.01255\n",
      "Epoch: 44, train_loss: 0.92955, time: 0.01285\n",
      "Epoch: 45, train_loss: 0.93031, time: 0.01228\n",
      "Epoch: 46, train_loss: 0.92959, time: 0.01260\n",
      "Epoch: 47, train_loss: 0.93142, time: 0.01250\n",
      "Epoch: 48, train_loss: 0.93124, time: 0.01264\n",
      "Epoch: 49, train_loss: 0.93434, time: 0.01263\n",
      "Epoch: 50, train_loss: 0.92990, time: 0.01248\n",
      "Epoch: 51, train_loss: 0.93121, time: 0.01297\n",
      "Epoch: 52, train_loss: 0.93090, time: 0.01308\n",
      "Epoch: 53, train_loss: 0.92999, time: 0.01260\n",
      "Epoch: 54, train_loss: 0.92760, time: 0.01255\n",
      "Epoch: 55, train_loss: 0.93339, time: 0.01290\n",
      "Epoch: 56, train_loss: 0.93145, time: 0.01265\n",
      "Epoch: 57, train_loss: 0.93371, time: 0.01272\n",
      "Epoch: 58, train_loss: 0.93092, time: 0.01219\n",
      "Epoch: 59, train_loss: 0.92789, time: 0.01271\n",
      "Epoch: 60, train_loss: 0.93088, time: 0.01270\n",
      "Epoch: 61, train_loss: 0.92885, time: 0.01285\n",
      "Epoch: 62, train_loss: 0.93129, time: 0.01291\n",
      "Epoch: 63, train_loss: 0.92980, time: 0.01267\n",
      "Epoch: 64, train_loss: 0.93012, time: 0.01249\n",
      "Epoch: 65, train_loss: 0.92721, time: 0.01260\n",
      "Epoch: 66, train_loss: 0.93044, time: 0.01252\n",
      "Epoch: 67, train_loss: 0.92887, time: 0.01327\n",
      "Epoch: 68, train_loss: 0.92773, time: 0.01249\n",
      "Epoch: 69, train_loss: 0.92846, time: 0.01232\n",
      "Epoch: 70, train_loss: 0.92958, time: 0.01273\n",
      "Epoch: 71, train_loss: 0.92680, time: 0.01233\n",
      "Epoch: 72, train_loss: 0.92926, time: 0.01277\n",
      "Epoch: 73, train_loss: 0.92881, time: 0.01295\n",
      "Epoch: 74, train_loss: 0.93267, time: 0.01262\n",
      "Epoch: 75, train_loss: 0.92986, time: 0.01264\n",
      "Epoch: 76, train_loss: 0.92813, time: 0.01252\n",
      "Epoch: 77, train_loss: 0.93129, time: 0.01260\n",
      "Epoch: 78, train_loss: 0.92865, time: 0.01281\n",
      "Epoch: 79, train_loss: 0.93210, time: 0.01267\n",
      "Epoch: 80, train_loss: 0.93255, time: 0.01298\n",
      "Epoch: 81, train_loss: 0.93302, time: 0.01238\n",
      "Epoch: 82, train_loss: 0.93236, time: 0.01256\n",
      "Epoch: 83, train_loss: 0.93121, time: 0.01304\n",
      "Epoch: 84, train_loss: 0.93189, time: 0.01287\n",
      "Epoch: 85, train_loss: 0.93355, time: 0.01253\n",
      "Epoch: 86, train_loss: 0.93093, time: 0.01218\n",
      "Epoch: 87, train_loss: 0.92861, time: 0.01266\n",
      "Epoch: 88, train_loss: 0.93187, time: 0.01301\n",
      "Epoch: 89, train_loss: 0.93444, time: 0.01253\n",
      "Epoch: 90, train_loss: 0.93417, time: 0.01294\n",
      "Epoch: 91, train_loss: 0.93005, time: 0.01242\n",
      "Epoch: 92, train_loss: 0.92927, time: 0.01258\n",
      "Epoch: 93, train_loss: 0.92907, time: 0.01250\n",
      "Epoch: 94, train_loss: 0.93572, time: 0.01285\n",
      "Epoch: 95, train_loss: 0.92910, time: 0.01242\n",
      "Epoch: 96, train_loss: 0.92895, time: 0.01264\n",
      "Epoch: 97, train_loss: 0.92972, time: 0.01252\n",
      "Epoch: 98, train_loss: 0.92983, time: 0.01265\n",
      "Epoch: 99, train_loss: 0.92677, time: 0.01284\n",
      "Epoch: 100, train_loss: 0.93207, time: 0.01304\n",
      "Epoch: 101, train_loss: 0.93042, time: 0.01241\n",
      "Epoch: 102, train_loss: 0.92946, time: 0.01281\n",
      "Epoch: 103, train_loss: 0.92951, time: 0.01269\n",
      "Epoch: 104, train_loss: 0.93264, time: 0.01273\n",
      "Epoch: 105, train_loss: 0.93245, time: 0.01276\n",
      "Epoch: 106, train_loss: 0.92652, time: 0.01260\n",
      "Epoch: 107, train_loss: 0.92699, time: 0.01266\n",
      "Epoch: 108, train_loss: 0.92921, time: 0.01257\n",
      "Epoch: 109, train_loss: 0.92786, time: 0.01262\n",
      "Epoch: 110, train_loss: 0.93595, time: 0.01277\n",
      "Epoch: 111, train_loss: 0.92883, time: 0.01257\n",
      "Epoch: 112, train_loss: 0.92629, time: 0.01239\n",
      "Epoch: 113, train_loss: 0.92956, time: 0.01325\n",
      "Epoch: 114, train_loss: 0.92943, time: 0.01271\n",
      "Epoch: 115, train_loss: 0.92817, time: 0.01287\n",
      "Epoch: 116, train_loss: 0.92911, time: 0.01312\n",
      "Epoch: 117, train_loss: 0.92917, time: 0.01266\n",
      "Epoch: 118, train_loss: 0.92868, time: 0.01294\n",
      "Epoch: 119, train_loss: 0.92973, time: 0.01276\n",
      "Epoch: 120, train_loss: 0.92593, time: 0.01278\n",
      "Epoch: 121, train_loss: 0.92912, time: 0.01317\n",
      "Epoch: 122, train_loss: 0.93106, time: 0.01268\n",
      "Epoch: 123, train_loss: 0.92655, time: 0.01286\n",
      "Epoch: 124, train_loss: 0.92801, time: 0.01285\n",
      "Epoch: 125, train_loss: 0.92939, time: 0.01251\n",
      "Epoch: 126, train_loss: 0.93171, time: 0.01273\n",
      "Epoch: 127, train_loss: 0.93266, time: 0.01275\n",
      "Epoch: 128, train_loss: 0.93123, time: 0.01243\n",
      "Epoch: 129, train_loss: 0.93675, time: 0.01267\n",
      "Epoch: 130, train_loss: 0.93129, time: 0.01260\n",
      "Epoch: 131, train_loss: 0.92923, time: 0.01269\n",
      "Epoch: 132, train_loss: 0.92984, time: 0.01232\n",
      "Epoch: 133, train_loss: 0.93491, time: 0.01274\n",
      "Epoch: 134, train_loss: 0.92879, time: 0.01292\n",
      "Epoch: 135, train_loss: 0.93107, time: 0.01246\n",
      "Epoch: 136, train_loss: 0.93289, time: 0.01294\n",
      "Epoch: 137, train_loss: 0.93066, time: 0.01282\n",
      "Epoch: 138, train_loss: 0.93150, time: 0.01246\n",
      "Epoch: 139, train_loss: 0.93086, time: 0.01265\n",
      "Epoch: 140, train_loss: 0.92998, time: 0.01272\n",
      "Epoch: 141, train_loss: 0.92798, time: 0.01209\n",
      "Epoch: 142, train_loss: 0.93216, time: 0.01262\n",
      "Epoch: 143, train_loss: 0.93090, time: 0.01225\n",
      "Epoch: 144, train_loss: 0.93189, time: 0.01254\n",
      "Epoch: 145, train_loss: 0.93063, time: 0.01313\n",
      "Epoch: 146, train_loss: 0.93168, time: 0.01293\n",
      "Epoch: 147, train_loss: 0.93330, time: 0.01313\n",
      "Epoch: 148, train_loss: 0.92885, time: 0.01276\n",
      "Epoch: 149, train_loss: 0.92890, time: 0.01264\n",
      "Epoch: 150, train_loss: 0.93142, time: 0.01256\n",
      "Epoch: 151, train_loss: 0.93086, time: 0.01274\n",
      "Epoch: 152, train_loss: 0.93123, time: 0.01269\n",
      "Epoch: 153, train_loss: 0.93331, time: 0.01290\n",
      "Epoch: 154, train_loss: 0.93084, time: 0.01317\n",
      "Epoch: 155, train_loss: 0.93112, time: 0.01315\n",
      "Epoch: 156, train_loss: 0.92907, time: 0.01267\n",
      "Epoch: 157, train_loss: 0.92937, time: 0.01268\n",
      "Epoch: 158, train_loss: 0.93047, time: 0.01240\n",
      "Epoch: 159, train_loss: 0.92977, time: 0.01317\n",
      "Epoch: 160, train_loss: 0.92825, time: 0.01270\n",
      "Epoch: 161, train_loss: 0.92991, time: 0.01243\n",
      "Epoch: 162, train_loss: 0.92928, time: 0.01261\n",
      "Epoch: 163, train_loss: 0.93140, time: 0.01266\n",
      "Epoch: 164, train_loss: 0.92908, time: 0.01264\n",
      "Epoch: 165, train_loss: 0.93061, time: 0.01279\n",
      "Epoch: 166, train_loss: 0.93043, time: 0.01339\n",
      "Epoch: 167, train_loss: 0.93378, time: 0.01261\n",
      "Epoch: 168, train_loss: 0.93185, time: 0.01272\n",
      "Epoch: 169, train_loss: 0.92693, time: 0.01244\n",
      "Epoch: 170, train_loss: 0.93321, time: 0.01267\n",
      "Epoch: 171, train_loss: 0.93126, time: 0.01258\n",
      "Epoch: 172, train_loss: 0.93159, time: 0.01285\n",
      "Epoch: 173, train_loss: 0.92973, time: 0.01258\n",
      "Epoch: 174, train_loss: 0.93315, time: 0.01268\n",
      "Epoch: 175, train_loss: 0.92998, time: 0.01254\n",
      "Epoch: 176, train_loss: 0.93391, time: 0.01271\n",
      "Epoch: 177, train_loss: 0.92908, time: 0.01307\n",
      "Epoch: 178, train_loss: 0.93129, time: 0.01275\n",
      "Epoch: 179, train_loss: 0.93127, time: 0.01289\n",
      "Epoch: 180, train_loss: 0.93234, time: 0.01267\n",
      "Epoch: 181, train_loss: 0.92942, time: 0.01271\n",
      "Epoch: 182, train_loss: 0.93398, time: 0.01279\n",
      "Epoch: 183, train_loss: 0.93141, time: 0.01266\n",
      "Epoch: 184, train_loss: 0.92761, time: 0.01266\n",
      "Epoch: 185, train_loss: 0.92880, time: 0.01271\n",
      "Epoch: 186, train_loss: 0.93266, time: 0.01271\n",
      "Epoch: 187, train_loss: 0.93257, time: 0.01273\n",
      "Epoch: 188, train_loss: 0.93115, time: 0.01255\n",
      "Epoch: 189, train_loss: 0.93336, time: 0.01266\n",
      "Epoch: 190, train_loss: 0.93159, time: 0.01262\n",
      "Epoch: 191, train_loss: 0.92984, time: 0.01273\n",
      "Epoch: 192, train_loss: 0.93144, time: 0.01269\n",
      "Epoch: 193, train_loss: 0.93077, time: 0.01239\n",
      "Epoch: 194, train_loss: 0.93165, time: 0.01301\n",
      "Epoch: 195, train_loss: 0.93384, time: 0.01318\n",
      "Epoch: 196, train_loss: 0.93233, time: 0.01289\n",
      "Epoch: 197, train_loss: 0.93135, time: 0.01388\n",
      "Epoch: 198, train_loss: 0.93343, time: 0.01245\n",
      "Epoch: 199, train_loss: 0.93147, time: 0.01307\n",
      "Epoch: 200, train_loss: 0.93183, time: 0.01260\n",
      "pairwise precision 0.50386 recall 0.98463 f1 0.66660\n",
      "average until now [0.5339797496518189, 0.8365711075209684, 0.6518722427878121]\n",
      "27 names 169.4275839328766 avg time 6.275095701217651\n",
      "Loading jian_feng dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 465 nodes, 5322 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97952, time: 0.11817\n",
      "Epoch: 2, train_loss: 0.97642, time: 0.01996\n",
      "Epoch: 3, train_loss: 0.97604, time: 0.01944\n",
      "Epoch: 4, train_loss: 0.97431, time: 0.01901\n",
      "Epoch: 5, train_loss: 0.97312, time: 0.01841\n",
      "Epoch: 6, train_loss: 0.97424, time: 0.01889\n",
      "Epoch: 7, train_loss: 0.97371, time: 0.01880\n",
      "Epoch: 8, train_loss: 0.97491, time: 0.01841\n",
      "Epoch: 9, train_loss: 0.97453, time: 0.01871\n",
      "Epoch: 10, train_loss: 0.97377, time: 0.01879\n",
      "Epoch: 11, train_loss: 0.97445, time: 0.01896\n",
      "Epoch: 12, train_loss: 0.97355, time: 0.01908\n",
      "Epoch: 13, train_loss: 0.97539, time: 0.01842\n",
      "Epoch: 14, train_loss: 0.97480, time: 0.01797\n",
      "Epoch: 15, train_loss: 0.97426, time: 0.01822\n",
      "Epoch: 16, train_loss: 0.97432, time: 0.01894\n",
      "Epoch: 17, train_loss: 0.97399, time: 0.01825\n",
      "Epoch: 18, train_loss: 0.97356, time: 0.01771\n",
      "Epoch: 19, train_loss: 0.97311, time: 0.01805\n",
      "Epoch: 20, train_loss: 0.97368, time: 0.01917\n",
      "Epoch: 21, train_loss: 0.97516, time: 0.01765\n",
      "Epoch: 22, train_loss: 0.97319, time: 0.01796\n",
      "Epoch: 23, train_loss: 0.97238, time: 0.01735\n",
      "Epoch: 24, train_loss: 0.97304, time: 0.01832\n",
      "Epoch: 25, train_loss: 0.97274, time: 0.01957\n",
      "Epoch: 26, train_loss: 0.97450, time: 0.01941\n",
      "Epoch: 27, train_loss: 0.97305, time: 0.02028\n",
      "Epoch: 28, train_loss: 0.97305, time: 0.01928\n",
      "Epoch: 29, train_loss: 0.97383, time: 0.02004\n",
      "Epoch: 30, train_loss: 0.97455, time: 0.01842\n",
      "Epoch: 31, train_loss: 0.97390, time: 0.01934\n",
      "Epoch: 32, train_loss: 0.97277, time: 0.01998\n",
      "Epoch: 33, train_loss: 0.97371, time: 0.02015\n",
      "Epoch: 34, train_loss: 0.97249, time: 0.02007\n",
      "Epoch: 35, train_loss: 0.97353, time: 0.01952\n",
      "Epoch: 36, train_loss: 0.97333, time: 0.01927\n",
      "Epoch: 37, train_loss: 0.97569, time: 0.01891\n",
      "Epoch: 38, train_loss: 0.97364, time: 0.01857\n",
      "Epoch: 39, train_loss: 0.97401, time: 0.01941\n",
      "Epoch: 40, train_loss: 0.97434, time: 0.01936\n",
      "Epoch: 41, train_loss: 0.97329, time: 0.01932\n",
      "Epoch: 42, train_loss: 0.97369, time: 0.01974\n",
      "Epoch: 43, train_loss: 0.97298, time: 0.01912\n",
      "Epoch: 44, train_loss: 0.97234, time: 0.01923\n",
      "Epoch: 45, train_loss: 0.97196, time: 0.01998\n",
      "Epoch: 46, train_loss: 0.97337, time: 0.01981\n",
      "Epoch: 47, train_loss: 0.97433, time: 0.01911\n",
      "Epoch: 48, train_loss: 0.97304, time: 0.01924\n",
      "Epoch: 49, train_loss: 0.97439, time: 0.01937\n",
      "Epoch: 50, train_loss: 0.97404, time: 0.02043\n",
      "Epoch: 51, train_loss: 0.97355, time: 0.01969\n",
      "Epoch: 52, train_loss: 0.97299, time: 0.01978\n",
      "Epoch: 53, train_loss: 0.97315, time: 0.02001\n",
      "Epoch: 54, train_loss: 0.97248, time: 0.01994\n",
      "Epoch: 55, train_loss: 0.97379, time: 0.01893\n",
      "Epoch: 56, train_loss: 0.97300, time: 0.01912\n",
      "Epoch: 57, train_loss: 0.97444, time: 0.01937\n",
      "Epoch: 58, train_loss: 0.97348, time: 0.01883\n",
      "Epoch: 59, train_loss: 0.97474, time: 0.01786\n",
      "Epoch: 60, train_loss: 0.97258, time: 0.01850\n",
      "Epoch: 61, train_loss: 0.97282, time: 0.01880\n",
      "Epoch: 62, train_loss: 0.97492, time: 0.01894\n",
      "Epoch: 63, train_loss: 0.97306, time: 0.01932\n",
      "Epoch: 64, train_loss: 0.97384, time: 0.01839\n",
      "Epoch: 65, train_loss: 0.97262, time: 0.01914\n",
      "Epoch: 66, train_loss: 0.97416, time: 0.01911\n",
      "Epoch: 67, train_loss: 0.97355, time: 0.01844\n",
      "Epoch: 68, train_loss: 0.97343, time: 0.01912\n",
      "Epoch: 69, train_loss: 0.97347, time: 0.01815\n",
      "Epoch: 70, train_loss: 0.97325, time: 0.01830\n",
      "Epoch: 71, train_loss: 0.97399, time: 0.01908\n",
      "Epoch: 72, train_loss: 0.97624, time: 0.01948\n",
      "Epoch: 73, train_loss: 0.97505, time: 0.01855\n",
      "Epoch: 74, train_loss: 0.97259, time: 0.01929\n",
      "Epoch: 75, train_loss: 0.97354, time: 0.01876\n",
      "Epoch: 76, train_loss: 0.97259, time: 0.01918\n",
      "Epoch: 77, train_loss: 0.97367, time: 0.01870\n",
      "Epoch: 78, train_loss: 0.97303, time: 0.01845\n",
      "Epoch: 79, train_loss: 0.97377, time: 0.01906\n",
      "Epoch: 80, train_loss: 0.97485, time: 0.01822\n",
      "Epoch: 81, train_loss: 0.97322, time: 0.01978\n",
      "Epoch: 82, train_loss: 0.97392, time: 0.01946\n",
      "Epoch: 83, train_loss: 0.97309, time: 0.01983\n",
      "Epoch: 84, train_loss: 0.97237, time: 0.01976\n",
      "Epoch: 85, train_loss: 0.97446, time: 0.01989\n",
      "Epoch: 86, train_loss: 0.97363, time: 0.01975\n",
      "Epoch: 87, train_loss: 0.97494, time: 0.02010\n",
      "Epoch: 88, train_loss: 0.97370, time: 0.01934\n",
      "Epoch: 89, train_loss: 0.97393, time: 0.01896\n",
      "Epoch: 90, train_loss: 0.97285, time: 0.01976\n",
      "Epoch: 91, train_loss: 0.97435, time: 0.01801\n",
      "Epoch: 92, train_loss: 0.97300, time: 0.01892\n",
      "Epoch: 93, train_loss: 0.97409, time: 0.01891\n",
      "Epoch: 94, train_loss: 0.97242, time: 0.01890\n",
      "Epoch: 95, train_loss: 0.97304, time: 0.01750\n",
      "Epoch: 96, train_loss: 0.97258, time: 0.01818\n",
      "Epoch: 97, train_loss: 0.97280, time: 0.01819\n",
      "Epoch: 98, train_loss: 0.97424, time: 0.01707\n",
      "Epoch: 99, train_loss: 0.97324, time: 0.01829\n",
      "Epoch: 100, train_loss: 0.97439, time: 0.01863\n",
      "Epoch: 101, train_loss: 0.97298, time: 0.01930\n",
      "Epoch: 102, train_loss: 0.97405, time: 0.01842\n",
      "Epoch: 103, train_loss: 0.97353, time: 0.01876\n",
      "Epoch: 104, train_loss: 0.97372, time: 0.01951\n",
      "Epoch: 105, train_loss: 0.97211, time: 0.02022\n",
      "Epoch: 106, train_loss: 0.97442, time: 0.01938\n",
      "Epoch: 107, train_loss: 0.97310, time: 0.01924\n",
      "Epoch: 108, train_loss: 0.97233, time: 0.01879\n",
      "Epoch: 109, train_loss: 0.97319, time: 0.01791\n",
      "Epoch: 110, train_loss: 0.97445, time: 0.01957\n",
      "Epoch: 111, train_loss: 0.97312, time: 0.01899\n",
      "Epoch: 112, train_loss: 0.97291, time: 0.01918\n",
      "Epoch: 113, train_loss: 0.97431, time: 0.01986\n",
      "Epoch: 114, train_loss: 0.97347, time: 0.01849\n",
      "Epoch: 115, train_loss: 0.97439, time: 0.01817\n",
      "Epoch: 116, train_loss: 0.97472, time: 0.01957\n",
      "Epoch: 117, train_loss: 0.97525, time: 0.01978\n",
      "Epoch: 118, train_loss: 0.97398, time: 0.01918\n",
      "Epoch: 119, train_loss: 0.97222, time: 0.01973\n",
      "Epoch: 120, train_loss: 0.97404, time: 0.01875\n",
      "Epoch: 121, train_loss: 0.97291, time: 0.01893\n",
      "Epoch: 122, train_loss: 0.97376, time: 0.01831\n",
      "Epoch: 123, train_loss: 0.97523, time: 0.01897\n",
      "Epoch: 124, train_loss: 0.97377, time: 0.01923\n",
      "Epoch: 125, train_loss: 0.97280, time: 0.01882\n",
      "Epoch: 126, train_loss: 0.97367, time: 0.01808\n",
      "Epoch: 127, train_loss: 0.97337, time: 0.01866\n",
      "Epoch: 128, train_loss: 0.97413, time: 0.01940\n",
      "Epoch: 129, train_loss: 0.97409, time: 0.01954\n",
      "Epoch: 130, train_loss: 0.97396, time: 0.01877\n",
      "Epoch: 131, train_loss: 0.97347, time: 0.01946\n",
      "Epoch: 132, train_loss: 0.97396, time: 0.02001\n",
      "Epoch: 133, train_loss: 0.97389, time: 0.01903\n",
      "Epoch: 134, train_loss: 0.97334, time: 0.01921\n",
      "Epoch: 135, train_loss: 0.97291, time: 0.01932\n",
      "Epoch: 136, train_loss: 0.97411, time: 0.01863\n",
      "Epoch: 137, train_loss: 0.97281, time: 0.01881\n",
      "Epoch: 138, train_loss: 0.97507, time: 0.01897\n",
      "Epoch: 139, train_loss: 0.97408, time: 0.01750\n",
      "Epoch: 140, train_loss: 0.97277, time: 0.01767\n",
      "Epoch: 141, train_loss: 0.97393, time: 0.01840\n",
      "Epoch: 142, train_loss: 0.97289, time: 0.02015\n",
      "Epoch: 143, train_loss: 0.97402, time: 0.01973\n",
      "Epoch: 144, train_loss: 0.97508, time: 0.01904\n",
      "Epoch: 145, train_loss: 0.97296, time: 0.01915\n",
      "Epoch: 146, train_loss: 0.97429, time: 0.01955\n",
      "Epoch: 147, train_loss: 0.97362, time: 0.01978\n",
      "Epoch: 148, train_loss: 0.97420, time: 0.01935\n",
      "Epoch: 149, train_loss: 0.97254, time: 0.01899\n",
      "Epoch: 150, train_loss: 0.97292, time: 0.01845\n",
      "Epoch: 151, train_loss: 0.97296, time: 0.01891\n",
      "Epoch: 152, train_loss: 0.97561, time: 0.01919\n",
      "Epoch: 153, train_loss: 0.97345, time: 0.01881\n",
      "Epoch: 154, train_loss: 0.97393, time: 0.01868\n",
      "Epoch: 155, train_loss: 0.97319, time: 0.01866\n",
      "Epoch: 156, train_loss: 0.97346, time: 0.01987\n",
      "Epoch: 157, train_loss: 0.97327, time: 0.01975\n",
      "Epoch: 158, train_loss: 0.97350, time: 0.01938\n",
      "Epoch: 159, train_loss: 0.97503, time: 0.01899\n",
      "Epoch: 160, train_loss: 0.97425, time: 0.01877\n",
      "Epoch: 161, train_loss: 0.97306, time: 0.01776\n",
      "Epoch: 162, train_loss: 0.97417, time: 0.01840\n",
      "Epoch: 163, train_loss: 0.97254, time: 0.01848\n",
      "Epoch: 164, train_loss: 0.97398, time: 0.01866\n",
      "Epoch: 165, train_loss: 0.97438, time: 0.01923\n",
      "Epoch: 166, train_loss: 0.97331, time: 0.01892\n",
      "Epoch: 167, train_loss: 0.97387, time: 0.01941\n",
      "Epoch: 168, train_loss: 0.97385, time: 0.01913\n",
      "Epoch: 169, train_loss: 0.97264, time: 0.01883\n",
      "Epoch: 170, train_loss: 0.97165, time: 0.01919\n",
      "Epoch: 171, train_loss: 0.97303, time: 0.01862\n",
      "Epoch: 172, train_loss: 0.97415, time: 0.01922\n",
      "Epoch: 173, train_loss: 0.97446, time: 0.02012\n",
      "Epoch: 174, train_loss: 0.97290, time: 0.01999\n",
      "Epoch: 175, train_loss: 0.97297, time: 0.02017\n",
      "Epoch: 176, train_loss: 0.97268, time: 0.01977\n",
      "Epoch: 177, train_loss: 0.97366, time: 0.02032\n",
      "Epoch: 178, train_loss: 0.97344, time: 0.01988\n",
      "Epoch: 179, train_loss: 0.97451, time: 0.01937\n",
      "Epoch: 180, train_loss: 0.97262, time: 0.02019\n",
      "Epoch: 181, train_loss: 0.97304, time: 0.01964\n",
      "Epoch: 182, train_loss: 0.97224, time: 0.01944\n",
      "Epoch: 183, train_loss: 0.97415, time: 0.01965\n",
      "Epoch: 184, train_loss: 0.97251, time: 0.01951\n",
      "Epoch: 185, train_loss: 0.97355, time: 0.01942\n",
      "Epoch: 186, train_loss: 0.97367, time: 0.01874\n",
      "Epoch: 187, train_loss: 0.97206, time: 0.01801\n",
      "Epoch: 188, train_loss: 0.97387, time: 0.01836\n",
      "Epoch: 189, train_loss: 0.97470, time: 0.01982\n",
      "Epoch: 190, train_loss: 0.97253, time: 0.01936\n",
      "Epoch: 191, train_loss: 0.97434, time: 0.01900\n",
      "Epoch: 192, train_loss: 0.97292, time: 0.01790\n",
      "Epoch: 193, train_loss: 0.97381, time: 0.01876\n",
      "Epoch: 194, train_loss: 0.97356, time: 0.01892\n",
      "Epoch: 195, train_loss: 0.97419, time: 0.01763\n",
      "Epoch: 196, train_loss: 0.97362, time: 0.01775\n",
      "Epoch: 197, train_loss: 0.97370, time: 0.01908\n",
      "Epoch: 198, train_loss: 0.97286, time: 0.01936\n",
      "Epoch: 199, train_loss: 0.97507, time: 0.02017\n",
      "Epoch: 200, train_loss: 0.97215, time: 0.01886\n",
      "pairwise precision 0.10945 recall 0.89698 f1 0.19509\n",
      "average until now [0.5188179422630191, 0.8387286626918687, 0.6410792488546195]\n",
      "28 names 173.4417760372162 avg time 6.194349144186292\n",
      "Loading hua_fu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 415 nodes, 8657 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.95173, time: 0.12138\n",
      "Epoch: 2, train_loss: 0.95194, time: 0.02209\n",
      "Epoch: 3, train_loss: 0.94702, time: 0.02040\n",
      "Epoch: 4, train_loss: 0.94893, time: 0.02040\n",
      "Epoch: 5, train_loss: 0.94808, time: 0.02080\n",
      "Epoch: 6, train_loss: 0.94980, time: 0.02039\n",
      "Epoch: 7, train_loss: 0.94778, time: 0.02019\n",
      "Epoch: 8, train_loss: 0.94999, time: 0.01939\n",
      "Epoch: 9, train_loss: 0.94595, time: 0.01920\n",
      "Epoch: 10, train_loss: 0.94926, time: 0.01966\n",
      "Epoch: 11, train_loss: 0.94775, time: 0.02078\n",
      "Epoch: 12, train_loss: 0.94876, time: 0.02027\n",
      "Epoch: 13, train_loss: 0.94914, time: 0.02066\n",
      "Epoch: 14, train_loss: 0.94765, time: 0.02025\n",
      "Epoch: 15, train_loss: 0.94821, time: 0.01827\n",
      "Epoch: 16, train_loss: 0.94987, time: 0.01995\n",
      "Epoch: 17, train_loss: 0.94774, time: 0.01957\n",
      "Epoch: 18, train_loss: 0.94941, time: 0.02008\n",
      "Epoch: 19, train_loss: 0.94976, time: 0.02166\n",
      "Epoch: 20, train_loss: 0.94759, time: 0.02146\n",
      "Epoch: 21, train_loss: 0.94979, time: 0.02123\n",
      "Epoch: 22, train_loss: 0.94812, time: 0.02066\n",
      "Epoch: 23, train_loss: 0.94700, time: 0.02142\n",
      "Epoch: 24, train_loss: 0.94779, time: 0.02151\n",
      "Epoch: 25, train_loss: 0.94691, time: 0.02082\n",
      "Epoch: 26, train_loss: 0.94724, time: 0.01968\n",
      "Epoch: 27, train_loss: 0.94842, time: 0.02119\n",
      "Epoch: 28, train_loss: 0.94842, time: 0.02085\n",
      "Epoch: 29, train_loss: 0.94689, time: 0.02116\n",
      "Epoch: 30, train_loss: 0.94836, time: 0.02111\n",
      "Epoch: 31, train_loss: 0.94770, time: 0.02089\n",
      "Epoch: 32, train_loss: 0.94778, time: 0.01963\n",
      "Epoch: 33, train_loss: 0.94766, time: 0.02021\n",
      "Epoch: 34, train_loss: 0.94839, time: 0.02040\n",
      "Epoch: 35, train_loss: 0.94872, time: 0.02010\n",
      "Epoch: 36, train_loss: 0.95009, time: 0.02075\n",
      "Epoch: 37, train_loss: 0.94889, time: 0.02085\n",
      "Epoch: 38, train_loss: 0.94962, time: 0.02061\n",
      "Epoch: 39, train_loss: 0.94675, time: 0.02115\n",
      "Epoch: 40, train_loss: 0.94814, time: 0.01960\n",
      "Epoch: 41, train_loss: 0.94766, time: 0.02111\n",
      "Epoch: 42, train_loss: 0.94801, time: 0.02146\n",
      "Epoch: 43, train_loss: 0.94763, time: 0.02069\n",
      "Epoch: 44, train_loss: 0.95087, time: 0.02122\n",
      "Epoch: 45, train_loss: 0.94622, time: 0.02129\n",
      "Epoch: 46, train_loss: 0.94616, time: 0.02125\n",
      "Epoch: 47, train_loss: 0.94688, time: 0.02117\n",
      "Epoch: 48, train_loss: 0.94729, time: 0.02039\n",
      "Epoch: 49, train_loss: 0.94765, time: 0.02054\n",
      "Epoch: 50, train_loss: 0.94817, time: 0.02186\n",
      "Epoch: 51, train_loss: 0.94821, time: 0.02211\n",
      "Epoch: 52, train_loss: 0.94870, time: 0.02119\n",
      "Epoch: 53, train_loss: 0.94640, time: 0.01930\n",
      "Epoch: 54, train_loss: 0.94678, time: 0.01998\n",
      "Epoch: 55, train_loss: 0.94823, time: 0.02123\n",
      "Epoch: 56, train_loss: 0.94896, time: 0.02117\n",
      "Epoch: 57, train_loss: 0.94821, time: 0.02011\n",
      "Epoch: 58, train_loss: 0.94720, time: 0.02093\n",
      "Epoch: 59, train_loss: 0.94814, time: 0.02112\n",
      "Epoch: 60, train_loss: 0.94773, time: 0.02163\n",
      "Epoch: 61, train_loss: 0.94805, time: 0.02199\n",
      "Epoch: 62, train_loss: 0.94754, time: 0.02173\n",
      "Epoch: 63, train_loss: 0.94787, time: 0.02092\n",
      "Epoch: 64, train_loss: 0.94704, time: 0.02087\n",
      "Epoch: 65, train_loss: 0.95039, time: 0.02128\n",
      "Epoch: 66, train_loss: 0.94991, time: 0.02044\n",
      "Epoch: 67, train_loss: 0.94885, time: 0.02026\n",
      "Epoch: 68, train_loss: 0.94799, time: 0.01987\n",
      "Epoch: 69, train_loss: 0.94742, time: 0.02032\n",
      "Epoch: 70, train_loss: 0.94888, time: 0.02035\n",
      "Epoch: 71, train_loss: 0.94708, time: 0.02044\n",
      "Epoch: 72, train_loss: 0.94805, time: 0.02003\n",
      "Epoch: 73, train_loss: 0.94793, time: 0.01993\n",
      "Epoch: 74, train_loss: 0.94813, time: 0.01983\n",
      "Epoch: 75, train_loss: 0.94774, time: 0.02081\n",
      "Epoch: 76, train_loss: 0.94622, time: 0.02101\n",
      "Epoch: 77, train_loss: 0.94762, time: 0.02144\n",
      "Epoch: 78, train_loss: 0.94815, time: 0.02112\n",
      "Epoch: 79, train_loss: 0.94876, time: 0.01984\n",
      "Epoch: 80, train_loss: 0.94812, time: 0.02141\n",
      "Epoch: 81, train_loss: 0.94831, time: 0.02110\n",
      "Epoch: 82, train_loss: 0.94792, time: 0.02115\n",
      "Epoch: 83, train_loss: 0.94779, time: 0.02160\n",
      "Epoch: 84, train_loss: 0.94906, time: 0.02120\n",
      "Epoch: 85, train_loss: 0.94836, time: 0.02069\n",
      "Epoch: 86, train_loss: 0.94771, time: 0.02136\n",
      "Epoch: 87, train_loss: 0.94728, time: 0.01978\n",
      "Epoch: 88, train_loss: 0.94910, time: 0.02029\n",
      "Epoch: 89, train_loss: 0.94871, time: 0.02036\n",
      "Epoch: 90, train_loss: 0.94779, time: 0.02050\n",
      "Epoch: 91, train_loss: 0.94847, time: 0.02084\n",
      "Epoch: 92, train_loss: 0.94825, time: 0.02056\n",
      "Epoch: 93, train_loss: 0.94744, time: 0.02118\n",
      "Epoch: 94, train_loss: 0.94798, time: 0.02110\n",
      "Epoch: 95, train_loss: 0.94821, time: 0.02093\n",
      "Epoch: 96, train_loss: 0.94698, time: 0.02050\n",
      "Epoch: 97, train_loss: 0.94738, time: 0.02045\n",
      "Epoch: 98, train_loss: 0.94778, time: 0.02028\n",
      "Epoch: 99, train_loss: 0.94841, time: 0.02004\n",
      "Epoch: 100, train_loss: 0.94996, time: 0.01999\n",
      "Epoch: 101, train_loss: 0.94861, time: 0.02127\n",
      "Epoch: 102, train_loss: 0.94878, time: 0.02053\n",
      "Epoch: 103, train_loss: 0.94781, time: 0.02092\n",
      "Epoch: 104, train_loss: 0.94811, time: 0.02027\n",
      "Epoch: 105, train_loss: 0.94684, time: 0.02033\n",
      "Epoch: 106, train_loss: 0.94621, time: 0.02016\n",
      "Epoch: 107, train_loss: 0.94760, time: 0.02058\n",
      "Epoch: 108, train_loss: 0.94818, time: 0.01900\n",
      "Epoch: 109, train_loss: 0.94755, time: 0.01938\n",
      "Epoch: 110, train_loss: 0.94750, time: 0.01925\n",
      "Epoch: 111, train_loss: 0.94737, time: 0.01956\n",
      "Epoch: 112, train_loss: 0.94972, time: 0.01996\n",
      "Epoch: 113, train_loss: 0.94892, time: 0.01917\n",
      "Epoch: 114, train_loss: 0.94806, time: 0.01978\n",
      "Epoch: 115, train_loss: 0.94932, time: 0.02114\n",
      "Epoch: 116, train_loss: 0.94814, time: 0.02131\n",
      "Epoch: 117, train_loss: 0.94746, time: 0.02073\n",
      "Epoch: 118, train_loss: 0.94732, time: 0.01956\n",
      "Epoch: 119, train_loss: 0.94863, time: 0.01926\n",
      "Epoch: 120, train_loss: 0.94902, time: 0.02014\n",
      "Epoch: 121, train_loss: 0.94932, time: 0.02102\n",
      "Epoch: 122, train_loss: 0.94843, time: 0.02176\n",
      "Epoch: 123, train_loss: 0.94949, time: 0.02051\n",
      "Epoch: 124, train_loss: 0.94753, time: 0.02085\n",
      "Epoch: 125, train_loss: 0.95055, time: 0.02028\n",
      "Epoch: 126, train_loss: 0.94817, time: 0.01961\n",
      "Epoch: 127, train_loss: 0.94946, time: 0.02027\n",
      "Epoch: 128, train_loss: 0.94906, time: 0.02038\n",
      "Epoch: 129, train_loss: 0.94758, time: 0.02001\n",
      "Epoch: 130, train_loss: 0.94661, time: 0.02054\n",
      "Epoch: 131, train_loss: 0.94908, time: 0.02077\n",
      "Epoch: 132, train_loss: 0.95004, time: 0.02125\n",
      "Epoch: 133, train_loss: 0.94833, time: 0.01929\n",
      "Epoch: 134, train_loss: 0.94965, time: 0.01943\n",
      "Epoch: 135, train_loss: 0.94725, time: 0.01942\n",
      "Epoch: 136, train_loss: 0.94773, time: 0.01926\n",
      "Epoch: 137, train_loss: 0.94757, time: 0.01929\n",
      "Epoch: 138, train_loss: 0.94792, time: 0.01867\n",
      "Epoch: 139, train_loss: 0.94747, time: 0.01935\n",
      "Epoch: 140, train_loss: 0.94765, time: 0.01942\n",
      "Epoch: 141, train_loss: 0.94726, time: 0.01989\n",
      "Epoch: 142, train_loss: 0.95023, time: 0.01923\n",
      "Epoch: 143, train_loss: 0.94802, time: 0.02024\n",
      "Epoch: 144, train_loss: 0.94779, time: 0.02027\n",
      "Epoch: 145, train_loss: 0.94832, time: 0.02049\n",
      "Epoch: 146, train_loss: 0.94726, time: 0.02024\n",
      "Epoch: 147, train_loss: 0.94862, time: 0.02117\n",
      "Epoch: 148, train_loss: 0.94751, time: 0.02129\n",
      "Epoch: 149, train_loss: 0.94833, time: 0.02036\n",
      "Epoch: 150, train_loss: 0.94876, time: 0.02103\n",
      "Epoch: 151, train_loss: 0.94823, time: 0.01987\n",
      "Epoch: 152, train_loss: 0.94874, time: 0.02117\n",
      "Epoch: 153, train_loss: 0.94654, time: 0.02023\n",
      "Epoch: 154, train_loss: 0.94736, time: 0.02107\n",
      "Epoch: 155, train_loss: 0.94680, time: 0.02035\n",
      "Epoch: 156, train_loss: 0.94746, time: 0.02064\n",
      "Epoch: 157, train_loss: 0.94818, time: 0.02139\n",
      "Epoch: 158, train_loss: 0.94722, time: 0.02077\n",
      "Epoch: 159, train_loss: 0.94946, time: 0.02098\n",
      "Epoch: 160, train_loss: 0.94791, time: 0.02036\n",
      "Epoch: 161, train_loss: 0.94814, time: 0.02033\n",
      "Epoch: 162, train_loss: 0.94953, time: 0.01979\n",
      "Epoch: 163, train_loss: 0.94754, time: 0.02062\n",
      "Epoch: 164, train_loss: 0.94913, time: 0.02067\n",
      "Epoch: 165, train_loss: 0.94750, time: 0.02099\n",
      "Epoch: 166, train_loss: 0.94703, time: 0.02013\n",
      "Epoch: 167, train_loss: 0.94959, time: 0.02127\n",
      "Epoch: 168, train_loss: 0.94774, time: 0.02074\n",
      "Epoch: 169, train_loss: 0.94617, time: 0.02082\n",
      "Epoch: 170, train_loss: 0.94834, time: 0.02107\n",
      "Epoch: 171, train_loss: 0.94823, time: 0.02036\n",
      "Epoch: 172, train_loss: 0.94658, time: 0.02000\n",
      "Epoch: 173, train_loss: 0.94686, time: 0.02099\n",
      "Epoch: 174, train_loss: 0.94892, time: 0.02068\n",
      "Epoch: 175, train_loss: 0.94838, time: 0.02084\n",
      "Epoch: 176, train_loss: 0.94734, time: 0.02249\n",
      "Epoch: 177, train_loss: 0.94960, time: 0.01947\n",
      "Epoch: 178, train_loss: 0.94696, time: 0.01905\n",
      "Epoch: 179, train_loss: 0.94836, time: 0.01935\n",
      "Epoch: 180, train_loss: 0.94754, time: 0.01942\n",
      "Epoch: 181, train_loss: 0.94880, time: 0.02058\n",
      "Epoch: 182, train_loss: 0.94798, time: 0.02063\n",
      "Epoch: 183, train_loss: 0.94856, time: 0.02015\n",
      "Epoch: 184, train_loss: 0.94981, time: 0.01982\n",
      "Epoch: 185, train_loss: 0.94770, time: 0.02141\n",
      "Epoch: 186, train_loss: 0.94703, time: 0.02162\n",
      "Epoch: 187, train_loss: 0.94825, time: 0.02081\n",
      "Epoch: 188, train_loss: 0.94929, time: 0.01951\n",
      "Epoch: 189, train_loss: 0.94641, time: 0.02042\n",
      "Epoch: 190, train_loss: 0.94739, time: 0.02064\n",
      "Epoch: 191, train_loss: 0.94854, time: 0.02190\n",
      "Epoch: 192, train_loss: 0.94674, time: 0.02024\n",
      "Epoch: 193, train_loss: 0.94791, time: 0.02053\n",
      "Epoch: 194, train_loss: 0.94945, time: 0.02159\n",
      "Epoch: 195, train_loss: 0.94927, time: 0.02098\n",
      "Epoch: 196, train_loss: 0.94705, time: 0.02062\n",
      "Epoch: 197, train_loss: 0.94814, time: 0.02012\n",
      "Epoch: 198, train_loss: 0.94698, time: 0.02130\n",
      "Epoch: 199, train_loss: 0.94896, time: 0.02090\n",
      "Epoch: 200, train_loss: 0.94584, time: 0.02032\n",
      "pairwise precision 0.17430 recall 0.94612 f1 0.29437\n",
      "average until now [0.5069379853472519, 0.8424319004557664, 0.6329780068497028]\n",
      "29 names 177.75216603279114 avg time 6.129385035613487\n",
      "Loading mei_xu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 98 nodes, 406 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.96820, time: 0.10779\n",
      "Epoch: 2, train_loss: 0.95839, time: 0.00999\n",
      "Epoch: 3, train_loss: 0.95965, time: 0.00878\n",
      "Epoch: 4, train_loss: 0.96306, time: 0.00878\n",
      "Epoch: 5, train_loss: 0.96195, time: 0.00871\n",
      "Epoch: 6, train_loss: 0.96061, time: 0.00872\n",
      "Epoch: 7, train_loss: 0.96324, time: 0.00873\n",
      "Epoch: 8, train_loss: 0.95466, time: 0.00860\n",
      "Epoch: 9, train_loss: 0.96084, time: 0.00866\n",
      "Epoch: 10, train_loss: 0.96364, time: 0.00870\n",
      "Epoch: 11, train_loss: 0.95806, time: 0.00872\n",
      "Epoch: 12, train_loss: 0.96428, time: 0.00866\n",
      "Epoch: 13, train_loss: 0.96673, time: 0.00868\n",
      "Epoch: 14, train_loss: 0.96205, time: 0.00866\n",
      "Epoch: 15, train_loss: 0.95786, time: 0.00879\n",
      "Epoch: 16, train_loss: 0.95879, time: 0.00874\n",
      "Epoch: 17, train_loss: 0.96018, time: 0.00867\n",
      "Epoch: 18, train_loss: 0.96046, time: 0.00865\n",
      "Epoch: 19, train_loss: 0.95556, time: 0.00872\n",
      "Epoch: 20, train_loss: 0.96235, time: 0.00873\n",
      "Epoch: 21, train_loss: 0.95899, time: 0.00861\n",
      "Epoch: 22, train_loss: 0.95773, time: 0.00879\n",
      "Epoch: 23, train_loss: 0.96351, time: 0.00865\n",
      "Epoch: 24, train_loss: 0.95486, time: 0.00885\n",
      "Epoch: 25, train_loss: 0.96396, time: 0.00881\n",
      "Epoch: 26, train_loss: 0.96427, time: 0.00867\n",
      "Epoch: 27, train_loss: 0.95818, time: 0.00864\n",
      "Epoch: 28, train_loss: 0.96163, time: 0.00875\n",
      "Epoch: 29, train_loss: 0.95961, time: 0.00872\n",
      "Epoch: 30, train_loss: 0.96113, time: 0.00872\n",
      "Epoch: 31, train_loss: 0.95980, time: 0.00870\n",
      "Epoch: 32, train_loss: 0.95513, time: 0.00866\n",
      "Epoch: 33, train_loss: 0.95993, time: 0.00873\n",
      "Epoch: 34, train_loss: 0.95580, time: 0.00872\n",
      "Epoch: 35, train_loss: 0.96578, time: 0.00872\n",
      "Epoch: 36, train_loss: 0.95605, time: 0.00872\n",
      "Epoch: 37, train_loss: 0.96354, time: 0.00848\n",
      "Epoch: 38, train_loss: 0.95676, time: 0.00850\n",
      "Epoch: 39, train_loss: 0.96147, time: 0.00874\n",
      "Epoch: 40, train_loss: 0.96253, time: 0.00872\n",
      "Epoch: 41, train_loss: 0.95901, time: 0.00881\n",
      "Epoch: 42, train_loss: 0.95582, time: 0.00883\n",
      "Epoch: 43, train_loss: 0.96957, time: 0.00859\n",
      "Epoch: 44, train_loss: 0.95416, time: 0.00856\n",
      "Epoch: 45, train_loss: 0.96359, time: 0.00864\n",
      "Epoch: 46, train_loss: 0.96949, time: 0.00864\n",
      "Epoch: 47, train_loss: 0.95709, time: 0.00874\n",
      "Epoch: 48, train_loss: 0.96108, time: 0.00877\n",
      "Epoch: 49, train_loss: 0.96171, time: 0.00878\n",
      "Epoch: 50, train_loss: 0.96499, time: 0.00868\n",
      "Epoch: 51, train_loss: 0.96113, time: 0.00867\n",
      "Epoch: 52, train_loss: 0.96484, time: 0.00874\n",
      "Epoch: 53, train_loss: 0.97072, time: 0.00866\n",
      "Epoch: 54, train_loss: 0.95860, time: 0.00865\n",
      "Epoch: 55, train_loss: 0.96055, time: 0.00872\n",
      "Epoch: 56, train_loss: 0.95791, time: 0.00876\n",
      "Epoch: 57, train_loss: 0.96272, time: 0.00880\n",
      "Epoch: 58, train_loss: 0.96211, time: 0.00871\n",
      "Epoch: 59, train_loss: 0.96100, time: 0.00866\n",
      "Epoch: 60, train_loss: 0.96798, time: 0.00867\n",
      "Epoch: 61, train_loss: 0.95670, time: 0.00871\n",
      "Epoch: 62, train_loss: 0.96223, time: 0.00868\n",
      "Epoch: 63, train_loss: 0.95388, time: 0.00863\n",
      "Epoch: 64, train_loss: 0.95562, time: 0.00873\n",
      "Epoch: 65, train_loss: 0.95958, time: 0.00868\n",
      "Epoch: 66, train_loss: 0.95618, time: 0.00866\n",
      "Epoch: 67, train_loss: 0.96528, time: 0.00853\n",
      "Epoch: 68, train_loss: 0.96063, time: 0.00861\n",
      "Epoch: 69, train_loss: 0.96097, time: 0.00862\n",
      "Epoch: 70, train_loss: 0.96668, time: 0.00872\n",
      "Epoch: 71, train_loss: 0.96216, time: 0.00863\n",
      "Epoch: 72, train_loss: 0.96150, time: 0.00878\n",
      "Epoch: 73, train_loss: 0.95974, time: 0.00863\n",
      "Epoch: 74, train_loss: 0.96454, time: 0.00867\n",
      "Epoch: 75, train_loss: 0.95881, time: 0.00855\n",
      "Epoch: 76, train_loss: 0.95700, time: 0.00857\n",
      "Epoch: 77, train_loss: 0.96258, time: 0.00867\n",
      "Epoch: 78, train_loss: 0.96225, time: 0.00857\n",
      "Epoch: 79, train_loss: 0.95480, time: 0.00862\n",
      "Epoch: 80, train_loss: 0.95913, time: 0.00870\n",
      "Epoch: 81, train_loss: 0.96250, time: 0.00862\n",
      "Epoch: 82, train_loss: 0.96201, time: 0.00862\n",
      "Epoch: 83, train_loss: 0.96132, time: 0.00861\n",
      "Epoch: 84, train_loss: 0.96262, time: 0.00867\n",
      "Epoch: 85, train_loss: 0.95909, time: 0.00864\n",
      "Epoch: 86, train_loss: 0.95808, time: 0.00864\n",
      "Epoch: 87, train_loss: 0.96325, time: 0.00869\n",
      "Epoch: 88, train_loss: 0.96520, time: 0.00861\n",
      "Epoch: 89, train_loss: 0.96086, time: 0.00862\n",
      "Epoch: 90, train_loss: 0.96032, time: 0.00863\n",
      "Epoch: 91, train_loss: 0.95991, time: 0.00863\n",
      "Epoch: 92, train_loss: 0.95984, time: 0.00858\n",
      "Epoch: 93, train_loss: 0.96960, time: 0.00866\n",
      "Epoch: 94, train_loss: 0.96618, time: 0.00862\n",
      "Epoch: 95, train_loss: 0.96210, time: 0.00863\n",
      "Epoch: 96, train_loss: 0.96643, time: 0.00880\n",
      "Epoch: 97, train_loss: 0.96174, time: 0.00886\n",
      "Epoch: 98, train_loss: 0.97284, time: 0.00864\n",
      "Epoch: 99, train_loss: 0.96103, time: 0.00872\n",
      "Epoch: 100, train_loss: 0.96376, time: 0.00868\n",
      "Epoch: 101, train_loss: 0.96146, time: 0.00876\n",
      "Epoch: 102, train_loss: 0.96371, time: 0.00866\n",
      "Epoch: 103, train_loss: 0.96834, time: 0.00860\n",
      "Epoch: 104, train_loss: 0.96381, time: 0.00861\n",
      "Epoch: 105, train_loss: 0.95850, time: 0.00864\n",
      "Epoch: 106, train_loss: 0.95984, time: 0.00865\n",
      "Epoch: 107, train_loss: 0.96769, time: 0.00869\n",
      "Epoch: 108, train_loss: 0.96340, time: 0.00887\n",
      "Epoch: 109, train_loss: 0.96037, time: 0.00888\n",
      "Epoch: 110, train_loss: 0.96615, time: 0.00882\n",
      "Epoch: 111, train_loss: 0.96877, time: 0.00868\n",
      "Epoch: 112, train_loss: 0.95906, time: 0.00864\n",
      "Epoch: 113, train_loss: 0.95874, time: 0.00861\n",
      "Epoch: 114, train_loss: 0.96590, time: 0.00872\n",
      "Epoch: 115, train_loss: 0.96164, time: 0.00870\n",
      "Epoch: 116, train_loss: 0.95808, time: 0.00861\n",
      "Epoch: 117, train_loss: 0.96122, time: 0.00873\n",
      "Epoch: 118, train_loss: 0.96206, time: 0.00879\n",
      "Epoch: 119, train_loss: 0.95958, time: 0.00879\n",
      "Epoch: 120, train_loss: 0.96584, time: 0.00875\n",
      "Epoch: 121, train_loss: 0.95582, time: 0.00874\n",
      "Epoch: 122, train_loss: 0.96547, time: 0.00856\n",
      "Epoch: 123, train_loss: 0.96136, time: 0.00864\n",
      "Epoch: 124, train_loss: 0.96310, time: 0.00865\n",
      "Epoch: 125, train_loss: 0.96480, time: 0.00867\n",
      "Epoch: 126, train_loss: 0.96229, time: 0.00867\n",
      "Epoch: 127, train_loss: 0.95919, time: 0.00872\n",
      "Epoch: 128, train_loss: 0.96738, time: 0.00869\n",
      "Epoch: 129, train_loss: 0.95472, time: 0.00867\n",
      "Epoch: 130, train_loss: 0.96346, time: 0.00864\n",
      "Epoch: 131, train_loss: 0.95880, time: 0.00865\n",
      "Epoch: 132, train_loss: 0.95925, time: 0.00875\n",
      "Epoch: 133, train_loss: 0.95693, time: 0.00871\n",
      "Epoch: 134, train_loss: 0.96410, time: 0.00873\n",
      "Epoch: 135, train_loss: 0.95861, time: 0.00872\n",
      "Epoch: 136, train_loss: 0.96086, time: 0.00882\n",
      "Epoch: 137, train_loss: 0.96187, time: 0.00859\n",
      "Epoch: 138, train_loss: 0.96447, time: 0.00863\n",
      "Epoch: 139, train_loss: 0.96674, time: 0.00877\n",
      "Epoch: 140, train_loss: 0.96158, time: 0.00879\n",
      "Epoch: 141, train_loss: 0.95956, time: 0.00863\n",
      "Epoch: 142, train_loss: 0.96613, time: 0.00865\n",
      "Epoch: 143, train_loss: 0.95991, time: 0.00866\n",
      "Epoch: 144, train_loss: 0.95772, time: 0.00889\n",
      "Epoch: 145, train_loss: 0.95354, time: 0.00871\n",
      "Epoch: 146, train_loss: 0.95629, time: 0.00857\n",
      "Epoch: 147, train_loss: 0.95577, time: 0.00869\n",
      "Epoch: 148, train_loss: 0.96332, time: 0.00859\n",
      "Epoch: 149, train_loss: 0.95595, time: 0.00856\n",
      "Epoch: 150, train_loss: 0.95978, time: 0.00867\n",
      "Epoch: 151, train_loss: 0.96125, time: 0.00868\n",
      "Epoch: 152, train_loss: 0.96038, time: 0.00864\n",
      "Epoch: 153, train_loss: 0.96779, time: 0.00873\n",
      "Epoch: 154, train_loss: 0.96385, time: 0.00865\n",
      "Epoch: 155, train_loss: 0.96086, time: 0.00867\n",
      "Epoch: 156, train_loss: 0.96364, time: 0.00861\n",
      "Epoch: 157, train_loss: 0.95509, time: 0.00872\n",
      "Epoch: 158, train_loss: 0.95812, time: 0.00865\n",
      "Epoch: 159, train_loss: 0.96091, time: 0.00871\n",
      "Epoch: 160, train_loss: 0.95907, time: 0.00864\n",
      "Epoch: 161, train_loss: 0.95657, time: 0.00860\n",
      "Epoch: 162, train_loss: 0.96149, time: 0.00871\n",
      "Epoch: 163, train_loss: 0.95332, time: 0.00875\n",
      "Epoch: 164, train_loss: 0.96077, time: 0.00864\n",
      "Epoch: 165, train_loss: 0.95148, time: 0.00871\n",
      "Epoch: 166, train_loss: 0.96201, time: 0.00871\n",
      "Epoch: 167, train_loss: 0.96206, time: 0.00866\n",
      "Epoch: 168, train_loss: 0.95684, time: 0.00878\n",
      "Epoch: 169, train_loss: 0.95767, time: 0.00877\n",
      "Epoch: 170, train_loss: 0.96056, time: 0.00868\n",
      "Epoch: 171, train_loss: 0.95934, time: 0.00871\n",
      "Epoch: 172, train_loss: 0.96057, time: 0.00869\n",
      "Epoch: 173, train_loss: 0.96212, time: 0.00858\n",
      "Epoch: 174, train_loss: 0.96572, time: 0.00864\n",
      "Epoch: 175, train_loss: 0.95656, time: 0.00872\n",
      "Epoch: 176, train_loss: 0.96480, time: 0.00869\n",
      "Epoch: 177, train_loss: 0.96489, time: 0.00865\n",
      "Epoch: 178, train_loss: 0.95769, time: 0.00876\n",
      "Epoch: 179, train_loss: 0.95828, time: 0.00863\n",
      "Epoch: 180, train_loss: 0.96137, time: 0.00859\n",
      "Epoch: 181, train_loss: 0.95935, time: 0.00854\n",
      "Epoch: 182, train_loss: 0.96312, time: 0.00864\n",
      "Epoch: 183, train_loss: 0.96554, time: 0.00871\n",
      "Epoch: 184, train_loss: 0.95727, time: 0.00870\n",
      "Epoch: 185, train_loss: 0.96409, time: 0.00868\n",
      "Epoch: 186, train_loss: 0.95963, time: 0.00868\n",
      "Epoch: 187, train_loss: 0.96234, time: 0.00867\n",
      "Epoch: 188, train_loss: 0.96233, time: 0.00867\n",
      "Epoch: 189, train_loss: 0.96257, time: 0.00880\n",
      "Epoch: 190, train_loss: 0.96048, time: 0.00867\n",
      "Epoch: 191, train_loss: 0.96118, time: 0.00870\n",
      "Epoch: 192, train_loss: 0.96239, time: 0.00873\n",
      "Epoch: 193, train_loss: 0.95800, time: 0.00869\n",
      "Epoch: 194, train_loss: 0.95922, time: 0.00868\n",
      "Epoch: 195, train_loss: 0.96347, time: 0.00864\n",
      "Epoch: 196, train_loss: 0.95970, time: 0.00875\n",
      "Epoch: 197, train_loss: 0.96072, time: 0.00867\n",
      "Epoch: 198, train_loss: 0.95458, time: 0.00862\n",
      "Epoch: 199, train_loss: 0.95835, time: 0.00873\n",
      "Epoch: 200, train_loss: 0.96061, time: 0.00867\n",
      "pairwise precision 0.11369 recall 0.79559 f1 0.19895\n",
      "average until now [0.4938296668589682, 0.8408705431860651, 0.6222323441442833]\n",
      "30 names 179.61803555488586 avg time 5.987267851829529\n",
      "Loading ruijin_liao dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 236 nodes, 15170 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.72758, time: 0.12188\n",
      "Epoch: 2, train_loss: 0.72786, time: 0.02291\n",
      "Epoch: 3, train_loss: 0.72575, time: 0.02214\n",
      "Epoch: 4, train_loss: 0.72739, time: 0.02125\n",
      "Epoch: 5, train_loss: 0.72867, time: 0.02138\n",
      "Epoch: 6, train_loss: 0.72478, time: 0.02125\n",
      "Epoch: 7, train_loss: 0.72566, time: 0.02192\n",
      "Epoch: 8, train_loss: 0.72570, time: 0.02149\n",
      "Epoch: 9, train_loss: 0.72661, time: 0.02166\n",
      "Epoch: 10, train_loss: 0.72552, time: 0.02151\n",
      "Epoch: 11, train_loss: 0.72475, time: 0.02167\n",
      "Epoch: 12, train_loss: 0.72573, time: 0.02077\n",
      "Epoch: 13, train_loss: 0.72716, time: 0.02141\n",
      "Epoch: 14, train_loss: 0.72482, time: 0.02180\n",
      "Epoch: 15, train_loss: 0.72785, time: 0.02136\n",
      "Epoch: 16, train_loss: 0.72633, time: 0.02163\n",
      "Epoch: 17, train_loss: 0.72672, time: 0.02176\n",
      "Epoch: 18, train_loss: 0.72665, time: 0.02183\n",
      "Epoch: 19, train_loss: 0.72582, time: 0.02203\n",
      "Epoch: 20, train_loss: 0.72612, time: 0.02112\n",
      "Epoch: 21, train_loss: 0.72556, time: 0.02204\n",
      "Epoch: 22, train_loss: 0.72800, time: 0.02268\n",
      "Epoch: 23, train_loss: 0.72560, time: 0.02138\n",
      "Epoch: 24, train_loss: 0.72717, time: 0.02095\n",
      "Epoch: 25, train_loss: 0.72383, time: 0.02059\n",
      "Epoch: 26, train_loss: 0.72975, time: 0.02033\n",
      "Epoch: 27, train_loss: 0.72786, time: 0.02085\n",
      "Epoch: 28, train_loss: 0.72958, time: 0.02091\n",
      "Epoch: 29, train_loss: 0.72550, time: 0.02064\n",
      "Epoch: 30, train_loss: 0.72592, time: 0.02148\n",
      "Epoch: 31, train_loss: 0.72590, time: 0.02144\n",
      "Epoch: 32, train_loss: 0.72639, time: 0.02123\n",
      "Epoch: 33, train_loss: 0.72579, time: 0.02078\n",
      "Epoch: 34, train_loss: 0.72685, time: 0.02147\n",
      "Epoch: 35, train_loss: 0.73009, time: 0.02120\n",
      "Epoch: 36, train_loss: 0.72669, time: 0.02116\n",
      "Epoch: 37, train_loss: 0.72477, time: 0.02099\n",
      "Epoch: 38, train_loss: 0.72759, time: 0.02146\n",
      "Epoch: 39, train_loss: 0.72625, time: 0.02152\n",
      "Epoch: 40, train_loss: 0.72788, time: 0.02198\n",
      "Epoch: 41, train_loss: 0.72629, time: 0.02160\n",
      "Epoch: 42, train_loss: 0.72732, time: 0.02139\n",
      "Epoch: 43, train_loss: 0.72706, time: 0.02135\n",
      "Epoch: 44, train_loss: 0.72676, time: 0.02116\n",
      "Epoch: 45, train_loss: 0.72317, time: 0.02156\n",
      "Epoch: 46, train_loss: 0.72535, time: 0.02147\n",
      "Epoch: 47, train_loss: 0.72944, time: 0.02087\n",
      "Epoch: 48, train_loss: 0.72203, time: 0.02122\n",
      "Epoch: 49, train_loss: 0.72675, time: 0.02178\n",
      "Epoch: 50, train_loss: 0.73032, time: 0.02125\n",
      "Epoch: 51, train_loss: 0.72429, time: 0.02149\n",
      "Epoch: 52, train_loss: 0.72744, time: 0.02057\n",
      "Epoch: 53, train_loss: 0.72587, time: 0.02079\n",
      "Epoch: 54, train_loss: 0.72452, time: 0.02200\n",
      "Epoch: 55, train_loss: 0.72645, time: 0.02189\n",
      "Epoch: 56, train_loss: 0.72852, time: 0.02186\n",
      "Epoch: 57, train_loss: 0.72635, time: 0.02093\n",
      "Epoch: 58, train_loss: 0.72611, time: 0.02097\n",
      "Epoch: 59, train_loss: 0.72247, time: 0.02161\n",
      "Epoch: 60, train_loss: 0.72564, time: 0.02136\n",
      "Epoch: 61, train_loss: 0.72601, time: 0.02172\n",
      "Epoch: 62, train_loss: 0.72266, time: 0.02163\n",
      "Epoch: 63, train_loss: 0.72075, time: 0.02179\n",
      "Epoch: 64, train_loss: 0.72554, time: 0.02054\n",
      "Epoch: 65, train_loss: 0.72262, time: 0.02256\n",
      "Epoch: 66, train_loss: 0.72588, time: 0.02107\n",
      "Epoch: 67, train_loss: 0.72682, time: 0.02192\n",
      "Epoch: 68, train_loss: 0.71685, time: 0.02209\n",
      "Epoch: 69, train_loss: 0.71232, time: 0.02182\n",
      "Epoch: 70, train_loss: 0.72148, time: 0.02183\n",
      "Epoch: 71, train_loss: 0.72509, time: 0.02192\n",
      "Epoch: 72, train_loss: 0.72273, time: 0.02212\n",
      "Epoch: 73, train_loss: 0.71670, time: 0.02205\n",
      "Epoch: 74, train_loss: 0.71446, time: 0.02167\n",
      "Epoch: 75, train_loss: 0.71900, time: 0.02187\n",
      "Epoch: 76, train_loss: 0.72864, time: 0.02155\n",
      "Epoch: 77, train_loss: 0.71061, time: 0.02205\n",
      "Epoch: 78, train_loss: 0.71197, time: 0.02138\n",
      "Epoch: 79, train_loss: 0.71848, time: 0.02218\n",
      "Epoch: 80, train_loss: 0.73118, time: 0.02123\n",
      "Epoch: 81, train_loss: 0.71776, time: 0.02213\n",
      "Epoch: 82, train_loss: 0.73630, time: 0.02220\n",
      "Epoch: 83, train_loss: 0.72235, time: 0.02157\n",
      "Epoch: 84, train_loss: 0.71391, time: 0.02224\n",
      "Epoch: 85, train_loss: 0.71881, time: 0.02170\n",
      "Epoch: 86, train_loss: 0.70008, time: 0.02193\n",
      "Epoch: 87, train_loss: 0.71394, time: 0.02154\n",
      "Epoch: 88, train_loss: 0.70614, time: 0.02207\n",
      "Epoch: 89, train_loss: 0.72894, time: 0.02096\n",
      "Epoch: 90, train_loss: 0.70503, time: 0.02107\n",
      "Epoch: 91, train_loss: 0.71586, time: 0.02136\n",
      "Epoch: 92, train_loss: 0.70944, time: 0.02219\n",
      "Epoch: 93, train_loss: 0.70304, time: 0.02197\n",
      "Epoch: 94, train_loss: 0.71630, time: 0.02103\n",
      "Epoch: 95, train_loss: 0.71340, time: 0.02224\n",
      "Epoch: 96, train_loss: 0.70194, time: 0.02088\n",
      "Epoch: 97, train_loss: 0.70696, time: 0.02133\n",
      "Epoch: 98, train_loss: 0.72281, time: 0.02158\n",
      "Epoch: 99, train_loss: 0.70305, time: 0.02103\n",
      "Epoch: 100, train_loss: 0.71261, time: 0.02138\n",
      "Epoch: 101, train_loss: 0.72062, time: 0.02178\n",
      "Epoch: 102, train_loss: 0.72793, time: 0.02144\n",
      "Epoch: 103, train_loss: 0.72246, time: 0.02176\n",
      "Epoch: 104, train_loss: 0.71186, time: 0.02193\n",
      "Epoch: 105, train_loss: 0.72589, time: 0.02137\n",
      "Epoch: 106, train_loss: 0.71942, time: 0.02064\n",
      "Epoch: 107, train_loss: 0.72594, time: 0.02116\n",
      "Epoch: 108, train_loss: 0.72048, time: 0.02157\n",
      "Epoch: 109, train_loss: 0.70330, time: 0.02112\n",
      "Epoch: 110, train_loss: 0.70631, time: 0.02106\n",
      "Epoch: 111, train_loss: 0.71740, time: 0.02243\n",
      "Epoch: 112, train_loss: 0.73060, time: 0.02216\n",
      "Epoch: 113, train_loss: 0.70863, time: 0.02186\n",
      "Epoch: 114, train_loss: 0.72358, time: 0.02110\n",
      "Epoch: 115, train_loss: 0.72993, time: 0.02129\n",
      "Epoch: 116, train_loss: 0.73388, time: 0.02135\n",
      "Epoch: 117, train_loss: 0.72479, time: 0.02143\n",
      "Epoch: 118, train_loss: 0.72060, time: 0.02099\n",
      "Epoch: 119, train_loss: 0.71698, time: 0.02121\n",
      "Epoch: 120, train_loss: 0.70795, time: 0.02103\n",
      "Epoch: 121, train_loss: 0.71583, time: 0.02084\n",
      "Epoch: 122, train_loss: 0.70780, time: 0.02173\n",
      "Epoch: 123, train_loss: 0.70860, time: 0.02201\n",
      "Epoch: 124, train_loss: 0.70603, time: 0.02136\n",
      "Epoch: 125, train_loss: 0.71796, time: 0.02105\n",
      "Epoch: 126, train_loss: 0.72358, time: 0.02142\n",
      "Epoch: 127, train_loss: 0.71783, time: 0.02123\n",
      "Epoch: 128, train_loss: 0.70274, time: 0.02125\n",
      "Epoch: 129, train_loss: 0.72104, time: 0.02135\n",
      "Epoch: 130, train_loss: 0.70071, time: 0.02150\n",
      "Epoch: 131, train_loss: 0.70942, time: 0.02240\n",
      "Epoch: 132, train_loss: 0.70853, time: 0.02143\n",
      "Epoch: 133, train_loss: 0.71344, time: 0.02075\n",
      "Epoch: 134, train_loss: 0.71202, time: 0.02053\n",
      "Epoch: 135, train_loss: 0.72242, time: 0.02120\n",
      "Epoch: 136, train_loss: 0.71387, time: 0.02100\n",
      "Epoch: 137, train_loss: 0.73779, time: 0.02173\n",
      "Epoch: 138, train_loss: 0.72231, time: 0.02212\n",
      "Epoch: 139, train_loss: 0.74067, time: 0.02175\n",
      "Epoch: 140, train_loss: 0.70540, time: 0.02148\n",
      "Epoch: 141, train_loss: 0.72572, time: 0.02133\n",
      "Epoch: 142, train_loss: 0.72218, time: 0.02113\n",
      "Epoch: 143, train_loss: 0.70175, time: 0.02101\n",
      "Epoch: 144, train_loss: 0.72951, time: 0.02141\n",
      "Epoch: 145, train_loss: 0.72677, time: 0.02183\n",
      "Epoch: 146, train_loss: 0.72857, time: 0.02053\n",
      "Epoch: 147, train_loss: 0.70317, time: 0.02081\n",
      "Epoch: 148, train_loss: 0.71147, time: 0.02127\n",
      "Epoch: 149, train_loss: 0.72207, time: 0.02086\n",
      "Epoch: 150, train_loss: 0.70771, time: 0.02168\n",
      "Epoch: 151, train_loss: 0.70744, time: 0.02161\n",
      "Epoch: 152, train_loss: 0.72403, time: 0.02094\n",
      "Epoch: 153, train_loss: 0.71415, time: 0.02181\n",
      "Epoch: 154, train_loss: 0.71634, time: 0.02170\n",
      "Epoch: 155, train_loss: 0.70528, time: 0.02111\n",
      "Epoch: 156, train_loss: 0.71689, time: 0.02133\n",
      "Epoch: 157, train_loss: 0.71056, time: 0.02151\n",
      "Epoch: 158, train_loss: 0.70674, time: 0.02154\n",
      "Epoch: 159, train_loss: 0.71022, time: 0.02249\n",
      "Epoch: 160, train_loss: 0.73576, time: 0.02108\n",
      "Epoch: 161, train_loss: 0.70702, time: 0.02231\n",
      "Epoch: 162, train_loss: 0.71913, time: 0.02126\n",
      "Epoch: 163, train_loss: 0.70707, time: 0.02098\n",
      "Epoch: 164, train_loss: 0.70101, time: 0.02140\n",
      "Epoch: 165, train_loss: 0.70242, time: 0.02172\n",
      "Epoch: 166, train_loss: 0.69540, time: 0.02077\n",
      "Epoch: 167, train_loss: 0.68442, time: 0.02188\n",
      "Epoch: 168, train_loss: 0.70595, time: 0.02173\n",
      "Epoch: 169, train_loss: 0.72109, time: 0.02116\n",
      "Epoch: 170, train_loss: 0.71256, time: 0.02133\n",
      "Epoch: 171, train_loss: 0.72081, time: 0.02162\n",
      "Epoch: 172, train_loss: 0.73083, time: 0.02088\n",
      "Epoch: 173, train_loss: 0.70090, time: 0.02043\n",
      "Epoch: 174, train_loss: 0.70153, time: 0.02116\n",
      "Epoch: 175, train_loss: 0.71030, time: 0.02082\n",
      "Epoch: 176, train_loss: 0.68895, time: 0.02064\n",
      "Epoch: 177, train_loss: nan, time: 0.02120\n",
      "Epoch: 178, train_loss: nan, time: 0.02118\n",
      "Epoch: 179, train_loss: nan, time: 0.02159\n",
      "Epoch: 180, train_loss: nan, time: 0.02144\n",
      "Epoch: 181, train_loss: nan, time: 0.02236\n",
      "Epoch: 182, train_loss: nan, time: 0.02283\n",
      "Epoch: 183, train_loss: nan, time: 0.02137\n",
      "Epoch: 184, train_loss: nan, time: 0.02136\n",
      "Epoch: 185, train_loss: nan, time: 0.02154\n",
      "Epoch: 186, train_loss: nan, time: 0.02139\n",
      "Epoch: 187, train_loss: nan, time: 0.02107\n",
      "Epoch: 188, train_loss: nan, time: 0.02099\n",
      "Epoch: 189, train_loss: nan, time: 0.02130\n",
      "Epoch: 190, train_loss: nan, time: 0.02115\n",
      "Epoch: 191, train_loss: nan, time: 0.02158\n",
      "Epoch: 192, train_loss: nan, time: 0.02086\n",
      "Epoch: 193, train_loss: nan, time: 0.02030\n",
      "Epoch: 194, train_loss: nan, time: 0.02172\n",
      "Epoch: 195, train_loss: nan, time: 0.02205\n",
      "Epoch: 196, train_loss: nan, time: 0.02194\n",
      "Epoch: 197, train_loss: nan, time: 0.02224\n",
      "Epoch: 198, train_loss: nan, time: 0.02148\n",
      "Epoch: 199, train_loss: nan, time: 0.02113\n",
      "Epoch: 200, train_loss: nan, time: 0.02093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/netdb/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:1047: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/netdb/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:1052: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/netdb/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples left after removing NaN values for clustering\n",
      "警告: 由于 clusters_pred 为 None，精确度、召回率和 F1 分数被设置为 0。\n",
      "pairwise precision 0.00000 recall 0.00000 f1 0.00000\n",
      "average until now [0.4778996776054531, 0.8137456869542565, 0.6021603330428548]\n",
      "31 names 184.09586358070374 avg time 5.93857624453883\n",
      "Loading yu_ming_wang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 227 nodes, 1649 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97226, time: 0.11048\n",
      "Epoch: 2, train_loss: 0.96663, time: 0.01314\n",
      "Epoch: 3, train_loss: 0.97059, time: 0.01118\n",
      "Epoch: 4, train_loss: 0.97090, time: 0.01136\n",
      "Epoch: 5, train_loss: 0.96791, time: 0.01140\n",
      "Epoch: 6, train_loss: 0.96856, time: 0.01121\n",
      "Epoch: 7, train_loss: 0.96928, time: 0.01137\n",
      "Epoch: 8, train_loss: 0.96957, time: 0.01104\n",
      "Epoch: 9, train_loss: 0.96666, time: 0.01103\n",
      "Epoch: 10, train_loss: 0.96609, time: 0.01097\n",
      "Epoch: 11, train_loss: 0.97209, time: 0.01116\n",
      "Epoch: 12, train_loss: 0.97116, time: 0.01103\n",
      "Epoch: 13, train_loss: 0.96757, time: 0.01138\n",
      "Epoch: 14, train_loss: 0.97097, time: 0.01120\n",
      "Epoch: 15, train_loss: 0.96880, time: 0.01094\n",
      "Epoch: 16, train_loss: 0.96876, time: 0.01116\n",
      "Epoch: 17, train_loss: 0.96602, time: 0.01103\n",
      "Epoch: 18, train_loss: 0.96485, time: 0.01141\n",
      "Epoch: 19, train_loss: 0.96950, time: 0.01158\n",
      "Epoch: 20, train_loss: 0.96804, time: 0.01089\n",
      "Epoch: 21, train_loss: 0.96748, time: 0.01091\n",
      "Epoch: 22, train_loss: 0.97072, time: 0.01111\n",
      "Epoch: 23, train_loss: 0.96957, time: 0.01150\n",
      "Epoch: 24, train_loss: 0.96757, time: 0.01151\n",
      "Epoch: 25, train_loss: 0.96598, time: 0.01111\n",
      "Epoch: 26, train_loss: 0.96920, time: 0.01130\n",
      "Epoch: 27, train_loss: 0.96528, time: 0.01106\n",
      "Epoch: 28, train_loss: 0.96872, time: 0.01123\n",
      "Epoch: 29, train_loss: 0.96784, time: 0.01118\n",
      "Epoch: 30, train_loss: 0.96582, time: 0.01090\n",
      "Epoch: 31, train_loss: 0.96614, time: 0.01099\n",
      "Epoch: 32, train_loss: 0.96858, time: 0.01119\n",
      "Epoch: 33, train_loss: 0.96842, time: 0.01109\n",
      "Epoch: 34, train_loss: 0.96712, time: 0.01116\n",
      "Epoch: 35, train_loss: 0.96741, time: 0.01099\n",
      "Epoch: 36, train_loss: 0.96899, time: 0.01118\n",
      "Epoch: 37, train_loss: 0.96749, time: 0.01093\n",
      "Epoch: 38, train_loss: 0.96748, time: 0.01119\n",
      "Epoch: 39, train_loss: 0.96698, time: 0.01086\n",
      "Epoch: 40, train_loss: 0.96698, time: 0.01138\n",
      "Epoch: 41, train_loss: 0.96682, time: 0.01118\n",
      "Epoch: 42, train_loss: 0.96581, time: 0.01108\n",
      "Epoch: 43, train_loss: 0.96823, time: 0.01094\n",
      "Epoch: 44, train_loss: 0.96494, time: 0.01076\n",
      "Epoch: 45, train_loss: 0.96612, time: 0.01121\n",
      "Epoch: 46, train_loss: 0.96591, time: 0.01116\n",
      "Epoch: 47, train_loss: 0.96798, time: 0.01137\n",
      "Epoch: 48, train_loss: 0.96797, time: 0.01085\n",
      "Epoch: 49, train_loss: 0.97156, time: 0.01123\n",
      "Epoch: 50, train_loss: 0.97028, time: 0.01114\n",
      "Epoch: 51, train_loss: 0.96806, time: 0.01110\n",
      "Epoch: 52, train_loss: 0.96492, time: 0.01113\n",
      "Epoch: 53, train_loss: 0.96615, time: 0.01106\n",
      "Epoch: 54, train_loss: 0.96651, time: 0.01136\n",
      "Epoch: 55, train_loss: 0.96792, time: 0.01112\n",
      "Epoch: 56, train_loss: 0.96541, time: 0.01091\n",
      "Epoch: 57, train_loss: 0.96584, time: 0.01155\n",
      "Epoch: 58, train_loss: 0.96534, time: 0.01119\n",
      "Epoch: 59, train_loss: 0.96770, time: 0.01118\n",
      "Epoch: 60, train_loss: 0.96673, time: 0.01126\n",
      "Epoch: 61, train_loss: 0.96676, time: 0.01119\n",
      "Epoch: 62, train_loss: 0.96902, time: 0.01088\n",
      "Epoch: 63, train_loss: 0.96541, time: 0.01108\n",
      "Epoch: 64, train_loss: 0.96806, time: 0.01131\n",
      "Epoch: 65, train_loss: 0.96838, time: 0.01111\n",
      "Epoch: 66, train_loss: 0.96652, time: 0.01113\n",
      "Epoch: 67, train_loss: 0.96790, time: 0.01102\n",
      "Epoch: 68, train_loss: 0.96707, time: 0.01105\n",
      "Epoch: 69, train_loss: 0.96587, time: 0.01122\n",
      "Epoch: 70, train_loss: 0.96929, time: 0.01111\n",
      "Epoch: 71, train_loss: 0.96756, time: 0.01115\n",
      "Epoch: 72, train_loss: 0.96695, time: 0.01117\n",
      "Epoch: 73, train_loss: 0.96916, time: 0.01104\n",
      "Epoch: 74, train_loss: 0.96811, time: 0.01128\n",
      "Epoch: 75, train_loss: 0.96568, time: 0.01106\n",
      "Epoch: 76, train_loss: 0.96605, time: 0.01114\n",
      "Epoch: 77, train_loss: 0.96631, time: 0.01100\n",
      "Epoch: 78, train_loss: 0.96486, time: 0.01106\n",
      "Epoch: 79, train_loss: 0.96882, time: 0.01130\n",
      "Epoch: 80, train_loss: 0.96962, time: 0.01116\n",
      "Epoch: 81, train_loss: 0.96606, time: 0.01125\n",
      "Epoch: 82, train_loss: 0.96759, time: 0.01092\n",
      "Epoch: 83, train_loss: 0.96717, time: 0.01113\n",
      "Epoch: 84, train_loss: 0.96536, time: 0.01116\n",
      "Epoch: 85, train_loss: 0.96701, time: 0.01111\n",
      "Epoch: 86, train_loss: 0.97191, time: 0.01112\n",
      "Epoch: 87, train_loss: 0.96772, time: 0.01098\n",
      "Epoch: 88, train_loss: 0.96534, time: 0.01108\n",
      "Epoch: 89, train_loss: 0.96662, time: 0.01112\n",
      "Epoch: 90, train_loss: 0.97083, time: 0.01130\n",
      "Epoch: 91, train_loss: 0.96870, time: 0.01085\n",
      "Epoch: 92, train_loss: 0.96651, time: 0.01109\n",
      "Epoch: 93, train_loss: 0.96673, time: 0.01127\n",
      "Epoch: 94, train_loss: 0.96430, time: 0.01094\n",
      "Epoch: 95, train_loss: 0.96613, time: 0.01109\n",
      "Epoch: 96, train_loss: 0.96350, time: 0.01088\n",
      "Epoch: 97, train_loss: 0.97036, time: 0.01101\n",
      "Epoch: 98, train_loss: 0.96856, time: 0.01125\n",
      "Epoch: 99, train_loss: 0.96791, time: 0.01102\n",
      "Epoch: 100, train_loss: 0.96834, time: 0.01102\n",
      "Epoch: 101, train_loss: 0.96638, time: 0.01106\n",
      "Epoch: 102, train_loss: 0.96721, time: 0.01121\n",
      "Epoch: 103, train_loss: 0.96728, time: 0.01110\n",
      "Epoch: 104, train_loss: 0.96698, time: 0.01113\n",
      "Epoch: 105, train_loss: 0.96715, time: 0.01128\n",
      "Epoch: 106, train_loss: 0.96697, time: 0.01124\n",
      "Epoch: 107, train_loss: 0.96673, time: 0.01129\n",
      "Epoch: 108, train_loss: 0.96551, time: 0.01082\n",
      "Epoch: 109, train_loss: 0.96726, time: 0.01102\n",
      "Epoch: 110, train_loss: 0.96634, time: 0.01116\n",
      "Epoch: 111, train_loss: 0.96653, time: 0.01093\n",
      "Epoch: 112, train_loss: 0.96705, time: 0.01096\n",
      "Epoch: 113, train_loss: 0.96802, time: 0.01102\n",
      "Epoch: 114, train_loss: 0.96530, time: 0.01087\n",
      "Epoch: 115, train_loss: 0.96868, time: 0.01104\n",
      "Epoch: 116, train_loss: 0.96960, time: 0.01119\n",
      "Epoch: 117, train_loss: 0.96632, time: 0.01149\n",
      "Epoch: 118, train_loss: 0.96561, time: 0.01137\n",
      "Epoch: 119, train_loss: 0.96345, time: 0.01143\n",
      "Epoch: 120, train_loss: 0.96853, time: 0.01107\n",
      "Epoch: 121, train_loss: 0.96823, time: 0.01104\n",
      "Epoch: 122, train_loss: 0.96563, time: 0.01125\n",
      "Epoch: 123, train_loss: 0.96626, time: 0.01143\n",
      "Epoch: 124, train_loss: 0.96679, time: 0.01120\n",
      "Epoch: 125, train_loss: 0.96542, time: 0.01110\n",
      "Epoch: 126, train_loss: 0.96507, time: 0.01118\n",
      "Epoch: 127, train_loss: 0.96560, time: 0.01115\n",
      "Epoch: 128, train_loss: 0.96839, time: 0.01089\n",
      "Epoch: 129, train_loss: 0.97159, time: 0.01100\n",
      "Epoch: 130, train_loss: 0.96590, time: 0.01097\n",
      "Epoch: 131, train_loss: 0.96689, time: 0.01110\n",
      "Epoch: 132, train_loss: 0.96607, time: 0.01123\n",
      "Epoch: 133, train_loss: 0.97047, time: 0.01117\n",
      "Epoch: 134, train_loss: 0.96815, time: 0.01119\n",
      "Epoch: 135, train_loss: 0.96517, time: 0.01105\n",
      "Epoch: 136, train_loss: 0.96871, time: 0.01105\n",
      "Epoch: 137, train_loss: 0.96514, time: 0.01110\n",
      "Epoch: 138, train_loss: 0.97059, time: 0.01117\n",
      "Epoch: 139, train_loss: 0.96786, time: 0.01124\n",
      "Epoch: 140, train_loss: 0.96741, time: 0.01118\n",
      "Epoch: 141, train_loss: 0.96672, time: 0.01108\n",
      "Epoch: 142, train_loss: 0.96570, time: 0.01116\n",
      "Epoch: 143, train_loss: 0.96594, time: 0.01098\n",
      "Epoch: 144, train_loss: 0.96450, time: 0.01115\n",
      "Epoch: 145, train_loss: 0.96735, time: 0.01095\n",
      "Epoch: 146, train_loss: 0.96647, time: 0.01079\n",
      "Epoch: 147, train_loss: 0.96454, time: 0.01116\n",
      "Epoch: 148, train_loss: 0.96427, time: 0.01069\n",
      "Epoch: 149, train_loss: 0.96908, time: 0.01077\n",
      "Epoch: 150, train_loss: 0.96789, time: 0.01086\n",
      "Epoch: 151, train_loss: 0.96713, time: 0.01129\n",
      "Epoch: 152, train_loss: 0.96751, time: 0.01096\n",
      "Epoch: 153, train_loss: 0.96724, time: 0.01123\n",
      "Epoch: 154, train_loss: 0.96636, time: 0.01134\n",
      "Epoch: 155, train_loss: 0.96725, time: 0.01123\n",
      "Epoch: 156, train_loss: 0.96705, time: 0.01144\n",
      "Epoch: 157, train_loss: 0.96499, time: 0.01122\n",
      "Epoch: 158, train_loss: 0.96641, time: 0.01112\n",
      "Epoch: 159, train_loss: 0.96635, time: 0.01113\n",
      "Epoch: 160, train_loss: 0.96739, time: 0.01128\n",
      "Epoch: 161, train_loss: 0.96578, time: 0.01103\n",
      "Epoch: 162, train_loss: 0.96753, time: 0.01097\n",
      "Epoch: 163, train_loss: 0.96804, time: 0.01093\n",
      "Epoch: 164, train_loss: 0.96563, time: 0.01128\n",
      "Epoch: 165, train_loss: 0.96581, time: 0.01087\n",
      "Epoch: 166, train_loss: 0.96470, time: 0.01084\n",
      "Epoch: 167, train_loss: 0.96766, time: 0.01143\n",
      "Epoch: 168, train_loss: 0.97024, time: 0.01099\n",
      "Epoch: 169, train_loss: 0.96723, time: 0.01096\n",
      "Epoch: 170, train_loss: 0.96624, time: 0.01123\n",
      "Epoch: 171, train_loss: 0.96890, time: 0.01096\n",
      "Epoch: 172, train_loss: 0.96592, time: 0.01102\n",
      "Epoch: 173, train_loss: 0.96720, time: 0.01115\n",
      "Epoch: 174, train_loss: 0.96570, time: 0.01097\n",
      "Epoch: 175, train_loss: 0.96464, time: 0.01110\n",
      "Epoch: 176, train_loss: 0.96810, time: 0.01102\n",
      "Epoch: 177, train_loss: 0.96362, time: 0.01073\n",
      "Epoch: 178, train_loss: 0.96582, time: 0.01115\n",
      "Epoch: 179, train_loss: 0.96758, time: 0.01105\n",
      "Epoch: 180, train_loss: 0.96672, time: 0.01107\n",
      "Epoch: 181, train_loss: 0.96601, time: 0.01128\n",
      "Epoch: 182, train_loss: 0.96757, time: 0.01109\n",
      "Epoch: 183, train_loss: 0.96777, time: 0.01114\n",
      "Epoch: 184, train_loss: 0.96641, time: 0.01097\n",
      "Epoch: 185, train_loss: 0.96536, time: 0.01118\n",
      "Epoch: 186, train_loss: 0.96565, time: 0.01096\n",
      "Epoch: 187, train_loss: 0.96868, time: 0.01080\n",
      "Epoch: 188, train_loss: 0.96532, time: 0.01089\n",
      "Epoch: 189, train_loss: 0.96617, time: 0.01111\n",
      "Epoch: 190, train_loss: 0.96631, time: 0.01094\n",
      "Epoch: 191, train_loss: 0.96661, time: 0.01072\n",
      "Epoch: 192, train_loss: 0.96670, time: 0.01113\n",
      "Epoch: 193, train_loss: 0.97000, time: 0.01119\n",
      "Epoch: 194, train_loss: 0.96566, time: 0.01089\n",
      "Epoch: 195, train_loss: 0.96739, time: 0.01097\n",
      "Epoch: 196, train_loss: 0.96503, time: 0.01124\n",
      "Epoch: 197, train_loss: 0.96603, time: 0.01125\n",
      "Epoch: 198, train_loss: 0.96705, time: 0.01113\n",
      "Epoch: 199, train_loss: 0.96531, time: 0.01113\n",
      "Epoch: 200, train_loss: 0.96483, time: 0.01095\n",
      "pairwise precision 0.21985 recall 0.91485 f1 0.35450\n",
      "average until now [0.4698354771414504, 0.8169051560525589, 0.596563151690564]\n",
      "32 names 186.47352027893066 avg time 5.827297508716583\n",
      "Loading bo_hong dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 321 nodes, 6283 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.93880, time: 0.11625\n",
      "Epoch: 2, train_loss: 0.93885, time: 0.01810\n",
      "Epoch: 3, train_loss: 0.94013, time: 0.01707\n",
      "Epoch: 4, train_loss: 0.93807, time: 0.01705\n",
      "Epoch: 5, train_loss: 0.94086, time: 0.01659\n",
      "Epoch: 6, train_loss: 0.93992, time: 0.01658\n",
      "Epoch: 7, train_loss: 0.93515, time: 0.01615\n",
      "Epoch: 8, train_loss: 0.93698, time: 0.01673\n",
      "Epoch: 9, train_loss: 0.93667, time: 0.01687\n",
      "Epoch: 10, train_loss: 0.93702, time: 0.01657\n",
      "Epoch: 11, train_loss: 0.93725, time: 0.01651\n",
      "Epoch: 12, train_loss: 0.93319, time: 0.01657\n",
      "Epoch: 13, train_loss: 0.93178, time: 0.01646\n",
      "Epoch: 14, train_loss: 0.94007, time: 0.01668\n",
      "Epoch: 15, train_loss: 0.92943, time: 0.01637\n",
      "Epoch: 16, train_loss: 0.92655, time: 0.01730\n",
      "Epoch: 17, train_loss: 0.93685, time: 0.01696\n",
      "Epoch: 18, train_loss: 0.92975, time: 0.01704\n",
      "Epoch: 19, train_loss: 0.93626, time: 0.01644\n",
      "Epoch: 20, train_loss: 0.93237, time: 0.01625\n",
      "Epoch: 21, train_loss: 0.93636, time: 0.01680\n",
      "Epoch: 22, train_loss: 0.93533, time: 0.01681\n",
      "Epoch: 23, train_loss: 0.93537, time: 0.01664\n",
      "Epoch: 24, train_loss: 0.93699, time: 0.01687\n",
      "Epoch: 25, train_loss: 0.93498, time: 0.01647\n",
      "Epoch: 26, train_loss: 0.93212, time: 0.01670\n",
      "Epoch: 27, train_loss: 0.93270, time: 0.01678\n",
      "Epoch: 28, train_loss: 0.93241, time: 0.01855\n",
      "Epoch: 29, train_loss: 0.94065, time: 0.01801\n",
      "Epoch: 30, train_loss: 0.93619, time: 0.01735\n",
      "Epoch: 31, train_loss: 0.94150, time: 0.01738\n",
      "Epoch: 32, train_loss: 0.93160, time: 0.01830\n",
      "Epoch: 33, train_loss: 0.93548, time: 0.01706\n",
      "Epoch: 34, train_loss: 0.93768, time: 0.01693\n",
      "Epoch: 35, train_loss: 0.93638, time: 0.01718\n",
      "Epoch: 36, train_loss: 0.93324, time: 0.01736\n",
      "Epoch: 37, train_loss: 0.93284, time: 0.01713\n",
      "Epoch: 38, train_loss: 0.93496, time: 0.01708\n",
      "Epoch: 39, train_loss: 0.93885, time: 0.01735\n",
      "Epoch: 40, train_loss: 0.93582, time: 0.01658\n",
      "Epoch: 41, train_loss: 0.93156, time: 0.01697\n",
      "Epoch: 42, train_loss: 0.93704, time: 0.01710\n",
      "Epoch: 43, train_loss: 0.93068, time: 0.01752\n",
      "Epoch: 44, train_loss: 0.93226, time: 0.01751\n",
      "Epoch: 45, train_loss: 0.93475, time: 0.01668\n",
      "Epoch: 46, train_loss: 0.93021, time: 0.01754\n",
      "Epoch: 47, train_loss: 0.93185, time: 0.01736\n",
      "Epoch: 48, train_loss: 0.93650, time: 0.01656\n",
      "Epoch: 49, train_loss: 0.94083, time: 0.01722\n",
      "Epoch: 50, train_loss: 0.93632, time: 0.01642\n",
      "Epoch: 51, train_loss: 0.93829, time: 0.01711\n",
      "Epoch: 52, train_loss: 0.92728, time: 0.01736\n",
      "Epoch: 53, train_loss: 0.92876, time: 0.01694\n",
      "Epoch: 54, train_loss: 0.93163, time: 0.01690\n",
      "Epoch: 55, train_loss: 0.93162, time: 0.01696\n",
      "Epoch: 56, train_loss: 0.93099, time: 0.01716\n",
      "Epoch: 57, train_loss: 0.92579, time: 0.01677\n",
      "Epoch: 58, train_loss: 0.93562, time: 0.01641\n",
      "Epoch: 59, train_loss: 0.93813, time: 0.01602\n",
      "Epoch: 60, train_loss: 0.92806, time: 0.01589\n",
      "Epoch: 61, train_loss: 0.93087, time: 0.01689\n",
      "Epoch: 62, train_loss: 0.93004, time: 0.01601\n",
      "Epoch: 63, train_loss: 0.93207, time: 0.01632\n",
      "Epoch: 64, train_loss: 0.93825, time: 0.01635\n",
      "Epoch: 65, train_loss: 0.93502, time: 0.01713\n",
      "Epoch: 66, train_loss: 0.93948, time: 0.01654\n",
      "Epoch: 67, train_loss: 0.93725, time: 0.01638\n",
      "Epoch: 68, train_loss: 0.92962, time: 0.01640\n",
      "Epoch: 69, train_loss: 0.93309, time: 0.01687\n",
      "Epoch: 70, train_loss: 0.93088, time: 0.01672\n",
      "Epoch: 71, train_loss: 0.93084, time: 0.01720\n",
      "Epoch: 72, train_loss: 0.93109, time: 0.01708\n",
      "Epoch: 73, train_loss: 0.93327, time: 0.01782\n",
      "Epoch: 74, train_loss: 0.92759, time: 0.01696\n",
      "Epoch: 75, train_loss: 0.93351, time: 0.01660\n",
      "Epoch: 76, train_loss: 0.92972, time: 0.01675\n",
      "Epoch: 77, train_loss: 0.93089, time: 0.01699\n",
      "Epoch: 78, train_loss: 0.93110, time: 0.01668\n",
      "Epoch: 79, train_loss: 0.93496, time: 0.01622\n",
      "Epoch: 80, train_loss: 0.92801, time: 0.01652\n",
      "Epoch: 81, train_loss: 0.92875, time: 0.01722\n",
      "Epoch: 82, train_loss: 0.93142, time: 0.01709\n",
      "Epoch: 83, train_loss: 0.92695, time: 0.01707\n",
      "Epoch: 84, train_loss: 0.93625, time: 0.01671\n",
      "Epoch: 85, train_loss: 0.93183, time: 0.01652\n",
      "Epoch: 86, train_loss: 0.92766, time: 0.01670\n",
      "Epoch: 87, train_loss: 0.92552, time: 0.01696\n",
      "Epoch: 88, train_loss: 0.93461, time: 0.01710\n",
      "Epoch: 89, train_loss: 0.92892, time: 0.01644\n",
      "Epoch: 90, train_loss: 0.92913, time: 0.01665\n",
      "Epoch: 91, train_loss: 0.93337, time: 0.01666\n",
      "Epoch: 92, train_loss: 0.93620, time: 0.01687\n",
      "Epoch: 93, train_loss: 0.93217, time: 0.01838\n",
      "Epoch: 94, train_loss: 0.92580, time: 0.01834\n",
      "Epoch: 95, train_loss: 0.93313, time: 0.01850\n",
      "Epoch: 96, train_loss: 0.93294, time: 0.01818\n",
      "Epoch: 97, train_loss: 0.93132, time: 0.01772\n",
      "Epoch: 98, train_loss: 0.93345, time: 0.01799\n",
      "Epoch: 99, train_loss: 0.92894, time: 0.01710\n",
      "Epoch: 100, train_loss: 0.93280, time: 0.01812\n",
      "Epoch: 101, train_loss: 0.93379, time: 0.01738\n",
      "Epoch: 102, train_loss: 0.92796, time: 0.01768\n",
      "Epoch: 103, train_loss: 0.93198, time: 0.01818\n",
      "Epoch: 104, train_loss: 0.93235, time: 0.01803\n",
      "Epoch: 105, train_loss: 0.93127, time: 0.01850\n",
      "Epoch: 106, train_loss: 0.93370, time: 0.01802\n",
      "Epoch: 107, train_loss: 0.93258, time: 0.01798\n",
      "Epoch: 108, train_loss: 0.93480, time: 0.01653\n",
      "Epoch: 109, train_loss: 0.92946, time: 0.01718\n",
      "Epoch: 110, train_loss: 0.93040, time: 0.01765\n",
      "Epoch: 111, train_loss: 0.92784, time: 0.01717\n",
      "Epoch: 112, train_loss: 0.93725, time: 0.01611\n",
      "Epoch: 113, train_loss: 0.92234, time: 0.01732\n",
      "Epoch: 114, train_loss: 0.93957, time: 0.01746\n",
      "Epoch: 115, train_loss: 0.92666, time: 0.01723\n",
      "Epoch: 116, train_loss: 0.93257, time: 0.01652\n",
      "Epoch: 117, train_loss: 0.93418, time: 0.01722\n",
      "Epoch: 118, train_loss: 0.93110, time: 0.01651\n",
      "Epoch: 119, train_loss: 0.92881, time: 0.01712\n",
      "Epoch: 120, train_loss: 0.92987, time: 0.01647\n",
      "Epoch: 121, train_loss: 0.92283, time: 0.01728\n",
      "Epoch: 122, train_loss: 0.92744, time: 0.01637\n",
      "Epoch: 123, train_loss: 0.92899, time: 0.01691\n",
      "Epoch: 124, train_loss: 0.93127, time: 0.01662\n",
      "Epoch: 125, train_loss: 0.93520, time: 0.01631\n",
      "Epoch: 126, train_loss: 0.93351, time: 0.01651\n",
      "Epoch: 127, train_loss: 0.93394, time: 0.01643\n",
      "Epoch: 128, train_loss: 0.93593, time: 0.01637\n",
      "Epoch: 129, train_loss: 0.93104, time: 0.01679\n",
      "Epoch: 130, train_loss: 0.92797, time: 0.01725\n",
      "Epoch: 131, train_loss: 0.93499, time: 0.01715\n",
      "Epoch: 132, train_loss: 0.93410, time: 0.01657\n",
      "Epoch: 133, train_loss: 0.93987, time: 0.01649\n",
      "Epoch: 134, train_loss: 0.93389, time: 0.01661\n",
      "Epoch: 135, train_loss: 0.92877, time: 0.01683\n",
      "Epoch: 136, train_loss: 0.93385, time: 0.01648\n",
      "Epoch: 137, train_loss: 0.92921, time: 0.01658\n",
      "Epoch: 138, train_loss: 0.93031, time: 0.01647\n",
      "Epoch: 139, train_loss: 0.93721, time: 0.01608\n",
      "Epoch: 140, train_loss: 0.93711, time: 0.01613\n",
      "Epoch: 141, train_loss: 0.93229, time: 0.01644\n",
      "Epoch: 142, train_loss: 0.92967, time: 0.01663\n",
      "Epoch: 143, train_loss: 0.93074, time: 0.01645\n",
      "Epoch: 144, train_loss: 0.93390, time: 0.01649\n",
      "Epoch: 145, train_loss: 0.92941, time: 0.01582\n",
      "Epoch: 146, train_loss: 0.92832, time: 0.01624\n",
      "Epoch: 147, train_loss: 0.93718, time: 0.01624\n",
      "Epoch: 148, train_loss: 0.93228, time: 0.01656\n",
      "Epoch: 149, train_loss: 0.92312, time: 0.01642\n",
      "Epoch: 150, train_loss: 0.92571, time: 0.01704\n",
      "Epoch: 151, train_loss: 0.92944, time: 0.01720\n",
      "Epoch: 152, train_loss: 0.93810, time: 0.01702\n",
      "Epoch: 153, train_loss: 0.93009, time: 0.01698\n",
      "Epoch: 154, train_loss: 0.93152, time: 0.01660\n",
      "Epoch: 155, train_loss: 0.93163, time: 0.01708\n",
      "Epoch: 156, train_loss: 0.93091, time: 0.01723\n",
      "Epoch: 157, train_loss: 0.93023, time: 0.01733\n",
      "Epoch: 158, train_loss: 0.92924, time: 0.01724\n",
      "Epoch: 159, train_loss: 0.93306, time: 0.01684\n",
      "Epoch: 160, train_loss: 0.93393, time: 0.01628\n",
      "Epoch: 161, train_loss: 0.93351, time: 0.01656\n",
      "Epoch: 162, train_loss: 0.92531, time: 0.01644\n",
      "Epoch: 163, train_loss: 0.93220, time: 0.01612\n",
      "Epoch: 164, train_loss: 0.94304, time: 0.01697\n",
      "Epoch: 165, train_loss: 0.92607, time: 0.01699\n",
      "Epoch: 166, train_loss: 0.93317, time: 0.01676\n",
      "Epoch: 167, train_loss: 0.93637, time: 0.01730\n",
      "Epoch: 168, train_loss: 0.92300, time: 0.01687\n",
      "Epoch: 169, train_loss: 0.93101, time: 0.01722\n",
      "Epoch: 170, train_loss: 0.93291, time: 0.01685\n",
      "Epoch: 171, train_loss: 0.92578, time: 0.01661\n",
      "Epoch: 172, train_loss: 0.92801, time: 0.01674\n",
      "Epoch: 173, train_loss: 0.92312, time: 0.01669\n",
      "Epoch: 174, train_loss: 0.92144, time: 0.01630\n",
      "Epoch: 175, train_loss: 0.92560, time: 0.01618\n",
      "Epoch: 176, train_loss: 0.93373, time: 0.01655\n",
      "Epoch: 177, train_loss: 0.92502, time: 0.01621\n",
      "Epoch: 178, train_loss: 0.93050, time: 0.01656\n",
      "Epoch: 179, train_loss: 0.92948, time: 0.01696\n",
      "Epoch: 180, train_loss: 0.92960, time: 0.01607\n",
      "Epoch: 181, train_loss: 0.92703, time: 0.01671\n",
      "Epoch: 182, train_loss: 0.92815, time: 0.01603\n",
      "Epoch: 183, train_loss: 0.93617, time: 0.01584\n",
      "Epoch: 184, train_loss: 0.92991, time: 0.01621\n",
      "Epoch: 185, train_loss: 0.93546, time: 0.01658\n",
      "Epoch: 186, train_loss: 0.93420, time: 0.01671\n",
      "Epoch: 187, train_loss: 0.93147, time: 0.01689\n",
      "Epoch: 188, train_loss: 0.93123, time: 0.01708\n",
      "Epoch: 189, train_loss: 0.93190, time: 0.01698\n",
      "Epoch: 190, train_loss: 0.92285, time: 0.01676\n",
      "Epoch: 191, train_loss: 0.92536, time: 0.01631\n",
      "Epoch: 192, train_loss: 0.93565, time: 0.01704\n",
      "Epoch: 193, train_loss: 0.93823, time: 0.01685\n",
      "Epoch: 194, train_loss: 0.93467, time: 0.01649\n",
      "Epoch: 195, train_loss: 0.92990, time: 0.01636\n",
      "Epoch: 196, train_loss: 0.93012, time: 0.01667\n",
      "Epoch: 197, train_loss: 0.92982, time: 0.01642\n",
      "Epoch: 198, train_loss: 0.93084, time: 0.01633\n",
      "Epoch: 199, train_loss: 0.92818, time: 0.01615\n",
      "Epoch: 200, train_loss: 0.93262, time: 0.01653\n",
      "pairwise precision 0.22927 recall 0.70399 f1 0.34590\n",
      "average until now [0.46254576030517774, 0.8134833348400525, 0.5897565643930059]\n",
      "33 names 190.02637767791748 avg time 5.758375081149015\n",
      "Loading weiming_zhu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 160 nodes, 4595 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.82096, time: 0.11204\n",
      "Epoch: 2, train_loss: 0.81381, time: 0.01331\n",
      "Epoch: 3, train_loss: 0.81988, time: 0.01202\n",
      "Epoch: 4, train_loss: 0.81813, time: 0.01232\n",
      "Epoch: 5, train_loss: 0.81166, time: 0.01205\n",
      "Epoch: 6, train_loss: 0.80898, time: 0.01242\n",
      "Epoch: 7, train_loss: 0.81424, time: 0.01211\n",
      "Epoch: 8, train_loss: 0.79679, time: 0.01194\n",
      "Epoch: 9, train_loss: 0.79313, time: 0.01219\n",
      "Epoch: 10, train_loss: 0.79468, time: 0.01190\n",
      "Epoch: 11, train_loss: 0.79424, time: 0.01243\n",
      "Epoch: 12, train_loss: 0.79348, time: 0.01217\n",
      "Epoch: 13, train_loss: 0.79898, time: 0.01188\n",
      "Epoch: 14, train_loss: 0.79110, time: 0.01206\n",
      "Epoch: 15, train_loss: 0.79028, time: 0.01214\n",
      "Epoch: 16, train_loss: 0.78743, time: 0.01202\n",
      "Epoch: 17, train_loss: 0.77702, time: 0.01204\n",
      "Epoch: 18, train_loss: 0.77425, time: 0.01238\n",
      "Epoch: 19, train_loss: 0.78509, time: 0.01244\n",
      "Epoch: 20, train_loss: 0.78265, time: 0.01211\n",
      "Epoch: 21, train_loss: 0.79676, time: 0.01201\n",
      "Epoch: 22, train_loss: 0.80484, time: 0.01205\n",
      "Epoch: 23, train_loss: 0.79285, time: 0.01239\n",
      "Epoch: 24, train_loss: 0.79506, time: 0.01216\n",
      "Epoch: 25, train_loss: 0.77078, time: 0.01220\n",
      "Epoch: 26, train_loss: 0.79409, time: 0.01198\n",
      "Epoch: 27, train_loss: 0.80396, time: 0.01223\n",
      "Epoch: 28, train_loss: 0.78114, time: 0.01194\n",
      "Epoch: 29, train_loss: 0.80371, time: 0.01223\n",
      "Epoch: 30, train_loss: 0.79279, time: 0.01202\n",
      "Epoch: 31, train_loss: 0.80819, time: 0.01185\n",
      "Epoch: 32, train_loss: 0.79250, time: 0.01175\n",
      "Epoch: 33, train_loss: 0.78453, time: 0.01194\n",
      "Epoch: 34, train_loss: 0.78876, time: 0.01209\n",
      "Epoch: 35, train_loss: 0.78886, time: 0.01238\n",
      "Epoch: 36, train_loss: 0.79590, time: 0.01220\n",
      "Epoch: 37, train_loss: 0.79166, time: 0.01187\n",
      "Epoch: 38, train_loss: 0.80088, time: 0.01199\n",
      "Epoch: 39, train_loss: 0.78069, time: 0.01231\n",
      "Epoch: 40, train_loss: 0.78674, time: 0.01231\n",
      "Epoch: 41, train_loss: 0.78012, time: 0.01231\n",
      "Epoch: 42, train_loss: 0.78353, time: 0.01192\n",
      "Epoch: 43, train_loss: 0.78879, time: 0.01187\n",
      "Epoch: 44, train_loss: 0.78567, time: 0.01193\n",
      "Epoch: 45, train_loss: 0.77975, time: 0.01228\n",
      "Epoch: 46, train_loss: 0.79358, time: 0.01216\n",
      "Epoch: 47, train_loss: 0.77497, time: 0.01227\n",
      "Epoch: 48, train_loss: 0.78953, time: 0.01226\n",
      "Epoch: 49, train_loss: 0.81002, time: 0.01195\n",
      "Epoch: 50, train_loss: 0.80382, time: 0.01216\n",
      "Epoch: 51, train_loss: 0.78015, time: 0.01217\n",
      "Epoch: 52, train_loss: 0.79613, time: 0.01235\n",
      "Epoch: 53, train_loss: 0.78137, time: 0.01228\n",
      "Epoch: 54, train_loss: 0.78791, time: 0.01215\n",
      "Epoch: 55, train_loss: 0.78444, time: 0.01201\n",
      "Epoch: 56, train_loss: 0.77977, time: 0.01223\n",
      "Epoch: 57, train_loss: 0.79594, time: 0.01212\n",
      "Epoch: 58, train_loss: 0.79135, time: 0.01259\n",
      "Epoch: 59, train_loss: 0.78199, time: 0.01207\n",
      "Epoch: 60, train_loss: 0.79517, time: 0.01241\n",
      "Epoch: 61, train_loss: 0.78772, time: 0.01217\n",
      "Epoch: 62, train_loss: 0.79957, time: 0.01178\n",
      "Epoch: 63, train_loss: 0.79146, time: 0.01238\n",
      "Epoch: 64, train_loss: 0.78616, time: 0.01207\n",
      "Epoch: 65, train_loss: 0.78238, time: 0.01186\n",
      "Epoch: 66, train_loss: 0.78633, time: 0.01213\n",
      "Epoch: 67, train_loss: 0.79229, time: 0.01186\n",
      "Epoch: 68, train_loss: 0.78879, time: 0.01235\n",
      "Epoch: 69, train_loss: 0.77966, time: 0.01230\n",
      "Epoch: 70, train_loss: 0.77061, time: 0.01204\n",
      "Epoch: 71, train_loss: 0.78818, time: 0.01190\n",
      "Epoch: 72, train_loss: 0.80424, time: 0.01183\n",
      "Epoch: 73, train_loss: 0.79248, time: 0.01230\n",
      "Epoch: 74, train_loss: 0.79760, time: 0.01205\n",
      "Epoch: 75, train_loss: 0.80047, time: 0.01255\n",
      "Epoch: 76, train_loss: 0.78552, time: 0.01192\n",
      "Epoch: 77, train_loss: 0.78490, time: 0.01220\n",
      "Epoch: 78, train_loss: 0.78778, time: 0.01225\n",
      "Epoch: 79, train_loss: 0.79883, time: 0.01195\n",
      "Epoch: 80, train_loss: 0.78774, time: 0.01215\n",
      "Epoch: 81, train_loss: 0.78604, time: 0.01170\n",
      "Epoch: 82, train_loss: 0.78768, time: 0.01232\n",
      "Epoch: 83, train_loss: 0.78744, time: 0.01223\n",
      "Epoch: 84, train_loss: 0.79294, time: 0.01251\n",
      "Epoch: 85, train_loss: 0.78818, time: 0.01238\n",
      "Epoch: 86, train_loss: 0.79434, time: 0.01259\n",
      "Epoch: 87, train_loss: 0.79006, time: 0.01217\n",
      "Epoch: 88, train_loss: 0.79346, time: 0.01222\n",
      "Epoch: 89, train_loss: 0.78763, time: 0.01199\n",
      "Epoch: 90, train_loss: 0.79594, time: 0.01182\n",
      "Epoch: 91, train_loss: 0.77955, time: 0.01214\n",
      "Epoch: 92, train_loss: 0.79516, time: 0.01231\n",
      "Epoch: 93, train_loss: 0.78459, time: 0.01208\n",
      "Epoch: 94, train_loss: 0.78564, time: 0.01183\n",
      "Epoch: 95, train_loss: 0.79030, time: 0.01194\n",
      "Epoch: 96, train_loss: 0.78987, time: 0.01184\n",
      "Epoch: 97, train_loss: 0.79169, time: 0.01210\n",
      "Epoch: 98, train_loss: 0.80121, time: 0.01187\n",
      "Epoch: 99, train_loss: 0.78527, time: 0.01233\n",
      "Epoch: 100, train_loss: 0.79561, time: 0.01231\n",
      "Epoch: 101, train_loss: 0.78787, time: 0.01238\n",
      "Epoch: 102, train_loss: 0.79687, time: 0.01235\n",
      "Epoch: 103, train_loss: 0.78042, time: 0.01240\n",
      "Epoch: 104, train_loss: 0.79530, time: 0.01231\n",
      "Epoch: 105, train_loss: 0.77909, time: 0.01218\n",
      "Epoch: 106, train_loss: 0.78389, time: 0.01220\n",
      "Epoch: 107, train_loss: 0.78098, time: 0.01243\n",
      "Epoch: 108, train_loss: 0.78839, time: 0.01196\n",
      "Epoch: 109, train_loss: 0.77875, time: 0.01236\n",
      "Epoch: 110, train_loss: 0.78679, time: 0.01251\n",
      "Epoch: 111, train_loss: 0.79754, time: 0.01204\n",
      "Epoch: 112, train_loss: 0.78549, time: 0.01229\n",
      "Epoch: 113, train_loss: 0.79226, time: 0.01211\n",
      "Epoch: 114, train_loss: 0.79781, time: 0.01234\n",
      "Epoch: 115, train_loss: 0.79289, time: 0.01182\n",
      "Epoch: 116, train_loss: 0.78376, time: 0.01220\n",
      "Epoch: 117, train_loss: 0.79001, time: 0.01195\n",
      "Epoch: 118, train_loss: 0.78062, time: 0.01166\n",
      "Epoch: 119, train_loss: 0.77563, time: 0.01190\n",
      "Epoch: 120, train_loss: 0.78078, time: 0.01224\n",
      "Epoch: 121, train_loss: 0.78745, time: 0.01195\n",
      "Epoch: 122, train_loss: 0.78926, time: 0.01215\n",
      "Epoch: 123, train_loss: 0.78909, time: 0.01209\n",
      "Epoch: 124, train_loss: 0.80659, time: 0.01201\n",
      "Epoch: 125, train_loss: 0.77715, time: 0.01182\n",
      "Epoch: 126, train_loss: 0.78584, time: 0.01227\n",
      "Epoch: 127, train_loss: 0.78745, time: 0.01206\n",
      "Epoch: 128, train_loss: 0.77302, time: 0.01220\n",
      "Epoch: 129, train_loss: 0.78440, time: 0.01215\n",
      "Epoch: 130, train_loss: 0.77877, time: 0.01242\n",
      "Epoch: 131, train_loss: 0.78648, time: 0.01202\n",
      "Epoch: 132, train_loss: 0.76227, time: 0.01212\n",
      "Epoch: 133, train_loss: 0.80151, time: 0.01215\n",
      "Epoch: 134, train_loss: 0.77031, time: 0.01195\n",
      "Epoch: 135, train_loss: 0.77790, time: 0.01207\n",
      "Epoch: 136, train_loss: 0.79954, time: 0.01204\n",
      "Epoch: 137, train_loss: 0.79001, time: 0.01268\n",
      "Epoch: 138, train_loss: 0.79316, time: 0.01237\n",
      "Epoch: 139, train_loss: 0.78268, time: 0.01232\n",
      "Epoch: 140, train_loss: 0.78193, time: 0.01221\n",
      "Epoch: 141, train_loss: 0.78782, time: 0.01188\n",
      "Epoch: 142, train_loss: 0.80093, time: 0.01220\n",
      "Epoch: 143, train_loss: 0.80416, time: 0.01236\n",
      "Epoch: 144, train_loss: 0.78544, time: 0.01218\n",
      "Epoch: 145, train_loss: 0.78390, time: 0.01217\n",
      "Epoch: 146, train_loss: 0.78136, time: 0.01230\n",
      "Epoch: 147, train_loss: 0.77202, time: 0.01201\n",
      "Epoch: 148, train_loss: 0.78522, time: 0.01228\n",
      "Epoch: 149, train_loss: 0.79553, time: 0.01208\n",
      "Epoch: 150, train_loss: 0.77685, time: 0.01217\n",
      "Epoch: 151, train_loss: 0.79649, time: 0.01219\n",
      "Epoch: 152, train_loss: 0.78349, time: 0.01206\n",
      "Epoch: 153, train_loss: 0.76124, time: 0.01226\n",
      "Epoch: 154, train_loss: 0.79403, time: 0.01289\n",
      "Epoch: 155, train_loss: 0.78210, time: 0.01199\n",
      "Epoch: 156, train_loss: 0.80111, time: 0.01250\n",
      "Epoch: 157, train_loss: 0.77487, time: 0.01212\n",
      "Epoch: 158, train_loss: 0.77802, time: 0.01238\n",
      "Epoch: 159, train_loss: 0.79708, time: 0.01221\n",
      "Epoch: 160, train_loss: 0.79566, time: 0.01202\n",
      "Epoch: 161, train_loss: 0.77719, time: 0.01200\n",
      "Epoch: 162, train_loss: 0.77907, time: 0.01175\n",
      "Epoch: 163, train_loss: 0.77559, time: 0.01207\n",
      "Epoch: 164, train_loss: 0.77901, time: 0.01226\n",
      "Epoch: 165, train_loss: 0.79448, time: 0.01246\n",
      "Epoch: 166, train_loss: 0.78660, time: 0.01210\n",
      "Epoch: 167, train_loss: 0.79095, time: 0.01220\n",
      "Epoch: 168, train_loss: 0.79533, time: 0.01220\n",
      "Epoch: 169, train_loss: 0.77630, time: 0.01230\n",
      "Epoch: 170, train_loss: 0.78609, time: 0.01206\n",
      "Epoch: 171, train_loss: 0.78728, time: 0.01215\n",
      "Epoch: 172, train_loss: 0.79187, time: 0.01235\n",
      "Epoch: 173, train_loss: 0.77170, time: 0.01199\n",
      "Epoch: 174, train_loss: 0.80689, time: 0.01212\n",
      "Epoch: 175, train_loss: 0.77732, time: 0.01197\n",
      "Epoch: 176, train_loss: 0.78329, time: 0.01202\n",
      "Epoch: 177, train_loss: 0.79415, time: 0.01205\n",
      "Epoch: 178, train_loss: 0.78602, time: 0.01233\n",
      "Epoch: 179, train_loss: 0.77593, time: 0.01207\n",
      "Epoch: 180, train_loss: 0.77878, time: 0.01190\n",
      "Epoch: 181, train_loss: 0.77425, time: 0.01211\n",
      "Epoch: 182, train_loss: 0.78138, time: 0.01179\n",
      "Epoch: 183, train_loss: 0.77824, time: 0.01196\n",
      "Epoch: 184, train_loss: 0.78635, time: 0.01308\n",
      "Epoch: 185, train_loss: 0.79849, time: 0.01206\n",
      "Epoch: 186, train_loss: 0.78554, time: 0.01228\n",
      "Epoch: 187, train_loss: 0.78347, time: 0.01236\n",
      "Epoch: 188, train_loss: 0.76936, time: 0.01222\n",
      "Epoch: 189, train_loss: 0.77432, time: 0.01237\n",
      "Epoch: 190, train_loss: 0.76480, time: 0.01213\n",
      "Epoch: 191, train_loss: 0.78548, time: 0.01273\n",
      "Epoch: 192, train_loss: 0.78621, time: 0.01208\n",
      "Epoch: 193, train_loss: 0.78716, time: 0.01243\n",
      "Epoch: 194, train_loss: 0.78607, time: 0.01233\n",
      "Epoch: 195, train_loss: 0.77836, time: 0.01216\n",
      "Epoch: 196, train_loss: 0.77856, time: 0.01191\n",
      "Epoch: 197, train_loss: 0.77061, time: 0.01224\n",
      "Epoch: 198, train_loss: 0.77528, time: 0.01202\n",
      "Epoch: 199, train_loss: 0.78952, time: 0.01172\n",
      "Epoch: 200, train_loss: 0.78642, time: 0.01186\n",
      "pairwise precision 0.85540 recall 0.68325 f1 0.75969\n",
      "average until now [0.474100427298602, 0.8096528711694635, 0.5980226460069055]\n",
      "34 names 192.60372114181519 avg time 5.664815327700446\n",
      "Loading xu_xu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 555 nodes, 5626 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98913, time: 0.12018\n",
      "Epoch: 2, train_loss: 0.98225, time: 0.02165\n",
      "Epoch: 3, train_loss: 0.98135, time: 0.02147\n",
      "Epoch: 4, train_loss: 0.98207, time: 0.01999\n",
      "Epoch: 5, train_loss: 0.97994, time: 0.02137\n",
      "Epoch: 6, train_loss: 0.98046, time: 0.02170\n",
      "Epoch: 7, train_loss: 0.98017, time: 0.02045\n",
      "Epoch: 8, train_loss: 0.98171, time: 0.02074\n",
      "Epoch: 9, train_loss: 0.98119, time: 0.02065\n",
      "Epoch: 10, train_loss: 0.98080, time: 0.02010\n",
      "Epoch: 11, train_loss: 0.98139, time: 0.02119\n",
      "Epoch: 12, train_loss: 0.98033, time: 0.02076\n",
      "Epoch: 13, train_loss: 0.98075, time: 0.02181\n",
      "Epoch: 14, train_loss: 0.97944, time: 0.02136\n",
      "Epoch: 15, train_loss: 0.98037, time: 0.02126\n",
      "Epoch: 16, train_loss: 0.97911, time: 0.02151\n",
      "Epoch: 17, train_loss: 0.98051, time: 0.02147\n",
      "Epoch: 18, train_loss: 0.98011, time: 0.02060\n",
      "Epoch: 19, train_loss: 0.97980, time: 0.02116\n",
      "Epoch: 20, train_loss: 0.98029, time: 0.02140\n",
      "Epoch: 21, train_loss: 0.97957, time: 0.02133\n",
      "Epoch: 22, train_loss: 0.97912, time: 0.02156\n",
      "Epoch: 23, train_loss: 0.98004, time: 0.02218\n",
      "Epoch: 24, train_loss: 0.98027, time: 0.02142\n",
      "Epoch: 25, train_loss: 0.98039, time: 0.02128\n",
      "Epoch: 26, train_loss: 0.97907, time: 0.02176\n",
      "Epoch: 27, train_loss: 0.97973, time: 0.02036\n",
      "Epoch: 28, train_loss: 0.97994, time: 0.02071\n",
      "Epoch: 29, train_loss: 0.98062, time: 0.02207\n",
      "Epoch: 30, train_loss: 0.97988, time: 0.02058\n",
      "Epoch: 31, train_loss: 0.97938, time: 0.02042\n",
      "Epoch: 32, train_loss: 0.97876, time: 0.02191\n",
      "Epoch: 33, train_loss: 0.97961, time: 0.02148\n",
      "Epoch: 34, train_loss: 0.98042, time: 0.02179\n",
      "Epoch: 35, train_loss: 0.97981, time: 0.02176\n",
      "Epoch: 36, train_loss: 0.97966, time: 0.02114\n",
      "Epoch: 37, train_loss: 0.97957, time: 0.02127\n",
      "Epoch: 38, train_loss: 0.97923, time: 0.02148\n",
      "Epoch: 39, train_loss: 0.98046, time: 0.02145\n",
      "Epoch: 40, train_loss: 0.98021, time: 0.02094\n",
      "Epoch: 41, train_loss: 0.97993, time: 0.02177\n",
      "Epoch: 42, train_loss: 0.97903, time: 0.02203\n",
      "Epoch: 43, train_loss: 0.97967, time: 0.02177\n",
      "Epoch: 44, train_loss: 0.98038, time: 0.02023\n",
      "Epoch: 45, train_loss: 0.98023, time: 0.02114\n",
      "Epoch: 46, train_loss: 0.98001, time: 0.01964\n",
      "Epoch: 47, train_loss: 0.97922, time: 0.02123\n",
      "Epoch: 48, train_loss: 0.97897, time: 0.02174\n",
      "Epoch: 49, train_loss: 0.97928, time: 0.02198\n",
      "Epoch: 50, train_loss: 0.97953, time: 0.02211\n",
      "Epoch: 51, train_loss: 0.97997, time: 0.02085\n",
      "Epoch: 52, train_loss: 0.97916, time: 0.02128\n",
      "Epoch: 53, train_loss: 0.97951, time: 0.02003\n",
      "Epoch: 54, train_loss: 0.98096, time: 0.02114\n",
      "Epoch: 55, train_loss: 0.98067, time: 0.02176\n",
      "Epoch: 56, train_loss: 0.97802, time: 0.02163\n",
      "Epoch: 57, train_loss: 0.98118, time: 0.02203\n",
      "Epoch: 58, train_loss: 0.98017, time: 0.02115\n",
      "Epoch: 59, train_loss: 0.97974, time: 0.02035\n",
      "Epoch: 60, train_loss: 0.98183, time: 0.02191\n",
      "Epoch: 61, train_loss: 0.98112, time: 0.02070\n",
      "Epoch: 62, train_loss: 0.97938, time: 0.02041\n",
      "Epoch: 63, train_loss: 0.98063, time: 0.02169\n",
      "Epoch: 64, train_loss: 0.97898, time: 0.02140\n",
      "Epoch: 65, train_loss: 0.98063, time: 0.02166\n",
      "Epoch: 66, train_loss: 0.98031, time: 0.02096\n",
      "Epoch: 67, train_loss: 0.97996, time: 0.02193\n",
      "Epoch: 68, train_loss: 0.98003, time: 0.02127\n",
      "Epoch: 69, train_loss: 0.98075, time: 0.02169\n",
      "Epoch: 70, train_loss: 0.97879, time: 0.02097\n",
      "Epoch: 71, train_loss: 0.97933, time: 0.02097\n",
      "Epoch: 72, train_loss: 0.97863, time: 0.02109\n",
      "Epoch: 73, train_loss: 0.97862, time: 0.02096\n",
      "Epoch: 74, train_loss: 0.98028, time: 0.02112\n",
      "Epoch: 75, train_loss: 0.97969, time: 0.02039\n",
      "Epoch: 76, train_loss: 0.97943, time: 0.02158\n",
      "Epoch: 77, train_loss: 0.97973, time: 0.02187\n",
      "Epoch: 78, train_loss: 0.98053, time: 0.02223\n",
      "Epoch: 79, train_loss: 0.98075, time: 0.02216\n",
      "Epoch: 80, train_loss: 0.97916, time: 0.02218\n",
      "Epoch: 81, train_loss: 0.97896, time: 0.02107\n",
      "Epoch: 82, train_loss: 0.97984, time: 0.02094\n",
      "Epoch: 83, train_loss: 0.98077, time: 0.02092\n",
      "Epoch: 84, train_loss: 0.97984, time: 0.02053\n",
      "Epoch: 85, train_loss: 0.98058, time: 0.02030\n",
      "Epoch: 86, train_loss: 0.97983, time: 0.02040\n",
      "Epoch: 87, train_loss: 0.97961, time: 0.02071\n",
      "Epoch: 88, train_loss: 0.98012, time: 0.02034\n",
      "Epoch: 89, train_loss: 0.97970, time: 0.02096\n",
      "Epoch: 90, train_loss: 0.97964, time: 0.02010\n",
      "Epoch: 91, train_loss: 0.98176, time: 0.02007\n",
      "Epoch: 92, train_loss: 0.98177, time: 0.01855\n",
      "Epoch: 93, train_loss: 0.97961, time: 0.02031\n",
      "Epoch: 94, train_loss: 0.98008, time: 0.02176\n",
      "Epoch: 95, train_loss: 0.97897, time: 0.02059\n",
      "Epoch: 96, train_loss: 0.98093, time: 0.02133\n",
      "Epoch: 97, train_loss: 0.97928, time: 0.02171\n",
      "Epoch: 98, train_loss: 0.97914, time: 0.02039\n",
      "Epoch: 99, train_loss: 0.97985, time: 0.02148\n",
      "Epoch: 100, train_loss: 0.98034, time: 0.02142\n",
      "Epoch: 101, train_loss: 0.97852, time: 0.02090\n",
      "Epoch: 102, train_loss: 0.97988, time: 0.02103\n",
      "Epoch: 103, train_loss: 0.97887, time: 0.02019\n",
      "Epoch: 104, train_loss: 0.97908, time: 0.02088\n",
      "Epoch: 105, train_loss: 0.97998, time: 0.02248\n",
      "Epoch: 106, train_loss: 0.98136, time: 0.02085\n",
      "Epoch: 107, train_loss: 0.97945, time: 0.02160\n",
      "Epoch: 108, train_loss: 0.98068, time: 0.02150\n",
      "Epoch: 109, train_loss: 0.98002, time: 0.02153\n",
      "Epoch: 110, train_loss: 0.98010, time: 0.02146\n",
      "Epoch: 111, train_loss: 0.97954, time: 0.02087\n",
      "Epoch: 112, train_loss: 0.98024, time: 0.02149\n",
      "Epoch: 113, train_loss: 0.97898, time: 0.02108\n",
      "Epoch: 114, train_loss: 0.98003, time: 0.02121\n",
      "Epoch: 115, train_loss: 0.97971, time: 0.02113\n",
      "Epoch: 116, train_loss: 0.97958, time: 0.02067\n",
      "Epoch: 117, train_loss: 0.97930, time: 0.02138\n",
      "Epoch: 118, train_loss: 0.97980, time: 0.02022\n",
      "Epoch: 119, train_loss: 0.98002, time: 0.02073\n",
      "Epoch: 120, train_loss: 0.98027, time: 0.02159\n",
      "Epoch: 121, train_loss: 0.97927, time: 0.02175\n",
      "Epoch: 122, train_loss: 0.97923, time: 0.02130\n",
      "Epoch: 123, train_loss: 0.97879, time: 0.02133\n",
      "Epoch: 124, train_loss: 0.97996, time: 0.01999\n",
      "Epoch: 125, train_loss: 0.98032, time: 0.02073\n",
      "Epoch: 126, train_loss: 0.98024, time: 0.02076\n",
      "Epoch: 127, train_loss: 0.97940, time: 0.02103\n",
      "Epoch: 128, train_loss: 0.98011, time: 0.02089\n",
      "Epoch: 129, train_loss: 0.97912, time: 0.02149\n",
      "Epoch: 130, train_loss: 0.97954, time: 0.02118\n",
      "Epoch: 131, train_loss: 0.98015, time: 0.02165\n",
      "Epoch: 132, train_loss: 0.97909, time: 0.02204\n",
      "Epoch: 133, train_loss: 0.97978, time: 0.02150\n",
      "Epoch: 134, train_loss: 0.97904, time: 0.02190\n",
      "Epoch: 135, train_loss: 0.97917, time: 0.02179\n",
      "Epoch: 136, train_loss: 0.97950, time: 0.02198\n",
      "Epoch: 137, train_loss: 0.97978, time: 0.02086\n",
      "Epoch: 138, train_loss: 0.97955, time: 0.02187\n",
      "Epoch: 139, train_loss: 0.97991, time: 0.02236\n",
      "Epoch: 140, train_loss: 0.97946, time: 0.02193\n",
      "Epoch: 141, train_loss: 0.97889, time: 0.02158\n",
      "Epoch: 142, train_loss: 0.98001, time: 0.02035\n",
      "Epoch: 143, train_loss: 0.97902, time: 0.02123\n",
      "Epoch: 144, train_loss: 0.97931, time: 0.02150\n",
      "Epoch: 145, train_loss: 0.97987, time: 0.02153\n",
      "Epoch: 146, train_loss: 0.97900, time: 0.02126\n",
      "Epoch: 147, train_loss: 0.97907, time: 0.02007\n",
      "Epoch: 148, train_loss: 0.97884, time: 0.01849\n",
      "Epoch: 149, train_loss: 0.97878, time: 0.01918\n",
      "Epoch: 150, train_loss: 0.98002, time: 0.02001\n",
      "Epoch: 151, train_loss: 0.97965, time: 0.02124\n",
      "Epoch: 152, train_loss: 0.97857, time: 0.02079\n",
      "Epoch: 153, train_loss: 0.97932, time: 0.02180\n",
      "Epoch: 154, train_loss: 0.98029, time: 0.02472\n",
      "Epoch: 155, train_loss: 0.98137, time: 0.02182\n",
      "Epoch: 156, train_loss: 0.97936, time: 0.02165\n",
      "Epoch: 157, train_loss: 0.97947, time: 0.02194\n",
      "Epoch: 158, train_loss: 0.97919, time: 0.02074\n",
      "Epoch: 159, train_loss: 0.98054, time: 0.02198\n",
      "Epoch: 160, train_loss: 0.97919, time: 0.02094\n",
      "Epoch: 161, train_loss: 0.97995, time: 0.02158\n",
      "Epoch: 162, train_loss: 0.97963, time: 0.02144\n",
      "Epoch: 163, train_loss: 0.97955, time: 0.02031\n",
      "Epoch: 164, train_loss: 0.97968, time: 0.02134\n",
      "Epoch: 165, train_loss: 0.97948, time: 0.02146\n",
      "Epoch: 166, train_loss: 0.98008, time: 0.02175\n",
      "Epoch: 167, train_loss: 0.97992, time: 0.02140\n",
      "Epoch: 168, train_loss: 0.97826, time: 0.02020\n",
      "Epoch: 169, train_loss: 0.97936, time: 0.02095\n",
      "Epoch: 170, train_loss: 0.97958, time: 0.02066\n",
      "Epoch: 171, train_loss: 0.97947, time: 0.02024\n",
      "Epoch: 172, train_loss: 0.97966, time: 0.02049\n",
      "Epoch: 173, train_loss: 0.97947, time: 0.02072\n",
      "Epoch: 174, train_loss: 0.98093, time: 0.02054\n",
      "Epoch: 175, train_loss: 0.98011, time: 0.02182\n",
      "Epoch: 176, train_loss: 0.98133, time: 0.02103\n",
      "Epoch: 177, train_loss: 0.98037, time: 0.02102\n",
      "Epoch: 178, train_loss: 0.97922, time: 0.02106\n",
      "Epoch: 179, train_loss: 0.97915, time: 0.02139\n",
      "Epoch: 180, train_loss: 0.98070, time: 0.01920\n",
      "Epoch: 181, train_loss: 0.97928, time: 0.01978\n",
      "Epoch: 182, train_loss: 0.97949, time: 0.02010\n",
      "Epoch: 183, train_loss: 0.97935, time: 0.02044\n",
      "Epoch: 184, train_loss: 0.97992, time: 0.02131\n",
      "Epoch: 185, train_loss: 0.98057, time: 0.02082\n",
      "Epoch: 186, train_loss: 0.97981, time: 0.02085\n",
      "Epoch: 187, train_loss: 0.98072, time: 0.02144\n",
      "Epoch: 188, train_loss: 0.97921, time: 0.02050\n",
      "Epoch: 189, train_loss: 0.98100, time: 0.01996\n",
      "Epoch: 190, train_loss: 0.98051, time: 0.02103\n",
      "Epoch: 191, train_loss: 0.98034, time: 0.02061\n",
      "Epoch: 192, train_loss: 0.98023, time: 0.02067\n",
      "Epoch: 193, train_loss: 0.98114, time: 0.02075\n",
      "Epoch: 194, train_loss: 0.97927, time: 0.02110\n",
      "Epoch: 195, train_loss: 0.97985, time: 0.02139\n",
      "Epoch: 196, train_loss: 0.97874, time: 0.02153\n",
      "Epoch: 197, train_loss: 0.98013, time: 0.02081\n",
      "Epoch: 198, train_loss: 0.98076, time: 0.02183\n",
      "Epoch: 199, train_loss: 0.98002, time: 0.02103\n",
      "Epoch: 200, train_loss: 0.97947, time: 0.02212\n",
      "pairwise precision 0.09281 recall 0.90023 f1 0.16827\n",
      "average until now [0.46320631863623973, 0.8122407913471836, 0.5899657678647371]\n",
      "35 names 197.05592346191406 avg time 5.630169241768973\n",
      "Loading rong_yu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 273 nodes, 2562 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97078, time: 0.11348\n",
      "Epoch: 2, train_loss: 0.96928, time: 0.01392\n",
      "Epoch: 3, train_loss: 0.96593, time: 0.01256\n",
      "Epoch: 4, train_loss: 0.96342, time: 0.01224\n",
      "Epoch: 5, train_loss: 0.96836, time: 0.01232\n",
      "Epoch: 6, train_loss: 0.96611, time: 0.01237\n",
      "Epoch: 7, train_loss: 0.96292, time: 0.01241\n",
      "Epoch: 8, train_loss: 0.96556, time: 0.01233\n",
      "Epoch: 9, train_loss: 0.96286, time: 0.01244\n",
      "Epoch: 10, train_loss: 0.96575, time: 0.01209\n",
      "Epoch: 11, train_loss: 0.96537, time: 0.01221\n",
      "Epoch: 12, train_loss: 0.96644, time: 0.01241\n",
      "Epoch: 13, train_loss: 0.96651, time: 0.01249\n",
      "Epoch: 14, train_loss: 0.96386, time: 0.01252\n",
      "Epoch: 15, train_loss: 0.96669, time: 0.01261\n",
      "Epoch: 16, train_loss: 0.96272, time: 0.01259\n",
      "Epoch: 17, train_loss: 0.96384, time: 0.01224\n",
      "Epoch: 18, train_loss: 0.96678, time: 0.01281\n",
      "Epoch: 19, train_loss: 0.96307, time: 0.01249\n",
      "Epoch: 20, train_loss: 0.96514, time: 0.01248\n",
      "Epoch: 21, train_loss: 0.96454, time: 0.01246\n",
      "Epoch: 22, train_loss: 0.96354, time: 0.01265\n",
      "Epoch: 23, train_loss: 0.96608, time: 0.01234\n",
      "Epoch: 24, train_loss: 0.96767, time: 0.01264\n",
      "Epoch: 25, train_loss: 0.96729, time: 0.01278\n",
      "Epoch: 26, train_loss: 0.96656, time: 0.01216\n",
      "Epoch: 27, train_loss: 0.96538, time: 0.01221\n",
      "Epoch: 28, train_loss: 0.96629, time: 0.01247\n",
      "Epoch: 29, train_loss: 0.96546, time: 0.01211\n",
      "Epoch: 30, train_loss: 0.96568, time: 0.01229\n",
      "Epoch: 31, train_loss: 0.96475, time: 0.01233\n",
      "Epoch: 32, train_loss: 0.96420, time: 0.01241\n",
      "Epoch: 33, train_loss: 0.96814, time: 0.01235\n",
      "Epoch: 34, train_loss: 0.96446, time: 0.01220\n",
      "Epoch: 35, train_loss: 0.96270, time: 0.01259\n",
      "Epoch: 36, train_loss: 0.96392, time: 0.01238\n",
      "Epoch: 37, train_loss: 0.96559, time: 0.01215\n",
      "Epoch: 38, train_loss: 0.96398, time: 0.01262\n",
      "Epoch: 39, train_loss: 0.96423, time: 0.01222\n",
      "Epoch: 40, train_loss: 0.96486, time: 0.01248\n",
      "Epoch: 41, train_loss: 0.96439, time: 0.01260\n",
      "Epoch: 42, train_loss: 0.96460, time: 0.01238\n",
      "Epoch: 43, train_loss: 0.96361, time: 0.01224\n",
      "Epoch: 44, train_loss: 0.96547, time: 0.01260\n",
      "Epoch: 45, train_loss: 0.96192, time: 0.01262\n",
      "Epoch: 46, train_loss: 0.96423, time: 0.01252\n",
      "Epoch: 47, train_loss: 0.96571, time: 0.01216\n",
      "Epoch: 48, train_loss: 0.96589, time: 0.01266\n",
      "Epoch: 49, train_loss: 0.96422, time: 0.01273\n",
      "Epoch: 50, train_loss: 0.96681, time: 0.01276\n",
      "Epoch: 51, train_loss: 0.96339, time: 0.01270\n",
      "Epoch: 52, train_loss: 0.96542, time: 0.01266\n",
      "Epoch: 53, train_loss: 0.96525, time: 0.01252\n",
      "Epoch: 54, train_loss: 0.96447, time: 0.01272\n",
      "Epoch: 55, train_loss: 0.96295, time: 0.01277\n",
      "Epoch: 56, train_loss: 0.96684, time: 0.01251\n",
      "Epoch: 57, train_loss: 0.96402, time: 0.01242\n",
      "Epoch: 58, train_loss: 0.96526, time: 0.01294\n",
      "Epoch: 59, train_loss: 0.96309, time: 0.01218\n",
      "Epoch: 60, train_loss: 0.96199, time: 0.01239\n",
      "Epoch: 61, train_loss: 0.96729, time: 0.01228\n",
      "Epoch: 62, train_loss: 0.96344, time: 0.01249\n",
      "Epoch: 63, train_loss: 0.96514, time: 0.01265\n",
      "Epoch: 64, train_loss: 0.96371, time: 0.01287\n",
      "Epoch: 65, train_loss: 0.96378, time: 0.01252\n",
      "Epoch: 66, train_loss: 0.96478, time: 0.01222\n",
      "Epoch: 67, train_loss: 0.96175, time: 0.01235\n",
      "Epoch: 68, train_loss: 0.96531, time: 0.01269\n",
      "Epoch: 69, train_loss: 0.96557, time: 0.01235\n",
      "Epoch: 70, train_loss: 0.96295, time: 0.01226\n",
      "Epoch: 71, train_loss: 0.96249, time: 0.01264\n",
      "Epoch: 72, train_loss: 0.96512, time: 0.01267\n",
      "Epoch: 73, train_loss: 0.96279, time: 0.01265\n",
      "Epoch: 74, train_loss: 0.96385, time: 0.01237\n",
      "Epoch: 75, train_loss: 0.96394, time: 0.01250\n",
      "Epoch: 76, train_loss: 0.96346, time: 0.01240\n",
      "Epoch: 77, train_loss: 0.96170, time: 0.01232\n",
      "Epoch: 78, train_loss: 0.96397, time: 0.01239\n",
      "Epoch: 79, train_loss: 0.96231, time: 0.01240\n",
      "Epoch: 80, train_loss: 0.96499, time: 0.01189\n",
      "Epoch: 81, train_loss: 0.96260, time: 0.01202\n",
      "Epoch: 82, train_loss: 0.96745, time: 0.01219\n",
      "Epoch: 83, train_loss: 0.96352, time: 0.01248\n",
      "Epoch: 84, train_loss: 0.96574, time: 0.01233\n",
      "Epoch: 85, train_loss: 0.96651, time: 0.01262\n",
      "Epoch: 86, train_loss: 0.96399, time: 0.01254\n",
      "Epoch: 87, train_loss: 0.96399, time: 0.01227\n",
      "Epoch: 88, train_loss: 0.96633, time: 0.01241\n",
      "Epoch: 89, train_loss: 0.96520, time: 0.01224\n",
      "Epoch: 90, train_loss: 0.96508, time: 0.01268\n",
      "Epoch: 91, train_loss: 0.96455, time: 0.01225\n",
      "Epoch: 92, train_loss: 0.96218, time: 0.01219\n",
      "Epoch: 93, train_loss: 0.96326, time: 0.01210\n",
      "Epoch: 94, train_loss: 0.96376, time: 0.01241\n",
      "Epoch: 95, train_loss: 0.96500, time: 0.01247\n",
      "Epoch: 96, train_loss: 0.96597, time: 0.01235\n",
      "Epoch: 97, train_loss: 0.96422, time: 0.01205\n",
      "Epoch: 98, train_loss: 0.96674, time: 0.01224\n",
      "Epoch: 99, train_loss: 0.96530, time: 0.01209\n",
      "Epoch: 100, train_loss: 0.97087, time: 0.01266\n",
      "Epoch: 101, train_loss: 0.96198, time: 0.01237\n",
      "Epoch: 102, train_loss: 0.96413, time: 0.01259\n",
      "Epoch: 103, train_loss: 0.96322, time: 0.01253\n",
      "Epoch: 104, train_loss: 0.96277, time: 0.01259\n",
      "Epoch: 105, train_loss: 0.96342, time: 0.01242\n",
      "Epoch: 106, train_loss: 0.96384, time: 0.01259\n",
      "Epoch: 107, train_loss: 0.96471, time: 0.01253\n",
      "Epoch: 108, train_loss: 0.96350, time: 0.01254\n",
      "Epoch: 109, train_loss: 0.96290, time: 0.01277\n",
      "Epoch: 110, train_loss: 0.96468, time: 0.01252\n",
      "Epoch: 111, train_loss: 0.96228, time: 0.01247\n",
      "Epoch: 112, train_loss: 0.96381, time: 0.01244\n",
      "Epoch: 113, train_loss: 0.96633, time: 0.01209\n",
      "Epoch: 114, train_loss: 0.96775, time: 0.01226\n",
      "Epoch: 115, train_loss: 0.96483, time: 0.01203\n",
      "Epoch: 116, train_loss: 0.96460, time: 0.01227\n",
      "Epoch: 117, train_loss: 0.96499, time: 0.01196\n",
      "Epoch: 118, train_loss: 0.96445, time: 0.01218\n",
      "Epoch: 119, train_loss: 0.96353, time: 0.01210\n",
      "Epoch: 120, train_loss: 0.96392, time: 0.01209\n",
      "Epoch: 121, train_loss: 0.96445, time: 0.01245\n",
      "Epoch: 122, train_loss: 0.96466, time: 0.01241\n",
      "Epoch: 123, train_loss: 0.96396, time: 0.01291\n",
      "Epoch: 124, train_loss: 0.96304, time: 0.01240\n",
      "Epoch: 125, train_loss: 0.96464, time: 0.01216\n",
      "Epoch: 126, train_loss: 0.96678, time: 0.01215\n",
      "Epoch: 127, train_loss: 0.96443, time: 0.01245\n",
      "Epoch: 128, train_loss: 0.96454, time: 0.01252\n",
      "Epoch: 129, train_loss: 0.96600, time: 0.01262\n",
      "Epoch: 130, train_loss: 0.96450, time: 0.01243\n",
      "Epoch: 131, train_loss: 0.96485, time: 0.01225\n",
      "Epoch: 132, train_loss: 0.96422, time: 0.01243\n",
      "Epoch: 133, train_loss: 0.96358, time: 0.01262\n",
      "Epoch: 134, train_loss: 0.96277, time: 0.01245\n",
      "Epoch: 135, train_loss: 0.96670, time: 0.01282\n",
      "Epoch: 136, train_loss: 0.96409, time: 0.01260\n",
      "Epoch: 137, train_loss: 0.96425, time: 0.01246\n",
      "Epoch: 138, train_loss: 0.96469, time: 0.01245\n",
      "Epoch: 139, train_loss: 0.96762, time: 0.01212\n",
      "Epoch: 140, train_loss: 0.96366, time: 0.01217\n",
      "Epoch: 141, train_loss: 0.96382, time: 0.01236\n",
      "Epoch: 142, train_loss: 0.96528, time: 0.01256\n",
      "Epoch: 143, train_loss: 0.96509, time: 0.01273\n",
      "Epoch: 144, train_loss: 0.96596, time: 0.01237\n",
      "Epoch: 145, train_loss: 0.96321, time: 0.01281\n",
      "Epoch: 146, train_loss: 0.96248, time: 0.01228\n",
      "Epoch: 147, train_loss: 0.96208, time: 0.01232\n",
      "Epoch: 148, train_loss: 0.96304, time: 0.01231\n",
      "Epoch: 149, train_loss: 0.96705, time: 0.01261\n",
      "Epoch: 150, train_loss: 0.96311, time: 0.01258\n",
      "Epoch: 151, train_loss: 0.96436, time: 0.01242\n",
      "Epoch: 152, train_loss: 0.96390, time: 0.01249\n",
      "Epoch: 153, train_loss: 0.96640, time: 0.01268\n",
      "Epoch: 154, train_loss: 0.96672, time: 0.01255\n",
      "Epoch: 155, train_loss: 0.96505, time: 0.01263\n",
      "Epoch: 156, train_loss: 0.96327, time: 0.01230\n",
      "Epoch: 157, train_loss: 0.96460, time: 0.01248\n",
      "Epoch: 158, train_loss: 0.96139, time: 0.01255\n",
      "Epoch: 159, train_loss: 0.96704, time: 0.01229\n",
      "Epoch: 160, train_loss: 0.96427, time: 0.01254\n",
      "Epoch: 161, train_loss: 0.96364, time: 0.01262\n",
      "Epoch: 162, train_loss: 0.96211, time: 0.01229\n",
      "Epoch: 163, train_loss: 0.96363, time: 0.01272\n",
      "Epoch: 164, train_loss: 0.96395, time: 0.01274\n",
      "Epoch: 165, train_loss: 0.96327, time: 0.01254\n",
      "Epoch: 166, train_loss: 0.96360, time: 0.01252\n",
      "Epoch: 167, train_loss: 0.96483, time: 0.01216\n",
      "Epoch: 168, train_loss: 0.96286, time: 0.01257\n",
      "Epoch: 169, train_loss: 0.96573, time: 0.01298\n",
      "Epoch: 170, train_loss: 0.96476, time: 0.01316\n",
      "Epoch: 171, train_loss: 0.96511, time: 0.01282\n",
      "Epoch: 172, train_loss: 0.96290, time: 0.01224\n",
      "Epoch: 173, train_loss: 0.96570, time: 0.01258\n",
      "Epoch: 174, train_loss: 0.96554, time: 0.01251\n",
      "Epoch: 175, train_loss: 0.96431, time: 0.01228\n",
      "Epoch: 176, train_loss: 0.96316, time: 0.01258\n",
      "Epoch: 177, train_loss: 0.96451, time: 0.01224\n",
      "Epoch: 178, train_loss: 0.96506, time: 0.01238\n",
      "Epoch: 179, train_loss: 0.96325, time: 0.01226\n",
      "Epoch: 180, train_loss: 0.96279, time: 0.01244\n",
      "Epoch: 181, train_loss: 0.96420, time: 0.01224\n",
      "Epoch: 182, train_loss: 0.96685, time: 0.01270\n",
      "Epoch: 183, train_loss: 0.96570, time: 0.01266\n",
      "Epoch: 184, train_loss: 0.96230, time: 0.01262\n",
      "Epoch: 185, train_loss: 0.96502, time: 0.01279\n",
      "Epoch: 186, train_loss: 0.96291, time: 0.01233\n",
      "Epoch: 187, train_loss: 0.96421, time: 0.01225\n",
      "Epoch: 188, train_loss: 0.96259, time: 0.01284\n",
      "Epoch: 189, train_loss: 0.96482, time: 0.01227\n",
      "Epoch: 190, train_loss: 0.96312, time: 0.01261\n",
      "Epoch: 191, train_loss: 0.96418, time: 0.01236\n",
      "Epoch: 192, train_loss: 0.96527, time: 0.01225\n",
      "Epoch: 193, train_loss: 0.96323, time: 0.01233\n",
      "Epoch: 194, train_loss: 0.96540, time: 0.01222\n",
      "Epoch: 195, train_loss: 0.96480, time: 0.01254\n",
      "Epoch: 196, train_loss: 0.96509, time: 0.01282\n",
      "Epoch: 197, train_loss: 0.96361, time: 0.01235\n",
      "Epoch: 198, train_loss: 0.96307, time: 0.01207\n",
      "Epoch: 199, train_loss: 0.96534, time: 0.01221\n",
      "Epoch: 200, train_loss: 0.96324, time: 0.01251\n",
      "pairwise precision 0.31308 recall 0.98272 f1 0.47487\n",
      "average until now [0.4590360825814114, 0.8169763182425193, 0.5878024515211235]\n",
      "36 names 199.711350440979 avg time 5.547537512249416\n",
      "Loading yong_tian dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 262 nodes, 1638 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98084, time: 0.11159\n",
      "Epoch: 2, train_loss: 0.97946, time: 0.01365\n",
      "Epoch: 3, train_loss: 0.97753, time: 0.01244\n",
      "Epoch: 4, train_loss: 0.97984, time: 0.01154\n",
      "Epoch: 5, train_loss: 0.97347, time: 0.01125\n",
      "Epoch: 6, train_loss: 0.97427, time: 0.01163\n",
      "Epoch: 7, train_loss: 0.97730, time: 0.01152\n",
      "Epoch: 8, train_loss: 0.97398, time: 0.01191\n",
      "Epoch: 9, train_loss: 0.97555, time: 0.01174\n",
      "Epoch: 10, train_loss: 0.97498, time: 0.01137\n",
      "Epoch: 11, train_loss: 0.97287, time: 0.01169\n",
      "Epoch: 12, train_loss: 0.97319, time: 0.01155\n",
      "Epoch: 13, train_loss: 0.97523, time: 0.01141\n",
      "Epoch: 14, train_loss: 0.97564, time: 0.01133\n",
      "Epoch: 15, train_loss: 0.97444, time: 0.01136\n",
      "Epoch: 16, train_loss: 0.97164, time: 0.01174\n",
      "Epoch: 17, train_loss: 0.97436, time: 0.01170\n",
      "Epoch: 18, train_loss: 0.97515, time: 0.01136\n",
      "Epoch: 19, train_loss: 0.97516, time: 0.01153\n",
      "Epoch: 20, train_loss: 0.97356, time: 0.01149\n",
      "Epoch: 21, train_loss: 0.97743, time: 0.01100\n",
      "Epoch: 22, train_loss: 0.97946, time: 0.01154\n",
      "Epoch: 23, train_loss: 0.97605, time: 0.01149\n",
      "Epoch: 24, train_loss: 0.97562, time: 0.01143\n",
      "Epoch: 25, train_loss: 0.97593, time: 0.01153\n",
      "Epoch: 26, train_loss: 0.97352, time: 0.01164\n",
      "Epoch: 27, train_loss: 0.97292, time: 0.01138\n",
      "Epoch: 28, train_loss: 0.97724, time: 0.01150\n",
      "Epoch: 29, train_loss: 0.97486, time: 0.01118\n",
      "Epoch: 30, train_loss: 0.97649, time: 0.01160\n",
      "Epoch: 31, train_loss: 0.97415, time: 0.01143\n",
      "Epoch: 32, train_loss: 0.97236, time: 0.01156\n",
      "Epoch: 33, train_loss: 0.97496, time: 0.01138\n",
      "Epoch: 34, train_loss: 0.97362, time: 0.01128\n",
      "Epoch: 35, train_loss: 0.97581, time: 0.01146\n",
      "Epoch: 36, train_loss: 0.97393, time: 0.01169\n",
      "Epoch: 37, train_loss: 0.97436, time: 0.01145\n",
      "Epoch: 38, train_loss: 0.97442, time: 0.01149\n",
      "Epoch: 39, train_loss: 0.97659, time: 0.01156\n",
      "Epoch: 40, train_loss: 0.97786, time: 0.01167\n",
      "Epoch: 41, train_loss: 0.97562, time: 0.01141\n",
      "Epoch: 42, train_loss: 0.97450, time: 0.01174\n",
      "Epoch: 43, train_loss: 0.97426, time: 0.01143\n",
      "Epoch: 44, train_loss: 0.97519, time: 0.01123\n",
      "Epoch: 45, train_loss: 0.97363, time: 0.01137\n",
      "Epoch: 46, train_loss: 0.97321, time: 0.01164\n",
      "Epoch: 47, train_loss: 0.97497, time: 0.01131\n",
      "Epoch: 48, train_loss: 0.97459, time: 0.01151\n",
      "Epoch: 49, train_loss: 0.97434, time: 0.01157\n",
      "Epoch: 50, train_loss: 0.97245, time: 0.01174\n",
      "Epoch: 51, train_loss: 0.97425, time: 0.01160\n",
      "Epoch: 52, train_loss: 0.97229, time: 0.01146\n",
      "Epoch: 53, train_loss: 0.97729, time: 0.01101\n",
      "Epoch: 54, train_loss: 0.97516, time: 0.01150\n",
      "Epoch: 55, train_loss: 0.97557, time: 0.01135\n",
      "Epoch: 56, train_loss: 0.97477, time: 0.01146\n",
      "Epoch: 57, train_loss: 0.97542, time: 0.01168\n",
      "Epoch: 58, train_loss: 0.97277, time: 0.01163\n",
      "Epoch: 59, train_loss: 0.97367, time: 0.01172\n",
      "Epoch: 60, train_loss: 0.97375, time: 0.01146\n",
      "Epoch: 61, train_loss: 0.97665, time: 0.01117\n",
      "Epoch: 62, train_loss: 0.97617, time: 0.01156\n",
      "Epoch: 63, train_loss: 0.97193, time: 0.01141\n",
      "Epoch: 64, train_loss: 0.97507, time: 0.01162\n",
      "Epoch: 65, train_loss: 0.97309, time: 0.01169\n",
      "Epoch: 66, train_loss: 0.97348, time: 0.01159\n",
      "Epoch: 67, train_loss: 0.97337, time: 0.01115\n",
      "Epoch: 68, train_loss: 0.97579, time: 0.01177\n",
      "Epoch: 69, train_loss: 0.97375, time: 0.01138\n",
      "Epoch: 70, train_loss: 0.97471, time: 0.01127\n",
      "Epoch: 71, train_loss: 0.97680, time: 0.01125\n",
      "Epoch: 72, train_loss: 0.97603, time: 0.01197\n",
      "Epoch: 73, train_loss: 0.97704, time: 0.01152\n",
      "Epoch: 74, train_loss: 0.97468, time: 0.01152\n",
      "Epoch: 75, train_loss: 0.97554, time: 0.01136\n",
      "Epoch: 76, train_loss: 0.97523, time: 0.01159\n",
      "Epoch: 77, train_loss: 0.97503, time: 0.01175\n",
      "Epoch: 78, train_loss: 0.97328, time: 0.01165\n",
      "Epoch: 79, train_loss: 0.97113, time: 0.01149\n",
      "Epoch: 80, train_loss: 0.97673, time: 0.01173\n",
      "Epoch: 81, train_loss: 0.97490, time: 0.01164\n",
      "Epoch: 82, train_loss: 0.97420, time: 0.01168\n",
      "Epoch: 83, train_loss: 0.97313, time: 0.01166\n",
      "Epoch: 84, train_loss: 0.97300, time: 0.01152\n",
      "Epoch: 85, train_loss: 0.97596, time: 0.01169\n",
      "Epoch: 86, train_loss: 0.97520, time: 0.01174\n",
      "Epoch: 87, train_loss: 0.97420, time: 0.01146\n",
      "Epoch: 88, train_loss: 0.97255, time: 0.01147\n",
      "Epoch: 89, train_loss: 0.97455, time: 0.01145\n",
      "Epoch: 90, train_loss: 0.97529, time: 0.01153\n",
      "Epoch: 91, train_loss: 0.97489, time: 0.01162\n",
      "Epoch: 92, train_loss: 0.97391, time: 0.01419\n",
      "Epoch: 93, train_loss: 0.97602, time: 0.01154\n",
      "Epoch: 94, train_loss: 0.97497, time: 0.01147\n",
      "Epoch: 95, train_loss: 0.97839, time: 0.01146\n",
      "Epoch: 96, train_loss: 0.97581, time: 0.01126\n",
      "Epoch: 97, train_loss: 0.97610, time: 0.01121\n",
      "Epoch: 98, train_loss: 0.97330, time: 0.01176\n",
      "Epoch: 99, train_loss: 0.97214, time: 0.01179\n",
      "Epoch: 100, train_loss: 0.97515, time: 0.01261\n",
      "Epoch: 101, train_loss: 0.97563, time: 0.01159\n",
      "Epoch: 102, train_loss: 0.97563, time: 0.01161\n",
      "Epoch: 103, train_loss: 0.97400, time: 0.01135\n",
      "Epoch: 104, train_loss: 0.97649, time: 0.01150\n",
      "Epoch: 105, train_loss: 0.97429, time: 0.01180\n",
      "Epoch: 106, train_loss: 0.97430, time: 0.01201\n",
      "Epoch: 107, train_loss: 0.97240, time: 0.01156\n",
      "Epoch: 108, train_loss: 0.97408, time: 0.01170\n",
      "Epoch: 109, train_loss: 0.97648, time: 0.01135\n",
      "Epoch: 110, train_loss: 0.97281, time: 0.01168\n",
      "Epoch: 111, train_loss: 0.97390, time: 0.01177\n",
      "Epoch: 112, train_loss: 0.97600, time: 0.01167\n",
      "Epoch: 113, train_loss: 0.97511, time: 0.01181\n",
      "Epoch: 114, train_loss: 0.97512, time: 0.01189\n",
      "Epoch: 115, train_loss: 0.97581, time: 0.01128\n",
      "Epoch: 116, train_loss: 0.97987, time: 0.01147\n",
      "Epoch: 117, train_loss: 0.97562, time: 0.01133\n",
      "Epoch: 118, train_loss: 0.97511, time: 0.01139\n",
      "Epoch: 119, train_loss: 0.97279, time: 0.01172\n",
      "Epoch: 120, train_loss: 0.97323, time: 0.01140\n",
      "Epoch: 121, train_loss: 0.97422, time: 0.01167\n",
      "Epoch: 122, train_loss: 0.97517, time: 0.01137\n",
      "Epoch: 123, train_loss: 0.97598, time: 0.01160\n",
      "Epoch: 124, train_loss: 0.97507, time: 0.01177\n",
      "Epoch: 125, train_loss: 0.97779, time: 0.01162\n",
      "Epoch: 126, train_loss: 0.97748, time: 0.01159\n",
      "Epoch: 127, train_loss: 0.97518, time: 0.01125\n",
      "Epoch: 128, train_loss: 0.97347, time: 0.01132\n",
      "Epoch: 129, train_loss: 0.97735, time: 0.01155\n",
      "Epoch: 130, train_loss: 0.97661, time: 0.01148\n",
      "Epoch: 131, train_loss: 0.97576, time: 0.01141\n",
      "Epoch: 132, train_loss: 0.97773, time: 0.01157\n",
      "Epoch: 133, train_loss: 0.97389, time: 0.01170\n",
      "Epoch: 134, train_loss: 0.97639, time: 0.01184\n",
      "Epoch: 135, train_loss: 0.97492, time: 0.01172\n",
      "Epoch: 136, train_loss: 0.97584, time: 0.01126\n",
      "Epoch: 137, train_loss: 0.97405, time: 0.01164\n",
      "Epoch: 138, train_loss: 0.97615, time: 0.01170\n",
      "Epoch: 139, train_loss: 0.97315, time: 0.01145\n",
      "Epoch: 140, train_loss: 0.97214, time: 0.01161\n",
      "Epoch: 141, train_loss: 0.97364, time: 0.01123\n",
      "Epoch: 142, train_loss: 0.97415, time: 0.01158\n",
      "Epoch: 143, train_loss: 0.97650, time: 0.01187\n",
      "Epoch: 144, train_loss: 0.97568, time: 0.01163\n",
      "Epoch: 145, train_loss: 0.97485, time: 0.01157\n",
      "Epoch: 146, train_loss: 0.97536, time: 0.01180\n",
      "Epoch: 147, train_loss: 0.97762, time: 0.01127\n",
      "Epoch: 148, train_loss: 0.97553, time: 0.01158\n",
      "Epoch: 149, train_loss: 0.97505, time: 0.01165\n",
      "Epoch: 150, train_loss: 0.97807, time: 0.01130\n",
      "Epoch: 151, train_loss: 0.97472, time: 0.01140\n",
      "Epoch: 152, train_loss: 0.97516, time: 0.01157\n",
      "Epoch: 153, train_loss: 0.97486, time: 0.01144\n",
      "Epoch: 154, train_loss: 0.97260, time: 0.01154\n",
      "Epoch: 155, train_loss: 0.97664, time: 0.01122\n",
      "Epoch: 156, train_loss: 0.97456, time: 0.01172\n",
      "Epoch: 157, train_loss: 0.97314, time: 0.01168\n",
      "Epoch: 158, train_loss: 0.97516, time: 0.01170\n",
      "Epoch: 159, train_loss: 0.97391, time: 0.01141\n",
      "Epoch: 160, train_loss: 0.97672, time: 0.01114\n",
      "Epoch: 161, train_loss: 0.97536, time: 0.01146\n",
      "Epoch: 162, train_loss: 0.97594, time: 0.01160\n",
      "Epoch: 163, train_loss: 0.97343, time: 0.01140\n",
      "Epoch: 164, train_loss: 0.97699, time: 0.01133\n",
      "Epoch: 165, train_loss: 0.97267, time: 0.01147\n",
      "Epoch: 166, train_loss: 0.97471, time: 0.01142\n",
      "Epoch: 167, train_loss: 0.97256, time: 0.01127\n",
      "Epoch: 168, train_loss: 0.97626, time: 0.01129\n",
      "Epoch: 169, train_loss: 0.97259, time: 0.01179\n",
      "Epoch: 170, train_loss: 0.97542, time: 0.01185\n",
      "Epoch: 171, train_loss: 0.97500, time: 0.01192\n",
      "Epoch: 172, train_loss: 0.97493, time: 0.01153\n",
      "Epoch: 173, train_loss: 0.97399, time: 0.01141\n",
      "Epoch: 174, train_loss: 0.97755, time: 0.01160\n",
      "Epoch: 175, train_loss: 0.97410, time: 0.01151\n",
      "Epoch: 176, train_loss: 0.97615, time: 0.01155\n",
      "Epoch: 177, train_loss: 0.97153, time: 0.01152\n",
      "Epoch: 178, train_loss: 0.97347, time: 0.01140\n",
      "Epoch: 179, train_loss: 0.97439, time: 0.01153\n",
      "Epoch: 180, train_loss: 0.97553, time: 0.01154\n",
      "Epoch: 181, train_loss: 0.97717, time: 0.01136\n",
      "Epoch: 182, train_loss: 0.97230, time: 0.01182\n",
      "Epoch: 183, train_loss: 0.97589, time: 0.01149\n",
      "Epoch: 184, train_loss: 0.97010, time: 0.01144\n",
      "Epoch: 185, train_loss: 0.97729, time: 0.01143\n",
      "Epoch: 186, train_loss: 0.97415, time: 0.01201\n",
      "Epoch: 187, train_loss: 0.97776, time: 0.01152\n",
      "Epoch: 188, train_loss: 0.97226, time: 0.01174\n",
      "Epoch: 189, train_loss: 0.97539, time: 0.01189\n",
      "Epoch: 190, train_loss: 0.97543, time: 0.01149\n",
      "Epoch: 191, train_loss: 0.97508, time: 0.01147\n",
      "Epoch: 192, train_loss: 0.97984, time: 0.01166\n",
      "Epoch: 193, train_loss: 0.97670, time: 0.01141\n",
      "Epoch: 194, train_loss: 0.97257, time: 0.01133\n",
      "Epoch: 195, train_loss: 0.97384, time: 0.01177\n",
      "Epoch: 196, train_loss: 0.97274, time: 0.01145\n",
      "Epoch: 197, train_loss: 0.97286, time: 0.01192\n",
      "Epoch: 198, train_loss: 0.97498, time: 0.01177\n",
      "Epoch: 199, train_loss: 0.97317, time: 0.01137\n",
      "Epoch: 200, train_loss: 0.97401, time: 0.01150\n",
      "pairwise precision 0.10915 recall 0.92626 f1 0.19528\n",
      "average until now [0.44957962533180934, 0.8199298497289437, 0.5807341526487853]\n",
      "37 names 202.18487215042114 avg time 5.464456004065436\n",
      "Loading lu_han dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 366 nodes, 1961 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99231, time: 0.11394\n",
      "Epoch: 2, train_loss: 0.98793, time: 0.01598\n",
      "Epoch: 3, train_loss: 0.98942, time: 0.01447\n",
      "Epoch: 4, train_loss: 0.98248, time: 0.01456\n",
      "Epoch: 5, train_loss: 0.98353, time: 0.01431\n",
      "Epoch: 6, train_loss: 0.98419, time: 0.01448\n",
      "Epoch: 7, train_loss: 0.98422, time: 0.01513\n",
      "Epoch: 8, train_loss: 0.98263, time: 0.01345\n",
      "Epoch: 9, train_loss: 0.98422, time: 0.01346\n",
      "Epoch: 10, train_loss: 0.98644, time: 0.01368\n",
      "Epoch: 11, train_loss: 0.98331, time: 0.01377\n",
      "Epoch: 12, train_loss: 0.98352, time: 0.01332\n",
      "Epoch: 13, train_loss: 0.98466, time: 0.01404\n",
      "Epoch: 14, train_loss: 0.98399, time: 0.01444\n",
      "Epoch: 15, train_loss: 0.98396, time: 0.01423\n",
      "Epoch: 16, train_loss: 0.98551, time: 0.01437\n",
      "Epoch: 17, train_loss: 0.98473, time: 0.01456\n",
      "Epoch: 18, train_loss: 0.98368, time: 0.01399\n",
      "Epoch: 19, train_loss: 0.98370, time: 0.01449\n",
      "Epoch: 20, train_loss: 0.98416, time: 0.01394\n",
      "Epoch: 21, train_loss: 0.98256, time: 0.01414\n",
      "Epoch: 22, train_loss: 0.98282, time: 0.01450\n",
      "Epoch: 23, train_loss: 0.98298, time: 0.01393\n",
      "Epoch: 24, train_loss: 0.98367, time: 0.01415\n",
      "Epoch: 25, train_loss: 0.98394, time: 0.01454\n",
      "Epoch: 26, train_loss: 0.98427, time: 0.01444\n",
      "Epoch: 27, train_loss: 0.98139, time: 0.01390\n",
      "Epoch: 28, train_loss: 0.98248, time: 0.01453\n",
      "Epoch: 29, train_loss: 0.98706, time: 0.01378\n",
      "Epoch: 30, train_loss: 0.98427, time: 0.01443\n",
      "Epoch: 31, train_loss: 0.98388, time: 0.01394\n",
      "Epoch: 32, train_loss: 0.98609, time: 0.01376\n",
      "Epoch: 33, train_loss: 0.98227, time: 0.01420\n",
      "Epoch: 34, train_loss: 0.98354, time: 0.01406\n",
      "Epoch: 35, train_loss: 0.98340, time: 0.01417\n",
      "Epoch: 36, train_loss: 0.98415, time: 0.01452\n",
      "Epoch: 37, train_loss: 0.98440, time: 0.01458\n",
      "Epoch: 38, train_loss: 0.98373, time: 0.01425\n",
      "Epoch: 39, train_loss: 0.98340, time: 0.01379\n",
      "Epoch: 40, train_loss: 0.98467, time: 0.01412\n",
      "Epoch: 41, train_loss: 0.98449, time: 0.01365\n",
      "Epoch: 42, train_loss: 0.98569, time: 0.01397\n",
      "Epoch: 43, train_loss: 0.98537, time: 0.01460\n",
      "Epoch: 44, train_loss: 0.98280, time: 0.01421\n",
      "Epoch: 45, train_loss: 0.98294, time: 0.01419\n",
      "Epoch: 46, train_loss: 0.98609, time: 0.01377\n",
      "Epoch: 47, train_loss: 0.98302, time: 0.01397\n",
      "Epoch: 48, train_loss: 0.98549, time: 0.01425\n",
      "Epoch: 49, train_loss: 0.98539, time: 0.01398\n",
      "Epoch: 50, train_loss: 0.98180, time: 0.01397\n",
      "Epoch: 51, train_loss: 0.98400, time: 0.01340\n",
      "Epoch: 52, train_loss: 0.98384, time: 0.01465\n",
      "Epoch: 53, train_loss: 0.98303, time: 0.01433\n",
      "Epoch: 54, train_loss: 0.98233, time: 0.01437\n",
      "Epoch: 55, train_loss: 0.98413, time: 0.01468\n",
      "Epoch: 56, train_loss: 0.98350, time: 0.01426\n",
      "Epoch: 57, train_loss: 0.98411, time: 0.01435\n",
      "Epoch: 58, train_loss: 0.98435, time: 0.01411\n",
      "Epoch: 59, train_loss: 0.98411, time: 0.01454\n",
      "Epoch: 60, train_loss: 0.98316, time: 0.01408\n",
      "Epoch: 61, train_loss: 0.98220, time: 0.01437\n",
      "Epoch: 62, train_loss: 0.98226, time: 0.01459\n",
      "Epoch: 63, train_loss: 0.98230, time: 0.01438\n",
      "Epoch: 64, train_loss: 0.98511, time: 0.01470\n",
      "Epoch: 65, train_loss: 0.98341, time: 0.01431\n",
      "Epoch: 66, train_loss: 0.98413, time: 0.01465\n",
      "Epoch: 67, train_loss: 0.98413, time: 0.01427\n",
      "Epoch: 68, train_loss: 0.98454, time: 0.01416\n",
      "Epoch: 69, train_loss: 0.98400, time: 0.01404\n",
      "Epoch: 70, train_loss: 0.98360, time: 0.01442\n",
      "Epoch: 71, train_loss: 0.98398, time: 0.01471\n",
      "Epoch: 72, train_loss: 0.98381, time: 0.01406\n",
      "Epoch: 73, train_loss: 0.98458, time: 0.01444\n",
      "Epoch: 74, train_loss: 0.98395, time: 0.01407\n",
      "Epoch: 75, train_loss: 0.98511, time: 0.01438\n",
      "Epoch: 76, train_loss: 0.98279, time: 0.01445\n",
      "Epoch: 77, train_loss: 0.98601, time: 0.01440\n",
      "Epoch: 78, train_loss: 0.98224, time: 0.01439\n",
      "Epoch: 79, train_loss: 0.98335, time: 0.01459\n",
      "Epoch: 80, train_loss: 0.98402, time: 0.01456\n",
      "Epoch: 81, train_loss: 0.98425, time: 0.01431\n",
      "Epoch: 82, train_loss: 0.98387, time: 0.01425\n",
      "Epoch: 83, train_loss: 0.98163, time: 0.01424\n",
      "Epoch: 84, train_loss: 0.98451, time: 0.01456\n",
      "Epoch: 85, train_loss: 0.98233, time: 0.01424\n",
      "Epoch: 86, train_loss: 0.98399, time: 0.01457\n",
      "Epoch: 87, train_loss: 0.98428, time: 0.01343\n",
      "Epoch: 88, train_loss: 0.98194, time: 0.01399\n",
      "Epoch: 89, train_loss: 0.98486, time: 0.01332\n",
      "Epoch: 90, train_loss: 0.98357, time: 0.01403\n",
      "Epoch: 91, train_loss: 0.98358, time: 0.01436\n",
      "Epoch: 92, train_loss: 0.98202, time: 0.01463\n",
      "Epoch: 93, train_loss: 0.98252, time: 0.01460\n",
      "Epoch: 94, train_loss: 0.98199, time: 0.01454\n",
      "Epoch: 95, train_loss: 0.98413, time: 0.01443\n",
      "Epoch: 96, train_loss: 0.98279, time: 0.01455\n",
      "Epoch: 97, train_loss: 0.98353, time: 0.01416\n",
      "Epoch: 98, train_loss: 0.98336, time: 0.01436\n",
      "Epoch: 99, train_loss: 0.98260, time: 0.01462\n",
      "Epoch: 100, train_loss: 0.98536, time: 0.01392\n",
      "Epoch: 101, train_loss: 0.98442, time: 0.01447\n",
      "Epoch: 102, train_loss: 0.98424, time: 0.01488\n",
      "Epoch: 103, train_loss: 0.98239, time: 0.01420\n",
      "Epoch: 104, train_loss: 0.98568, time: 0.01465\n",
      "Epoch: 105, train_loss: 0.98349, time: 0.01424\n",
      "Epoch: 106, train_loss: 0.98429, time: 0.01405\n",
      "Epoch: 107, train_loss: 0.98578, time: 0.01444\n",
      "Epoch: 108, train_loss: 0.98394, time: 0.01323\n",
      "Epoch: 109, train_loss: 0.98371, time: 0.01478\n",
      "Epoch: 110, train_loss: 0.98383, time: 0.01419\n",
      "Epoch: 111, train_loss: 0.98456, time: 0.01451\n",
      "Epoch: 112, train_loss: 0.98505, time: 0.01464\n",
      "Epoch: 113, train_loss: 0.98352, time: 0.01407\n",
      "Epoch: 114, train_loss: 0.98282, time: 0.01428\n",
      "Epoch: 115, train_loss: 0.98330, time: 0.01402\n",
      "Epoch: 116, train_loss: 0.98362, time: 0.01434\n",
      "Epoch: 117, train_loss: 0.98292, time: 0.01481\n",
      "Epoch: 118, train_loss: 0.98545, time: 0.01424\n",
      "Epoch: 119, train_loss: 0.98278, time: 0.01418\n",
      "Epoch: 120, train_loss: 0.98404, time: 0.01400\n",
      "Epoch: 121, train_loss: 0.98366, time: 0.01428\n",
      "Epoch: 122, train_loss: 0.98396, time: 0.01405\n",
      "Epoch: 123, train_loss: 0.98298, time: 0.01397\n",
      "Epoch: 124, train_loss: 0.98257, time: 0.01460\n",
      "Epoch: 125, train_loss: 0.98353, time: 0.01458\n",
      "Epoch: 126, train_loss: 0.98383, time: 0.01420\n",
      "Epoch: 127, train_loss: 0.98450, time: 0.01457\n",
      "Epoch: 128, train_loss: 0.98436, time: 0.01441\n",
      "Epoch: 129, train_loss: 0.98453, time: 0.01430\n",
      "Epoch: 130, train_loss: 0.98313, time: 0.01470\n",
      "Epoch: 131, train_loss: 0.98344, time: 0.01463\n",
      "Epoch: 132, train_loss: 0.98464, time: 0.01456\n",
      "Epoch: 133, train_loss: 0.98276, time: 0.01407\n",
      "Epoch: 134, train_loss: 0.98423, time: 0.01415\n",
      "Epoch: 135, train_loss: 0.98459, time: 0.01432\n",
      "Epoch: 136, train_loss: 0.98383, time: 0.01468\n",
      "Epoch: 137, train_loss: 0.98174, time: 0.01477\n",
      "Epoch: 138, train_loss: 0.98523, time: 0.01468\n",
      "Epoch: 139, train_loss: 0.98335, time: 0.01464\n",
      "Epoch: 140, train_loss: 0.98518, time: 0.01413\n",
      "Epoch: 141, train_loss: 0.98342, time: 0.01414\n",
      "Epoch: 142, train_loss: 0.98272, time: 0.01444\n",
      "Epoch: 143, train_loss: 0.98238, time: 0.01448\n",
      "Epoch: 144, train_loss: 0.98384, time: 0.01379\n",
      "Epoch: 145, train_loss: 0.98626, time: 0.01460\n",
      "Epoch: 146, train_loss: 0.98263, time: 0.01412\n",
      "Epoch: 147, train_loss: 0.98431, time: 0.01463\n",
      "Epoch: 148, train_loss: 0.98395, time: 0.01447\n",
      "Epoch: 149, train_loss: 0.98339, time: 0.01421\n",
      "Epoch: 150, train_loss: 0.98649, time: 0.01455\n",
      "Epoch: 151, train_loss: 0.98562, time: 0.01385\n",
      "Epoch: 152, train_loss: 0.98426, time: 0.01374\n",
      "Epoch: 153, train_loss: 0.98330, time: 0.01408\n",
      "Epoch: 154, train_loss: 0.98207, time: 0.01350\n",
      "Epoch: 155, train_loss: 0.98416, time: 0.01344\n",
      "Epoch: 156, train_loss: 0.98171, time: 0.01415\n",
      "Epoch: 157, train_loss: 0.98350, time: 0.01438\n",
      "Epoch: 158, train_loss: 0.98364, time: 0.01456\n",
      "Epoch: 159, train_loss: 0.98347, time: 0.01455\n",
      "Epoch: 160, train_loss: 0.98547, time: 0.01432\n",
      "Epoch: 161, train_loss: 0.98289, time: 0.01434\n",
      "Epoch: 162, train_loss: 0.98273, time: 0.01372\n",
      "Epoch: 163, train_loss: 0.98289, time: 0.01370\n",
      "Epoch: 164, train_loss: 0.98383, time: 0.01358\n",
      "Epoch: 165, train_loss: 0.98379, time: 0.01363\n",
      "Epoch: 166, train_loss: 0.98407, time: 0.01348\n",
      "Epoch: 167, train_loss: 0.98502, time: 0.01377\n",
      "Epoch: 168, train_loss: 0.98387, time: 0.01460\n",
      "Epoch: 169, train_loss: 0.98417, time: 0.01476\n",
      "Epoch: 170, train_loss: 0.98143, time: 0.01454\n",
      "Epoch: 171, train_loss: 0.98160, time: 0.01411\n",
      "Epoch: 172, train_loss: 0.98232, time: 0.01380\n",
      "Epoch: 173, train_loss: 0.98362, time: 0.01402\n",
      "Epoch: 174, train_loss: 0.98387, time: 0.01426\n",
      "Epoch: 175, train_loss: 0.98244, time: 0.01366\n",
      "Epoch: 176, train_loss: 0.98258, time: 0.01428\n",
      "Epoch: 177, train_loss: 0.98510, time: 0.01446\n",
      "Epoch: 178, train_loss: 0.98414, time: 0.01431\n",
      "Epoch: 179, train_loss: 0.98328, time: 0.01451\n",
      "Epoch: 180, train_loss: 0.98486, time: 0.01417\n",
      "Epoch: 181, train_loss: 0.98341, time: 0.01414\n",
      "Epoch: 182, train_loss: 0.98431, time: 0.01477\n",
      "Epoch: 183, train_loss: 0.98442, time: 0.01400\n",
      "Epoch: 184, train_loss: 0.98236, time: 0.01420\n",
      "Epoch: 185, train_loss: 0.98473, time: 0.01478\n",
      "Epoch: 186, train_loss: 0.98466, time: 0.01429\n",
      "Epoch: 187, train_loss: 0.98181, time: 0.01452\n",
      "Epoch: 188, train_loss: 0.98228, time: 0.01452\n",
      "Epoch: 189, train_loss: 0.98445, time: 0.01447\n",
      "Epoch: 190, train_loss: 0.98562, time: 0.01462\n",
      "Epoch: 191, train_loss: 0.98313, time: 0.01403\n",
      "Epoch: 192, train_loss: 0.98292, time: 0.01443\n",
      "Epoch: 193, train_loss: 0.98355, time: 0.01447\n",
      "Epoch: 194, train_loss: 0.98549, time: 0.01464\n",
      "Epoch: 195, train_loss: 0.98308, time: 0.01432\n",
      "Epoch: 196, train_loss: 0.98448, time: 0.01451\n",
      "Epoch: 197, train_loss: 0.98291, time: 0.01506\n",
      "Epoch: 198, train_loss: 0.98181, time: 0.01410\n",
      "Epoch: 199, train_loss: 0.98720, time: 0.01426\n",
      "Epoch: 200, train_loss: 0.98305, time: 0.01427\n",
      "pairwise precision 0.13643 recall 0.88557 f1 0.23643\n",
      "average until now [0.44133883280063063, 0.8216571914668145, 0.5742365278695759]\n",
      "38 names 205.2262511253357 avg time 5.400690819087782\n",
      "Loading lin_huang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 553 nodes, 4176 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99230, time: 0.11973\n",
      "Epoch: 2, train_loss: 0.98808, time: 0.02037\n",
      "Epoch: 3, train_loss: 0.98567, time: 0.02009\n",
      "Epoch: 4, train_loss: 0.98608, time: 0.02006\n",
      "Epoch: 5, train_loss: 0.98519, time: 0.02028\n",
      "Epoch: 6, train_loss: 0.98511, time: 0.01933\n",
      "Epoch: 7, train_loss: 0.98541, time: 0.01997\n",
      "Epoch: 8, train_loss: 0.98470, time: 0.01995\n",
      "Epoch: 9, train_loss: 0.98485, time: 0.02068\n",
      "Epoch: 10, train_loss: 0.98385, time: 0.01994\n",
      "Epoch: 11, train_loss: 0.98497, time: 0.02066\n",
      "Epoch: 12, train_loss: 0.98379, time: 0.02079\n",
      "Epoch: 13, train_loss: 0.98581, time: 0.02061\n",
      "Epoch: 14, train_loss: 0.98418, time: 0.01997\n",
      "Epoch: 15, train_loss: 0.98496, time: 0.01898\n",
      "Epoch: 16, train_loss: 0.98508, time: 0.01994\n",
      "Epoch: 17, train_loss: 0.98470, time: 0.02024\n",
      "Epoch: 18, train_loss: 0.98366, time: 0.02026\n",
      "Epoch: 19, train_loss: 0.98418, time: 0.01957\n",
      "Epoch: 20, train_loss: 0.98381, time: 0.02035\n",
      "Epoch: 21, train_loss: 0.98352, time: 0.02067\n",
      "Epoch: 22, train_loss: 0.98378, time: 0.02057\n",
      "Epoch: 23, train_loss: 0.98489, time: 0.02046\n",
      "Epoch: 24, train_loss: 0.98432, time: 0.01990\n",
      "Epoch: 25, train_loss: 0.98315, time: 0.02013\n",
      "Epoch: 26, train_loss: 0.98430, time: 0.01890\n",
      "Epoch: 27, train_loss: 0.98390, time: 0.01973\n",
      "Epoch: 28, train_loss: 0.98492, time: 0.02066\n",
      "Epoch: 29, train_loss: 0.98441, time: 0.02069\n",
      "Epoch: 30, train_loss: 0.98517, time: 0.01997\n",
      "Epoch: 31, train_loss: 0.98400, time: 0.02070\n",
      "Epoch: 32, train_loss: 0.98398, time: 0.01989\n",
      "Epoch: 33, train_loss: 0.98351, time: 0.01986\n",
      "Epoch: 34, train_loss: 0.98415, time: 0.02096\n",
      "Epoch: 35, train_loss: 0.98411, time: 0.02065\n",
      "Epoch: 36, train_loss: 0.98467, time: 0.02032\n",
      "Epoch: 37, train_loss: 0.98428, time: 0.02035\n",
      "Epoch: 38, train_loss: 0.98405, time: 0.02034\n",
      "Epoch: 39, train_loss: 0.98421, time: 0.02049\n",
      "Epoch: 40, train_loss: 0.98461, time: 0.02095\n",
      "Epoch: 41, train_loss: 0.98422, time: 0.02125\n",
      "Epoch: 42, train_loss: 0.98449, time: 0.02062\n",
      "Epoch: 43, train_loss: 0.98375, time: 0.01995\n",
      "Epoch: 44, train_loss: 0.98505, time: 0.02009\n",
      "Epoch: 45, train_loss: 0.98441, time: 0.02047\n",
      "Epoch: 46, train_loss: 0.98399, time: 0.02003\n",
      "Epoch: 47, train_loss: 0.98449, time: 0.02035\n",
      "Epoch: 48, train_loss: 0.98418, time: 0.02025\n",
      "Epoch: 49, train_loss: 0.98419, time: 0.02019\n",
      "Epoch: 50, train_loss: 0.98581, time: 0.02018\n",
      "Epoch: 51, train_loss: 0.98443, time: 0.02036\n",
      "Epoch: 52, train_loss: 0.98530, time: 0.02106\n",
      "Epoch: 53, train_loss: 0.98441, time: 0.02043\n",
      "Epoch: 54, train_loss: 0.98341, time: 0.02047\n",
      "Epoch: 55, train_loss: 0.98335, time: 0.02044\n",
      "Epoch: 56, train_loss: 0.98420, time: 0.02007\n",
      "Epoch: 57, train_loss: 0.98352, time: 0.02055\n",
      "Epoch: 58, train_loss: 0.98424, time: 0.01947\n",
      "Epoch: 59, train_loss: 0.98398, time: 0.02028\n",
      "Epoch: 60, train_loss: 0.98520, time: 0.02056\n",
      "Epoch: 61, train_loss: 0.98337, time: 0.02096\n",
      "Epoch: 62, train_loss: 0.98453, time: 0.02014\n",
      "Epoch: 63, train_loss: 0.98564, time: 0.02079\n",
      "Epoch: 64, train_loss: 0.98357, time: 0.02056\n",
      "Epoch: 65, train_loss: 0.98495, time: 0.01975\n",
      "Epoch: 66, train_loss: 0.98455, time: 0.01966\n",
      "Epoch: 67, train_loss: 0.98408, time: 0.02015\n",
      "Epoch: 68, train_loss: 0.98427, time: 0.02028\n",
      "Epoch: 69, train_loss: 0.98374, time: 0.02069\n",
      "Epoch: 70, train_loss: 0.98337, time: 0.02096\n",
      "Epoch: 71, train_loss: 0.98379, time: 0.02027\n",
      "Epoch: 72, train_loss: 0.98434, time: 0.02084\n",
      "Epoch: 73, train_loss: 0.98525, time: 0.02046\n",
      "Epoch: 74, train_loss: 0.98358, time: 0.02057\n",
      "Epoch: 75, train_loss: 0.98533, time: 0.02095\n",
      "Epoch: 76, train_loss: 0.98429, time: 0.02093\n",
      "Epoch: 77, train_loss: 0.98488, time: 0.02038\n",
      "Epoch: 78, train_loss: 0.98404, time: 0.02015\n",
      "Epoch: 79, train_loss: 0.98477, time: 0.02064\n",
      "Epoch: 80, train_loss: 0.98280, time: 0.02038\n",
      "Epoch: 81, train_loss: 0.98421, time: 0.02068\n",
      "Epoch: 82, train_loss: 0.98630, time: 0.02050\n",
      "Epoch: 83, train_loss: 0.98516, time: 0.02040\n",
      "Epoch: 84, train_loss: 0.98515, time: 0.02070\n",
      "Epoch: 85, train_loss: 0.98493, time: 0.01978\n",
      "Epoch: 86, train_loss: 0.98518, time: 0.02047\n",
      "Epoch: 87, train_loss: 0.98414, time: 0.02029\n",
      "Epoch: 88, train_loss: 0.98406, time: 0.01918\n",
      "Epoch: 89, train_loss: 0.98232, time: 0.01967\n",
      "Epoch: 90, train_loss: 0.98448, time: 0.01997\n",
      "Epoch: 91, train_loss: 0.98324, time: 0.02014\n",
      "Epoch: 92, train_loss: 0.98574, time: 0.02046\n",
      "Epoch: 93, train_loss: 0.98504, time: 0.02054\n",
      "Epoch: 94, train_loss: 0.98299, time: 0.02034\n",
      "Epoch: 95, train_loss: 0.98366, time: 0.02014\n",
      "Epoch: 96, train_loss: 0.98325, time: 0.01984\n",
      "Epoch: 97, train_loss: 0.98356, time: 0.01971\n",
      "Epoch: 98, train_loss: 0.98460, time: 0.02048\n",
      "Epoch: 99, train_loss: 0.98538, time: 0.02060\n",
      "Epoch: 100, train_loss: 0.98499, time: 0.02064\n",
      "Epoch: 101, train_loss: 0.98306, time: 0.02032\n",
      "Epoch: 102, train_loss: 0.98479, time: 0.02004\n",
      "Epoch: 103, train_loss: 0.98506, time: 0.02036\n",
      "Epoch: 104, train_loss: 0.98329, time: 0.02123\n",
      "Epoch: 105, train_loss: 0.98475, time: 0.02023\n",
      "Epoch: 106, train_loss: 0.98500, time: 0.02096\n",
      "Epoch: 107, train_loss: 0.98392, time: 0.02067\n",
      "Epoch: 108, train_loss: 0.98482, time: 0.02057\n",
      "Epoch: 109, train_loss: 0.98353, time: 0.02091\n",
      "Epoch: 110, train_loss: 0.98435, time: 0.02066\n",
      "Epoch: 111, train_loss: 0.98375, time: 0.02038\n",
      "Epoch: 112, train_loss: 0.98529, time: 0.02012\n",
      "Epoch: 113, train_loss: 0.98453, time: 0.02053\n",
      "Epoch: 114, train_loss: 0.98391, time: 0.01965\n",
      "Epoch: 115, train_loss: 0.98298, time: 0.02076\n",
      "Epoch: 116, train_loss: 0.98379, time: 0.02085\n",
      "Epoch: 117, train_loss: 0.98421, time: 0.02068\n",
      "Epoch: 118, train_loss: 0.98395, time: 0.02045\n",
      "Epoch: 119, train_loss: 0.98490, time: 0.01991\n",
      "Epoch: 120, train_loss: 0.98432, time: 0.02025\n",
      "Epoch: 121, train_loss: 0.98401, time: 0.02024\n",
      "Epoch: 122, train_loss: 0.98487, time: 0.02034\n",
      "Epoch: 123, train_loss: 0.98457, time: 0.02108\n",
      "Epoch: 124, train_loss: 0.98473, time: 0.02046\n",
      "Epoch: 125, train_loss: 0.98438, time: 0.02057\n",
      "Epoch: 126, train_loss: 0.98564, time: 0.02078\n",
      "Epoch: 127, train_loss: 0.98650, time: 0.02098\n",
      "Epoch: 128, train_loss: 0.98388, time: 0.02005\n",
      "Epoch: 129, train_loss: 0.98420, time: 0.02064\n",
      "Epoch: 130, train_loss: 0.98423, time: 0.02052\n",
      "Epoch: 131, train_loss: 0.98446, time: 0.02076\n",
      "Epoch: 132, train_loss: 0.98424, time: 0.02109\n",
      "Epoch: 133, train_loss: 0.98552, time: 0.02046\n",
      "Epoch: 134, train_loss: 0.98616, time: 0.02087\n",
      "Epoch: 135, train_loss: 0.98387, time: 0.02062\n",
      "Epoch: 136, train_loss: 0.98515, time: 0.02135\n",
      "Epoch: 137, train_loss: 0.98461, time: 0.02103\n",
      "Epoch: 138, train_loss: 0.98480, time: 0.02006\n",
      "Epoch: 139, train_loss: 0.98545, time: 0.02020\n",
      "Epoch: 140, train_loss: 0.98338, time: 0.02088\n",
      "Epoch: 141, train_loss: 0.98430, time: 0.02008\n",
      "Epoch: 142, train_loss: 0.98520, time: 0.02069\n",
      "Epoch: 143, train_loss: 0.98394, time: 0.02111\n",
      "Epoch: 144, train_loss: 0.98392, time: 0.02113\n",
      "Epoch: 145, train_loss: 0.98472, time: 0.02097\n",
      "Epoch: 146, train_loss: 0.98462, time: 0.02117\n",
      "Epoch: 147, train_loss: 0.98380, time: 0.01919\n",
      "Epoch: 148, train_loss: 0.98503, time: 0.01990\n",
      "Epoch: 149, train_loss: 0.98508, time: 0.02063\n",
      "Epoch: 150, train_loss: 0.98339, time: 0.02074\n",
      "Epoch: 151, train_loss: 0.98530, time: 0.02128\n",
      "Epoch: 152, train_loss: 0.98444, time: 0.01988\n",
      "Epoch: 153, train_loss: 0.98371, time: 0.02047\n",
      "Epoch: 154, train_loss: 0.98510, time: 0.01904\n",
      "Epoch: 155, train_loss: 0.98490, time: 0.02032\n",
      "Epoch: 156, train_loss: 0.98413, time: 0.02113\n",
      "Epoch: 157, train_loss: 0.98525, time: 0.02044\n",
      "Epoch: 158, train_loss: 0.98411, time: 0.02084\n",
      "Epoch: 159, train_loss: 0.98491, time: 0.02021\n",
      "Epoch: 160, train_loss: 0.98497, time: 0.02054\n",
      "Epoch: 161, train_loss: 0.98517, time: 0.02063\n",
      "Epoch: 162, train_loss: 0.98355, time: 0.01892\n",
      "Epoch: 163, train_loss: 0.98487, time: 0.02113\n",
      "Epoch: 164, train_loss: 0.98536, time: 0.01980\n",
      "Epoch: 165, train_loss: 0.98620, time: 0.02037\n",
      "Epoch: 166, train_loss: 0.98501, time: 0.02092\n",
      "Epoch: 167, train_loss: 0.98471, time: 0.02132\n",
      "Epoch: 168, train_loss: 0.98401, time: 0.02118\n",
      "Epoch: 169, train_loss: 0.98437, time: 0.02112\n",
      "Epoch: 170, train_loss: 0.98378, time: 0.02075\n",
      "Epoch: 171, train_loss: 0.98457, time: 0.02165\n",
      "Epoch: 172, train_loss: 0.98332, time: 0.02098\n",
      "Epoch: 173, train_loss: 0.98439, time: 0.02047\n",
      "Epoch: 174, train_loss: 0.98630, time: 0.02179\n",
      "Epoch: 175, train_loss: 0.98349, time: 0.02126\n",
      "Epoch: 176, train_loss: 0.98587, time: 0.02129\n",
      "Epoch: 177, train_loss: 0.98416, time: 0.02067\n",
      "Epoch: 178, train_loss: 0.98450, time: 0.02112\n",
      "Epoch: 179, train_loss: 0.98420, time: 0.02042\n",
      "Epoch: 180, train_loss: 0.98452, time: 0.02081\n",
      "Epoch: 181, train_loss: 0.98467, time: 0.02050\n",
      "Epoch: 182, train_loss: 0.98410, time: 0.02054\n",
      "Epoch: 183, train_loss: 0.98663, time: 0.02054\n",
      "Epoch: 184, train_loss: 0.98336, time: 0.02101\n",
      "Epoch: 185, train_loss: 0.98361, time: 0.02050\n",
      "Epoch: 186, train_loss: 0.98317, time: 0.02105\n",
      "Epoch: 187, train_loss: 0.98440, time: 0.02155\n",
      "Epoch: 188, train_loss: 0.98406, time: 0.02168\n",
      "Epoch: 189, train_loss: 0.98466, time: 0.02101\n",
      "Epoch: 190, train_loss: 0.98433, time: 0.02141\n",
      "Epoch: 191, train_loss: 0.98459, time: 0.02121\n",
      "Epoch: 192, train_loss: 0.98501, time: 0.02037\n",
      "Epoch: 193, train_loss: 0.98452, time: 0.02154\n",
      "Epoch: 194, train_loss: 0.98346, time: 0.02115\n",
      "Epoch: 195, train_loss: 0.98372, time: 0.02091\n",
      "Epoch: 196, train_loss: 0.98449, time: 0.02067\n",
      "Epoch: 197, train_loss: 0.98365, time: 0.02092\n",
      "Epoch: 198, train_loss: 0.98303, time: 0.02027\n",
      "Epoch: 199, train_loss: 0.98431, time: 0.02088\n",
      "Epoch: 200, train_loss: 0.98385, time: 0.02031\n",
      "pairwise precision 0.13565 recall 0.89653 f1 0.23565\n",
      "average until now [0.4335007267359411, 0.8235770932024247, 0.5680177674980053]\n",
      "39 names 209.54663681983948 avg time 5.372990687688191\n",
      "Loading kexin_xu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 203 nodes, 5529 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.86664, time: 0.11431\n",
      "Epoch: 2, train_loss: 0.86308, time: 0.01531\n",
      "Epoch: 3, train_loss: 0.86776, time: 0.01456\n",
      "Epoch: 4, train_loss: 0.86517, time: 0.01471\n",
      "Epoch: 5, train_loss: 0.86821, time: 0.01366\n",
      "Epoch: 6, train_loss: 0.86465, time: 0.01368\n",
      "Epoch: 7, train_loss: 0.86646, time: 0.01375\n",
      "Epoch: 8, train_loss: 0.86475, time: 0.01357\n",
      "Epoch: 9, train_loss: 0.86749, time: 0.01363\n",
      "Epoch: 10, train_loss: 0.86190, time: 0.01402\n",
      "Epoch: 11, train_loss: 0.86417, time: 0.01343\n",
      "Epoch: 12, train_loss: 0.86416, time: 0.01396\n",
      "Epoch: 13, train_loss: 0.86277, time: 0.01411\n",
      "Epoch: 14, train_loss: 0.86885, time: 0.01361\n",
      "Epoch: 15, train_loss: 0.86482, time: 0.01378\n",
      "Epoch: 16, train_loss: 0.86403, time: 0.01379\n",
      "Epoch: 17, train_loss: 0.86666, time: 0.01357\n",
      "Epoch: 18, train_loss: 0.86475, time: 0.01375\n",
      "Epoch: 19, train_loss: 0.86819, time: 0.01380\n",
      "Epoch: 20, train_loss: 0.86568, time: 0.01362\n",
      "Epoch: 21, train_loss: 0.86802, time: 0.01375\n",
      "Epoch: 22, train_loss: 0.86516, time: 0.01341\n",
      "Epoch: 23, train_loss: 0.86602, time: 0.01325\n",
      "Epoch: 24, train_loss: 0.86757, time: 0.01333\n",
      "Epoch: 25, train_loss: 0.86841, time: 0.01358\n",
      "Epoch: 26, train_loss: 0.86263, time: 0.01376\n",
      "Epoch: 27, train_loss: 0.86619, time: 0.01348\n",
      "Epoch: 28, train_loss: 0.86333, time: 0.01378\n",
      "Epoch: 29, train_loss: 0.86784, time: 0.01362\n",
      "Epoch: 30, train_loss: 0.86750, time: 0.01353\n",
      "Epoch: 31, train_loss: 0.86458, time: 0.01360\n",
      "Epoch: 32, train_loss: 0.86534, time: 0.01322\n",
      "Epoch: 33, train_loss: 0.86716, time: 0.01350\n",
      "Epoch: 34, train_loss: 0.86344, time: 0.01374\n",
      "Epoch: 35, train_loss: 0.86610, time: 0.01354\n",
      "Epoch: 36, train_loss: 0.86566, time: 0.01346\n",
      "Epoch: 37, train_loss: 0.86459, time: 0.01333\n",
      "Epoch: 38, train_loss: 0.86498, time: 0.01324\n",
      "Epoch: 39, train_loss: 0.86463, time: 0.01349\n",
      "Epoch: 40, train_loss: 0.86480, time: 0.01353\n",
      "Epoch: 41, train_loss: 0.86651, time: 0.01362\n",
      "Epoch: 42, train_loss: 0.86762, time: 0.01362\n",
      "Epoch: 43, train_loss: 0.86691, time: 0.01346\n",
      "Epoch: 44, train_loss: 0.86436, time: 0.01330\n",
      "Epoch: 45, train_loss: 0.86384, time: 0.01366\n",
      "Epoch: 46, train_loss: 0.86628, time: 0.01353\n",
      "Epoch: 47, train_loss: 0.86724, time: 0.01349\n",
      "Epoch: 48, train_loss: 0.86082, time: 0.01369\n",
      "Epoch: 49, train_loss: 0.86551, time: 0.01386\n",
      "Epoch: 50, train_loss: 0.86304, time: 0.01360\n",
      "Epoch: 51, train_loss: 0.86675, time: 0.01367\n",
      "Epoch: 52, train_loss: 0.86537, time: 0.01362\n",
      "Epoch: 53, train_loss: 0.86586, time: 0.01376\n",
      "Epoch: 54, train_loss: 0.86417, time: 0.01392\n",
      "Epoch: 55, train_loss: 0.86214, time: 0.01360\n",
      "Epoch: 56, train_loss: 0.86553, time: 0.01362\n",
      "Epoch: 57, train_loss: 0.86307, time: 0.01374\n",
      "Epoch: 58, train_loss: 0.86438, time: 0.01361\n",
      "Epoch: 59, train_loss: 0.86507, time: 0.01344\n",
      "Epoch: 60, train_loss: 0.86465, time: 0.01291\n",
      "Epoch: 61, train_loss: 0.86509, time: 0.01381\n",
      "Epoch: 62, train_loss: 0.86635, time: 0.01354\n",
      "Epoch: 63, train_loss: 0.86711, time: 0.01390\n",
      "Epoch: 64, train_loss: 0.86553, time: 0.01346\n",
      "Epoch: 65, train_loss: 0.86496, time: 0.01341\n",
      "Epoch: 66, train_loss: 0.86734, time: 0.01375\n",
      "Epoch: 67, train_loss: 0.86655, time: 0.01380\n",
      "Epoch: 68, train_loss: 0.86606, time: 0.01336\n",
      "Epoch: 69, train_loss: 0.86734, time: 0.01341\n",
      "Epoch: 70, train_loss: 0.86647, time: 0.01372\n",
      "Epoch: 71, train_loss: 0.86438, time: 0.01365\n",
      "Epoch: 72, train_loss: 0.86779, time: 0.01363\n",
      "Epoch: 73, train_loss: 0.86647, time: 0.01350\n",
      "Epoch: 74, train_loss: 0.86777, time: 0.01350\n",
      "Epoch: 75, train_loss: 0.86514, time: 0.01341\n",
      "Epoch: 76, train_loss: 0.86366, time: 0.01360\n",
      "Epoch: 77, train_loss: 0.86431, time: 0.01347\n",
      "Epoch: 78, train_loss: 0.86675, time: 0.01331\n",
      "Epoch: 79, train_loss: 0.86924, time: 0.01325\n",
      "Epoch: 80, train_loss: 0.86582, time: 0.01311\n",
      "Epoch: 81, train_loss: 0.86349, time: 0.01330\n",
      "Epoch: 82, train_loss: 0.86575, time: 0.01311\n",
      "Epoch: 83, train_loss: 0.86250, time: 0.01320\n",
      "Epoch: 84, train_loss: 0.86329, time: 0.01341\n",
      "Epoch: 85, train_loss: 0.86503, time: 0.01343\n",
      "Epoch: 86, train_loss: 0.86481, time: 0.01382\n",
      "Epoch: 87, train_loss: 0.86302, time: 0.01361\n",
      "Epoch: 88, train_loss: 0.86594, time: 0.01338\n",
      "Epoch: 89, train_loss: 0.86704, time: 0.01395\n",
      "Epoch: 90, train_loss: 0.86373, time: 0.01383\n",
      "Epoch: 91, train_loss: 0.86601, time: 0.01377\n",
      "Epoch: 92, train_loss: 0.86270, time: 0.01349\n",
      "Epoch: 93, train_loss: 0.86916, time: 0.01312\n",
      "Epoch: 94, train_loss: 0.86140, time: 0.01389\n",
      "Epoch: 95, train_loss: 0.86539, time: 0.01395\n",
      "Epoch: 96, train_loss: 0.86466, time: 0.01391\n",
      "Epoch: 97, train_loss: 0.86730, time: 0.01366\n",
      "Epoch: 98, train_loss: 0.86432, time: 0.01369\n",
      "Epoch: 99, train_loss: 0.86509, time: 0.01397\n",
      "Epoch: 100, train_loss: 0.86457, time: 0.01358\n",
      "Epoch: 101, train_loss: 0.86433, time: 0.01338\n",
      "Epoch: 102, train_loss: 0.86540, time: 0.01385\n",
      "Epoch: 103, train_loss: 0.86572, time: 0.01344\n",
      "Epoch: 104, train_loss: 0.86254, time: 0.01348\n",
      "Epoch: 105, train_loss: 0.86504, time: 0.01374\n",
      "Epoch: 106, train_loss: 0.86082, time: 0.01354\n",
      "Epoch: 107, train_loss: 0.86522, time: 0.01340\n",
      "Epoch: 108, train_loss: 0.86352, time: 0.01332\n",
      "Epoch: 109, train_loss: 0.86443, time: 0.01401\n",
      "Epoch: 110, train_loss: 0.86283, time: 0.01363\n",
      "Epoch: 111, train_loss: 0.86530, time: 0.01362\n",
      "Epoch: 112, train_loss: 0.86595, time: 0.01373\n",
      "Epoch: 113, train_loss: 0.86404, time: 0.01344\n",
      "Epoch: 114, train_loss: 0.86276, time: 0.01375\n",
      "Epoch: 115, train_loss: 0.86001, time: 0.01386\n",
      "Epoch: 116, train_loss: 0.86662, time: 0.01379\n",
      "Epoch: 117, train_loss: 0.86449, time: 0.01353\n",
      "Epoch: 118, train_loss: 0.86608, time: 0.01393\n",
      "Epoch: 119, train_loss: 0.86611, time: 0.01361\n",
      "Epoch: 120, train_loss: 0.86719, time: 0.01334\n",
      "Epoch: 121, train_loss: 0.86314, time: 0.01360\n",
      "Epoch: 122, train_loss: 0.86815, time: 0.01342\n",
      "Epoch: 123, train_loss: 0.86356, time: 0.01391\n",
      "Epoch: 124, train_loss: 0.86327, time: 0.01397\n",
      "Epoch: 125, train_loss: 0.86673, time: 0.01377\n",
      "Epoch: 126, train_loss: 0.86644, time: 0.01358\n",
      "Epoch: 127, train_loss: 0.86331, time: 0.01350\n",
      "Epoch: 128, train_loss: 0.86747, time: 0.01360\n",
      "Epoch: 129, train_loss: 0.86195, time: 0.01350\n",
      "Epoch: 130, train_loss: 0.86014, time: 0.01349\n",
      "Epoch: 131, train_loss: 0.86642, time: 0.01334\n",
      "Epoch: 132, train_loss: 0.86327, time: 0.01326\n",
      "Epoch: 133, train_loss: 0.86399, time: 0.01372\n",
      "Epoch: 134, train_loss: 0.86555, time: 0.01379\n",
      "Epoch: 135, train_loss: 0.86944, time: 0.01315\n",
      "Epoch: 136, train_loss: 0.86286, time: 0.01335\n",
      "Epoch: 137, train_loss: 0.86871, time: 0.01341\n",
      "Epoch: 138, train_loss: 0.86172, time: 0.01292\n",
      "Epoch: 139, train_loss: 0.86364, time: 0.01399\n",
      "Epoch: 140, train_loss: 0.86209, time: 0.01393\n",
      "Epoch: 141, train_loss: 0.86250, time: 0.01365\n",
      "Epoch: 142, train_loss: 0.86435, time: 0.01363\n",
      "Epoch: 143, train_loss: 0.86225, time: 0.01348\n",
      "Epoch: 144, train_loss: 0.86573, time: 0.01336\n",
      "Epoch: 145, train_loss: 0.86452, time: 0.01370\n",
      "Epoch: 146, train_loss: 0.86552, time: 0.01358\n",
      "Epoch: 147, train_loss: 0.86463, time: 0.01333\n",
      "Epoch: 148, train_loss: 0.86385, time: 0.01363\n",
      "Epoch: 149, train_loss: 0.86445, time: 0.01347\n",
      "Epoch: 150, train_loss: 0.86489, time: 0.01363\n",
      "Epoch: 151, train_loss: 0.86398, time: 0.01401\n",
      "Epoch: 152, train_loss: 0.86453, time: 0.01361\n",
      "Epoch: 153, train_loss: 0.86687, time: 0.01368\n",
      "Epoch: 154, train_loss: 0.86251, time: 0.01389\n",
      "Epoch: 155, train_loss: 0.86287, time: 0.01400\n",
      "Epoch: 156, train_loss: 0.86887, time: 0.01359\n",
      "Epoch: 157, train_loss: 0.86408, time: 0.01368\n",
      "Epoch: 158, train_loss: 0.86383, time: 0.01362\n",
      "Epoch: 159, train_loss: 0.86504, time: 0.01474\n",
      "Epoch: 160, train_loss: 0.86529, time: 0.01361\n",
      "Epoch: 161, train_loss: 0.86401, time: 0.01389\n",
      "Epoch: 162, train_loss: 0.86280, time: 0.01349\n",
      "Epoch: 163, train_loss: 0.86425, time: 0.01344\n",
      "Epoch: 164, train_loss: 0.86478, time: 0.01364\n",
      "Epoch: 165, train_loss: 0.86531, time: 0.01354\n",
      "Epoch: 166, train_loss: 0.86610, time: 0.01397\n",
      "Epoch: 167, train_loss: 0.86737, time: 0.01374\n",
      "Epoch: 168, train_loss: 0.86398, time: 0.01370\n",
      "Epoch: 169, train_loss: 0.86674, time: 0.01388\n",
      "Epoch: 170, train_loss: 0.86805, time: 0.01368\n",
      "Epoch: 171, train_loss: 0.86464, time: 0.01391\n",
      "Epoch: 172, train_loss: 0.86526, time: 0.01338\n",
      "Epoch: 173, train_loss: 0.86485, time: 0.01389\n",
      "Epoch: 174, train_loss: 0.86515, time: 0.01347\n",
      "Epoch: 175, train_loss: 0.86814, time: 0.01359\n",
      "Epoch: 176, train_loss: 0.86413, time: 0.01366\n",
      "Epoch: 177, train_loss: 0.86392, time: 0.01345\n",
      "Epoch: 178, train_loss: 0.86629, time: 0.01351\n",
      "Epoch: 179, train_loss: 0.86565, time: 0.01365\n",
      "Epoch: 180, train_loss: 0.86343, time: 0.01368\n",
      "Epoch: 181, train_loss: 0.86399, time: 0.01381\n",
      "Epoch: 182, train_loss: 0.86483, time: 0.01389\n",
      "Epoch: 183, train_loss: 0.86185, time: 0.01362\n",
      "Epoch: 184, train_loss: 0.86610, time: 0.01376\n",
      "Epoch: 185, train_loss: 0.86452, time: 0.01385\n",
      "Epoch: 186, train_loss: 0.86648, time: 0.01341\n",
      "Epoch: 187, train_loss: 0.86554, time: 0.01359\n",
      "Epoch: 188, train_loss: 0.86138, time: 0.01326\n",
      "Epoch: 189, train_loss: 0.86413, time: 0.01366\n",
      "Epoch: 190, train_loss: 0.86777, time: 0.01358\n",
      "Epoch: 191, train_loss: 0.86209, time: 0.01346\n",
      "Epoch: 192, train_loss: 0.86245, time: 0.01358\n",
      "Epoch: 193, train_loss: 0.86485, time: 0.01388\n",
      "Epoch: 194, train_loss: 0.86364, time: 0.01344\n",
      "Epoch: 195, train_loss: 0.86184, time: 0.01333\n",
      "Epoch: 196, train_loss: 0.86441, time: 0.01324\n",
      "Epoch: 197, train_loss: 0.86428, time: 0.01398\n",
      "Epoch: 198, train_loss: 0.86873, time: 0.01365\n",
      "Epoch: 199, train_loss: 0.86455, time: 0.01368\n",
      "Epoch: 200, train_loss: 0.86320, time: 0.01384\n",
      "pairwise precision 0.83954 recall 0.98737 f1 0.90747\n",
      "average until now [0.4436516645559985, 0.8276720050210982, 0.577662591393675]\n",
      "40 names 212.43019843101501 avg time 5.310754960775375\n",
      "Loading wei_quan dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 174 nodes, 799 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97445, time: 0.10884\n",
      "Epoch: 2, train_loss: 0.97217, time: 0.01146\n",
      "Epoch: 3, train_loss: 0.98055, time: 0.01022\n",
      "Epoch: 4, train_loss: 0.97693, time: 0.00965\n",
      "Epoch: 5, train_loss: 0.97258, time: 0.01041\n",
      "Epoch: 6, train_loss: 0.97173, time: 0.01012\n",
      "Epoch: 7, train_loss: 0.97460, time: 0.00978\n",
      "Epoch: 8, train_loss: 0.97422, time: 0.00988\n",
      "Epoch: 9, train_loss: 0.97665, time: 0.00987\n",
      "Epoch: 10, train_loss: 0.97105, time: 0.00976\n",
      "Epoch: 11, train_loss: 0.97002, time: 0.00998\n",
      "Epoch: 12, train_loss: 0.97058, time: 0.00984\n",
      "Epoch: 13, train_loss: 0.97826, time: 0.00985\n",
      "Epoch: 14, train_loss: 0.97278, time: 0.00964\n",
      "Epoch: 15, train_loss: 0.97519, time: 0.00991\n",
      "Epoch: 16, train_loss: 0.97357, time: 0.00977\n",
      "Epoch: 17, train_loss: 0.97441, time: 0.00984\n",
      "Epoch: 18, train_loss: 0.97672, time: 0.01000\n",
      "Epoch: 19, train_loss: 0.97230, time: 0.00983\n",
      "Epoch: 20, train_loss: 0.97373, time: 0.01000\n",
      "Epoch: 21, train_loss: 0.97320, time: 0.00983\n",
      "Epoch: 22, train_loss: 0.97388, time: 0.00990\n",
      "Epoch: 23, train_loss: 0.97276, time: 0.00979\n",
      "Epoch: 24, train_loss: 0.97490, time: 0.00975\n",
      "Epoch: 25, train_loss: 0.97172, time: 0.00982\n",
      "Epoch: 26, train_loss: 0.97379, time: 0.01017\n",
      "Epoch: 27, train_loss: 0.97435, time: 0.00978\n",
      "Epoch: 28, train_loss: 0.97758, time: 0.00963\n",
      "Epoch: 29, train_loss: 0.97405, time: 0.01008\n",
      "Epoch: 30, train_loss: 0.97470, time: 0.00991\n",
      "Epoch: 31, train_loss: 0.97651, time: 0.00997\n",
      "Epoch: 32, train_loss: 0.97478, time: 0.00993\n",
      "Epoch: 33, train_loss: 0.97432, time: 0.00986\n",
      "Epoch: 34, train_loss: 0.97664, time: 0.00980\n",
      "Epoch: 35, train_loss: 0.97666, time: 0.00970\n",
      "Epoch: 36, train_loss: 0.97130, time: 0.00991\n",
      "Epoch: 37, train_loss: 0.97171, time: 0.00976\n",
      "Epoch: 38, train_loss: 0.97555, time: 0.00976\n",
      "Epoch: 39, train_loss: 0.97155, time: 0.00987\n",
      "Epoch: 40, train_loss: 0.97527, time: 0.00978\n",
      "Epoch: 41, train_loss: 0.97181, time: 0.00996\n",
      "Epoch: 42, train_loss: 0.97288, time: 0.01006\n",
      "Epoch: 43, train_loss: 0.97054, time: 0.00993\n",
      "Epoch: 44, train_loss: 0.97731, time: 0.00980\n",
      "Epoch: 45, train_loss: 0.97241, time: 0.00984\n",
      "Epoch: 46, train_loss: 0.97479, time: 0.00993\n",
      "Epoch: 47, train_loss: 0.97132, time: 0.01034\n",
      "Epoch: 48, train_loss: 0.97070, time: 0.00989\n",
      "Epoch: 49, train_loss: 0.97407, time: 0.00982\n",
      "Epoch: 50, train_loss: 0.97426, time: 0.00987\n",
      "Epoch: 51, train_loss: 0.97853, time: 0.00996\n",
      "Epoch: 52, train_loss: 0.97142, time: 0.00962\n",
      "Epoch: 53, train_loss: 0.97514, time: 0.00976\n",
      "Epoch: 54, train_loss: 0.97068, time: 0.00987\n",
      "Epoch: 55, train_loss: 0.97353, time: 0.00981\n",
      "Epoch: 56, train_loss: 0.97928, time: 0.00984\n",
      "Epoch: 57, train_loss: 0.97470, time: 0.00988\n",
      "Epoch: 58, train_loss: 0.97238, time: 0.01004\n",
      "Epoch: 59, train_loss: 0.97625, time: 0.00976\n",
      "Epoch: 60, train_loss: 0.97454, time: 0.00983\n",
      "Epoch: 61, train_loss: 0.97822, time: 0.00998\n",
      "Epoch: 62, train_loss: 0.97048, time: 0.00996\n",
      "Epoch: 63, train_loss: 0.97203, time: 0.00986\n",
      "Epoch: 64, train_loss: 0.97168, time: 0.00986\n",
      "Epoch: 65, train_loss: 0.97627, time: 0.00989\n",
      "Epoch: 66, train_loss: 0.97692, time: 0.00987\n",
      "Epoch: 67, train_loss: 0.96875, time: 0.00987\n",
      "Epoch: 68, train_loss: 0.97256, time: 0.01028\n",
      "Epoch: 69, train_loss: 0.97122, time: 0.01000\n",
      "Epoch: 70, train_loss: 0.97245, time: 0.00987\n",
      "Epoch: 71, train_loss: 0.97627, time: 0.00970\n",
      "Epoch: 72, train_loss: 0.97252, time: 0.00976\n",
      "Epoch: 73, train_loss: 0.97462, time: 0.00985\n",
      "Epoch: 74, train_loss: 0.97339, time: 0.00988\n",
      "Epoch: 75, train_loss: 0.96996, time: 0.00986\n",
      "Epoch: 76, train_loss: 0.97394, time: 0.00989\n",
      "Epoch: 77, train_loss: 0.97431, time: 0.00980\n",
      "Epoch: 78, train_loss: 0.97070, time: 0.00958\n",
      "Epoch: 79, train_loss: 0.97328, time: 0.01001\n",
      "Epoch: 80, train_loss: 0.97534, time: 0.00987\n",
      "Epoch: 81, train_loss: 0.97482, time: 0.00981\n",
      "Epoch: 82, train_loss: 0.97524, time: 0.00991\n",
      "Epoch: 83, train_loss: 0.97491, time: 0.01007\n",
      "Epoch: 84, train_loss: 0.97322, time: 0.00962\n",
      "Epoch: 85, train_loss: 0.97523, time: 0.01005\n",
      "Epoch: 86, train_loss: 0.98147, time: 0.00993\n",
      "Epoch: 87, train_loss: 0.97224, time: 0.00986\n",
      "Epoch: 88, train_loss: 0.97458, time: 0.00966\n",
      "Epoch: 89, train_loss: 0.97514, time: 0.01038\n",
      "Epoch: 90, train_loss: 0.97696, time: 0.00995\n",
      "Epoch: 91, train_loss: 0.97022, time: 0.00984\n",
      "Epoch: 92, train_loss: 0.97240, time: 0.00997\n",
      "Epoch: 93, train_loss: 0.97412, time: 0.00979\n",
      "Epoch: 94, train_loss: 0.97198, time: 0.00991\n",
      "Epoch: 95, train_loss: 0.97422, time: 0.00980\n",
      "Epoch: 96, train_loss: 0.97264, time: 0.00983\n",
      "Epoch: 97, train_loss: 0.97428, time: 0.00984\n",
      "Epoch: 98, train_loss: 0.97296, time: 0.00989\n",
      "Epoch: 99, train_loss: 0.97731, time: 0.00993\n",
      "Epoch: 100, train_loss: 0.97417, time: 0.00980\n",
      "Epoch: 101, train_loss: 0.97278, time: 0.00984\n",
      "Epoch: 102, train_loss: 0.97453, time: 0.00983\n",
      "Epoch: 103, train_loss: 0.97437, time: 0.00996\n",
      "Epoch: 104, train_loss: 0.97215, time: 0.00978\n",
      "Epoch: 105, train_loss: 0.96941, time: 0.00985\n",
      "Epoch: 106, train_loss: 0.97315, time: 0.00990\n",
      "Epoch: 107, train_loss: 0.97410, time: 0.00994\n",
      "Epoch: 108, train_loss: 0.97521, time: 0.00997\n",
      "Epoch: 109, train_loss: 0.97359, time: 0.01000\n",
      "Epoch: 110, train_loss: 0.97308, time: 0.01032\n",
      "Epoch: 111, train_loss: 0.97340, time: 0.01006\n",
      "Epoch: 112, train_loss: 0.97178, time: 0.01001\n",
      "Epoch: 113, train_loss: 0.97141, time: 0.00962\n",
      "Epoch: 114, train_loss: 0.97017, time: 0.00999\n",
      "Epoch: 115, train_loss: 0.97565, time: 0.00999\n",
      "Epoch: 116, train_loss: 0.97244, time: 0.00982\n",
      "Epoch: 117, train_loss: 0.97610, time: 0.00983\n",
      "Epoch: 118, train_loss: 0.97084, time: 0.00987\n",
      "Epoch: 119, train_loss: 0.97652, time: 0.00990\n",
      "Epoch: 120, train_loss: 0.97153, time: 0.00981\n",
      "Epoch: 121, train_loss: 0.97262, time: 0.00976\n",
      "Epoch: 122, train_loss: 0.97413, time: 0.00982\n",
      "Epoch: 123, train_loss: 0.97511, time: 0.00999\n",
      "Epoch: 124, train_loss: 0.97585, time: 0.00995\n",
      "Epoch: 125, train_loss: 0.97174, time: 0.00996\n",
      "Epoch: 126, train_loss: 0.97602, time: 0.00990\n",
      "Epoch: 127, train_loss: 0.97388, time: 0.00973\n",
      "Epoch: 128, train_loss: 0.97369, time: 0.00982\n",
      "Epoch: 129, train_loss: 0.97325, time: 0.00982\n",
      "Epoch: 130, train_loss: 0.97097, time: 0.00997\n",
      "Epoch: 131, train_loss: 0.97282, time: 0.01050\n",
      "Epoch: 132, train_loss: 0.97559, time: 0.01024\n",
      "Epoch: 133, train_loss: 0.97451, time: 0.00994\n",
      "Epoch: 134, train_loss: 0.97168, time: 0.00997\n",
      "Epoch: 135, train_loss: 0.97320, time: 0.00993\n",
      "Epoch: 136, train_loss: 0.97226, time: 0.00980\n",
      "Epoch: 137, train_loss: 0.97337, time: 0.00984\n",
      "Epoch: 138, train_loss: 0.97305, time: 0.00984\n",
      "Epoch: 139, train_loss: 0.97155, time: 0.00999\n",
      "Epoch: 140, train_loss: 0.97543, time: 0.00986\n",
      "Epoch: 141, train_loss: 0.97577, time: 0.01010\n",
      "Epoch: 142, train_loss: 0.97183, time: 0.00960\n",
      "Epoch: 143, train_loss: 0.97114, time: 0.00992\n",
      "Epoch: 144, train_loss: 0.97505, time: 0.00993\n",
      "Epoch: 145, train_loss: 0.97387, time: 0.00994\n",
      "Epoch: 146, train_loss: 0.97041, time: 0.00995\n",
      "Epoch: 147, train_loss: 0.97226, time: 0.01002\n",
      "Epoch: 148, train_loss: 0.97943, time: 0.01001\n",
      "Epoch: 149, train_loss: 0.97570, time: 0.01000\n",
      "Epoch: 150, train_loss: 0.97201, time: 0.00987\n",
      "Epoch: 151, train_loss: 0.97320, time: 0.01005\n",
      "Epoch: 152, train_loss: 0.96930, time: 0.01024\n",
      "Epoch: 153, train_loss: 0.97117, time: 0.01003\n",
      "Epoch: 154, train_loss: 0.97299, time: 0.00967\n",
      "Epoch: 155, train_loss: 0.97396, time: 0.00990\n",
      "Epoch: 156, train_loss: 0.97294, time: 0.00975\n",
      "Epoch: 157, train_loss: 0.97570, time: 0.00991\n",
      "Epoch: 158, train_loss: 0.97558, time: 0.00976\n",
      "Epoch: 159, train_loss: 0.97272, time: 0.00999\n",
      "Epoch: 160, train_loss: 0.97238, time: 0.00994\n",
      "Epoch: 161, train_loss: 0.97691, time: 0.00981\n",
      "Epoch: 162, train_loss: 0.97344, time: 0.00983\n",
      "Epoch: 163, train_loss: 0.97312, time: 0.00980\n",
      "Epoch: 164, train_loss: 0.97656, time: 0.00995\n",
      "Epoch: 165, train_loss: 0.97609, time: 0.00989\n",
      "Epoch: 166, train_loss: 0.97460, time: 0.00982\n",
      "Epoch: 167, train_loss: 0.97469, time: 0.00992\n",
      "Epoch: 168, train_loss: 0.97170, time: 0.00989\n",
      "Epoch: 169, train_loss: 0.97368, time: 0.00987\n",
      "Epoch: 170, train_loss: 0.97912, time: 0.00990\n",
      "Epoch: 171, train_loss: 0.97264, time: 0.00971\n",
      "Epoch: 172, train_loss: 0.97196, time: 0.00978\n",
      "Epoch: 173, train_loss: 0.97776, time: 0.01034\n",
      "Epoch: 174, train_loss: 0.97449, time: 0.01004\n",
      "Epoch: 175, train_loss: 0.97682, time: 0.00984\n",
      "Epoch: 176, train_loss: 0.98057, time: 0.00981\n",
      "Epoch: 177, train_loss: 0.97419, time: 0.01005\n",
      "Epoch: 178, train_loss: 0.97552, time: 0.00992\n",
      "Epoch: 179, train_loss: 0.97383, time: 0.01001\n",
      "Epoch: 180, train_loss: 0.97548, time: 0.00997\n",
      "Epoch: 181, train_loss: 0.97615, time: 0.00992\n",
      "Epoch: 182, train_loss: 0.97058, time: 0.00999\n",
      "Epoch: 183, train_loss: 0.97767, time: 0.00978\n",
      "Epoch: 184, train_loss: 0.97042, time: 0.00977\n",
      "Epoch: 185, train_loss: 0.97367, time: 0.00986\n",
      "Epoch: 186, train_loss: 0.97225, time: 0.00979\n",
      "Epoch: 187, train_loss: 0.97515, time: 0.00993\n",
      "Epoch: 188, train_loss: 0.97466, time: 0.00982\n",
      "Epoch: 189, train_loss: 0.97341, time: 0.00976\n",
      "Epoch: 190, train_loss: 0.97596, time: 0.00974\n",
      "Epoch: 191, train_loss: 0.96926, time: 0.00979\n",
      "Epoch: 192, train_loss: 0.97587, time: 0.00982\n",
      "Epoch: 193, train_loss: 0.97484, time: 0.00977\n",
      "Epoch: 194, train_loss: 0.97048, time: 0.01042\n",
      "Epoch: 195, train_loss: 0.97348, time: 0.01019\n",
      "Epoch: 196, train_loss: 0.97102, time: 0.00986\n",
      "Epoch: 197, train_loss: 0.97390, time: 0.00978\n",
      "Epoch: 198, train_loss: 0.97340, time: 0.00982\n",
      "Epoch: 199, train_loss: 0.97609, time: 0.00972\n",
      "Epoch: 200, train_loss: 0.97773, time: 0.00966\n",
      "pairwise precision 0.27485 recall 0.72000 f1 0.39783\n",
      "average until now [0.43953449652233134, 0.8250458585571688, 0.5735279922578934]\n",
      "41 names 214.55226254463196 avg time 5.232982013283706\n",
      "Loading tao_deng dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 306 nodes, 1979 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98554, time: 0.11294\n",
      "Epoch: 2, train_loss: 0.98180, time: 0.01445\n",
      "Epoch: 3, train_loss: 0.98011, time: 0.01279\n",
      "Epoch: 4, train_loss: 0.97855, time: 0.01253\n",
      "Epoch: 5, train_loss: 0.97679, time: 0.01320\n",
      "Epoch: 6, train_loss: 0.97755, time: 0.01246\n",
      "Epoch: 7, train_loss: 0.97600, time: 0.01272\n",
      "Epoch: 8, train_loss: 0.97771, time: 0.01250\n",
      "Epoch: 9, train_loss: 0.97838, time: 0.01258\n",
      "Epoch: 10, train_loss: 0.97936, time: 0.01269\n",
      "Epoch: 11, train_loss: 0.97918, time: 0.01226\n",
      "Epoch: 12, train_loss: 0.97721, time: 0.01222\n",
      "Epoch: 13, train_loss: 0.97623, time: 0.01214\n",
      "Epoch: 14, train_loss: 0.97760, time: 0.01252\n",
      "Epoch: 15, train_loss: 0.97583, time: 0.01260\n",
      "Epoch: 16, train_loss: 0.97647, time: 0.01270\n",
      "Epoch: 17, train_loss: 0.97705, time: 0.01260\n",
      "Epoch: 18, train_loss: 0.97777, time: 0.01311\n",
      "Epoch: 19, train_loss: 0.97963, time: 0.01276\n",
      "Epoch: 20, train_loss: 0.97586, time: 0.01245\n",
      "Epoch: 21, train_loss: 0.97761, time: 0.01280\n",
      "Epoch: 22, train_loss: 0.97752, time: 0.01283\n",
      "Epoch: 23, train_loss: 0.97892, time: 0.01284\n",
      "Epoch: 24, train_loss: 0.97746, time: 0.01291\n",
      "Epoch: 25, train_loss: 0.97832, time: 0.01236\n",
      "Epoch: 26, train_loss: 0.97830, time: 0.01240\n",
      "Epoch: 27, train_loss: 0.97797, time: 0.01293\n",
      "Epoch: 28, train_loss: 0.97813, time: 0.01250\n",
      "Epoch: 29, train_loss: 0.97731, time: 0.01246\n",
      "Epoch: 30, train_loss: 0.97734, time: 0.01276\n",
      "Epoch: 31, train_loss: 0.97645, time: 0.01239\n",
      "Epoch: 32, train_loss: 0.97567, time: 0.01283\n",
      "Epoch: 33, train_loss: 0.97647, time: 0.01285\n",
      "Epoch: 34, train_loss: 0.97753, time: 0.01300\n",
      "Epoch: 35, train_loss: 0.97857, time: 0.01237\n",
      "Epoch: 36, train_loss: 0.97613, time: 0.01243\n",
      "Epoch: 37, train_loss: 0.97597, time: 0.01308\n",
      "Epoch: 38, train_loss: 0.97860, time: 0.01292\n",
      "Epoch: 39, train_loss: 0.97597, time: 0.01328\n",
      "Epoch: 40, train_loss: 0.97698, time: 0.01300\n",
      "Epoch: 41, train_loss: 0.97944, time: 0.01236\n",
      "Epoch: 42, train_loss: 0.97706, time: 0.01263\n",
      "Epoch: 43, train_loss: 0.97608, time: 0.01260\n",
      "Epoch: 44, train_loss: 0.97822, time: 0.01266\n",
      "Epoch: 45, train_loss: 0.97642, time: 0.01298\n",
      "Epoch: 46, train_loss: 0.97671, time: 0.01231\n",
      "Epoch: 47, train_loss: 0.97878, time: 0.01251\n",
      "Epoch: 48, train_loss: 0.97740, time: 0.01273\n",
      "Epoch: 49, train_loss: 0.97755, time: 0.01299\n",
      "Epoch: 50, train_loss: 0.97878, time: 0.01283\n",
      "Epoch: 51, train_loss: 0.97543, time: 0.01251\n",
      "Epoch: 52, train_loss: 0.98036, time: 0.01256\n",
      "Epoch: 53, train_loss: 0.97770, time: 0.01271\n",
      "Epoch: 54, train_loss: 0.97851, time: 0.01276\n",
      "Epoch: 55, train_loss: 0.97852, time: 0.01242\n",
      "Epoch: 56, train_loss: 0.97788, time: 0.01255\n",
      "Epoch: 57, train_loss: 0.98035, time: 0.01276\n",
      "Epoch: 58, train_loss: 0.97769, time: 0.01239\n",
      "Epoch: 59, train_loss: 0.97484, time: 0.01255\n",
      "Epoch: 60, train_loss: 0.97786, time: 0.01268\n",
      "Epoch: 61, train_loss: 0.97930, time: 0.01282\n",
      "Epoch: 62, train_loss: 0.97796, time: 0.01267\n",
      "Epoch: 63, train_loss: 0.97761, time: 0.01252\n",
      "Epoch: 64, train_loss: 0.97440, time: 0.01244\n",
      "Epoch: 65, train_loss: 0.97782, time: 0.01349\n",
      "Epoch: 66, train_loss: 0.97723, time: 0.01268\n",
      "Epoch: 67, train_loss: 0.97657, time: 0.01202\n",
      "Epoch: 68, train_loss: 0.97810, time: 0.01261\n",
      "Epoch: 69, train_loss: 0.97792, time: 0.01273\n",
      "Epoch: 70, train_loss: 0.97671, time: 0.01217\n",
      "Epoch: 71, train_loss: 0.97678, time: 0.01269\n",
      "Epoch: 72, train_loss: 0.97854, time: 0.01220\n",
      "Epoch: 73, train_loss: 0.97640, time: 0.01277\n",
      "Epoch: 74, train_loss: 0.97769, time: 0.01232\n",
      "Epoch: 75, train_loss: 0.97900, time: 0.01251\n",
      "Epoch: 76, train_loss: 0.97704, time: 0.01275\n",
      "Epoch: 77, train_loss: 0.97826, time: 0.01262\n",
      "Epoch: 78, train_loss: 0.97706, time: 0.01280\n",
      "Epoch: 79, train_loss: 0.97810, time: 0.01254\n",
      "Epoch: 80, train_loss: 0.97924, time: 0.01259\n",
      "Epoch: 81, train_loss: 0.97751, time: 0.01283\n",
      "Epoch: 82, train_loss: 0.97783, time: 0.01274\n",
      "Epoch: 83, train_loss: 0.97658, time: 0.01243\n",
      "Epoch: 84, train_loss: 0.97758, time: 0.01238\n",
      "Epoch: 85, train_loss: 0.97608, time: 0.01246\n",
      "Epoch: 86, train_loss: 0.97827, time: 0.01262\n",
      "Epoch: 87, train_loss: 0.97676, time: 0.01255\n",
      "Epoch: 88, train_loss: 0.97567, time: 0.01292\n",
      "Epoch: 89, train_loss: 0.97703, time: 0.01248\n",
      "Epoch: 90, train_loss: 0.97633, time: 0.01273\n",
      "Epoch: 91, train_loss: 0.97534, time: 0.01298\n",
      "Epoch: 92, train_loss: 0.97579, time: 0.01286\n",
      "Epoch: 93, train_loss: 0.97779, time: 0.01275\n",
      "Epoch: 94, train_loss: 0.97794, time: 0.01274\n",
      "Epoch: 95, train_loss: 0.97429, time: 0.01283\n",
      "Epoch: 96, train_loss: 0.97737, time: 0.01251\n",
      "Epoch: 97, train_loss: 0.97847, time: 0.01275\n",
      "Epoch: 98, train_loss: 0.97406, time: 0.01272\n",
      "Epoch: 99, train_loss: 0.97740, time: 0.01228\n",
      "Epoch: 100, train_loss: 0.97966, time: 0.01189\n",
      "Epoch: 101, train_loss: 0.97836, time: 0.01227\n",
      "Epoch: 102, train_loss: 0.97713, time: 0.01253\n",
      "Epoch: 103, train_loss: 0.97672, time: 0.01192\n",
      "Epoch: 104, train_loss: 0.97797, time: 0.01249\n",
      "Epoch: 105, train_loss: 0.97691, time: 0.01243\n",
      "Epoch: 106, train_loss: 0.97962, time: 0.01189\n",
      "Epoch: 107, train_loss: 0.97464, time: 0.01191\n",
      "Epoch: 108, train_loss: 0.97862, time: 0.01205\n",
      "Epoch: 109, train_loss: 0.97796, time: 0.01240\n",
      "Epoch: 110, train_loss: 0.97759, time: 0.01235\n",
      "Epoch: 111, train_loss: 0.98013, time: 0.01238\n",
      "Epoch: 112, train_loss: 0.97797, time: 0.01259\n",
      "Epoch: 113, train_loss: 0.97838, time: 0.01269\n",
      "Epoch: 114, train_loss: 0.97999, time: 0.01278\n",
      "Epoch: 115, train_loss: 0.97729, time: 0.01258\n",
      "Epoch: 116, train_loss: 0.97791, time: 0.01290\n",
      "Epoch: 117, train_loss: 0.97745, time: 0.01243\n",
      "Epoch: 118, train_loss: 0.97695, time: 0.01298\n",
      "Epoch: 119, train_loss: 0.97775, time: 0.01243\n",
      "Epoch: 120, train_loss: 0.97685, time: 0.01227\n",
      "Epoch: 121, train_loss: 0.97702, time: 0.01274\n",
      "Epoch: 122, train_loss: 0.97810, time: 0.01284\n",
      "Epoch: 123, train_loss: 0.97850, time: 0.01209\n",
      "Epoch: 124, train_loss: 0.97980, time: 0.01242\n",
      "Epoch: 125, train_loss: 0.97424, time: 0.01269\n",
      "Epoch: 126, train_loss: 0.97695, time: 0.01269\n",
      "Epoch: 127, train_loss: 0.97754, time: 0.01271\n",
      "Epoch: 128, train_loss: 0.97852, time: 0.01272\n",
      "Epoch: 129, train_loss: 0.97688, time: 0.01275\n",
      "Epoch: 130, train_loss: 0.97625, time: 0.01281\n",
      "Epoch: 131, train_loss: 0.97598, time: 0.01259\n",
      "Epoch: 132, train_loss: 0.97885, time: 0.01241\n",
      "Epoch: 133, train_loss: 0.97740, time: 0.01261\n",
      "Epoch: 134, train_loss: 0.97780, time: 0.01282\n",
      "Epoch: 135, train_loss: 0.97589, time: 0.01244\n",
      "Epoch: 136, train_loss: 0.97773, time: 0.01255\n",
      "Epoch: 137, train_loss: 0.97684, time: 0.01241\n",
      "Epoch: 138, train_loss: 0.97690, time: 0.01244\n",
      "Epoch: 139, train_loss: 0.97624, time: 0.01259\n",
      "Epoch: 140, train_loss: 0.97718, time: 0.01202\n",
      "Epoch: 141, train_loss: 0.97702, time: 0.01245\n",
      "Epoch: 142, train_loss: 0.98069, time: 0.01305\n",
      "Epoch: 143, train_loss: 0.97707, time: 0.01233\n",
      "Epoch: 144, train_loss: 0.97746, time: 0.01282\n",
      "Epoch: 145, train_loss: 0.97774, time: 0.01266\n",
      "Epoch: 146, train_loss: 0.97762, time: 0.01321\n",
      "Epoch: 147, train_loss: 0.97893, time: 0.01291\n",
      "Epoch: 148, train_loss: 0.97601, time: 0.01309\n",
      "Epoch: 149, train_loss: 0.97792, time: 0.01313\n",
      "Epoch: 150, train_loss: 0.98208, time: 0.01289\n",
      "Epoch: 151, train_loss: 0.97922, time: 0.01246\n",
      "Epoch: 152, train_loss: 0.97650, time: 0.01239\n",
      "Epoch: 153, train_loss: 0.97898, time: 0.01271\n",
      "Epoch: 154, train_loss: 0.97626, time: 0.01273\n",
      "Epoch: 155, train_loss: 0.97567, time: 0.01305\n",
      "Epoch: 156, train_loss: 0.97751, time: 0.01264\n",
      "Epoch: 157, train_loss: 0.97518, time: 0.01239\n",
      "Epoch: 158, train_loss: 0.97845, time: 0.01244\n",
      "Epoch: 159, train_loss: 0.97717, time: 0.01170\n",
      "Epoch: 160, train_loss: 0.97790, time: 0.01220\n",
      "Epoch: 161, train_loss: 0.97635, time: 0.01182\n",
      "Epoch: 162, train_loss: 0.97752, time: 0.01215\n",
      "Epoch: 163, train_loss: 0.97783, time: 0.01209\n",
      "Epoch: 164, train_loss: 0.97882, time: 0.01285\n",
      "Epoch: 165, train_loss: 0.97854, time: 0.01281\n",
      "Epoch: 166, train_loss: 0.97840, time: 0.01296\n",
      "Epoch: 167, train_loss: 0.98005, time: 0.01244\n",
      "Epoch: 168, train_loss: 0.97839, time: 0.01279\n",
      "Epoch: 169, train_loss: 0.97660, time: 0.01216\n",
      "Epoch: 170, train_loss: 0.97699, time: 0.01272\n",
      "Epoch: 171, train_loss: 0.97750, time: 0.01207\n",
      "Epoch: 172, train_loss: 0.97796, time: 0.01236\n",
      "Epoch: 173, train_loss: 0.97786, time: 0.01237\n",
      "Epoch: 174, train_loss: 0.97891, time: 0.01220\n",
      "Epoch: 175, train_loss: 0.97611, time: 0.01255\n",
      "Epoch: 176, train_loss: 0.97703, time: 0.01284\n",
      "Epoch: 177, train_loss: 0.97659, time: 0.01253\n",
      "Epoch: 178, train_loss: 0.97951, time: 0.01295\n",
      "Epoch: 179, train_loss: 0.97590, time: 0.01291\n",
      "Epoch: 180, train_loss: 0.97621, time: 0.01273\n",
      "Epoch: 181, train_loss: 0.97763, time: 0.01241\n",
      "Epoch: 182, train_loss: 0.97612, time: 0.01228\n",
      "Epoch: 183, train_loss: 0.97856, time: 0.01279\n",
      "Epoch: 184, train_loss: 0.97645, time: 0.01250\n",
      "Epoch: 185, train_loss: 0.97990, time: 0.01224\n",
      "Epoch: 186, train_loss: 0.97774, time: 0.01282\n",
      "Epoch: 187, train_loss: 0.97715, time: 0.01253\n",
      "Epoch: 188, train_loss: 0.97905, time: 0.01279\n",
      "Epoch: 189, train_loss: 0.97596, time: 0.01262\n",
      "Epoch: 190, train_loss: 0.97816, time: 0.01261\n",
      "Epoch: 191, train_loss: 0.97702, time: 0.01257\n",
      "Epoch: 192, train_loss: 0.97843, time: 0.01263\n",
      "Epoch: 193, train_loss: 0.97909, time: 0.01266\n",
      "Epoch: 194, train_loss: 0.97701, time: 0.01320\n",
      "Epoch: 195, train_loss: 0.97799, time: 0.01263\n",
      "Epoch: 196, train_loss: 0.97745, time: 0.01272\n",
      "Epoch: 197, train_loss: 0.98015, time: 0.01279\n",
      "Epoch: 198, train_loss: 0.97597, time: 0.01251\n",
      "Epoch: 199, train_loss: 0.97709, time: 0.01279\n",
      "Epoch: 200, train_loss: 0.97668, time: 0.01232\n",
      "pairwise precision 0.16092 recall 0.92044 f1 0.27395\n",
      "average until now [0.43290090370898565, 0.8273170568629276, 0.5683878706303874]\n",
      "42 names 217.25100898742676 avg time 5.172643071129208\n",
      "Loading hongbin_li dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 286 nodes, 1519 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98426, time: 0.11406\n",
      "Epoch: 2, train_loss: 0.98215, time: 0.01428\n",
      "Epoch: 3, train_loss: 0.98776, time: 0.01249\n",
      "Epoch: 4, train_loss: 0.97831, time: 0.01201\n",
      "Epoch: 5, train_loss: 0.98008, time: 0.01212\n",
      "Epoch: 6, train_loss: 0.98001, time: 0.01171\n",
      "Epoch: 7, train_loss: 0.97882, time: 0.01171\n",
      "Epoch: 8, train_loss: 0.97817, time: 0.01184\n",
      "Epoch: 9, train_loss: 0.98002, time: 0.01196\n",
      "Epoch: 10, train_loss: 0.98429, time: 0.01179\n",
      "Epoch: 11, train_loss: 0.97874, time: 0.01190\n",
      "Epoch: 12, train_loss: 0.98043, time: 0.01200\n",
      "Epoch: 13, train_loss: 0.98376, time: 0.01193\n",
      "Epoch: 14, train_loss: 0.98135, time: 0.01234\n",
      "Epoch: 15, train_loss: 0.98111, time: 0.01229\n",
      "Epoch: 16, train_loss: 0.98274, time: 0.01207\n",
      "Epoch: 17, train_loss: 0.98036, time: 0.01170\n",
      "Epoch: 18, train_loss: 0.98010, time: 0.01225\n",
      "Epoch: 19, train_loss: 0.98075, time: 0.01196\n",
      "Epoch: 20, train_loss: 0.98128, time: 0.01201\n",
      "Epoch: 21, train_loss: 0.98254, time: 0.01190\n",
      "Epoch: 22, train_loss: 0.97886, time: 0.01203\n",
      "Epoch: 23, train_loss: 0.98371, time: 0.01235\n",
      "Epoch: 24, train_loss: 0.98218, time: 0.01181\n",
      "Epoch: 25, train_loss: 0.98243, time: 0.01181\n",
      "Epoch: 26, train_loss: 0.97919, time: 0.01195\n",
      "Epoch: 27, train_loss: 0.98342, time: 0.01190\n",
      "Epoch: 28, train_loss: 0.98030, time: 0.01230\n",
      "Epoch: 29, train_loss: 0.98197, time: 0.01186\n",
      "Epoch: 30, train_loss: 0.98008, time: 0.01196\n",
      "Epoch: 31, train_loss: 0.97923, time: 0.01236\n",
      "Epoch: 32, train_loss: 0.98097, time: 0.01210\n",
      "Epoch: 33, train_loss: 0.98103, time: 0.01191\n",
      "Epoch: 34, train_loss: 0.98127, time: 0.01200\n",
      "Epoch: 35, train_loss: 0.98150, time: 0.01245\n",
      "Epoch: 36, train_loss: 0.98261, time: 0.01205\n",
      "Epoch: 37, train_loss: 0.98202, time: 0.01220\n",
      "Epoch: 38, train_loss: 0.98166, time: 0.01219\n",
      "Epoch: 39, train_loss: 0.97871, time: 0.01185\n",
      "Epoch: 40, train_loss: 0.98050, time: 0.01210\n",
      "Epoch: 41, train_loss: 0.98206, time: 0.01170\n",
      "Epoch: 42, train_loss: 0.97956, time: 0.01181\n",
      "Epoch: 43, train_loss: 0.97972, time: 0.01162\n",
      "Epoch: 44, train_loss: 0.97942, time: 0.01215\n",
      "Epoch: 45, train_loss: 0.98063, time: 0.01170\n",
      "Epoch: 46, train_loss: 0.98153, time: 0.01195\n",
      "Epoch: 47, train_loss: 0.97978, time: 0.01204\n",
      "Epoch: 48, train_loss: 0.97961, time: 0.01205\n",
      "Epoch: 49, train_loss: 0.98190, time: 0.01210\n",
      "Epoch: 50, train_loss: 0.98085, time: 0.01196\n",
      "Epoch: 51, train_loss: 0.98134, time: 0.01221\n",
      "Epoch: 52, train_loss: 0.98024, time: 0.01231\n",
      "Epoch: 53, train_loss: 0.97952, time: 0.01205\n",
      "Epoch: 54, train_loss: 0.98169, time: 0.01204\n",
      "Epoch: 55, train_loss: 0.97913, time: 0.01169\n",
      "Epoch: 56, train_loss: 0.97985, time: 0.01196\n",
      "Epoch: 57, train_loss: 0.97974, time: 0.01179\n",
      "Epoch: 58, train_loss: 0.97950, time: 0.01181\n",
      "Epoch: 59, train_loss: 0.98062, time: 0.01178\n",
      "Epoch: 60, train_loss: 0.97948, time: 0.01195\n",
      "Epoch: 61, train_loss: 0.97929, time: 0.01224\n",
      "Epoch: 62, train_loss: 0.98172, time: 0.01198\n",
      "Epoch: 63, train_loss: 0.98050, time: 0.01182\n",
      "Epoch: 64, train_loss: 0.97705, time: 0.01230\n",
      "Epoch: 65, train_loss: 0.97976, time: 0.01190\n",
      "Epoch: 66, train_loss: 0.98047, time: 0.01208\n",
      "Epoch: 67, train_loss: 0.97952, time: 0.01238\n",
      "Epoch: 68, train_loss: 0.97980, time: 0.01214\n",
      "Epoch: 69, train_loss: 0.98051, time: 0.01289\n",
      "Epoch: 70, train_loss: 0.98090, time: 0.01212\n",
      "Epoch: 71, train_loss: 0.98048, time: 0.01209\n",
      "Epoch: 72, train_loss: 0.98166, time: 0.01166\n",
      "Epoch: 73, train_loss: 0.97968, time: 0.01231\n",
      "Epoch: 74, train_loss: 0.98034, time: 0.01218\n",
      "Epoch: 75, train_loss: 0.97997, time: 0.01160\n",
      "Epoch: 76, train_loss: 0.98194, time: 0.01193\n",
      "Epoch: 77, train_loss: 0.98076, time: 0.01216\n",
      "Epoch: 78, train_loss: 0.97921, time: 0.01221\n",
      "Epoch: 79, train_loss: 0.97744, time: 0.01222\n",
      "Epoch: 80, train_loss: 0.98255, time: 0.01188\n",
      "Epoch: 81, train_loss: 0.98257, time: 0.01176\n",
      "Epoch: 82, train_loss: 0.97936, time: 0.01199\n",
      "Epoch: 83, train_loss: 0.97916, time: 0.01191\n",
      "Epoch: 84, train_loss: 0.98060, time: 0.01220\n",
      "Epoch: 85, train_loss: 0.98107, time: 0.01211\n",
      "Epoch: 86, train_loss: 0.97943, time: 0.01231\n",
      "Epoch: 87, train_loss: 0.97926, time: 0.01215\n",
      "Epoch: 88, train_loss: 0.97966, time: 0.01221\n",
      "Epoch: 89, train_loss: 0.98164, time: 0.01233\n",
      "Epoch: 90, train_loss: 0.98027, time: 0.01208\n",
      "Epoch: 91, train_loss: 0.98048, time: 0.01216\n",
      "Epoch: 92, train_loss: 0.97940, time: 0.01174\n",
      "Epoch: 93, train_loss: 0.98087, time: 0.01182\n",
      "Epoch: 94, train_loss: 0.98095, time: 0.01243\n",
      "Epoch: 95, train_loss: 0.97810, time: 0.01214\n",
      "Epoch: 96, train_loss: 0.98092, time: 0.01193\n",
      "Epoch: 97, train_loss: 0.98052, time: 0.01209\n",
      "Epoch: 98, train_loss: 0.97777, time: 0.01189\n",
      "Epoch: 99, train_loss: 0.97957, time: 0.01174\n",
      "Epoch: 100, train_loss: 0.97974, time: 0.01194\n",
      "Epoch: 101, train_loss: 0.97875, time: 0.01208\n",
      "Epoch: 102, train_loss: 0.98056, time: 0.01209\n",
      "Epoch: 103, train_loss: 0.97885, time: 0.01278\n",
      "Epoch: 104, train_loss: 0.98177, time: 0.01339\n",
      "Epoch: 105, train_loss: 0.98111, time: 0.01196\n",
      "Epoch: 106, train_loss: 0.97727, time: 0.01184\n",
      "Epoch: 107, train_loss: 0.97933, time: 0.01167\n",
      "Epoch: 108, train_loss: 0.97842, time: 0.01179\n",
      "Epoch: 109, train_loss: 0.98269, time: 0.01235\n",
      "Epoch: 110, train_loss: 0.98020, time: 0.01214\n",
      "Epoch: 111, train_loss: 0.97972, time: 0.01183\n",
      "Epoch: 112, train_loss: 0.97922, time: 0.01184\n",
      "Epoch: 113, train_loss: 0.98059, time: 0.01192\n",
      "Epoch: 114, train_loss: 0.97944, time: 0.01223\n",
      "Epoch: 115, train_loss: 0.97980, time: 0.01181\n",
      "Epoch: 116, train_loss: 0.98052, time: 0.01222\n",
      "Epoch: 117, train_loss: 0.97978, time: 0.01227\n",
      "Epoch: 118, train_loss: 0.98080, time: 0.01204\n",
      "Epoch: 119, train_loss: 0.98107, time: 0.01187\n",
      "Epoch: 120, train_loss: 0.98050, time: 0.01238\n",
      "Epoch: 121, train_loss: 0.97965, time: 0.01182\n",
      "Epoch: 122, train_loss: 0.98114, time: 0.01211\n",
      "Epoch: 123, train_loss: 0.98319, time: 0.01221\n",
      "Epoch: 124, train_loss: 0.98013, time: 0.01222\n",
      "Epoch: 125, train_loss: 0.98295, time: 0.01142\n",
      "Epoch: 126, train_loss: 0.97763, time: 0.01146\n",
      "Epoch: 127, train_loss: 0.97935, time: 0.01147\n",
      "Epoch: 128, train_loss: 0.97962, time: 0.01198\n",
      "Epoch: 129, train_loss: 0.98009, time: 0.01190\n",
      "Epoch: 130, train_loss: 0.97996, time: 0.01180\n",
      "Epoch: 131, train_loss: 0.98193, time: 0.01213\n",
      "Epoch: 132, train_loss: 0.98288, time: 0.01189\n",
      "Epoch: 133, train_loss: 0.98188, time: 0.01196\n",
      "Epoch: 134, train_loss: 0.97957, time: 0.01219\n",
      "Epoch: 135, train_loss: 0.97967, time: 0.01198\n",
      "Epoch: 136, train_loss: 0.98061, time: 0.01173\n",
      "Epoch: 137, train_loss: 0.97980, time: 0.01208\n",
      "Epoch: 138, train_loss: 0.98067, time: 0.01194\n",
      "Epoch: 139, train_loss: 0.97963, time: 0.01188\n",
      "Epoch: 140, train_loss: 0.98076, time: 0.01187\n",
      "Epoch: 141, train_loss: 0.98174, time: 0.01201\n",
      "Epoch: 142, train_loss: 0.98035, time: 0.01188\n",
      "Epoch: 143, train_loss: 0.97964, time: 0.01188\n",
      "Epoch: 144, train_loss: 0.98167, time: 0.01202\n",
      "Epoch: 145, train_loss: 0.97999, time: 0.01190\n",
      "Epoch: 146, train_loss: 0.97891, time: 0.01185\n",
      "Epoch: 147, train_loss: 0.98120, time: 0.01198\n",
      "Epoch: 148, train_loss: 0.97963, time: 0.01180\n",
      "Epoch: 149, train_loss: 0.97987, time: 0.01233\n",
      "Epoch: 150, train_loss: 0.97833, time: 0.01153\n",
      "Epoch: 151, train_loss: 0.98083, time: 0.01194\n",
      "Epoch: 152, train_loss: 0.98021, time: 0.01201\n",
      "Epoch: 153, train_loss: 0.98010, time: 0.01192\n",
      "Epoch: 154, train_loss: 0.98004, time: 0.01209\n",
      "Epoch: 155, train_loss: 0.97994, time: 0.01219\n",
      "Epoch: 156, train_loss: 0.97984, time: 0.01229\n",
      "Epoch: 157, train_loss: 0.97959, time: 0.01207\n",
      "Epoch: 158, train_loss: 0.98076, time: 0.01195\n",
      "Epoch: 159, train_loss: 0.97959, time: 0.01210\n",
      "Epoch: 160, train_loss: 0.98015, time: 0.01213\n",
      "Epoch: 161, train_loss: 0.97756, time: 0.01213\n",
      "Epoch: 162, train_loss: 0.97976, time: 0.01155\n",
      "Epoch: 163, train_loss: 0.97968, time: 0.01199\n",
      "Epoch: 164, train_loss: 0.97809, time: 0.01150\n",
      "Epoch: 165, train_loss: 0.98047, time: 0.01205\n",
      "Epoch: 166, train_loss: 0.98089, time: 0.01182\n",
      "Epoch: 167, train_loss: 0.97853, time: 0.01199\n",
      "Epoch: 168, train_loss: 0.98140, time: 0.01184\n",
      "Epoch: 169, train_loss: 0.98113, time: 0.01201\n",
      "Epoch: 170, train_loss: 0.97912, time: 0.01193\n",
      "Epoch: 171, train_loss: 0.97817, time: 0.01229\n",
      "Epoch: 172, train_loss: 0.98159, time: 0.01189\n",
      "Epoch: 173, train_loss: 0.98114, time: 0.01181\n",
      "Epoch: 174, train_loss: 0.97928, time: 0.01191\n",
      "Epoch: 175, train_loss: 0.97779, time: 0.01189\n",
      "Epoch: 176, train_loss: 0.97966, time: 0.01209\n",
      "Epoch: 177, train_loss: 0.98004, time: 0.01208\n",
      "Epoch: 178, train_loss: 0.97860, time: 0.01209\n",
      "Epoch: 179, train_loss: 0.98124, time: 0.01197\n",
      "Epoch: 180, train_loss: 0.97886, time: 0.01221\n",
      "Epoch: 181, train_loss: 0.98028, time: 0.01154\n",
      "Epoch: 182, train_loss: 0.97889, time: 0.01201\n",
      "Epoch: 183, train_loss: 0.97901, time: 0.01215\n",
      "Epoch: 184, train_loss: 0.97962, time: 0.01221\n",
      "Epoch: 185, train_loss: 0.98068, time: 0.01230\n",
      "Epoch: 186, train_loss: 0.97813, time: 0.01189\n",
      "Epoch: 187, train_loss: 0.98119, time: 0.01182\n",
      "Epoch: 188, train_loss: 0.98111, time: 0.01259\n",
      "Epoch: 189, train_loss: 0.97919, time: 0.01194\n",
      "Epoch: 190, train_loss: 0.98008, time: 0.01191\n",
      "Epoch: 191, train_loss: 0.98066, time: 0.01195\n",
      "Epoch: 192, train_loss: 0.97864, time: 0.01183\n",
      "Epoch: 193, train_loss: 0.98243, time: 0.01190\n",
      "Epoch: 194, train_loss: 0.97788, time: 0.01214\n",
      "Epoch: 195, train_loss: 0.97998, time: 0.01189\n",
      "Epoch: 196, train_loss: 0.98022, time: 0.01195\n",
      "Epoch: 197, train_loss: 0.97886, time: 0.01203\n",
      "Epoch: 198, train_loss: 0.97910, time: 0.01206\n",
      "Epoch: 199, train_loss: 0.98188, time: 0.01221\n",
      "Epoch: 200, train_loss: 0.97901, time: 0.01172\n",
      "pairwise precision 0.14685 recall 0.88191 f1 0.25178\n",
      "average until now [0.4262486707729316, 0.8285867370158355, 0.5629168464339888]\n",
      "43 names 219.8220407962799 avg time 5.112140483634416\n",
      "Loading hua_bai dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 351 nodes, 3113 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97974, time: 0.11563\n",
      "Epoch: 2, train_loss: 0.97615, time: 0.01652\n",
      "Epoch: 3, train_loss: 0.97618, time: 0.01508\n",
      "Epoch: 4, train_loss: 0.97406, time: 0.01491\n",
      "Epoch: 5, train_loss: 0.97216, time: 0.01506\n",
      "Epoch: 6, train_loss: 0.97544, time: 0.01486\n",
      "Epoch: 7, train_loss: 0.97355, time: 0.01495\n",
      "Epoch: 8, train_loss: 0.97504, time: 0.01543\n",
      "Epoch: 9, train_loss: 0.97542, time: 0.01509\n",
      "Epoch: 10, train_loss: 0.97318, time: 0.01508\n",
      "Epoch: 11, train_loss: 0.97354, time: 0.01514\n",
      "Epoch: 12, train_loss: 0.97347, time: 0.01550\n",
      "Epoch: 13, train_loss: 0.97400, time: 0.01491\n",
      "Epoch: 14, train_loss: 0.97496, time: 0.01560\n",
      "Epoch: 15, train_loss: 0.97464, time: 0.01523\n",
      "Epoch: 16, train_loss: 0.97488, time: 0.01463\n",
      "Epoch: 17, train_loss: 0.97398, time: 0.01509\n",
      "Epoch: 18, train_loss: 0.97393, time: 0.01469\n",
      "Epoch: 19, train_loss: 0.97249, time: 0.01429\n",
      "Epoch: 20, train_loss: 0.97306, time: 0.01454\n",
      "Epoch: 21, train_loss: 0.97328, time: 0.01495\n",
      "Epoch: 22, train_loss: 0.97386, time: 0.01470\n",
      "Epoch: 23, train_loss: 0.97391, time: 0.01473\n",
      "Epoch: 24, train_loss: 0.97384, time: 0.01501\n",
      "Epoch: 25, train_loss: 0.97499, time: 0.01384\n",
      "Epoch: 26, train_loss: 0.97205, time: 0.01447\n",
      "Epoch: 27, train_loss: 0.97316, time: 0.01485\n",
      "Epoch: 28, train_loss: 0.97247, time: 0.01457\n",
      "Epoch: 29, train_loss: 0.97525, time: 0.01479\n",
      "Epoch: 30, train_loss: 0.97338, time: 0.01501\n",
      "Epoch: 31, train_loss: 0.97246, time: 0.01500\n",
      "Epoch: 32, train_loss: 0.97236, time: 0.01479\n",
      "Epoch: 33, train_loss: 0.97324, time: 0.01527\n",
      "Epoch: 34, train_loss: 0.97453, time: 0.01488\n",
      "Epoch: 35, train_loss: 0.97141, time: 0.01559\n",
      "Epoch: 36, train_loss: 0.97172, time: 0.01510\n",
      "Epoch: 37, train_loss: 0.97434, time: 0.01527\n",
      "Epoch: 38, train_loss: 0.97229, time: 0.01526\n",
      "Epoch: 39, train_loss: 0.97102, time: 0.01458\n",
      "Epoch: 40, train_loss: 0.97394, time: 0.01550\n",
      "Epoch: 41, train_loss: 0.97263, time: 0.01501\n",
      "Epoch: 42, train_loss: 0.97307, time: 0.01558\n",
      "Epoch: 43, train_loss: 0.97353, time: 0.01545\n",
      "Epoch: 44, train_loss: 0.97306, time: 0.01461\n",
      "Epoch: 45, train_loss: 0.97369, time: 0.01475\n",
      "Epoch: 46, train_loss: 0.97322, time: 0.01426\n",
      "Epoch: 47, train_loss: 0.97322, time: 0.01373\n",
      "Epoch: 48, train_loss: 0.97418, time: 0.01508\n",
      "Epoch: 49, train_loss: 0.97371, time: 0.01541\n",
      "Epoch: 50, train_loss: 0.97465, time: 0.01469\n",
      "Epoch: 51, train_loss: 0.97310, time: 0.01490\n",
      "Epoch: 52, train_loss: 0.97333, time: 0.01455\n",
      "Epoch: 53, train_loss: 0.97343, time: 0.01488\n",
      "Epoch: 54, train_loss: 0.97336, time: 0.01514\n",
      "Epoch: 55, train_loss: 0.97292, time: 0.01465\n",
      "Epoch: 56, train_loss: 0.97119, time: 0.01469\n",
      "Epoch: 57, train_loss: 0.97274, time: 0.01494\n",
      "Epoch: 58, train_loss: 0.97344, time: 0.01511\n",
      "Epoch: 59, train_loss: 0.97247, time: 0.01461\n",
      "Epoch: 60, train_loss: 0.97291, time: 0.01522\n",
      "Epoch: 61, train_loss: 0.97222, time: 0.01496\n",
      "Epoch: 62, train_loss: 0.97297, time: 0.01519\n",
      "Epoch: 63, train_loss: 0.97165, time: 0.01533\n",
      "Epoch: 64, train_loss: 0.97286, time: 0.01516\n",
      "Epoch: 65, train_loss: 0.97334, time: 0.01510\n",
      "Epoch: 66, train_loss: 0.97478, time: 0.01488\n",
      "Epoch: 67, train_loss: 0.97342, time: 0.01554\n",
      "Epoch: 68, train_loss: 0.97219, time: 0.01499\n",
      "Epoch: 69, train_loss: 0.97484, time: 0.01486\n",
      "Epoch: 70, train_loss: 0.97585, time: 0.01471\n",
      "Epoch: 71, train_loss: 0.97306, time: 0.01523\n",
      "Epoch: 72, train_loss: 0.97251, time: 0.01497\n",
      "Epoch: 73, train_loss: 0.97213, time: 0.01489\n",
      "Epoch: 74, train_loss: 0.97258, time: 0.01410\n",
      "Epoch: 75, train_loss: 0.97323, time: 0.01482\n",
      "Epoch: 76, train_loss: 0.97242, time: 0.01404\n",
      "Epoch: 77, train_loss: 0.97389, time: 0.01444\n",
      "Epoch: 78, train_loss: 0.97443, time: 0.01503\n",
      "Epoch: 79, train_loss: 0.97331, time: 0.01518\n",
      "Epoch: 80, train_loss: 0.97221, time: 0.01428\n",
      "Epoch: 81, train_loss: 0.97278, time: 0.01529\n",
      "Epoch: 82, train_loss: 0.97389, time: 0.01443\n",
      "Epoch: 83, train_loss: 0.97446, time: 0.01505\n",
      "Epoch: 84, train_loss: 0.97112, time: 0.01455\n",
      "Epoch: 85, train_loss: 0.97041, time: 0.01493\n",
      "Epoch: 86, train_loss: 0.97355, time: 0.01517\n",
      "Epoch: 87, train_loss: 0.97316, time: 0.01490\n",
      "Epoch: 88, train_loss: 0.97392, time: 0.01477\n",
      "Epoch: 89, train_loss: 0.97323, time: 0.01519\n",
      "Epoch: 90, train_loss: 0.97366, time: 0.01507\n",
      "Epoch: 91, train_loss: 0.97268, time: 0.01548\n",
      "Epoch: 92, train_loss: 0.97260, time: 0.01518\n",
      "Epoch: 93, train_loss: 0.97328, time: 0.01488\n",
      "Epoch: 94, train_loss: 0.97412, time: 0.01512\n",
      "Epoch: 95, train_loss: 0.97415, time: 0.01530\n",
      "Epoch: 96, train_loss: 0.97489, time: 0.01529\n",
      "Epoch: 97, train_loss: 0.97654, time: 0.01515\n",
      "Epoch: 98, train_loss: 0.97446, time: 0.01489\n",
      "Epoch: 99, train_loss: 0.97244, time: 0.01528\n",
      "Epoch: 100, train_loss: 0.97345, time: 0.01470\n",
      "Epoch: 101, train_loss: 0.97387, time: 0.01527\n",
      "Epoch: 102, train_loss: 0.97357, time: 0.01522\n",
      "Epoch: 103, train_loss: 0.97404, time: 0.01489\n",
      "Epoch: 104, train_loss: 0.97209, time: 0.01521\n",
      "Epoch: 105, train_loss: 0.97231, time: 0.01520\n",
      "Epoch: 106, train_loss: 0.97583, time: 0.01511\n",
      "Epoch: 107, train_loss: 0.97218, time: 0.01501\n",
      "Epoch: 108, train_loss: 0.97251, time: 0.01500\n",
      "Epoch: 109, train_loss: 0.97230, time: 0.01480\n",
      "Epoch: 110, train_loss: 0.97440, time: 0.01513\n",
      "Epoch: 111, train_loss: 0.97238, time: 0.01512\n",
      "Epoch: 112, train_loss: 0.97306, time: 0.01507\n",
      "Epoch: 113, train_loss: 0.97634, time: 0.01505\n",
      "Epoch: 114, train_loss: 0.97316, time: 0.01495\n",
      "Epoch: 115, train_loss: 0.97348, time: 0.01525\n",
      "Epoch: 116, train_loss: 0.97447, time: 0.01569\n",
      "Epoch: 117, train_loss: 0.97292, time: 0.01494\n",
      "Epoch: 118, train_loss: 0.97396, time: 0.01489\n",
      "Epoch: 119, train_loss: 0.97186, time: 0.01574\n",
      "Epoch: 120, train_loss: 0.97252, time: 0.01502\n",
      "Epoch: 121, train_loss: 0.97352, time: 0.01509\n",
      "Epoch: 122, train_loss: 0.97227, time: 0.01531\n",
      "Epoch: 123, train_loss: 0.97078, time: 0.01489\n",
      "Epoch: 124, train_loss: 0.97378, time: 0.01475\n",
      "Epoch: 125, train_loss: 0.97237, time: 0.01395\n",
      "Epoch: 126, train_loss: 0.97220, time: 0.01490\n",
      "Epoch: 127, train_loss: 0.97450, time: 0.01397\n",
      "Epoch: 128, train_loss: 0.97309, time: 0.01513\n",
      "Epoch: 129, train_loss: 0.97229, time: 0.01474\n",
      "Epoch: 130, train_loss: 0.97324, time: 0.01455\n",
      "Epoch: 131, train_loss: 0.97211, time: 0.01501\n",
      "Epoch: 132, train_loss: 0.97347, time: 0.01529\n",
      "Epoch: 133, train_loss: 0.97433, time: 0.01514\n",
      "Epoch: 134, train_loss: 0.97299, time: 0.01480\n",
      "Epoch: 135, train_loss: 0.97466, time: 0.01498\n",
      "Epoch: 136, train_loss: 0.97320, time: 0.01453\n",
      "Epoch: 137, train_loss: 0.97320, time: 0.01486\n",
      "Epoch: 138, train_loss: 0.97405, time: 0.01504\n",
      "Epoch: 139, train_loss: 0.97282, time: 0.01492\n",
      "Epoch: 140, train_loss: 0.97281, time: 0.01487\n",
      "Epoch: 141, train_loss: 0.97417, time: 0.01440\n",
      "Epoch: 142, train_loss: 0.97428, time: 0.01504\n",
      "Epoch: 143, train_loss: 0.97426, time: 0.01449\n",
      "Epoch: 144, train_loss: 0.97201, time: 0.01519\n",
      "Epoch: 145, train_loss: 0.97170, time: 0.01519\n",
      "Epoch: 146, train_loss: 0.97618, time: 0.01533\n",
      "Epoch: 147, train_loss: 0.97253, time: 0.01488\n",
      "Epoch: 148, train_loss: 0.97275, time: 0.01506\n",
      "Epoch: 149, train_loss: 0.97130, time: 0.01525\n",
      "Epoch: 150, train_loss: 0.97143, time: 0.01498\n",
      "Epoch: 151, train_loss: 0.97290, time: 0.01469\n",
      "Epoch: 152, train_loss: 0.97435, time: 0.01543\n",
      "Epoch: 153, train_loss: 0.97366, time: 0.01429\n",
      "Epoch: 154, train_loss: 0.97295, time: 0.01471\n",
      "Epoch: 155, train_loss: 0.97287, time: 0.01492\n",
      "Epoch: 156, train_loss: 0.97461, time: 0.01493\n",
      "Epoch: 157, train_loss: 0.97465, time: 0.01494\n",
      "Epoch: 158, train_loss: 0.97208, time: 0.01532\n",
      "Epoch: 159, train_loss: 0.97443, time: 0.01483\n",
      "Epoch: 160, train_loss: 0.97273, time: 0.01479\n",
      "Epoch: 161, train_loss: 0.97298, time: 0.01543\n",
      "Epoch: 162, train_loss: 0.97285, time: 0.01507\n",
      "Epoch: 163, train_loss: 0.97194, time: 0.01411\n",
      "Epoch: 164, train_loss: 0.97263, time: 0.01521\n",
      "Epoch: 165, train_loss: 0.97270, time: 0.01528\n",
      "Epoch: 166, train_loss: 0.97102, time: 0.01479\n",
      "Epoch: 167, train_loss: 0.97234, time: 0.01466\n",
      "Epoch: 168, train_loss: 0.97471, time: 0.01469\n",
      "Epoch: 169, train_loss: 0.97276, time: 0.01483\n",
      "Epoch: 170, train_loss: 0.97247, time: 0.01512\n",
      "Epoch: 171, train_loss: 0.97060, time: 0.01492\n",
      "Epoch: 172, train_loss: 0.97369, time: 0.01479\n",
      "Epoch: 173, train_loss: 0.97202, time: 0.01496\n",
      "Epoch: 174, train_loss: 0.97102, time: 0.01469\n",
      "Epoch: 175, train_loss: 0.97247, time: 0.01519\n",
      "Epoch: 176, train_loss: 0.97322, time: 0.01438\n",
      "Epoch: 177, train_loss: 0.97405, time: 0.01413\n",
      "Epoch: 178, train_loss: 0.97141, time: 0.01473\n",
      "Epoch: 179, train_loss: 0.97247, time: 0.01491\n",
      "Epoch: 180, train_loss: 0.97579, time: 0.01516\n",
      "Epoch: 181, train_loss: 0.97404, time: 0.01443\n",
      "Epoch: 182, train_loss: 0.97198, time: 0.01432\n",
      "Epoch: 183, train_loss: 0.97350, time: 0.01490\n",
      "Epoch: 184, train_loss: 0.97302, time: 0.01495\n",
      "Epoch: 185, train_loss: 0.97447, time: 0.01502\n",
      "Epoch: 186, train_loss: 0.97479, time: 0.01485\n",
      "Epoch: 187, train_loss: 0.97494, time: 0.01506\n",
      "Epoch: 188, train_loss: 0.97526, time: 0.01470\n",
      "Epoch: 189, train_loss: 0.97575, time: 0.01597\n",
      "Epoch: 190, train_loss: 0.97308, time: 0.01450\n",
      "Epoch: 191, train_loss: 0.97274, time: 0.01513\n",
      "Epoch: 192, train_loss: 0.97322, time: 0.01513\n",
      "Epoch: 193, train_loss: 0.97359, time: 0.01463\n",
      "Epoch: 194, train_loss: 0.97090, time: 0.01448\n",
      "Epoch: 195, train_loss: 0.97450, time: 0.01464\n",
      "Epoch: 196, train_loss: 0.97435, time: 0.01490\n",
      "Epoch: 197, train_loss: 0.97418, time: 0.01544\n",
      "Epoch: 198, train_loss: 0.97136, time: 0.01519\n",
      "Epoch: 199, train_loss: 0.97377, time: 0.01475\n",
      "Epoch: 200, train_loss: 0.97481, time: 0.01492\n",
      "pairwise precision 0.21178 recall 0.93001 f1 0.34500\n",
      "average until now [0.4213744272595521, 0.8308917044161253, 0.5591726985294433]\n",
      "44 names 222.99899053573608 avg time 5.068158875812184\n",
      "Loading mei_ling_chen dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 161 nodes, 656 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97955, time: 0.10841\n",
      "Epoch: 2, train_loss: 0.97713, time: 0.01117\n",
      "Epoch: 3, train_loss: 0.97312, time: 0.00980\n",
      "Epoch: 4, train_loss: 0.97279, time: 0.00961\n",
      "Epoch: 5, train_loss: 0.98278, time: 0.00954\n",
      "Epoch: 6, train_loss: 0.98189, time: 0.00967\n",
      "Epoch: 7, train_loss: 0.97864, time: 0.00962\n",
      "Epoch: 8, train_loss: 0.97405, time: 0.00957\n",
      "Epoch: 9, train_loss: 0.97532, time: 0.00943\n",
      "Epoch: 10, train_loss: 0.97208, time: 0.00960\n",
      "Epoch: 11, train_loss: 0.97495, time: 0.00975\n",
      "Epoch: 12, train_loss: 0.97314, time: 0.00938\n",
      "Epoch: 13, train_loss: 0.98324, time: 0.00949\n",
      "Epoch: 14, train_loss: 0.97544, time: 0.00932\n",
      "Epoch: 15, train_loss: 0.97285, time: 0.00951\n",
      "Epoch: 16, train_loss: 0.97408, time: 0.00950\n",
      "Epoch: 17, train_loss: 0.97538, time: 0.00949\n",
      "Epoch: 18, train_loss: 0.97540, time: 0.00951\n",
      "Epoch: 19, train_loss: 0.97450, time: 0.00958\n",
      "Epoch: 20, train_loss: 0.97929, time: 0.00946\n",
      "Epoch: 21, train_loss: 0.97532, time: 0.00934\n",
      "Epoch: 22, train_loss: 0.97746, time: 0.00921\n",
      "Epoch: 23, train_loss: 0.97353, time: 0.00944\n",
      "Epoch: 24, train_loss: 0.97557, time: 0.00921\n",
      "Epoch: 25, train_loss: 0.97289, time: 0.00945\n",
      "Epoch: 26, train_loss: 0.97671, time: 0.00962\n",
      "Epoch: 27, train_loss: 0.97470, time: 0.00945\n",
      "Epoch: 28, train_loss: 0.97764, time: 0.00962\n",
      "Epoch: 29, train_loss: 0.97868, time: 0.00953\n",
      "Epoch: 30, train_loss: 0.97341, time: 0.00952\n",
      "Epoch: 31, train_loss: 0.97542, time: 0.00949\n",
      "Epoch: 32, train_loss: 0.97294, time: 0.00975\n",
      "Epoch: 33, train_loss: 0.97321, time: 0.00966\n",
      "Epoch: 34, train_loss: 0.97508, time: 0.00962\n",
      "Epoch: 35, train_loss: 0.97708, time: 0.00958\n",
      "Epoch: 36, train_loss: 0.97239, time: 0.00970\n",
      "Epoch: 37, train_loss: 0.97641, time: 0.00956\n",
      "Epoch: 38, train_loss: 0.97333, time: 0.00942\n",
      "Epoch: 39, train_loss: 0.97965, time: 0.00959\n",
      "Epoch: 40, train_loss: 0.97470, time: 0.00958\n",
      "Epoch: 41, train_loss: 0.97538, time: 0.00952\n",
      "Epoch: 42, train_loss: 0.97654, time: 0.00950\n",
      "Epoch: 43, train_loss: 0.97287, time: 0.00977\n",
      "Epoch: 44, train_loss: 0.97276, time: 0.00953\n",
      "Epoch: 45, train_loss: 0.97385, time: 0.00950\n",
      "Epoch: 46, train_loss: 0.97937, time: 0.00948\n",
      "Epoch: 47, train_loss: 0.98073, time: 0.00952\n",
      "Epoch: 48, train_loss: 0.97533, time: 0.00962\n",
      "Epoch: 49, train_loss: 0.97988, time: 0.00951\n",
      "Epoch: 50, train_loss: 0.97481, time: 0.00954\n",
      "Epoch: 51, train_loss: 0.97679, time: 0.00938\n",
      "Epoch: 52, train_loss: 0.97292, time: 0.00953\n",
      "Epoch: 53, train_loss: 0.97954, time: 0.00968\n",
      "Epoch: 54, train_loss: 0.97500, time: 0.00954\n",
      "Epoch: 55, train_loss: 0.97321, time: 0.00946\n",
      "Epoch: 56, train_loss: 0.97661, time: 0.00955\n",
      "Epoch: 57, train_loss: 0.97470, time: 0.00947\n",
      "Epoch: 58, train_loss: 0.97641, time: 0.00960\n",
      "Epoch: 59, train_loss: 0.97045, time: 0.00951\n",
      "Epoch: 60, train_loss: 0.97687, time: 0.00959\n",
      "Epoch: 61, train_loss: 0.97858, time: 0.00964\n",
      "Epoch: 62, train_loss: 0.97783, time: 0.00948\n",
      "Epoch: 63, train_loss: 0.97838, time: 0.00945\n",
      "Epoch: 64, train_loss: 0.97500, time: 0.00951\n",
      "Epoch: 65, train_loss: 0.97186, time: 0.00956\n",
      "Epoch: 66, train_loss: 0.97666, time: 0.00956\n",
      "Epoch: 67, train_loss: 0.98194, time: 0.00942\n",
      "Epoch: 68, train_loss: 0.97538, time: 0.00943\n",
      "Epoch: 69, train_loss: 0.97340, time: 0.00962\n",
      "Epoch: 70, train_loss: 0.97437, time: 0.00968\n",
      "Epoch: 71, train_loss: 0.97417, time: 0.00954\n",
      "Epoch: 72, train_loss: 0.97529, time: 0.00938\n",
      "Epoch: 73, train_loss: 0.97463, time: 0.00951\n",
      "Epoch: 74, train_loss: 0.97719, time: 0.00952\n",
      "Epoch: 75, train_loss: 0.97574, time: 0.00958\n",
      "Epoch: 76, train_loss: 0.97142, time: 0.00961\n",
      "Epoch: 77, train_loss: 0.97723, time: 0.00953\n",
      "Epoch: 78, train_loss: 0.97481, time: 0.00957\n",
      "Epoch: 79, train_loss: 0.97585, time: 0.00957\n",
      "Epoch: 80, train_loss: 0.97264, time: 0.00965\n",
      "Epoch: 81, train_loss: 0.97437, time: 0.00963\n",
      "Epoch: 82, train_loss: 0.97542, time: 0.00964\n",
      "Epoch: 83, train_loss: 0.97553, time: 0.00969\n",
      "Epoch: 84, train_loss: 0.97566, time: 0.00963\n",
      "Epoch: 85, train_loss: 0.97389, time: 0.00958\n",
      "Epoch: 86, train_loss: 0.97400, time: 0.00947\n",
      "Epoch: 87, train_loss: 0.97272, time: 0.00951\n",
      "Epoch: 88, train_loss: 0.97651, time: 0.00941\n",
      "Epoch: 89, train_loss: 0.97770, time: 0.00960\n",
      "Epoch: 90, train_loss: 0.97553, time: 0.00953\n",
      "Epoch: 91, train_loss: 0.97771, time: 0.00959\n",
      "Epoch: 92, train_loss: 0.97585, time: 0.00961\n",
      "Epoch: 93, train_loss: 0.97150, time: 0.00940\n",
      "Epoch: 94, train_loss: 0.97400, time: 0.00956\n",
      "Epoch: 95, train_loss: 0.97051, time: 0.00956\n",
      "Epoch: 96, train_loss: 0.97583, time: 0.00968\n",
      "Epoch: 97, train_loss: 0.97477, time: 0.00950\n",
      "Epoch: 98, train_loss: 0.97413, time: 0.00946\n",
      "Epoch: 99, train_loss: 0.97312, time: 0.00959\n",
      "Epoch: 100, train_loss: 0.97309, time: 0.00968\n",
      "Epoch: 101, train_loss: 0.97046, time: 0.00976\n",
      "Epoch: 102, train_loss: 0.97577, time: 0.00955\n",
      "Epoch: 103, train_loss: 0.97579, time: 0.00963\n",
      "Epoch: 104, train_loss: 0.97488, time: 0.00967\n",
      "Epoch: 105, train_loss: 0.97697, time: 0.00943\n",
      "Epoch: 106, train_loss: 0.97472, time: 0.00954\n",
      "Epoch: 107, train_loss: 0.97677, time: 0.00952\n",
      "Epoch: 108, train_loss: 0.97665, time: 0.00949\n",
      "Epoch: 109, train_loss: 0.97351, time: 0.00960\n",
      "Epoch: 110, train_loss: 0.97679, time: 0.00944\n",
      "Epoch: 111, train_loss: 0.97058, time: 0.00951\n",
      "Epoch: 112, train_loss: 0.97353, time: 0.00951\n",
      "Epoch: 113, train_loss: 0.97656, time: 0.00943\n",
      "Epoch: 114, train_loss: 0.97666, time: 0.00947\n",
      "Epoch: 115, train_loss: 0.97538, time: 0.00947\n",
      "Epoch: 116, train_loss: 0.97497, time: 0.00966\n",
      "Epoch: 117, train_loss: 0.97287, time: 0.00971\n",
      "Epoch: 118, train_loss: 0.97500, time: 0.00960\n",
      "Epoch: 119, train_loss: 0.97470, time: 0.00950\n",
      "Epoch: 120, train_loss: 0.97526, time: 0.00959\n",
      "Epoch: 121, train_loss: 0.97730, time: 0.00945\n",
      "Epoch: 122, train_loss: 0.97485, time: 0.00942\n",
      "Epoch: 123, train_loss: 0.97156, time: 0.00945\n",
      "Epoch: 124, train_loss: 0.97332, time: 0.00954\n",
      "Epoch: 125, train_loss: 0.97061, time: 0.00943\n",
      "Epoch: 126, train_loss: 0.97807, time: 0.00953\n",
      "Epoch: 127, train_loss: 0.96988, time: 0.00964\n",
      "Epoch: 128, train_loss: 0.97598, time: 0.00951\n",
      "Epoch: 129, train_loss: 0.97842, time: 0.00960\n",
      "Epoch: 130, train_loss: 0.97325, time: 0.00944\n",
      "Epoch: 131, train_loss: 0.97677, time: 0.00941\n",
      "Epoch: 132, train_loss: 0.97315, time: 0.00956\n",
      "Epoch: 133, train_loss: 0.97668, time: 0.00955\n",
      "Epoch: 134, train_loss: 0.97352, time: 0.00945\n",
      "Epoch: 135, train_loss: 0.97582, time: 0.00953\n",
      "Epoch: 136, train_loss: 0.97901, time: 0.00975\n",
      "Epoch: 137, train_loss: 0.97223, time: 0.00942\n",
      "Epoch: 138, train_loss: 0.96873, time: 0.00941\n",
      "Epoch: 139, train_loss: 0.97131, time: 0.00975\n",
      "Epoch: 140, train_loss: 0.97232, time: 0.00950\n",
      "Epoch: 141, train_loss: 0.97513, time: 0.00947\n",
      "Epoch: 142, train_loss: 0.97232, time: 0.00953\n",
      "Epoch: 143, train_loss: 0.97698, time: 0.00961\n",
      "Epoch: 144, train_loss: 0.97158, time: 0.00956\n",
      "Epoch: 145, train_loss: 0.97882, time: 0.00953\n",
      "Epoch: 146, train_loss: 0.97285, time: 0.00949\n",
      "Epoch: 147, train_loss: 0.97654, time: 0.00958\n",
      "Epoch: 148, train_loss: 0.97426, time: 0.00965\n",
      "Epoch: 149, train_loss: 0.97293, time: 0.00965\n",
      "Epoch: 150, train_loss: 0.97551, time: 0.00956\n",
      "Epoch: 151, train_loss: 0.97613, time: 0.00948\n",
      "Epoch: 152, train_loss: 0.97796, time: 0.00954\n",
      "Epoch: 153, train_loss: 0.97313, time: 0.00952\n",
      "Epoch: 154, train_loss: 0.97369, time: 0.00947\n",
      "Epoch: 155, train_loss: 0.97291, time: 0.00946\n",
      "Epoch: 156, train_loss: 0.97804, time: 0.00953\n",
      "Epoch: 157, train_loss: 0.97745, time: 0.00957\n",
      "Epoch: 158, train_loss: 0.97606, time: 0.00959\n",
      "Epoch: 159, train_loss: 0.97458, time: 0.00942\n",
      "Epoch: 160, train_loss: 0.97353, time: 0.00954\n",
      "Epoch: 161, train_loss: 0.97681, time: 0.00959\n",
      "Epoch: 162, train_loss: 0.97306, time: 0.00949\n",
      "Epoch: 163, train_loss: 0.97489, time: 0.00954\n",
      "Epoch: 164, train_loss: 0.97723, time: 0.00970\n",
      "Epoch: 165, train_loss: 0.97426, time: 0.00966\n",
      "Epoch: 166, train_loss: 0.97704, time: 0.00964\n",
      "Epoch: 167, train_loss: 0.97410, time: 0.00954\n",
      "Epoch: 168, train_loss: 0.97442, time: 0.00943\n",
      "Epoch: 169, train_loss: 0.97532, time: 0.00950\n",
      "Epoch: 170, train_loss: 0.97414, time: 0.00950\n",
      "Epoch: 171, train_loss: 0.97508, time: 0.00962\n",
      "Epoch: 172, train_loss: 0.97337, time: 0.00950\n",
      "Epoch: 173, train_loss: 0.97398, time: 0.00956\n",
      "Epoch: 174, train_loss: 0.97518, time: 0.00933\n",
      "Epoch: 175, train_loss: 0.97345, time: 0.00927\n",
      "Epoch: 176, train_loss: 0.97458, time: 0.00938\n",
      "Epoch: 177, train_loss: 0.97031, time: 0.00968\n",
      "Epoch: 178, train_loss: 0.97711, time: 0.00954\n",
      "Epoch: 179, train_loss: 0.97630, time: 0.00937\n",
      "Epoch: 180, train_loss: 0.97435, time: 0.00961\n",
      "Epoch: 181, train_loss: 0.97640, time: 0.00957\n",
      "Epoch: 182, train_loss: 0.97447, time: 0.00946\n",
      "Epoch: 183, train_loss: 0.97380, time: 0.00966\n",
      "Epoch: 184, train_loss: 0.97564, time: 0.00954\n",
      "Epoch: 185, train_loss: 0.97161, time: 0.00943\n",
      "Epoch: 186, train_loss: 0.96880, time: 0.00960\n",
      "Epoch: 187, train_loss: 0.97388, time: 0.00952\n",
      "Epoch: 188, train_loss: 0.97238, time: 0.00966\n",
      "Epoch: 189, train_loss: 0.97224, time: 0.00964\n",
      "Epoch: 190, train_loss: 0.97441, time: 0.00965\n",
      "Epoch: 191, train_loss: 0.97094, time: 0.00949\n",
      "Epoch: 192, train_loss: 0.97705, time: 0.00958\n",
      "Epoch: 193, train_loss: 0.97318, time: 0.00941\n",
      "Epoch: 194, train_loss: 0.97185, time: 0.00945\n",
      "Epoch: 195, train_loss: 0.97565, time: 0.00956\n",
      "Epoch: 196, train_loss: 0.97876, time: 0.00957\n",
      "Epoch: 197, train_loss: 0.97351, time: 0.00945\n",
      "Epoch: 198, train_loss: 0.97259, time: 0.00945\n",
      "Epoch: 199, train_loss: 0.97522, time: 0.00941\n",
      "Epoch: 200, train_loss: 0.97570, time: 0.00957\n",
      "pairwise precision 0.40921 recall 0.83143 f1 0.54848\n",
      "average until now [0.42110420782874364, 0.8309037444523203, 0.5589374451688508]\n",
      "45 names 225.04647946357727 avg time 5.001032876968384\n",
      "Loading yanqing_wang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 107 nodes, 351 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98809, time: 0.10752\n",
      "Epoch: 2, train_loss: 0.97500, time: 0.00983\n",
      "Epoch: 3, train_loss: 0.98467, time: 0.00874\n",
      "Epoch: 4, train_loss: 0.97176, time: 0.00850\n",
      "Epoch: 5, train_loss: 0.97221, time: 0.00849\n",
      "Epoch: 6, train_loss: 0.97054, time: 0.00844\n",
      "Epoch: 7, train_loss: 0.97113, time: 0.00835\n",
      "Epoch: 8, train_loss: 0.97306, time: 0.00848\n",
      "Epoch: 9, train_loss: 0.97352, time: 0.00844\n",
      "Epoch: 10, train_loss: 0.97775, time: 0.00844\n",
      "Epoch: 11, train_loss: 0.97880, time: 0.00846\n",
      "Epoch: 12, train_loss: 0.97247, time: 0.00838\n",
      "Epoch: 13, train_loss: 0.97078, time: 0.00846\n",
      "Epoch: 14, train_loss: 0.97001, time: 0.00831\n",
      "Epoch: 15, train_loss: 0.96544, time: 0.00842\n",
      "Epoch: 16, train_loss: 0.97517, time: 0.00848\n",
      "Epoch: 17, train_loss: 0.96641, time: 0.00842\n",
      "Epoch: 18, train_loss: 0.97503, time: 0.00854\n",
      "Epoch: 19, train_loss: 0.97085, time: 0.00847\n",
      "Epoch: 20, train_loss: 0.98005, time: 0.00855\n",
      "Epoch: 21, train_loss: 0.96712, time: 0.00832\n",
      "Epoch: 22, train_loss: 0.97803, time: 0.00831\n",
      "Epoch: 23, train_loss: 0.97106, time: 0.00831\n",
      "Epoch: 24, train_loss: 0.96849, time: 0.00849\n",
      "Epoch: 25, train_loss: 0.97180, time: 0.00867\n",
      "Epoch: 26, train_loss: 0.96733, time: 0.00852\n",
      "Epoch: 27, train_loss: 0.98006, time: 0.00841\n",
      "Epoch: 28, train_loss: 0.97100, time: 0.00829\n",
      "Epoch: 29, train_loss: 0.97919, time: 0.00835\n",
      "Epoch: 30, train_loss: 0.97577, time: 0.00854\n",
      "Epoch: 31, train_loss: 0.96718, time: 0.00847\n",
      "Epoch: 32, train_loss: 0.97097, time: 0.00839\n",
      "Epoch: 33, train_loss: 0.96625, time: 0.00839\n",
      "Epoch: 34, train_loss: 0.96725, time: 0.00839\n",
      "Epoch: 35, train_loss: 0.97094, time: 0.00823\n",
      "Epoch: 36, train_loss: 0.96896, time: 0.00833\n",
      "Epoch: 37, train_loss: 0.97637, time: 0.00827\n",
      "Epoch: 38, train_loss: 0.96967, time: 0.00828\n",
      "Epoch: 39, train_loss: 0.97412, time: 0.00845\n",
      "Epoch: 40, train_loss: 0.97380, time: 0.00848\n",
      "Epoch: 41, train_loss: 0.97936, time: 0.00840\n",
      "Epoch: 42, train_loss: 0.97156, time: 0.00852\n",
      "Epoch: 43, train_loss: 0.97063, time: 0.00844\n",
      "Epoch: 44, train_loss: 0.97403, time: 0.00840\n",
      "Epoch: 45, train_loss: 0.97079, time: 0.00842\n",
      "Epoch: 46, train_loss: 0.97344, time: 0.00850\n",
      "Epoch: 47, train_loss: 0.97018, time: 0.00847\n",
      "Epoch: 48, train_loss: 0.97111, time: 0.00845\n",
      "Epoch: 49, train_loss: 0.96479, time: 0.00875\n",
      "Epoch: 50, train_loss: 0.96877, time: 0.00856\n",
      "Epoch: 51, train_loss: 0.97493, time: 0.00840\n",
      "Epoch: 52, train_loss: 0.97472, time: 0.00843\n",
      "Epoch: 53, train_loss: 0.96759, time: 0.00847\n",
      "Epoch: 54, train_loss: 0.97397, time: 0.00836\n",
      "Epoch: 55, train_loss: 0.97625, time: 0.00835\n",
      "Epoch: 56, train_loss: 0.96999, time: 0.00837\n",
      "Epoch: 57, train_loss: 0.97181, time: 0.00834\n",
      "Epoch: 58, train_loss: 0.97562, time: 0.00830\n",
      "Epoch: 59, train_loss: 0.97685, time: 0.00836\n",
      "Epoch: 60, train_loss: 0.97374, time: 0.00833\n",
      "Epoch: 61, train_loss: 0.97973, time: 0.00844\n",
      "Epoch: 62, train_loss: 0.96697, time: 0.00848\n",
      "Epoch: 63, train_loss: 0.96909, time: 0.00841\n",
      "Epoch: 64, train_loss: 0.97029, time: 0.00835\n",
      "Epoch: 65, train_loss: 0.97361, time: 0.00846\n",
      "Epoch: 66, train_loss: 0.97507, time: 0.00845\n",
      "Epoch: 67, train_loss: 0.97172, time: 0.00842\n",
      "Epoch: 68, train_loss: 0.97153, time: 0.00838\n",
      "Epoch: 69, train_loss: 0.97246, time: 0.00839\n",
      "Epoch: 70, train_loss: 0.97161, time: 0.00855\n",
      "Epoch: 71, train_loss: 0.97212, time: 0.00849\n",
      "Epoch: 72, train_loss: 0.97562, time: 0.00840\n",
      "Epoch: 73, train_loss: 0.97116, time: 0.00864\n",
      "Epoch: 74, train_loss: 0.96865, time: 0.00846\n",
      "Epoch: 75, train_loss: 0.96988, time: 0.00849\n",
      "Epoch: 76, train_loss: 0.97165, time: 0.00846\n",
      "Epoch: 77, train_loss: 0.96796, time: 0.00857\n",
      "Epoch: 78, train_loss: 0.97097, time: 0.00849\n",
      "Epoch: 79, train_loss: 0.97437, time: 0.00851\n",
      "Epoch: 80, train_loss: 0.97567, time: 0.00834\n",
      "Epoch: 81, train_loss: 0.97582, time: 0.00840\n",
      "Epoch: 82, train_loss: 0.97554, time: 0.00845\n",
      "Epoch: 83, train_loss: 0.97345, time: 0.00833\n",
      "Epoch: 84, train_loss: 0.96646, time: 0.00837\n",
      "Epoch: 85, train_loss: 0.97451, time: 0.00851\n",
      "Epoch: 86, train_loss: 0.96897, time: 0.00859\n",
      "Epoch: 87, train_loss: 0.96730, time: 0.00853\n",
      "Epoch: 88, train_loss: 0.98239, time: 0.00850\n",
      "Epoch: 89, train_loss: 0.97076, time: 0.00852\n",
      "Epoch: 90, train_loss: 0.97412, time: 0.00850\n",
      "Epoch: 91, train_loss: 0.96888, time: 0.00838\n",
      "Epoch: 92, train_loss: 0.97331, time: 0.00842\n",
      "Epoch: 93, train_loss: 0.97200, time: 0.00840\n",
      "Epoch: 94, train_loss: 0.97613, time: 0.00837\n",
      "Epoch: 95, train_loss: 0.97041, time: 0.00849\n",
      "Epoch: 96, train_loss: 0.97115, time: 0.00856\n",
      "Epoch: 97, train_loss: 0.97476, time: 0.00862\n",
      "Epoch: 98, train_loss: 0.97638, time: 0.00855\n",
      "Epoch: 99, train_loss: 0.97115, time: 0.00848\n",
      "Epoch: 100, train_loss: 0.96804, time: 0.00847\n",
      "Epoch: 101, train_loss: 0.97505, time: 0.00834\n",
      "Epoch: 102, train_loss: 0.97708, time: 0.00835\n",
      "Epoch: 103, train_loss: 0.96826, time: 0.00838\n",
      "Epoch: 104, train_loss: 0.97125, time: 0.00843\n",
      "Epoch: 105, train_loss: 0.97063, time: 0.00852\n",
      "Epoch: 106, train_loss: 0.97121, time: 0.00840\n",
      "Epoch: 107, train_loss: 0.96565, time: 0.00839\n",
      "Epoch: 108, train_loss: 0.96703, time: 0.00835\n",
      "Epoch: 109, train_loss: 0.97448, time: 0.00842\n",
      "Epoch: 110, train_loss: 0.97027, time: 0.00849\n",
      "Epoch: 111, train_loss: 0.96218, time: 0.00856\n",
      "Epoch: 112, train_loss: 0.97079, time: 0.00843\n",
      "Epoch: 113, train_loss: 0.97340, time: 0.00849\n",
      "Epoch: 114, train_loss: 0.97367, time: 0.00858\n",
      "Epoch: 115, train_loss: 0.97034, time: 0.00849\n",
      "Epoch: 116, train_loss: 0.97530, time: 0.00857\n",
      "Epoch: 117, train_loss: 0.97532, time: 0.00843\n",
      "Epoch: 118, train_loss: 0.97114, time: 0.00850\n",
      "Epoch: 119, train_loss: 0.97256, time: 0.00841\n",
      "Epoch: 120, train_loss: 0.97021, time: 0.00839\n",
      "Epoch: 121, train_loss: 0.96818, time: 0.00868\n",
      "Epoch: 122, train_loss: 0.97465, time: 0.00839\n",
      "Epoch: 123, train_loss: 0.97165, time: 0.00842\n",
      "Epoch: 124, train_loss: 0.96956, time: 0.00845\n",
      "Epoch: 125, train_loss: 0.97145, time: 0.00854\n",
      "Epoch: 126, train_loss: 0.96718, time: 0.00848\n",
      "Epoch: 127, train_loss: 0.97683, time: 0.00846\n",
      "Epoch: 128, train_loss: 0.96943, time: 0.00848\n",
      "Epoch: 129, train_loss: 0.97282, time: 0.00845\n",
      "Epoch: 130, train_loss: 0.97128, time: 0.00838\n",
      "Epoch: 131, train_loss: 0.97690, time: 0.00829\n",
      "Epoch: 132, train_loss: 0.97099, time: 0.00846\n",
      "Epoch: 133, train_loss: 0.97003, time: 0.00848\n",
      "Epoch: 134, train_loss: 0.97334, time: 0.00857\n",
      "Epoch: 135, train_loss: 0.97731, time: 0.00853\n",
      "Epoch: 136, train_loss: 0.96922, time: 0.00849\n",
      "Epoch: 137, train_loss: 0.97305, time: 0.00845\n",
      "Epoch: 138, train_loss: 0.97259, time: 0.00838\n",
      "Epoch: 139, train_loss: 0.96857, time: 0.00852\n",
      "Epoch: 140, train_loss: 0.97133, time: 0.00847\n",
      "Epoch: 141, train_loss: 0.96863, time: 0.00844\n",
      "Epoch: 142, train_loss: 0.97243, time: 0.00851\n",
      "Epoch: 143, train_loss: 0.97004, time: 0.00844\n",
      "Epoch: 144, train_loss: 0.97138, time: 0.00846\n",
      "Epoch: 145, train_loss: 0.96535, time: 0.00858\n",
      "Epoch: 146, train_loss: 0.97022, time: 0.00848\n",
      "Epoch: 147, train_loss: 0.96736, time: 0.00855\n",
      "Epoch: 148, train_loss: 0.96778, time: 0.00849\n",
      "Epoch: 149, train_loss: 0.97085, time: 0.00849\n",
      "Epoch: 150, train_loss: 0.97059, time: 0.00831\n",
      "Epoch: 151, train_loss: 0.96968, time: 0.00836\n",
      "Epoch: 152, train_loss: 0.97709, time: 0.00849\n",
      "Epoch: 153, train_loss: 0.97307, time: 0.00844\n",
      "Epoch: 154, train_loss: 0.96789, time: 0.00854\n",
      "Epoch: 155, train_loss: 0.97250, time: 0.00841\n",
      "Epoch: 156, train_loss: 0.97108, time: 0.00851\n",
      "Epoch: 157, train_loss: 0.97345, time: 0.00842\n",
      "Epoch: 158, train_loss: 0.97243, time: 0.00854\n",
      "Epoch: 159, train_loss: 0.97463, time: 0.00866\n",
      "Epoch: 160, train_loss: 0.97725, time: 0.00849\n",
      "Epoch: 161, train_loss: 0.97804, time: 0.00841\n",
      "Epoch: 162, train_loss: 0.97128, time: 0.00851\n",
      "Epoch: 163, train_loss: 0.97468, time: 0.00856\n",
      "Epoch: 164, train_loss: 0.97056, time: 0.00854\n",
      "Epoch: 165, train_loss: 0.97723, time: 0.00839\n",
      "Epoch: 166, train_loss: 0.97292, time: 0.00852\n",
      "Epoch: 167, train_loss: 0.96987, time: 0.00852\n",
      "Epoch: 168, train_loss: 0.98130, time: 0.00850\n",
      "Epoch: 169, train_loss: 0.97492, time: 0.00866\n",
      "Epoch: 170, train_loss: 0.96793, time: 0.00851\n",
      "Epoch: 171, train_loss: 0.96834, time: 0.00854\n",
      "Epoch: 172, train_loss: 0.97068, time: 0.00849\n",
      "Epoch: 173, train_loss: 0.96982, time: 0.00849\n",
      "Epoch: 174, train_loss: 0.97152, time: 0.00849\n",
      "Epoch: 175, train_loss: 0.97002, time: 0.00850\n",
      "Epoch: 176, train_loss: 0.97580, time: 0.00847\n",
      "Epoch: 177, train_loss: 0.97264, time: 0.00835\n",
      "Epoch: 178, train_loss: 0.96832, time: 0.00837\n",
      "Epoch: 179, train_loss: 0.96906, time: 0.00844\n",
      "Epoch: 180, train_loss: 0.97003, time: 0.00849\n",
      "Epoch: 181, train_loss: 0.96906, time: 0.00851\n",
      "Epoch: 182, train_loss: 0.97554, time: 0.00837\n",
      "Epoch: 183, train_loss: 0.97253, time: 0.00833\n",
      "Epoch: 184, train_loss: 0.96782, time: 0.00846\n",
      "Epoch: 185, train_loss: 0.97763, time: 0.00855\n",
      "Epoch: 186, train_loss: 0.97671, time: 0.00858\n",
      "Epoch: 187, train_loss: 0.97250, time: 0.00851\n",
      "Epoch: 188, train_loss: 0.97191, time: 0.00841\n",
      "Epoch: 189, train_loss: 0.97153, time: 0.00846\n",
      "Epoch: 190, train_loss: 0.96657, time: 0.00856\n",
      "Epoch: 191, train_loss: 0.97536, time: 0.00843\n",
      "Epoch: 192, train_loss: 0.96896, time: 0.00849\n",
      "Epoch: 193, train_loss: 0.96535, time: 0.00867\n",
      "Epoch: 194, train_loss: 0.97106, time: 0.00845\n",
      "Epoch: 195, train_loss: 0.96788, time: 0.00854\n",
      "Epoch: 196, train_loss: 0.97201, time: 0.00851\n",
      "Epoch: 197, train_loss: 0.97304, time: 0.00842\n",
      "Epoch: 198, train_loss: 0.96687, time: 0.00847\n",
      "Epoch: 199, train_loss: 0.97198, time: 0.00849\n",
      "Epoch: 200, train_loss: 0.97465, time: 0.00848\n",
      "pairwise precision 0.20666 recall 0.82208 f1 0.33029\n",
      "average until now [0.41644238020465074, 0.8307118787485291, 0.554772803070389]\n",
      "46 names 226.87664151191711 avg time 4.932100902432981\n",
      "Loading xu_dong_zhang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 224 nodes, 1005 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99329, time: 0.11078\n",
      "Epoch: 2, train_loss: 0.98445, time: 0.01223\n",
      "Epoch: 3, train_loss: 0.98598, time: 0.01093\n",
      "Epoch: 4, train_loss: 0.98253, time: 0.01056\n",
      "Epoch: 5, train_loss: 0.97776, time: 0.01037\n",
      "Epoch: 6, train_loss: 0.98046, time: 0.01046\n",
      "Epoch: 7, train_loss: 0.97896, time: 0.01065\n",
      "Epoch: 8, train_loss: 0.98275, time: 0.01059\n",
      "Epoch: 9, train_loss: 0.97899, time: 0.01048\n",
      "Epoch: 10, train_loss: 0.98169, time: 0.01059\n",
      "Epoch: 11, train_loss: 0.97906, time: 0.01036\n",
      "Epoch: 12, train_loss: 0.97803, time: 0.01044\n",
      "Epoch: 13, train_loss: 0.98413, time: 0.01034\n",
      "Epoch: 14, train_loss: 0.98068, time: 0.01073\n",
      "Epoch: 15, train_loss: 0.98043, time: 0.01076\n",
      "Epoch: 16, train_loss: 0.98118, time: 0.01072\n",
      "Epoch: 17, train_loss: 0.98126, time: 0.01073\n",
      "Epoch: 18, train_loss: 0.97836, time: 0.01093\n",
      "Epoch: 19, train_loss: 0.97739, time: 0.01075\n",
      "Epoch: 20, train_loss: 0.98062, time: 0.01099\n",
      "Epoch: 21, train_loss: 0.97940, time: 0.01070\n",
      "Epoch: 22, train_loss: 0.98252, time: 0.01034\n",
      "Epoch: 23, train_loss: 0.97869, time: 0.01023\n",
      "Epoch: 24, train_loss: 0.97856, time: 0.01035\n",
      "Epoch: 25, train_loss: 0.98062, time: 0.01062\n",
      "Epoch: 26, train_loss: 0.97871, time: 0.01085\n",
      "Epoch: 27, train_loss: 0.98184, time: 0.01082\n",
      "Epoch: 28, train_loss: 0.97866, time: 0.01079\n",
      "Epoch: 29, train_loss: 0.97914, time: 0.01081\n",
      "Epoch: 30, train_loss: 0.98048, time: 0.01084\n",
      "Epoch: 31, train_loss: 0.98100, time: 0.01072\n",
      "Epoch: 32, train_loss: 0.97968, time: 0.01070\n",
      "Epoch: 33, train_loss: 0.97927, time: 0.01065\n",
      "Epoch: 34, train_loss: 0.98120, time: 0.01082\n",
      "Epoch: 35, train_loss: 0.98087, time: 0.01068\n",
      "Epoch: 36, train_loss: 0.97964, time: 0.01058\n",
      "Epoch: 37, train_loss: 0.97954, time: 0.01052\n",
      "Epoch: 38, train_loss: 0.98106, time: 0.01060\n",
      "Epoch: 39, train_loss: 0.97762, time: 0.01100\n",
      "Epoch: 40, train_loss: 0.98099, time: 0.01064\n",
      "Epoch: 41, train_loss: 0.97869, time: 0.01075\n",
      "Epoch: 42, train_loss: 0.98171, time: 0.01077\n",
      "Epoch: 43, train_loss: 0.97711, time: 0.01061\n",
      "Epoch: 44, train_loss: 0.98040, time: 0.01051\n",
      "Epoch: 45, train_loss: 0.97945, time: 0.01080\n",
      "Epoch: 46, train_loss: 0.97779, time: 0.01099\n",
      "Epoch: 47, train_loss: 0.98155, time: 0.01072\n",
      "Epoch: 48, train_loss: 0.97843, time: 0.01084\n",
      "Epoch: 49, train_loss: 0.97539, time: 0.01079\n",
      "Epoch: 50, train_loss: 0.98113, time: 0.01059\n",
      "Epoch: 51, train_loss: 0.97666, time: 0.01089\n",
      "Epoch: 52, train_loss: 0.98183, time: 0.01092\n",
      "Epoch: 53, train_loss: 0.97896, time: 0.01077\n",
      "Epoch: 54, train_loss: 0.98045, time: 0.01067\n",
      "Epoch: 55, train_loss: 0.98003, time: 0.01085\n",
      "Epoch: 56, train_loss: 0.97917, time: 0.01070\n",
      "Epoch: 57, train_loss: 0.97878, time: 0.01058\n",
      "Epoch: 58, train_loss: 0.98063, time: 0.01080\n",
      "Epoch: 59, train_loss: 0.97720, time: 0.01105\n",
      "Epoch: 60, train_loss: 0.97868, time: 0.01101\n",
      "Epoch: 61, train_loss: 0.97887, time: 0.01070\n",
      "Epoch: 62, train_loss: 0.97907, time: 0.01076\n",
      "Epoch: 63, train_loss: 0.97713, time: 0.01067\n",
      "Epoch: 64, train_loss: 0.97907, time: 0.01074\n",
      "Epoch: 65, train_loss: 0.97867, time: 0.01050\n",
      "Epoch: 66, train_loss: 0.97848, time: 0.01089\n",
      "Epoch: 67, train_loss: 0.98054, time: 0.01065\n",
      "Epoch: 68, train_loss: 0.98116, time: 0.01069\n",
      "Epoch: 69, train_loss: 0.97603, time: 0.01071\n",
      "Epoch: 70, train_loss: 0.97986, time: 0.01073\n",
      "Epoch: 71, train_loss: 0.97617, time: 0.01076\n",
      "Epoch: 72, train_loss: 0.98024, time: 0.01054\n",
      "Epoch: 73, train_loss: 0.97941, time: 0.01072\n",
      "Epoch: 74, train_loss: 0.98014, time: 0.01082\n",
      "Epoch: 75, train_loss: 0.97973, time: 0.01076\n",
      "Epoch: 76, train_loss: 0.98033, time: 0.01051\n",
      "Epoch: 77, train_loss: 0.97698, time: 0.01099\n",
      "Epoch: 78, train_loss: 0.97819, time: 0.01065\n",
      "Epoch: 79, train_loss: 0.97927, time: 0.01086\n",
      "Epoch: 80, train_loss: 0.97859, time: 0.01073\n",
      "Epoch: 81, train_loss: 0.97795, time: 0.01056\n",
      "Epoch: 82, train_loss: 0.98088, time: 0.01081\n",
      "Epoch: 83, train_loss: 0.97943, time: 0.01075\n",
      "Epoch: 84, train_loss: 0.97892, time: 0.01090\n",
      "Epoch: 85, train_loss: 0.97936, time: 0.01084\n",
      "Epoch: 86, train_loss: 0.97805, time: 0.01069\n",
      "Epoch: 87, train_loss: 0.97946, time: 0.01055\n",
      "Epoch: 88, train_loss: 0.97704, time: 0.01060\n",
      "Epoch: 89, train_loss: 0.97811, time: 0.01070\n",
      "Epoch: 90, train_loss: 0.97968, time: 0.01088\n",
      "Epoch: 91, train_loss: 0.97831, time: 0.01090\n",
      "Epoch: 92, train_loss: 0.97866, time: 0.01065\n",
      "Epoch: 93, train_loss: 0.98113, time: 0.01073\n",
      "Epoch: 94, train_loss: 0.98153, time: 0.01074\n",
      "Epoch: 95, train_loss: 0.97819, time: 0.01050\n",
      "Epoch: 96, train_loss: 0.97710, time: 0.01092\n",
      "Epoch: 97, train_loss: 0.97669, time: 0.01071\n",
      "Epoch: 98, train_loss: 0.97899, time: 0.01090\n",
      "Epoch: 99, train_loss: 0.97799, time: 0.01076\n",
      "Epoch: 100, train_loss: 0.97687, time: 0.01043\n",
      "Epoch: 101, train_loss: 0.97784, time: 0.01077\n",
      "Epoch: 102, train_loss: 0.98050, time: 0.01086\n",
      "Epoch: 103, train_loss: 0.97889, time: 0.01074\n",
      "Epoch: 104, train_loss: 0.97749, time: 0.01060\n",
      "Epoch: 105, train_loss: 0.98153, time: 0.01060\n",
      "Epoch: 106, train_loss: 0.97726, time: 0.01066\n",
      "Epoch: 107, train_loss: 0.97906, time: 0.01061\n",
      "Epoch: 108, train_loss: 0.97987, time: 0.01085\n",
      "Epoch: 109, train_loss: 0.97900, time: 0.01074\n",
      "Epoch: 110, train_loss: 0.97843, time: 0.01092\n",
      "Epoch: 111, train_loss: 0.97953, time: 0.01061\n",
      "Epoch: 112, train_loss: 0.97612, time: 0.01067\n",
      "Epoch: 113, train_loss: 0.98021, time: 0.01056\n",
      "Epoch: 114, train_loss: 0.97712, time: 0.01052\n",
      "Epoch: 115, train_loss: 0.97703, time: 0.01081\n",
      "Epoch: 116, train_loss: 0.98183, time: 0.01046\n",
      "Epoch: 117, train_loss: 0.97761, time: 0.01062\n",
      "Epoch: 118, train_loss: 0.98148, time: 0.01047\n",
      "Epoch: 119, train_loss: 0.98195, time: 0.01080\n",
      "Epoch: 120, train_loss: 0.98114, time: 0.01081\n",
      "Epoch: 121, train_loss: 0.97987, time: 0.01074\n",
      "Epoch: 122, train_loss: 0.97909, time: 0.01088\n",
      "Epoch: 123, train_loss: 0.98030, time: 0.01077\n",
      "Epoch: 124, train_loss: 0.97932, time: 0.01065\n",
      "Epoch: 125, train_loss: 0.97535, time: 0.01072\n",
      "Epoch: 126, train_loss: 0.97642, time: 0.01066\n",
      "Epoch: 127, train_loss: 0.97743, time: 0.01087\n",
      "Epoch: 128, train_loss: 0.97740, time: 0.01060\n",
      "Epoch: 129, train_loss: 0.97840, time: 0.01081\n",
      "Epoch: 130, train_loss: 0.97900, time: 0.01082\n",
      "Epoch: 131, train_loss: 0.97953, time: 0.01084\n",
      "Epoch: 132, train_loss: 0.98103, time: 0.01072\n",
      "Epoch: 133, train_loss: 0.98023, time: 0.01063\n",
      "Epoch: 134, train_loss: 0.97814, time: 0.01057\n",
      "Epoch: 135, train_loss: 0.97745, time: 0.01110\n",
      "Epoch: 136, train_loss: 0.98105, time: 0.01076\n",
      "Epoch: 137, train_loss: 0.97939, time: 0.01092\n",
      "Epoch: 138, train_loss: 0.98094, time: 0.01070\n",
      "Epoch: 139, train_loss: 0.97678, time: 0.01073\n",
      "Epoch: 140, train_loss: 0.97795, time: 0.01080\n",
      "Epoch: 141, train_loss: 0.98079, time: 0.01061\n",
      "Epoch: 142, train_loss: 0.97966, time: 0.01062\n",
      "Epoch: 143, train_loss: 0.97637, time: 0.01065\n",
      "Epoch: 144, train_loss: 0.97770, time: 0.01053\n",
      "Epoch: 145, train_loss: 0.97721, time: 0.01094\n",
      "Epoch: 146, train_loss: 0.97935, time: 0.01066\n",
      "Epoch: 147, train_loss: 0.98087, time: 0.01048\n",
      "Epoch: 148, train_loss: 0.97848, time: 0.01069\n",
      "Epoch: 149, train_loss: 0.97757, time: 0.01053\n",
      "Epoch: 150, train_loss: 0.97784, time: 0.01085\n",
      "Epoch: 151, train_loss: 0.97648, time: 0.01086\n",
      "Epoch: 152, train_loss: 0.97632, time: 0.01068\n",
      "Epoch: 153, train_loss: 0.98210, time: 0.01116\n",
      "Epoch: 154, train_loss: 0.97868, time: 0.01094\n",
      "Epoch: 155, train_loss: 0.97828, time: 0.01083\n",
      "Epoch: 156, train_loss: 0.97776, time: 0.01088\n",
      "Epoch: 157, train_loss: 0.98001, time: 0.01070\n",
      "Epoch: 158, train_loss: 0.97910, time: 0.01087\n",
      "Epoch: 159, train_loss: 0.97768, time: 0.01090\n",
      "Epoch: 160, train_loss: 0.97845, time: 0.01065\n",
      "Epoch: 161, train_loss: 0.97884, time: 0.01077\n",
      "Epoch: 162, train_loss: 0.97899, time: 0.01070\n",
      "Epoch: 163, train_loss: 0.97792, time: 0.01077\n",
      "Epoch: 164, train_loss: 0.97777, time: 0.01093\n",
      "Epoch: 165, train_loss: 0.97573, time: 0.01084\n",
      "Epoch: 166, train_loss: 0.97786, time: 0.01066\n",
      "Epoch: 167, train_loss: 0.98066, time: 0.01097\n",
      "Epoch: 168, train_loss: 0.97917, time: 0.01052\n",
      "Epoch: 169, train_loss: 0.97979, time: 0.01055\n",
      "Epoch: 170, train_loss: 0.97794, time: 0.01056\n",
      "Epoch: 171, train_loss: 0.97798, time: 0.01035\n",
      "Epoch: 172, train_loss: 0.97787, time: 0.01086\n",
      "Epoch: 173, train_loss: 0.97712, time: 0.01076\n",
      "Epoch: 174, train_loss: 0.97876, time: 0.01140\n",
      "Epoch: 175, train_loss: 0.97971, time: 0.01095\n",
      "Epoch: 176, train_loss: 0.98052, time: 0.01041\n",
      "Epoch: 177, train_loss: 0.97654, time: 0.01054\n",
      "Epoch: 178, train_loss: 0.98111, time: 0.01056\n",
      "Epoch: 179, train_loss: 0.97740, time: 0.01049\n",
      "Epoch: 180, train_loss: 0.97792, time: 0.01043\n",
      "Epoch: 181, train_loss: 0.97574, time: 0.01045\n",
      "Epoch: 182, train_loss: 0.98087, time: 0.01046\n",
      "Epoch: 183, train_loss: 0.97648, time: 0.01035\n",
      "Epoch: 184, train_loss: 0.97837, time: 0.01046\n",
      "Epoch: 185, train_loss: 0.97960, time: 0.01037\n",
      "Epoch: 186, train_loss: 0.97923, time: 0.01027\n",
      "Epoch: 187, train_loss: 0.97673, time: 0.01038\n",
      "Epoch: 188, train_loss: 0.97673, time: 0.01034\n",
      "Epoch: 189, train_loss: 0.97946, time: 0.01040\n",
      "Epoch: 190, train_loss: 0.97870, time: 0.01062\n",
      "Epoch: 191, train_loss: 0.97747, time: 0.01081\n",
      "Epoch: 192, train_loss: 0.97962, time: 0.01112\n",
      "Epoch: 193, train_loss: 0.98006, time: 0.01065\n",
      "Epoch: 194, train_loss: 0.97958, time: 0.01067\n",
      "Epoch: 195, train_loss: 0.97760, time: 0.01093\n",
      "Epoch: 196, train_loss: 0.97894, time: 0.01059\n",
      "Epoch: 197, train_loss: 0.97996, time: 0.01084\n",
      "Epoch: 198, train_loss: 0.97950, time: 0.01064\n",
      "Epoch: 199, train_loss: 0.97730, time: 0.01085\n",
      "Epoch: 200, train_loss: 0.98074, time: 0.01075\n",
      "pairwise precision 0.61954 recall 0.95499 f1 0.75153\n",
      "average until now [0.42076357109338586, 0.8333561282500697, 0.5591904834899311]\n",
      "47 names 229.1698682308197 avg time 4.87595464320893\n",
      "Loading qiang_shi dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 241 nodes, 1893 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97054, time: 0.11278\n",
      "Epoch: 2, train_loss: 0.97190, time: 0.01350\n",
      "Epoch: 3, train_loss: 0.97075, time: 0.01164\n",
      "Epoch: 4, train_loss: 0.96759, time: 0.01146\n",
      "Epoch: 5, train_loss: 0.96767, time: 0.01128\n",
      "Epoch: 6, train_loss: 0.96672, time: 0.01121\n",
      "Epoch: 7, train_loss: 0.97131, time: 0.01113\n",
      "Epoch: 8, train_loss: 0.96685, time: 0.01151\n",
      "Epoch: 9, train_loss: 0.96716, time: 0.01142\n",
      "Epoch: 10, train_loss: 0.96436, time: 0.01134\n",
      "Epoch: 11, train_loss: 0.96602, time: 0.01139\n",
      "Epoch: 12, train_loss: 0.96552, time: 0.01142\n",
      "Epoch: 13, train_loss: 0.96537, time: 0.01108\n",
      "Epoch: 14, train_loss: 0.96616, time: 0.01113\n",
      "Epoch: 15, train_loss: 0.96728, time: 0.01125\n",
      "Epoch: 16, train_loss: 0.96801, time: 0.01110\n",
      "Epoch: 17, train_loss: 0.96851, time: 0.01150\n",
      "Epoch: 18, train_loss: 0.97038, time: 0.01149\n",
      "Epoch: 19, train_loss: 0.96736, time: 0.01124\n",
      "Epoch: 20, train_loss: 0.96751, time: 0.01171\n",
      "Epoch: 21, train_loss: 0.96738, time: 0.01153\n",
      "Epoch: 22, train_loss: 0.96676, time: 0.01111\n",
      "Epoch: 23, train_loss: 0.96502, time: 0.01148\n",
      "Epoch: 24, train_loss: 0.96774, time: 0.01133\n",
      "Epoch: 25, train_loss: 0.97135, time: 0.01152\n",
      "Epoch: 26, train_loss: 0.96557, time: 0.01117\n",
      "Epoch: 27, train_loss: 0.96581, time: 0.01137\n",
      "Epoch: 28, train_loss: 0.96617, time: 0.01143\n",
      "Epoch: 29, train_loss: 0.96537, time: 0.01159\n",
      "Epoch: 30, train_loss: 0.96652, time: 0.01139\n",
      "Epoch: 31, train_loss: 0.96846, time: 0.01121\n",
      "Epoch: 32, train_loss: 0.96670, time: 0.01144\n",
      "Epoch: 33, train_loss: 0.96680, time: 0.01139\n",
      "Epoch: 34, train_loss: 0.96467, time: 0.01138\n",
      "Epoch: 35, train_loss: 0.96710, time: 0.01135\n",
      "Epoch: 36, train_loss: 0.96631, time: 0.01131\n",
      "Epoch: 37, train_loss: 0.96766, time: 0.01146\n",
      "Epoch: 38, train_loss: 0.96482, time: 0.01146\n",
      "Epoch: 39, train_loss: 0.96524, time: 0.01143\n",
      "Epoch: 40, train_loss: 0.96541, time: 0.01137\n",
      "Epoch: 41, train_loss: 0.96717, time: 0.01123\n",
      "Epoch: 42, train_loss: 0.96414, time: 0.01125\n",
      "Epoch: 43, train_loss: 0.96619, time: 0.01134\n",
      "Epoch: 44, train_loss: 0.96726, time: 0.01151\n",
      "Epoch: 45, train_loss: 0.96605, time: 0.01140\n",
      "Epoch: 46, train_loss: 0.96680, time: 0.01126\n",
      "Epoch: 47, train_loss: 0.96934, time: 0.01149\n",
      "Epoch: 48, train_loss: 0.96511, time: 0.01128\n",
      "Epoch: 49, train_loss: 0.96838, time: 0.01137\n",
      "Epoch: 50, train_loss: 0.96572, time: 0.01146\n",
      "Epoch: 51, train_loss: 0.96615, time: 0.01142\n",
      "Epoch: 52, train_loss: 0.96764, time: 0.01142\n",
      "Epoch: 53, train_loss: 0.96582, time: 0.01150\n",
      "Epoch: 54, train_loss: 0.96598, time: 0.01126\n",
      "Epoch: 55, train_loss: 0.96565, time: 0.01174\n",
      "Epoch: 56, train_loss: 0.96647, time: 0.01152\n",
      "Epoch: 57, train_loss: 0.96468, time: 0.01132\n",
      "Epoch: 58, train_loss: 0.96655, time: 0.01123\n",
      "Epoch: 59, train_loss: 0.96918, time: 0.01141\n",
      "Epoch: 60, train_loss: 0.96493, time: 0.01113\n",
      "Epoch: 61, train_loss: 0.96711, time: 0.01125\n",
      "Epoch: 62, train_loss: 0.96493, time: 0.01109\n",
      "Epoch: 63, train_loss: 0.96453, time: 0.01159\n",
      "Epoch: 64, train_loss: 0.96658, time: 0.01149\n",
      "Epoch: 65, train_loss: 0.96591, time: 0.01128\n",
      "Epoch: 66, train_loss: 0.96503, time: 0.01138\n",
      "Epoch: 67, train_loss: 0.96406, time: 0.01129\n",
      "Epoch: 68, train_loss: 0.96482, time: 0.01131\n",
      "Epoch: 69, train_loss: 0.96529, time: 0.01126\n",
      "Epoch: 70, train_loss: 0.96557, time: 0.01138\n",
      "Epoch: 71, train_loss: 0.96537, time: 0.01137\n",
      "Epoch: 72, train_loss: 0.96639, time: 0.01150\n",
      "Epoch: 73, train_loss: 0.96595, time: 0.01153\n",
      "Epoch: 74, train_loss: 0.96557, time: 0.01162\n",
      "Epoch: 75, train_loss: 0.96986, time: 0.01140\n",
      "Epoch: 76, train_loss: 0.96648, time: 0.01130\n",
      "Epoch: 77, train_loss: 0.96589, time: 0.01154\n",
      "Epoch: 78, train_loss: 0.96515, time: 0.01159\n",
      "Epoch: 79, train_loss: 0.96495, time: 0.01133\n",
      "Epoch: 80, train_loss: 0.96567, time: 0.01161\n",
      "Epoch: 81, train_loss: 0.96558, time: 0.01128\n",
      "Epoch: 82, train_loss: 0.96875, time: 0.01172\n",
      "Epoch: 83, train_loss: 0.96589, time: 0.01141\n",
      "Epoch: 84, train_loss: 0.96805, time: 0.01140\n",
      "Epoch: 85, train_loss: 0.96325, time: 0.01140\n",
      "Epoch: 86, train_loss: 0.96366, time: 0.01127\n",
      "Epoch: 87, train_loss: 0.96697, time: 0.01145\n",
      "Epoch: 88, train_loss: 0.96934, time: 0.01143\n",
      "Epoch: 89, train_loss: 0.96524, time: 0.01155\n",
      "Epoch: 90, train_loss: 0.96760, time: 0.01142\n",
      "Epoch: 91, train_loss: 0.96426, time: 0.01138\n",
      "Epoch: 92, train_loss: 0.96536, time: 0.01161\n",
      "Epoch: 93, train_loss: 0.96758, time: 0.01132\n",
      "Epoch: 94, train_loss: 0.96670, time: 0.01149\n",
      "Epoch: 95, train_loss: 0.96856, time: 0.01134\n",
      "Epoch: 96, train_loss: 0.96474, time: 0.01178\n",
      "Epoch: 97, train_loss: 0.96678, time: 0.01139\n",
      "Epoch: 98, train_loss: 0.96593, time: 0.01131\n",
      "Epoch: 99, train_loss: 0.96786, time: 0.01138\n",
      "Epoch: 100, train_loss: 0.96796, time: 0.01151\n",
      "Epoch: 101, train_loss: 0.96805, time: 0.01134\n",
      "Epoch: 102, train_loss: 0.96695, time: 0.01143\n",
      "Epoch: 103, train_loss: 0.96970, time: 0.01146\n",
      "Epoch: 104, train_loss: 0.96597, time: 0.01112\n",
      "Epoch: 105, train_loss: 0.96615, time: 0.01116\n",
      "Epoch: 106, train_loss: 0.96441, time: 0.01106\n",
      "Epoch: 107, train_loss: 0.96822, time: 0.01135\n",
      "Epoch: 108, train_loss: 0.96999, time: 0.01126\n",
      "Epoch: 109, train_loss: 0.96615, time: 0.01176\n",
      "Epoch: 110, train_loss: 0.96217, time: 0.01150\n",
      "Epoch: 111, train_loss: 0.96612, time: 0.01131\n",
      "Epoch: 112, train_loss: 0.96485, time: 0.01124\n",
      "Epoch: 113, train_loss: 0.96604, time: 0.01128\n",
      "Epoch: 114, train_loss: 0.96731, time: 0.01132\n",
      "Epoch: 115, train_loss: 0.96419, time: 0.01138\n",
      "Epoch: 116, train_loss: 0.96668, time: 0.01152\n",
      "Epoch: 117, train_loss: 0.96492, time: 0.01161\n",
      "Epoch: 118, train_loss: 0.96685, time: 0.01155\n",
      "Epoch: 119, train_loss: 0.96347, time: 0.01131\n",
      "Epoch: 120, train_loss: 0.96480, time: 0.01156\n",
      "Epoch: 121, train_loss: 0.96693, time: 0.01137\n",
      "Epoch: 122, train_loss: 0.96573, time: 0.01140\n",
      "Epoch: 123, train_loss: 0.97005, time: 0.01122\n",
      "Epoch: 124, train_loss: 0.96516, time: 0.01125\n",
      "Epoch: 125, train_loss: 0.96699, time: 0.01142\n",
      "Epoch: 126, train_loss: 0.96240, time: 0.01170\n",
      "Epoch: 127, train_loss: 0.96582, time: 0.01176\n",
      "Epoch: 128, train_loss: 0.96734, time: 0.01167\n",
      "Epoch: 129, train_loss: 0.96588, time: 0.01158\n",
      "Epoch: 130, train_loss: 0.96762, time: 0.01126\n",
      "Epoch: 131, train_loss: 0.96945, time: 0.01142\n",
      "Epoch: 132, train_loss: 0.96732, time: 0.01144\n",
      "Epoch: 133, train_loss: 0.96793, time: 0.01145\n",
      "Epoch: 134, train_loss: 0.96533, time: 0.01093\n",
      "Epoch: 135, train_loss: 0.96683, time: 0.01114\n",
      "Epoch: 136, train_loss: 0.96772, time: 0.01131\n",
      "Epoch: 137, train_loss: 0.96432, time: 0.01118\n",
      "Epoch: 138, train_loss: 0.96772, time: 0.01131\n",
      "Epoch: 139, train_loss: 0.96596, time: 0.01153\n",
      "Epoch: 140, train_loss: 0.96612, time: 0.01120\n",
      "Epoch: 141, train_loss: 0.96431, time: 0.01138\n",
      "Epoch: 142, train_loss: 0.96771, time: 0.01118\n",
      "Epoch: 143, train_loss: 0.96533, time: 0.01134\n",
      "Epoch: 144, train_loss: 0.96756, time: 0.01114\n",
      "Epoch: 145, train_loss: 0.96684, time: 0.01163\n",
      "Epoch: 146, train_loss: 0.96531, time: 0.01143\n",
      "Epoch: 147, train_loss: 0.97023, time: 0.01124\n",
      "Epoch: 148, train_loss: 0.96558, time: 0.01161\n",
      "Epoch: 149, train_loss: 0.96935, time: 0.01160\n",
      "Epoch: 150, train_loss: 0.96634, time: 0.01128\n",
      "Epoch: 151, train_loss: 0.96791, time: 0.01141\n",
      "Epoch: 152, train_loss: 0.96805, time: 0.01139\n",
      "Epoch: 153, train_loss: 0.96697, time: 0.01118\n",
      "Epoch: 154, train_loss: 0.96557, time: 0.01144\n",
      "Epoch: 155, train_loss: 0.96498, time: 0.01103\n",
      "Epoch: 156, train_loss: 0.96752, time: 0.01162\n",
      "Epoch: 157, train_loss: 0.96478, time: 0.01123\n",
      "Epoch: 158, train_loss: 0.96556, time: 0.01135\n",
      "Epoch: 159, train_loss: 0.96586, time: 0.01122\n",
      "Epoch: 160, train_loss: 0.96710, time: 0.01152\n",
      "Epoch: 161, train_loss: 0.96237, time: 0.01132\n",
      "Epoch: 162, train_loss: 0.96459, time: 0.01168\n",
      "Epoch: 163, train_loss: 0.96303, time: 0.01152\n",
      "Epoch: 164, train_loss: 0.96623, time: 0.01146\n",
      "Epoch: 165, train_loss: 0.96716, time: 0.01131\n",
      "Epoch: 166, train_loss: 0.96677, time: 0.01158\n",
      "Epoch: 167, train_loss: 0.96547, time: 0.01122\n",
      "Epoch: 168, train_loss: 0.97098, time: 0.01138\n",
      "Epoch: 169, train_loss: 0.96504, time: 0.01149\n",
      "Epoch: 170, train_loss: 0.96591, time: 0.01121\n",
      "Epoch: 171, train_loss: 0.96466, time: 0.01145\n",
      "Epoch: 172, train_loss: 0.96452, time: 0.01138\n",
      "Epoch: 173, train_loss: 0.96674, time: 0.01136\n",
      "Epoch: 174, train_loss: 0.96636, time: 0.01133\n",
      "Epoch: 175, train_loss: 0.96582, time: 0.01143\n",
      "Epoch: 176, train_loss: 0.96736, time: 0.01105\n",
      "Epoch: 177, train_loss: 0.96990, time: 0.01137\n",
      "Epoch: 178, train_loss: 0.96380, time: 0.01129\n",
      "Epoch: 179, train_loss: 0.96790, time: 0.01147\n",
      "Epoch: 180, train_loss: 0.96569, time: 0.01121\n",
      "Epoch: 181, train_loss: 0.96666, time: 0.01144\n",
      "Epoch: 182, train_loss: 0.96761, time: 0.01113\n",
      "Epoch: 183, train_loss: 0.96882, time: 0.01168\n",
      "Epoch: 184, train_loss: 0.96750, time: 0.01097\n",
      "Epoch: 185, train_loss: 0.96570, time: 0.01135\n",
      "Epoch: 186, train_loss: 0.96800, time: 0.01117\n",
      "Epoch: 187, train_loss: 0.96526, time: 0.01141\n",
      "Epoch: 188, train_loss: 0.96794, time: 0.01118\n",
      "Epoch: 189, train_loss: 0.96645, time: 0.01126\n",
      "Epoch: 190, train_loss: 0.96721, time: 0.01135\n",
      "Epoch: 191, train_loss: 0.96824, time: 0.01124\n",
      "Epoch: 192, train_loss: 0.96467, time: 0.01162\n",
      "Epoch: 193, train_loss: 0.96654, time: 0.01121\n",
      "Epoch: 194, train_loss: 0.96757, time: 0.01151\n",
      "Epoch: 195, train_loss: 0.96536, time: 0.01107\n",
      "Epoch: 196, train_loss: 0.96834, time: 0.01156\n",
      "Epoch: 197, train_loss: 0.96478, time: 0.01142\n",
      "Epoch: 198, train_loss: 0.96738, time: 0.01164\n",
      "Epoch: 199, train_loss: 0.96555, time: 0.01177\n",
      "Epoch: 200, train_loss: 0.96526, time: 0.01124\n",
      "pairwise precision 0.15365 recall 0.87483 f1 0.26139\n",
      "average until now [0.4151987215633319, 0.8342201786358826, 0.5544451930681152]\n",
      "48 names 231.60569310188293 avg time 4.8251186062892275\n",
      "Loading min_zheng dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 523 nodes, 2857 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99510, time: 0.11765\n",
      "Epoch: 2, train_loss: 0.99217, time: 0.01867\n",
      "Epoch: 3, train_loss: 0.98914, time: 0.01797\n",
      "Epoch: 4, train_loss: 0.98945, time: 0.01722\n",
      "Epoch: 5, train_loss: 0.98822, time: 0.01740\n",
      "Epoch: 6, train_loss: 0.98677, time: 0.01750\n",
      "Epoch: 7, train_loss: 0.98766, time: 0.01812\n",
      "Epoch: 8, train_loss: 0.98763, time: 0.01817\n",
      "Epoch: 9, train_loss: 0.98753, time: 0.01786\n",
      "Epoch: 10, train_loss: 0.98830, time: 0.01745\n",
      "Epoch: 11, train_loss: 0.98865, time: 0.01699\n",
      "Epoch: 12, train_loss: 0.98827, time: 0.01694\n",
      "Epoch: 13, train_loss: 0.98734, time: 0.01726\n",
      "Epoch: 14, train_loss: 0.98726, time: 0.01802\n",
      "Epoch: 15, train_loss: 0.98824, time: 0.01722\n",
      "Epoch: 16, train_loss: 0.98686, time: 0.01730\n",
      "Epoch: 17, train_loss: 0.98806, time: 0.01798\n",
      "Epoch: 18, train_loss: 0.98764, time: 0.01743\n",
      "Epoch: 19, train_loss: 0.98864, time: 0.01818\n",
      "Epoch: 20, train_loss: 0.98758, time: 0.01825\n",
      "Epoch: 21, train_loss: 0.98614, time: 0.01824\n",
      "Epoch: 22, train_loss: 0.98729, time: 0.01714\n",
      "Epoch: 23, train_loss: 0.98739, time: 0.01695\n",
      "Epoch: 24, train_loss: 0.98796, time: 0.01822\n",
      "Epoch: 25, train_loss: 0.98783, time: 0.01861\n",
      "Epoch: 26, train_loss: 0.98780, time: 0.01785\n",
      "Epoch: 27, train_loss: 0.98692, time: 0.01801\n",
      "Epoch: 28, train_loss: 0.98711, time: 0.01822\n",
      "Epoch: 29, train_loss: 0.98812, time: 0.01790\n",
      "Epoch: 30, train_loss: 0.98905, time: 0.01807\n",
      "Epoch: 31, train_loss: 0.98634, time: 0.01761\n",
      "Epoch: 32, train_loss: 0.98874, time: 0.01753\n",
      "Epoch: 33, train_loss: 0.98716, time: 0.01768\n",
      "Epoch: 34, train_loss: 0.98604, time: 0.01736\n",
      "Epoch: 35, train_loss: 0.98736, time: 0.01857\n",
      "Epoch: 36, train_loss: 0.98757, time: 0.01694\n",
      "Epoch: 37, train_loss: 0.98781, time: 0.01851\n",
      "Epoch: 38, train_loss: 0.98773, time: 0.01740\n",
      "Epoch: 39, train_loss: 0.98768, time: 0.01658\n",
      "Epoch: 40, train_loss: 0.98731, time: 0.01801\n",
      "Epoch: 41, train_loss: 0.98799, time: 0.01816\n",
      "Epoch: 42, train_loss: 0.98752, time: 0.01798\n",
      "Epoch: 43, train_loss: 0.98741, time: 0.01782\n",
      "Epoch: 44, train_loss: 0.98884, time: 0.01752\n",
      "Epoch: 45, train_loss: 0.98698, time: 0.01824\n",
      "Epoch: 46, train_loss: 0.98695, time: 0.01787\n",
      "Epoch: 47, train_loss: 0.98807, time: 0.01807\n",
      "Epoch: 48, train_loss: 0.98802, time: 0.01789\n",
      "Epoch: 49, train_loss: 0.98768, time: 0.01788\n",
      "Epoch: 50, train_loss: 0.98804, time: 0.01779\n",
      "Epoch: 51, train_loss: 0.98717, time: 0.01903\n",
      "Epoch: 52, train_loss: 0.98790, time: 0.01805\n",
      "Epoch: 53, train_loss: 0.98730, time: 0.01696\n",
      "Epoch: 54, train_loss: 0.98730, time: 0.01706\n",
      "Epoch: 55, train_loss: 0.98662, time: 0.01833\n",
      "Epoch: 56, train_loss: 0.98662, time: 0.01798\n",
      "Epoch: 57, train_loss: 0.98699, time: 0.01803\n",
      "Epoch: 58, train_loss: 0.98772, time: 0.01843\n",
      "Epoch: 59, train_loss: 0.98649, time: 0.01747\n",
      "Epoch: 60, train_loss: 0.98767, time: 0.01732\n",
      "Epoch: 61, train_loss: 0.98833, time: 0.01855\n",
      "Epoch: 62, train_loss: 0.98743, time: 0.01675\n",
      "Epoch: 63, train_loss: 0.98808, time: 0.01832\n",
      "Epoch: 64, train_loss: 0.98723, time: 0.01782\n",
      "Epoch: 65, train_loss: 0.98863, time: 0.01803\n",
      "Epoch: 66, train_loss: 0.98620, time: 0.01780\n",
      "Epoch: 67, train_loss: 0.98748, time: 0.01795\n",
      "Epoch: 68, train_loss: 0.98820, time: 0.01786\n",
      "Epoch: 69, train_loss: 0.98738, time: 0.01784\n",
      "Epoch: 70, train_loss: 0.98702, time: 0.01821\n",
      "Epoch: 71, train_loss: 0.98735, time: 0.01755\n",
      "Epoch: 72, train_loss: 0.98651, time: 0.01750\n",
      "Epoch: 73, train_loss: 0.98811, time: 0.01821\n",
      "Epoch: 74, train_loss: 0.98614, time: 0.01762\n",
      "Epoch: 75, train_loss: 0.98745, time: 0.01830\n",
      "Epoch: 76, train_loss: 0.98852, time: 0.01677\n",
      "Epoch: 77, train_loss: 0.98736, time: 0.01897\n",
      "Epoch: 78, train_loss: 0.98692, time: 0.01760\n",
      "Epoch: 79, train_loss: 0.98875, time: 0.01659\n",
      "Epoch: 80, train_loss: 0.98802, time: 0.01787\n",
      "Epoch: 81, train_loss: 0.98728, time: 0.01810\n",
      "Epoch: 82, train_loss: 0.98779, time: 0.01808\n",
      "Epoch: 83, train_loss: 0.98722, time: 0.01724\n",
      "Epoch: 84, train_loss: 0.98834, time: 0.01804\n",
      "Epoch: 85, train_loss: 0.98829, time: 0.01818\n",
      "Epoch: 86, train_loss: 0.98871, time: 0.01803\n",
      "Epoch: 87, train_loss: 0.98844, time: 0.01800\n",
      "Epoch: 88, train_loss: 0.98690, time: 0.01773\n",
      "Epoch: 89, train_loss: 0.98867, time: 0.01756\n",
      "Epoch: 90, train_loss: 0.98723, time: 0.01784\n",
      "Epoch: 91, train_loss: 0.98717, time: 0.01864\n",
      "Epoch: 92, train_loss: 0.98755, time: 0.01751\n",
      "Epoch: 93, train_loss: 0.98876, time: 0.01861\n",
      "Epoch: 94, train_loss: 0.98661, time: 0.01756\n",
      "Epoch: 95, train_loss: 0.98741, time: 0.01760\n",
      "Epoch: 96, train_loss: 0.98644, time: 0.01724\n",
      "Epoch: 97, train_loss: 0.98697, time: 0.01892\n",
      "Epoch: 98, train_loss: 0.98800, time: 0.01756\n",
      "Epoch: 99, train_loss: 0.98833, time: 0.01753\n",
      "Epoch: 100, train_loss: 0.98848, time: 0.01803\n",
      "Epoch: 101, train_loss: 0.98689, time: 0.01811\n",
      "Epoch: 102, train_loss: 0.98669, time: 0.01684\n",
      "Epoch: 103, train_loss: 0.98713, time: 0.01743\n",
      "Epoch: 104, train_loss: 0.98811, time: 0.01791\n",
      "Epoch: 105, train_loss: 0.98720, time: 0.01809\n",
      "Epoch: 106, train_loss: 0.98675, time: 0.01812\n",
      "Epoch: 107, train_loss: 0.98711, time: 0.01735\n",
      "Epoch: 108, train_loss: 0.98797, time: 0.01757\n",
      "Epoch: 109, train_loss: 0.98740, time: 0.01797\n",
      "Epoch: 110, train_loss: 0.98669, time: 0.01713\n",
      "Epoch: 111, train_loss: 0.98860, time: 0.01648\n",
      "Epoch: 112, train_loss: 0.98738, time: 0.01705\n",
      "Epoch: 113, train_loss: 0.98722, time: 0.01793\n",
      "Epoch: 114, train_loss: 0.98846, time: 0.01747\n",
      "Epoch: 115, train_loss: 0.98618, time: 0.01678\n",
      "Epoch: 116, train_loss: 0.98726, time: 0.01683\n",
      "Epoch: 117, train_loss: 0.98752, time: 0.01835\n",
      "Epoch: 118, train_loss: 0.98758, time: 0.01778\n",
      "Epoch: 119, train_loss: 0.98773, time: 0.01858\n",
      "Epoch: 120, train_loss: 0.98739, time: 0.01767\n",
      "Epoch: 121, train_loss: 0.98799, time: 0.01775\n",
      "Epoch: 122, train_loss: 0.98785, time: 0.01869\n",
      "Epoch: 123, train_loss: 0.98676, time: 0.01866\n",
      "Epoch: 124, train_loss: 0.98837, time: 0.01743\n",
      "Epoch: 125, train_loss: 0.98787, time: 0.01775\n",
      "Epoch: 126, train_loss: 0.98674, time: 0.01725\n",
      "Epoch: 127, train_loss: 0.98687, time: 0.01796\n",
      "Epoch: 128, train_loss: 0.98763, time: 0.01824\n",
      "Epoch: 129, train_loss: 0.98812, time: 0.01833\n",
      "Epoch: 130, train_loss: 0.98723, time: 0.01845\n",
      "Epoch: 131, train_loss: 0.98660, time: 0.01684\n",
      "Epoch: 132, train_loss: 0.98778, time: 0.01722\n",
      "Epoch: 133, train_loss: 0.98698, time: 0.01873\n",
      "Epoch: 134, train_loss: 0.98648, time: 0.01758\n",
      "Epoch: 135, train_loss: 0.98669, time: 0.01778\n",
      "Epoch: 136, train_loss: 0.98670, time: 0.01816\n",
      "Epoch: 137, train_loss: 0.98799, time: 0.01796\n",
      "Epoch: 138, train_loss: 0.98775, time: 0.01815\n",
      "Epoch: 139, train_loss: 0.98801, time: 0.01847\n",
      "Epoch: 140, train_loss: 0.98912, time: 0.01836\n",
      "Epoch: 141, train_loss: 0.98611, time: 0.01771\n",
      "Epoch: 142, train_loss: 0.98839, time: 0.01734\n",
      "Epoch: 143, train_loss: 0.98731, time: 0.01877\n",
      "Epoch: 144, train_loss: 0.98813, time: 0.01859\n",
      "Epoch: 145, train_loss: 0.98663, time: 0.01907\n",
      "Epoch: 146, train_loss: 0.98731, time: 0.01826\n",
      "Epoch: 147, train_loss: 0.98788, time: 0.01828\n",
      "Epoch: 148, train_loss: 0.98832, time: 0.01817\n",
      "Epoch: 149, train_loss: 0.98840, time: 0.01748\n",
      "Epoch: 150, train_loss: 0.98679, time: 0.01842\n",
      "Epoch: 151, train_loss: 0.98755, time: 0.01807\n",
      "Epoch: 152, train_loss: 0.98772, time: 0.01730\n",
      "Epoch: 153, train_loss: 0.98770, time: 0.01749\n",
      "Epoch: 154, train_loss: 0.98885, time: 0.01797\n",
      "Epoch: 155, train_loss: 0.98726, time: 0.01782\n",
      "Epoch: 156, train_loss: 0.98673, time: 0.01803\n",
      "Epoch: 157, train_loss: 0.98776, time: 0.01835\n",
      "Epoch: 158, train_loss: 0.98842, time: 0.01849\n",
      "Epoch: 159, train_loss: 0.98732, time: 0.01834\n",
      "Epoch: 160, train_loss: 0.98845, time: 0.01730\n",
      "Epoch: 161, train_loss: 0.98728, time: 0.01750\n",
      "Epoch: 162, train_loss: 0.98787, time: 0.01805\n",
      "Epoch: 163, train_loss: 0.98709, time: 0.01779\n",
      "Epoch: 164, train_loss: 0.98695, time: 0.01684\n",
      "Epoch: 165, train_loss: 0.98796, time: 0.01714\n",
      "Epoch: 166, train_loss: 0.98646, time: 0.01839\n",
      "Epoch: 167, train_loss: 0.98595, time: 0.01819\n",
      "Epoch: 168, train_loss: 0.98790, time: 0.01836\n",
      "Epoch: 169, train_loss: 0.98713, time: 0.01783\n",
      "Epoch: 170, train_loss: 0.98878, time: 0.01761\n",
      "Epoch: 171, train_loss: 0.98687, time: 0.01822\n",
      "Epoch: 172, train_loss: 0.98795, time: 0.01781\n",
      "Epoch: 173, train_loss: 0.98731, time: 0.01806\n",
      "Epoch: 174, train_loss: 0.98914, time: 0.01830\n",
      "Epoch: 175, train_loss: 0.98674, time: 0.01775\n",
      "Epoch: 176, train_loss: 0.98712, time: 0.01830\n",
      "Epoch: 177, train_loss: 0.98823, time: 0.01783\n",
      "Epoch: 178, train_loss: 0.98805, time: 0.01855\n",
      "Epoch: 179, train_loss: 0.98739, time: 0.01743\n",
      "Epoch: 180, train_loss: 0.98832, time: 0.01826\n",
      "Epoch: 181, train_loss: 0.98974, time: 0.01792\n",
      "Epoch: 182, train_loss: 0.98815, time: 0.01789\n",
      "Epoch: 183, train_loss: 0.98822, time: 0.01734\n",
      "Epoch: 184, train_loss: 0.98792, time: 0.01727\n",
      "Epoch: 185, train_loss: 0.98854, time: 0.01793\n",
      "Epoch: 186, train_loss: 0.98641, time: 0.01784\n",
      "Epoch: 187, train_loss: 0.98764, time: 0.01829\n",
      "Epoch: 188, train_loss: 0.98915, time: 0.01712\n",
      "Epoch: 189, train_loss: 0.98900, time: 0.01707\n",
      "Epoch: 190, train_loss: 0.98696, time: 0.01750\n",
      "Epoch: 191, train_loss: 0.98761, time: 0.01685\n",
      "Epoch: 192, train_loss: 0.98663, time: 0.01685\n",
      "Epoch: 193, train_loss: 0.98798, time: 0.01704\n",
      "Epoch: 194, train_loss: 0.98748, time: 0.01789\n",
      "Epoch: 195, train_loss: 0.98859, time: 0.01740\n",
      "Epoch: 196, train_loss: 0.98803, time: 0.01796\n",
      "Epoch: 197, train_loss: 0.98680, time: 0.01839\n",
      "Epoch: 198, train_loss: 0.98775, time: 0.01835\n",
      "Epoch: 199, train_loss: 0.98752, time: 0.01815\n",
      "Epoch: 200, train_loss: 0.98701, time: 0.01861\n",
      "pairwise precision 0.14638 recall 0.88543 f1 0.25122\n",
      "average until now [0.4097125239953022, 0.8352653610812453, 0.549758647758475]\n",
      "49 names 235.38501453399658 avg time 4.803775806816256\n",
      "Loading yue_tian dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 127 nodes, 2006 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.87862, time: 0.10992\n",
      "Epoch: 2, train_loss: 0.88271, time: 0.01160\n",
      "Epoch: 3, train_loss: 0.87978, time: 0.01028\n",
      "Epoch: 4, train_loss: 0.88101, time: 0.01015\n",
      "Epoch: 5, train_loss: 0.87779, time: 0.01026\n",
      "Epoch: 6, train_loss: 0.87229, time: 0.00993\n",
      "Epoch: 7, train_loss: 0.87523, time: 0.01005\n",
      "Epoch: 8, train_loss: 0.87678, time: 0.00996\n",
      "Epoch: 9, train_loss: 0.87638, time: 0.00980\n",
      "Epoch: 10, train_loss: 0.87853, time: 0.01009\n",
      "Epoch: 11, train_loss: 0.87158, time: 0.00999\n",
      "Epoch: 12, train_loss: 0.87621, time: 0.01021\n",
      "Epoch: 13, train_loss: 0.87652, time: 0.00980\n",
      "Epoch: 14, train_loss: 0.88004, time: 0.00994\n",
      "Epoch: 15, train_loss: 0.87755, time: 0.00985\n",
      "Epoch: 16, train_loss: 0.87475, time: 0.01016\n",
      "Epoch: 17, train_loss: 0.87348, time: 0.00980\n",
      "Epoch: 18, train_loss: 0.87821, time: 0.01008\n",
      "Epoch: 19, train_loss: 0.87555, time: 0.00997\n",
      "Epoch: 20, train_loss: 0.87654, time: 0.01013\n",
      "Epoch: 21, train_loss: 0.87957, time: 0.01028\n",
      "Epoch: 22, train_loss: 0.87578, time: 0.01030\n",
      "Epoch: 23, train_loss: 0.87384, time: 0.00975\n",
      "Epoch: 24, train_loss: 0.87818, time: 0.01013\n",
      "Epoch: 25, train_loss: 0.87558, time: 0.00996\n",
      "Epoch: 26, train_loss: 0.87543, time: 0.00996\n",
      "Epoch: 27, train_loss: 0.87748, time: 0.01004\n",
      "Epoch: 28, train_loss: 0.87723, time: 0.00992\n",
      "Epoch: 29, train_loss: 0.87691, time: 0.01017\n",
      "Epoch: 30, train_loss: 0.87428, time: 0.01007\n",
      "Epoch: 31, train_loss: 0.87931, time: 0.00981\n",
      "Epoch: 32, train_loss: 0.87246, time: 0.01005\n",
      "Epoch: 33, train_loss: 0.87463, time: 0.00989\n",
      "Epoch: 34, train_loss: 0.87124, time: 0.01008\n",
      "Epoch: 35, train_loss: 0.87523, time: 0.00977\n",
      "Epoch: 36, train_loss: 0.87372, time: 0.01011\n",
      "Epoch: 37, train_loss: 0.87543, time: 0.00986\n",
      "Epoch: 38, train_loss: 0.87463, time: 0.01018\n",
      "Epoch: 39, train_loss: 0.87404, time: 0.00988\n",
      "Epoch: 40, train_loss: 0.87587, time: 0.01011\n",
      "Epoch: 41, train_loss: 0.87745, time: 0.00984\n",
      "Epoch: 42, train_loss: 0.87974, time: 0.01034\n",
      "Epoch: 43, train_loss: 0.87427, time: 0.01004\n",
      "Epoch: 44, train_loss: 0.87875, time: 0.00999\n",
      "Epoch: 45, train_loss: 0.88302, time: 0.01000\n",
      "Epoch: 46, train_loss: 0.87431, time: 0.00987\n",
      "Epoch: 47, train_loss: 0.87921, time: 0.00992\n",
      "Epoch: 48, train_loss: 0.86744, time: 0.00985\n",
      "Epoch: 49, train_loss: 0.87998, time: 0.01014\n",
      "Epoch: 50, train_loss: 0.88201, time: 0.00992\n",
      "Epoch: 51, train_loss: 0.87600, time: 0.00996\n",
      "Epoch: 52, train_loss: 0.87549, time: 0.00991\n",
      "Epoch: 53, train_loss: 0.87549, time: 0.01005\n",
      "Epoch: 54, train_loss: 0.87493, time: 0.00970\n",
      "Epoch: 55, train_loss: 0.87956, time: 0.00984\n",
      "Epoch: 56, train_loss: 0.87523, time: 0.00967\n",
      "Epoch: 57, train_loss: 0.87666, time: 0.01004\n",
      "Epoch: 58, train_loss: 0.88312, time: 0.00979\n",
      "Epoch: 59, train_loss: 0.87998, time: 0.01005\n",
      "Epoch: 60, train_loss: 0.87527, time: 0.00987\n",
      "Epoch: 61, train_loss: 0.87865, time: 0.01002\n",
      "Epoch: 62, train_loss: 0.87599, time: 0.00978\n",
      "Epoch: 63, train_loss: 0.87411, time: 0.01051\n",
      "Epoch: 64, train_loss: 0.88141, time: 0.00996\n",
      "Epoch: 65, train_loss: 0.87328, time: 0.00997\n",
      "Epoch: 66, train_loss: 0.87255, time: 0.00982\n",
      "Epoch: 67, train_loss: 0.87123, time: 0.00989\n",
      "Epoch: 68, train_loss: 0.87771, time: 0.01003\n",
      "Epoch: 69, train_loss: 0.87383, time: 0.00969\n",
      "Epoch: 70, train_loss: 0.87140, time: 0.00988\n",
      "Epoch: 71, train_loss: 0.87806, time: 0.00967\n",
      "Epoch: 72, train_loss: 0.88177, time: 0.00992\n",
      "Epoch: 73, train_loss: 0.86669, time: 0.01036\n",
      "Epoch: 74, train_loss: 0.87319, time: 0.01002\n",
      "Epoch: 75, train_loss: 0.86970, time: 0.00981\n",
      "Epoch: 76, train_loss: 0.87607, time: 0.01013\n",
      "Epoch: 77, train_loss: 0.86268, time: 0.00987\n",
      "Epoch: 78, train_loss: 0.86210, time: 0.01018\n",
      "Epoch: 79, train_loss: 0.87263, time: 0.00983\n",
      "Epoch: 80, train_loss: 0.86982, time: 0.01016\n",
      "Epoch: 81, train_loss: 0.87509, time: 0.00991\n",
      "Epoch: 82, train_loss: 0.86504, time: 0.01019\n",
      "Epoch: 83, train_loss: 0.87918, time: 0.00980\n",
      "Epoch: 84, train_loss: 0.87412, time: 0.01058\n",
      "Epoch: 85, train_loss: 0.88314, time: 0.00990\n",
      "Epoch: 86, train_loss: 0.86965, time: 0.01006\n",
      "Epoch: 87, train_loss: 0.87711, time: 0.00986\n",
      "Epoch: 88, train_loss: 0.88266, time: 0.01004\n",
      "Epoch: 89, train_loss: 0.87039, time: 0.00981\n",
      "Epoch: 90, train_loss: 0.88295, time: 0.01023\n",
      "Epoch: 91, train_loss: 0.88080, time: 0.00984\n",
      "Epoch: 92, train_loss: 0.87278, time: 0.01007\n",
      "Epoch: 93, train_loss: 0.87194, time: 0.00990\n",
      "Epoch: 94, train_loss: 0.85824, time: 0.01010\n",
      "Epoch: 95, train_loss: 0.88543, time: 0.00982\n",
      "Epoch: 96, train_loss: 0.86723, time: 0.01007\n",
      "Epoch: 97, train_loss: 0.88321, time: 0.00983\n",
      "Epoch: 98, train_loss: 0.87203, time: 0.01006\n",
      "Epoch: 99, train_loss: 0.89420, time: 0.00980\n",
      "Epoch: 100, train_loss: 0.88303, time: 0.01003\n",
      "Epoch: 101, train_loss: 0.86237, time: 0.00981\n",
      "Epoch: 102, train_loss: 0.87233, time: 0.01001\n",
      "Epoch: 103, train_loss: 0.88264, time: 0.00992\n",
      "Epoch: 104, train_loss: 0.86828, time: 0.01002\n",
      "Epoch: 105, train_loss: 0.84531, time: 0.01017\n",
      "Epoch: 106, train_loss: 0.87092, time: 0.01021\n",
      "Epoch: 107, train_loss: 0.88546, time: 0.00970\n",
      "Epoch: 108, train_loss: 0.87250, time: 0.01018\n",
      "Epoch: 109, train_loss: 0.84941, time: 0.00975\n",
      "Epoch: 110, train_loss: 0.86280, time: 0.01001\n",
      "Epoch: 111, train_loss: 0.86450, time: 0.00992\n",
      "Epoch: 112, train_loss: 0.86636, time: 0.00991\n",
      "Epoch: 113, train_loss: 0.86746, time: 0.00985\n",
      "Epoch: 114, train_loss: 0.86121, time: 0.01001\n",
      "Epoch: 115, train_loss: 0.86906, time: 0.00996\n",
      "Epoch: 116, train_loss: 0.85092, time: 0.01006\n",
      "Epoch: 117, train_loss: 0.87475, time: 0.00996\n",
      "Epoch: 118, train_loss: 0.87970, time: 0.00969\n",
      "Epoch: 119, train_loss: 0.87015, time: 0.00993\n",
      "Epoch: 120, train_loss: 0.87485, time: 0.01009\n",
      "Epoch: 121, train_loss: 0.85612, time: 0.00992\n",
      "Epoch: 122, train_loss: 0.85446, time: 0.01003\n",
      "Epoch: 123, train_loss: 0.87099, time: 0.00983\n",
      "Epoch: 124, train_loss: 0.86303, time: 0.00996\n",
      "Epoch: 125, train_loss: 0.87362, time: 0.00983\n",
      "Epoch: 126, train_loss: 0.84536, time: 0.01039\n",
      "Epoch: 127, train_loss: 0.86670, time: 0.00995\n",
      "Epoch: 128, train_loss: 0.87108, time: 0.01002\n",
      "Epoch: 129, train_loss: 0.87251, time: 0.00982\n",
      "Epoch: 130, train_loss: 0.86001, time: 0.01024\n",
      "Epoch: 131, train_loss: 0.86432, time: 0.00978\n",
      "Epoch: 132, train_loss: 0.86526, time: 0.00999\n",
      "Epoch: 133, train_loss: 0.86258, time: 0.00994\n",
      "Epoch: 134, train_loss: 0.85634, time: 0.00993\n",
      "Epoch: 135, train_loss: 0.88030, time: 0.00988\n",
      "Epoch: 136, train_loss: 0.87293, time: 0.00996\n",
      "Epoch: 137, train_loss: 0.86151, time: 0.00974\n",
      "Epoch: 138, train_loss: 0.87540, time: 0.01000\n",
      "Epoch: 139, train_loss: 0.86319, time: 0.00979\n",
      "Epoch: 140, train_loss: 0.88178, time: 0.01005\n",
      "Epoch: 141, train_loss: 0.87313, time: 0.00974\n",
      "Epoch: 142, train_loss: 0.84908, time: 0.01000\n",
      "Epoch: 143, train_loss: 0.84658, time: 0.00974\n",
      "Epoch: 144, train_loss: 0.86688, time: 0.01015\n",
      "Epoch: 145, train_loss: 0.87487, time: 0.00971\n",
      "Epoch: 146, train_loss: 0.86491, time: 0.00994\n",
      "Epoch: 147, train_loss: 0.88945, time: 0.01027\n",
      "Epoch: 148, train_loss: 0.87520, time: 0.01012\n",
      "Epoch: 149, train_loss: 0.87985, time: 0.00983\n",
      "Epoch: 150, train_loss: 0.86845, time: 0.00997\n",
      "Epoch: 151, train_loss: 0.86011, time: 0.00984\n",
      "Epoch: 152, train_loss: 0.87618, time: 0.01003\n",
      "Epoch: 153, train_loss: 0.87592, time: 0.00977\n",
      "Epoch: 154, train_loss: 0.85139, time: 0.01013\n",
      "Epoch: 155, train_loss: 0.87826, time: 0.00989\n",
      "Epoch: 156, train_loss: 0.86748, time: 0.00998\n",
      "Epoch: 157, train_loss: 0.87270, time: 0.00989\n",
      "Epoch: 158, train_loss: 0.87562, time: 0.01005\n",
      "Epoch: 159, train_loss: 0.87898, time: 0.00975\n",
      "Epoch: 160, train_loss: 0.88215, time: 0.00998\n",
      "Epoch: 161, train_loss: 0.87812, time: 0.00974\n",
      "Epoch: 162, train_loss: 0.87995, time: 0.00980\n",
      "Epoch: 163, train_loss: 0.85065, time: 0.00986\n",
      "Epoch: 164, train_loss: 0.88025, time: 0.00977\n",
      "Epoch: 165, train_loss: 0.85810, time: 0.00971\n",
      "Epoch: 166, train_loss: 0.87114, time: 0.00988\n",
      "Epoch: 167, train_loss: 0.85807, time: 0.00969\n",
      "Epoch: 168, train_loss: 0.84004, time: 0.01013\n",
      "Epoch: 169, train_loss: 0.86221, time: 0.00980\n",
      "Epoch: 170, train_loss: 0.84839, time: 0.00992\n",
      "Epoch: 171, train_loss: 0.86520, time: 0.00967\n",
      "Epoch: 172, train_loss: 0.87933, time: 0.00988\n",
      "Epoch: 173, train_loss: 0.85884, time: 0.00970\n",
      "Epoch: 174, train_loss: 0.87493, time: 0.01000\n",
      "Epoch: 175, train_loss: 0.87136, time: 0.00989\n",
      "Epoch: 176, train_loss: 0.87688, time: 0.00987\n",
      "Epoch: 177, train_loss: 0.86587, time: 0.00980\n",
      "Epoch: 178, train_loss: 0.87571, time: 0.01023\n",
      "Epoch: 179, train_loss: 0.86188, time: 0.01012\n",
      "Epoch: 180, train_loss: 0.88478, time: 0.01008\n",
      "Epoch: 181, train_loss: 0.87807, time: 0.00995\n",
      "Epoch: 182, train_loss: 0.86537, time: 0.01011\n",
      "Epoch: 183, train_loss: 0.85282, time: 0.01018\n",
      "Epoch: 184, train_loss: 0.86809, time: 0.00981\n",
      "Epoch: 185, train_loss: 0.85581, time: 0.01009\n",
      "Epoch: 186, train_loss: 0.85543, time: 0.00985\n",
      "Epoch: 187, train_loss: 0.86064, time: 0.01002\n",
      "Epoch: 188, train_loss: 0.85101, time: 0.00979\n",
      "Epoch: 189, train_loss: 0.88701, time: 0.01040\n",
      "Epoch: 190, train_loss: 0.86522, time: 0.00997\n",
      "Epoch: 191, train_loss: 0.84413, time: 0.01003\n",
      "Epoch: 192, train_loss: 0.86805, time: 0.00988\n",
      "Epoch: 193, train_loss: 0.87693, time: 0.00994\n",
      "Epoch: 194, train_loss: 0.86908, time: 0.00979\n",
      "Epoch: 195, train_loss: 0.88050, time: 0.00993\n",
      "Epoch: 196, train_loss: 0.85639, time: 0.01005\n",
      "Epoch: 197, train_loss: 0.87417, time: 0.00987\n",
      "Epoch: 198, train_loss: 0.86763, time: 0.01000\n",
      "Epoch: 199, train_loss: 0.88301, time: 0.01004\n",
      "Epoch: 200, train_loss: 0.87261, time: 0.00988\n",
      "pairwise precision 0.51323 recall 0.48893 f1 0.50079\n",
      "average until now [0.4117829373208325, 0.8283385887829594, 0.5501004377480537]\n",
      "50 names 237.51555728912354 avg time 4.7503111457824705\n",
      "Loading jian_shao dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 162 nodes, 1561 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.94357, time: 0.11036\n",
      "Epoch: 2, train_loss: 0.94743, time: 0.01199\n",
      "Epoch: 3, train_loss: 0.94557, time: 0.01015\n",
      "Epoch: 4, train_loss: 0.94235, time: 0.01012\n",
      "Epoch: 5, train_loss: 0.94140, time: 0.01009\n",
      "Epoch: 6, train_loss: 0.93690, time: 0.01031\n",
      "Epoch: 7, train_loss: 0.93994, time: 0.01021\n",
      "Epoch: 8, train_loss: 0.94149, time: 0.01034\n",
      "Epoch: 9, train_loss: 0.94254, time: 0.01030\n",
      "Epoch: 10, train_loss: 0.94000, time: 0.01020\n",
      "Epoch: 11, train_loss: 0.94124, time: 0.00996\n",
      "Epoch: 12, train_loss: 0.94031, time: 0.01024\n",
      "Epoch: 13, train_loss: 0.94152, time: 0.01001\n",
      "Epoch: 14, train_loss: 0.93969, time: 0.01027\n",
      "Epoch: 15, train_loss: 0.94103, time: 0.00993\n",
      "Epoch: 16, train_loss: 0.93942, time: 0.01012\n",
      "Epoch: 17, train_loss: 0.93835, time: 0.01020\n",
      "Epoch: 18, train_loss: 0.93819, time: 0.00988\n",
      "Epoch: 19, train_loss: 0.94172, time: 0.01021\n",
      "Epoch: 20, train_loss: 0.94003, time: 0.00988\n",
      "Epoch: 21, train_loss: 0.93929, time: 0.01084\n",
      "Epoch: 22, train_loss: 0.93742, time: 0.01022\n",
      "Epoch: 23, train_loss: 0.93915, time: 0.01000\n",
      "Epoch: 24, train_loss: 0.94283, time: 0.01025\n",
      "Epoch: 25, train_loss: 0.93840, time: 0.01017\n",
      "Epoch: 26, train_loss: 0.94515, time: 0.01041\n",
      "Epoch: 27, train_loss: 0.94452, time: 0.01004\n",
      "Epoch: 28, train_loss: 0.93951, time: 0.01041\n",
      "Epoch: 29, train_loss: 0.94031, time: 0.00999\n",
      "Epoch: 30, train_loss: 0.94292, time: 0.01043\n",
      "Epoch: 31, train_loss: 0.93856, time: 0.01001\n",
      "Epoch: 32, train_loss: 0.94547, time: 0.01019\n",
      "Epoch: 33, train_loss: 0.94349, time: 0.01026\n",
      "Epoch: 34, train_loss: 0.94452, time: 0.00989\n",
      "Epoch: 35, train_loss: 0.93892, time: 0.01021\n",
      "Epoch: 36, train_loss: 0.93932, time: 0.01008\n",
      "Epoch: 37, train_loss: 0.94374, time: 0.01023\n",
      "Epoch: 38, train_loss: 0.94072, time: 0.00981\n",
      "Epoch: 39, train_loss: 0.94045, time: 0.01025\n",
      "Epoch: 40, train_loss: 0.93665, time: 0.00993\n",
      "Epoch: 41, train_loss: 0.94243, time: 0.01064\n",
      "Epoch: 42, train_loss: 0.94049, time: 0.01033\n",
      "Epoch: 43, train_loss: 0.93925, time: 0.01012\n",
      "Epoch: 44, train_loss: 0.93899, time: 0.01026\n",
      "Epoch: 45, train_loss: 0.94186, time: 0.00988\n",
      "Epoch: 46, train_loss: 0.94815, time: 0.01014\n",
      "Epoch: 47, train_loss: 0.94575, time: 0.01015\n",
      "Epoch: 48, train_loss: 0.94002, time: 0.00987\n",
      "Epoch: 49, train_loss: 0.93605, time: 0.01013\n",
      "Epoch: 50, train_loss: 0.94293, time: 0.01002\n",
      "Epoch: 51, train_loss: 0.94280, time: 0.01019\n",
      "Epoch: 52, train_loss: 0.94212, time: 0.00985\n",
      "Epoch: 53, train_loss: 0.94214, time: 0.01024\n",
      "Epoch: 54, train_loss: 0.94557, time: 0.01022\n",
      "Epoch: 55, train_loss: 0.93852, time: 0.01034\n",
      "Epoch: 56, train_loss: 0.94218, time: 0.01012\n",
      "Epoch: 57, train_loss: 0.94399, time: 0.01005\n",
      "Epoch: 58, train_loss: 0.94206, time: 0.01014\n",
      "Epoch: 59, train_loss: 0.93694, time: 0.01022\n",
      "Epoch: 60, train_loss: 0.93842, time: 0.00995\n",
      "Epoch: 61, train_loss: 0.94110, time: 0.01030\n",
      "Epoch: 62, train_loss: 0.93838, time: 0.00996\n",
      "Epoch: 63, train_loss: 0.94169, time: 0.00993\n",
      "Epoch: 64, train_loss: 0.94042, time: 0.00996\n",
      "Epoch: 65, train_loss: 0.94284, time: 0.01030\n",
      "Epoch: 66, train_loss: 0.93938, time: 0.01031\n",
      "Epoch: 67, train_loss: 0.93813, time: 0.01002\n",
      "Epoch: 68, train_loss: 0.93940, time: 0.01046\n",
      "Epoch: 69, train_loss: 0.94431, time: 0.01007\n",
      "Epoch: 70, train_loss: 0.94528, time: 0.01018\n",
      "Epoch: 71, train_loss: 0.94417, time: 0.00990\n",
      "Epoch: 72, train_loss: 0.93957, time: 0.01021\n",
      "Epoch: 73, train_loss: 0.93813, time: 0.01027\n",
      "Epoch: 74, train_loss: 0.94130, time: 0.01002\n",
      "Epoch: 75, train_loss: 0.93824, time: 0.01014\n",
      "Epoch: 76, train_loss: 0.94068, time: 0.01012\n",
      "Epoch: 77, train_loss: 0.93920, time: 0.01025\n",
      "Epoch: 78, train_loss: 0.94164, time: 0.01000\n",
      "Epoch: 79, train_loss: 0.94421, time: 0.01038\n",
      "Epoch: 80, train_loss: 0.94242, time: 0.00978\n",
      "Epoch: 81, train_loss: 0.94184, time: 0.01039\n",
      "Epoch: 82, train_loss: 0.94283, time: 0.01050\n",
      "Epoch: 83, train_loss: 0.94127, time: 0.01005\n",
      "Epoch: 84, train_loss: 0.94201, time: 0.01021\n",
      "Epoch: 85, train_loss: 0.94032, time: 0.01006\n",
      "Epoch: 86, train_loss: 0.94526, time: 0.00999\n",
      "Epoch: 87, train_loss: 0.94022, time: 0.00996\n",
      "Epoch: 88, train_loss: 0.93765, time: 0.01023\n",
      "Epoch: 89, train_loss: 0.94014, time: 0.01012\n",
      "Epoch: 90, train_loss: 0.93977, time: 0.01014\n",
      "Epoch: 91, train_loss: 0.93782, time: 0.00996\n",
      "Epoch: 92, train_loss: 0.94519, time: 0.01037\n",
      "Epoch: 93, train_loss: 0.94145, time: 0.01020\n",
      "Epoch: 94, train_loss: 0.94528, time: 0.01010\n",
      "Epoch: 95, train_loss: 0.93977, time: 0.00990\n",
      "Epoch: 96, train_loss: 0.94335, time: 0.01002\n",
      "Epoch: 97, train_loss: 0.93641, time: 0.01035\n",
      "Epoch: 98, train_loss: 0.94409, time: 0.01016\n",
      "Epoch: 99, train_loss: 0.93803, time: 0.01011\n",
      "Epoch: 100, train_loss: 0.93885, time: 0.01032\n",
      "Epoch: 101, train_loss: 0.94036, time: 0.01057\n",
      "Epoch: 102, train_loss: 0.93716, time: 0.01039\n",
      "Epoch: 103, train_loss: 0.94124, time: 0.00992\n",
      "Epoch: 104, train_loss: 0.93668, time: 0.01032\n",
      "Epoch: 105, train_loss: 0.94314, time: 0.00996\n",
      "Epoch: 106, train_loss: 0.93876, time: 0.01009\n",
      "Epoch: 107, train_loss: 0.94130, time: 0.01008\n",
      "Epoch: 108, train_loss: 0.94184, time: 0.01022\n",
      "Epoch: 109, train_loss: 0.94306, time: 0.00989\n",
      "Epoch: 110, train_loss: 0.94265, time: 0.01028\n",
      "Epoch: 111, train_loss: 0.94028, time: 0.01024\n",
      "Epoch: 112, train_loss: 0.94106, time: 0.01011\n",
      "Epoch: 113, train_loss: 0.94330, time: 0.01029\n",
      "Epoch: 114, train_loss: 0.94028, time: 0.01013\n",
      "Epoch: 115, train_loss: 0.93963, time: 0.00992\n",
      "Epoch: 116, train_loss: 0.94290, time: 0.01016\n",
      "Epoch: 117, train_loss: 0.94340, time: 0.01017\n",
      "Epoch: 118, train_loss: 0.94250, time: 0.01015\n",
      "Epoch: 119, train_loss: 0.94283, time: 0.00995\n",
      "Epoch: 120, train_loss: 0.94152, time: 0.01021\n",
      "Epoch: 121, train_loss: 0.93710, time: 0.01058\n",
      "Epoch: 122, train_loss: 0.93901, time: 0.01009\n",
      "Epoch: 123, train_loss: 0.94006, time: 0.01035\n",
      "Epoch: 124, train_loss: 0.93991, time: 0.00994\n",
      "Epoch: 125, train_loss: 0.93820, time: 0.01012\n",
      "Epoch: 126, train_loss: 0.93967, time: 0.01028\n",
      "Epoch: 127, train_loss: 0.93783, time: 0.01020\n",
      "Epoch: 128, train_loss: 0.94387, time: 0.01017\n",
      "Epoch: 129, train_loss: 0.94066, time: 0.00998\n",
      "Epoch: 130, train_loss: 0.94330, time: 0.01022\n",
      "Epoch: 131, train_loss: 0.93822, time: 0.01009\n",
      "Epoch: 132, train_loss: 0.93832, time: 0.01013\n",
      "Epoch: 133, train_loss: 0.94314, time: 0.01027\n",
      "Epoch: 134, train_loss: 0.94082, time: 0.01024\n",
      "Epoch: 135, train_loss: 0.93929, time: 0.01010\n",
      "Epoch: 136, train_loss: 0.93871, time: 0.01012\n",
      "Epoch: 137, train_loss: 0.94393, time: 0.01026\n",
      "Epoch: 138, train_loss: 0.93899, time: 0.01029\n",
      "Epoch: 139, train_loss: 0.94431, time: 0.01008\n",
      "Epoch: 140, train_loss: 0.94215, time: 0.00989\n",
      "Epoch: 141, train_loss: 0.94087, time: 0.01073\n",
      "Epoch: 142, train_loss: 0.94143, time: 0.01050\n",
      "Epoch: 143, train_loss: 0.93757, time: 0.01012\n",
      "Epoch: 144, train_loss: 0.93553, time: 0.01027\n",
      "Epoch: 145, train_loss: 0.93426, time: 0.01000\n",
      "Epoch: 146, train_loss: 0.94031, time: 0.01018\n",
      "Epoch: 147, train_loss: 0.94246, time: 0.00991\n",
      "Epoch: 148, train_loss: 0.94678, time: 0.01021\n",
      "Epoch: 149, train_loss: 0.94036, time: 0.01017\n",
      "Epoch: 150, train_loss: 0.94009, time: 0.01055\n",
      "Epoch: 151, train_loss: 0.94104, time: 0.01006\n",
      "Epoch: 152, train_loss: 0.94351, time: 0.00995\n",
      "Epoch: 153, train_loss: 0.94267, time: 0.01030\n",
      "Epoch: 154, train_loss: 0.94138, time: 0.01021\n",
      "Epoch: 155, train_loss: 0.94199, time: 0.01011\n",
      "Epoch: 156, train_loss: 0.94227, time: 0.01039\n",
      "Epoch: 157, train_loss: 0.93801, time: 0.01007\n",
      "Epoch: 158, train_loss: 0.93875, time: 0.01025\n",
      "Epoch: 159, train_loss: 0.93594, time: 0.01020\n",
      "Epoch: 160, train_loss: 0.94087, time: 0.01032\n",
      "Epoch: 161, train_loss: 0.94156, time: 0.01058\n",
      "Epoch: 162, train_loss: 0.93899, time: 0.01041\n",
      "Epoch: 163, train_loss: 0.94324, time: 0.01013\n",
      "Epoch: 164, train_loss: 0.94084, time: 0.01028\n",
      "Epoch: 165, train_loss: 0.93771, time: 0.01026\n",
      "Epoch: 166, train_loss: 0.93995, time: 0.01020\n",
      "Epoch: 167, train_loss: 0.94344, time: 0.01023\n",
      "Epoch: 168, train_loss: 0.93941, time: 0.00995\n",
      "Epoch: 169, train_loss: 0.94238, time: 0.01024\n",
      "Epoch: 170, train_loss: 0.94260, time: 0.00992\n",
      "Epoch: 171, train_loss: 0.94057, time: 0.01018\n",
      "Epoch: 172, train_loss: 0.93816, time: 0.01001\n",
      "Epoch: 173, train_loss: 0.94113, time: 0.01028\n",
      "Epoch: 174, train_loss: 0.93907, time: 0.01012\n",
      "Epoch: 175, train_loss: 0.94185, time: 0.01023\n",
      "Epoch: 176, train_loss: 0.94092, time: 0.01022\n",
      "Epoch: 177, train_loss: 0.93754, time: 0.00985\n",
      "Epoch: 178, train_loss: 0.94239, time: 0.00998\n",
      "Epoch: 179, train_loss: 0.94072, time: 0.01046\n",
      "Epoch: 180, train_loss: 0.94031, time: 0.01003\n",
      "Epoch: 181, train_loss: 0.94191, time: 0.01075\n",
      "Epoch: 182, train_loss: 0.93963, time: 0.01046\n",
      "Epoch: 183, train_loss: 0.93779, time: 0.01007\n",
      "Epoch: 184, train_loss: 0.94106, time: 0.01021\n",
      "Epoch: 185, train_loss: 0.93780, time: 0.00995\n",
      "Epoch: 186, train_loss: 0.94465, time: 0.01005\n",
      "Epoch: 187, train_loss: 0.93768, time: 0.01020\n",
      "Epoch: 188, train_loss: 0.94052, time: 0.01034\n",
      "Epoch: 189, train_loss: 0.94061, time: 0.01036\n",
      "Epoch: 190, train_loss: 0.94183, time: 0.01028\n",
      "Epoch: 191, train_loss: 0.94067, time: 0.01003\n",
      "Epoch: 192, train_loss: 0.94089, time: 0.01027\n",
      "Epoch: 193, train_loss: 0.93991, time: 0.01030\n",
      "Epoch: 194, train_loss: 0.94086, time: 0.01033\n",
      "Epoch: 195, train_loss: 0.93970, time: 0.01030\n",
      "Epoch: 196, train_loss: 0.94471, time: 0.01000\n",
      "Epoch: 197, train_loss: 0.94017, time: 0.01033\n",
      "Epoch: 198, train_loss: 0.94007, time: 0.01034\n",
      "Epoch: 199, train_loss: 0.93654, time: 0.01016\n",
      "Epoch: 200, train_loss: 0.94171, time: 0.01024\n",
      "pairwise precision 0.28304 recall 0.96077 f1 0.43727\n",
      "average until now [0.40925860862806646, 0.8309352183483084, 0.5484100693363078]\n",
      "51 names 239.69326758384705 avg time 4.699867991840138\n",
      "Loading tian_chen dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 180 nodes, 697 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98598, time: 0.11059\n",
      "Epoch: 2, train_loss: 0.98479, time: 0.01140\n",
      "Epoch: 3, train_loss: 0.97388, time: 0.01005\n",
      "Epoch: 4, train_loss: 0.98643, time: 0.00976\n",
      "Epoch: 5, train_loss: 0.97758, time: 0.00977\n",
      "Epoch: 6, train_loss: 0.97872, time: 0.00976\n",
      "Epoch: 7, train_loss: 0.97835, time: 0.00982\n",
      "Epoch: 8, train_loss: 0.98094, time: 0.00996\n",
      "Epoch: 9, train_loss: 0.98174, time: 0.00998\n",
      "Epoch: 10, train_loss: 0.97921, time: 0.01031\n",
      "Epoch: 11, train_loss: 0.97557, time: 0.00975\n",
      "Epoch: 12, train_loss: 0.98058, time: 0.00975\n",
      "Epoch: 13, train_loss: 0.97925, time: 0.00982\n",
      "Epoch: 14, train_loss: 0.98238, time: 0.00960\n",
      "Epoch: 15, train_loss: 0.97938, time: 0.00980\n",
      "Epoch: 16, train_loss: 0.97707, time: 0.00984\n",
      "Epoch: 17, train_loss: 0.98003, time: 0.00970\n",
      "Epoch: 18, train_loss: 0.97767, time: 0.00990\n",
      "Epoch: 19, train_loss: 0.97893, time: 0.00984\n",
      "Epoch: 20, train_loss: 0.97638, time: 0.00989\n",
      "Epoch: 21, train_loss: 0.97747, time: 0.00966\n",
      "Epoch: 22, train_loss: 0.98002, time: 0.00979\n",
      "Epoch: 23, train_loss: 0.97999, time: 0.00980\n",
      "Epoch: 24, train_loss: 0.98139, time: 0.01003\n",
      "Epoch: 25, train_loss: 0.97605, time: 0.00998\n",
      "Epoch: 26, train_loss: 0.98010, time: 0.00977\n",
      "Epoch: 27, train_loss: 0.97893, time: 0.00988\n",
      "Epoch: 28, train_loss: 0.98148, time: 0.00986\n",
      "Epoch: 29, train_loss: 0.98047, time: 0.00994\n",
      "Epoch: 30, train_loss: 0.98005, time: 0.00990\n",
      "Epoch: 31, train_loss: 0.97413, time: 0.01048\n",
      "Epoch: 32, train_loss: 0.98068, time: 0.01012\n",
      "Epoch: 33, train_loss: 0.97990, time: 0.00986\n",
      "Epoch: 34, train_loss: 0.98073, time: 0.00981\n",
      "Epoch: 35, train_loss: 0.98244, time: 0.00990\n",
      "Epoch: 36, train_loss: 0.97772, time: 0.00986\n",
      "Epoch: 37, train_loss: 0.97501, time: 0.00991\n",
      "Epoch: 38, train_loss: 0.97948, time: 0.00988\n",
      "Epoch: 39, train_loss: 0.97763, time: 0.00962\n",
      "Epoch: 40, train_loss: 0.98017, time: 0.00992\n",
      "Epoch: 41, train_loss: 0.98182, time: 0.00996\n",
      "Epoch: 42, train_loss: 0.97973, time: 0.00976\n",
      "Epoch: 43, train_loss: 0.98657, time: 0.01015\n",
      "Epoch: 44, train_loss: 0.97902, time: 0.00993\n",
      "Epoch: 45, train_loss: 0.97935, time: 0.00980\n",
      "Epoch: 46, train_loss: 0.97713, time: 0.00982\n",
      "Epoch: 47, train_loss: 0.98169, time: 0.00978\n",
      "Epoch: 48, train_loss: 0.97695, time: 0.00994\n",
      "Epoch: 49, train_loss: 0.98105, time: 0.00979\n",
      "Epoch: 50, train_loss: 0.97795, time: 0.00978\n",
      "Epoch: 51, train_loss: 0.97871, time: 0.00980\n",
      "Epoch: 52, train_loss: 0.97857, time: 0.01040\n",
      "Epoch: 53, train_loss: 0.98150, time: 0.01011\n",
      "Epoch: 54, train_loss: 0.98004, time: 0.01014\n",
      "Epoch: 55, train_loss: 0.97508, time: 0.00982\n",
      "Epoch: 56, train_loss: 0.97612, time: 0.00988\n",
      "Epoch: 57, train_loss: 0.97566, time: 0.00992\n",
      "Epoch: 58, train_loss: 0.97544, time: 0.00983\n",
      "Epoch: 59, train_loss: 0.97596, time: 0.01003\n",
      "Epoch: 60, train_loss: 0.97683, time: 0.01000\n",
      "Epoch: 61, train_loss: 0.98122, time: 0.00989\n",
      "Epoch: 62, train_loss: 0.97726, time: 0.00999\n",
      "Epoch: 63, train_loss: 0.97547, time: 0.00991\n",
      "Epoch: 64, train_loss: 0.97712, time: 0.00994\n",
      "Epoch: 65, train_loss: 0.97564, time: 0.00991\n",
      "Epoch: 66, train_loss: 0.97893, time: 0.00972\n",
      "Epoch: 67, train_loss: 0.98028, time: 0.00981\n",
      "Epoch: 68, train_loss: 0.97762, time: 0.00993\n",
      "Epoch: 69, train_loss: 0.98208, time: 0.00972\n",
      "Epoch: 70, train_loss: 0.98503, time: 0.00960\n",
      "Epoch: 71, train_loss: 0.97571, time: 0.00998\n",
      "Epoch: 72, train_loss: 0.97617, time: 0.01001\n",
      "Epoch: 73, train_loss: 0.97685, time: 0.01028\n",
      "Epoch: 74, train_loss: 0.97403, time: 0.01003\n",
      "Epoch: 75, train_loss: 0.97593, time: 0.00998\n",
      "Epoch: 76, train_loss: 0.97749, time: 0.01001\n",
      "Epoch: 77, train_loss: 0.98338, time: 0.00990\n",
      "Epoch: 78, train_loss: 0.97530, time: 0.00981\n",
      "Epoch: 79, train_loss: 0.97847, time: 0.00996\n",
      "Epoch: 80, train_loss: 0.98052, time: 0.01005\n",
      "Epoch: 81, train_loss: 0.97810, time: 0.00978\n",
      "Epoch: 82, train_loss: 0.97725, time: 0.01007\n",
      "Epoch: 83, train_loss: 0.97868, time: 0.00983\n",
      "Epoch: 84, train_loss: 0.97591, time: 0.00972\n",
      "Epoch: 85, train_loss: 0.97887, time: 0.00986\n",
      "Epoch: 86, train_loss: 0.97851, time: 0.00985\n",
      "Epoch: 87, train_loss: 0.97918, time: 0.00984\n",
      "Epoch: 88, train_loss: 0.97374, time: 0.00978\n",
      "Epoch: 89, train_loss: 0.97988, time: 0.00972\n",
      "Epoch: 90, train_loss: 0.97771, time: 0.00984\n",
      "Epoch: 91, train_loss: 0.97676, time: 0.00969\n",
      "Epoch: 92, train_loss: 0.97948, time: 0.00968\n",
      "Epoch: 93, train_loss: 0.97625, time: 0.00977\n",
      "Epoch: 94, train_loss: 0.98104, time: 0.01028\n",
      "Epoch: 95, train_loss: 0.97586, time: 0.01013\n",
      "Epoch: 96, train_loss: 0.98056, time: 0.00978\n",
      "Epoch: 97, train_loss: 0.97986, time: 0.00994\n",
      "Epoch: 98, train_loss: 0.98008, time: 0.00987\n",
      "Epoch: 99, train_loss: 0.97561, time: 0.00992\n",
      "Epoch: 100, train_loss: 0.97712, time: 0.01010\n",
      "Epoch: 101, train_loss: 0.97843, time: 0.00990\n",
      "Epoch: 102, train_loss: 0.97839, time: 0.00988\n",
      "Epoch: 103, train_loss: 0.97895, time: 0.01002\n",
      "Epoch: 104, train_loss: 0.97773, time: 0.00980\n",
      "Epoch: 105, train_loss: 0.97484, time: 0.00991\n",
      "Epoch: 106, train_loss: 0.97652, time: 0.00991\n",
      "Epoch: 107, train_loss: 0.97688, time: 0.00982\n",
      "Epoch: 108, train_loss: 0.97529, time: 0.00987\n",
      "Epoch: 109, train_loss: 0.97691, time: 0.01012\n",
      "Epoch: 110, train_loss: 0.98047, time: 0.00983\n",
      "Epoch: 111, train_loss: 0.97771, time: 0.00978\n",
      "Epoch: 112, train_loss: 0.97999, time: 0.00970\n",
      "Epoch: 113, train_loss: 0.97681, time: 0.00990\n",
      "Epoch: 114, train_loss: 0.97522, time: 0.00985\n",
      "Epoch: 115, train_loss: 0.97503, time: 0.01032\n",
      "Epoch: 116, train_loss: 0.97706, time: 0.01005\n",
      "Epoch: 117, train_loss: 0.98143, time: 0.00993\n",
      "Epoch: 118, train_loss: 0.97891, time: 0.00982\n",
      "Epoch: 119, train_loss: 0.97641, time: 0.00989\n",
      "Epoch: 120, train_loss: 0.97757, time: 0.00988\n",
      "Epoch: 121, train_loss: 0.97678, time: 0.00978\n",
      "Epoch: 122, train_loss: 0.97682, time: 0.00970\n",
      "Epoch: 123, train_loss: 0.98159, time: 0.00976\n",
      "Epoch: 124, train_loss: 0.97852, time: 0.00961\n",
      "Epoch: 125, train_loss: 0.98110, time: 0.00995\n",
      "Epoch: 126, train_loss: 0.97725, time: 0.00998\n",
      "Epoch: 127, train_loss: 0.97493, time: 0.00988\n",
      "Epoch: 128, train_loss: 0.97857, time: 0.00996\n",
      "Epoch: 129, train_loss: 0.97500, time: 0.00965\n",
      "Epoch: 130, train_loss: 0.98112, time: 0.00987\n",
      "Epoch: 131, train_loss: 0.98115, time: 0.00999\n",
      "Epoch: 132, train_loss: 0.97494, time: 0.00982\n",
      "Epoch: 133, train_loss: 0.97832, time: 0.00984\n",
      "Epoch: 134, train_loss: 0.98010, time: 0.00987\n",
      "Epoch: 135, train_loss: 0.97899, time: 0.01000\n",
      "Epoch: 136, train_loss: 0.97735, time: 0.01037\n",
      "Epoch: 137, train_loss: 0.97710, time: 0.00992\n",
      "Epoch: 138, train_loss: 0.97653, time: 0.00981\n",
      "Epoch: 139, train_loss: 0.97702, time: 0.00985\n",
      "Epoch: 140, train_loss: 0.97518, time: 0.00973\n",
      "Epoch: 141, train_loss: 0.97360, time: 0.00972\n",
      "Epoch: 142, train_loss: 0.97521, time: 0.00982\n",
      "Epoch: 143, train_loss: 0.98083, time: 0.00982\n",
      "Epoch: 144, train_loss: 0.97335, time: 0.00962\n",
      "Epoch: 145, train_loss: 0.97566, time: 0.00976\n",
      "Epoch: 146, train_loss: 0.97931, time: 0.00984\n",
      "Epoch: 147, train_loss: 0.97629, time: 0.00970\n",
      "Epoch: 148, train_loss: 0.97607, time: 0.00998\n",
      "Epoch: 149, train_loss: 0.97609, time: 0.00990\n",
      "Epoch: 150, train_loss: 0.97490, time: 0.00978\n",
      "Epoch: 151, train_loss: 0.98088, time: 0.00970\n",
      "Epoch: 152, train_loss: 0.97937, time: 0.00985\n",
      "Epoch: 153, train_loss: 0.97760, time: 0.00993\n",
      "Epoch: 154, train_loss: 0.97770, time: 0.00994\n",
      "Epoch: 155, train_loss: 0.97647, time: 0.00975\n",
      "Epoch: 156, train_loss: 0.97498, time: 0.00985\n",
      "Epoch: 157, train_loss: 0.97613, time: 0.01031\n",
      "Epoch: 158, train_loss: 0.98034, time: 0.00988\n",
      "Epoch: 159, train_loss: 0.97501, time: 0.00950\n",
      "Epoch: 160, train_loss: 0.97431, time: 0.00968\n",
      "Epoch: 161, train_loss: 0.97942, time: 0.00971\n",
      "Epoch: 162, train_loss: 0.97357, time: 0.00973\n",
      "Epoch: 163, train_loss: 0.97786, time: 0.00985\n",
      "Epoch: 164, train_loss: 0.98256, time: 0.00973\n",
      "Epoch: 165, train_loss: 0.97611, time: 0.00984\n",
      "Epoch: 166, train_loss: 0.97689, time: 0.00985\n",
      "Epoch: 167, train_loss: 0.97519, time: 0.00975\n",
      "Epoch: 168, train_loss: 0.97595, time: 0.00973\n",
      "Epoch: 169, train_loss: 0.97895, time: 0.00979\n",
      "Epoch: 170, train_loss: 0.97720, time: 0.00986\n",
      "Epoch: 171, train_loss: 0.97731, time: 0.00958\n",
      "Epoch: 172, train_loss: 0.97391, time: 0.00990\n",
      "Epoch: 173, train_loss: 0.97773, time: 0.00996\n",
      "Epoch: 174, train_loss: 0.97703, time: 0.00988\n",
      "Epoch: 175, train_loss: 0.97939, time: 0.00995\n",
      "Epoch: 176, train_loss: 0.97849, time: 0.00992\n",
      "Epoch: 177, train_loss: 0.97581, time: 0.00982\n",
      "Epoch: 178, train_loss: 0.97941, time: 0.01020\n",
      "Epoch: 179, train_loss: 0.97836, time: 0.00984\n",
      "Epoch: 180, train_loss: 0.97625, time: 0.00989\n",
      "Epoch: 181, train_loss: 0.97794, time: 0.01001\n",
      "Epoch: 182, train_loss: 0.97749, time: 0.00981\n",
      "Epoch: 183, train_loss: 0.97760, time: 0.00987\n",
      "Epoch: 184, train_loss: 0.97755, time: 0.00976\n",
      "Epoch: 185, train_loss: 0.97810, time: 0.00988\n",
      "Epoch: 186, train_loss: 0.97419, time: 0.00975\n",
      "Epoch: 187, train_loss: 0.98005, time: 0.01001\n",
      "Epoch: 188, train_loss: 0.97601, time: 0.00994\n",
      "Epoch: 189, train_loss: 0.97894, time: 0.00977\n",
      "Epoch: 190, train_loss: 0.97799, time: 0.00986\n",
      "Epoch: 191, train_loss: 0.97644, time: 0.00975\n",
      "Epoch: 192, train_loss: 0.97783, time: 0.00983\n",
      "Epoch: 193, train_loss: 0.97750, time: 0.00995\n",
      "Epoch: 194, train_loss: 0.97765, time: 0.00982\n",
      "Epoch: 195, train_loss: 0.97770, time: 0.00971\n",
      "Epoch: 196, train_loss: 0.97899, time: 0.00986\n",
      "Epoch: 197, train_loss: 0.97868, time: 0.00983\n",
      "Epoch: 198, train_loss: 0.97993, time: 0.00972\n",
      "Epoch: 199, train_loss: 0.97564, time: 0.01019\n",
      "Epoch: 200, train_loss: 0.98115, time: 0.00985\n",
      "pairwise precision 0.09884 recall 0.87821 f1 0.17768\n",
      "average until now [0.40328899053220035, 0.8318442550763242, 0.543218524968875]\n",
      "52 names 241.8147053718567 avg time 4.650282795612629\n",
      "Loading yan_yan_li dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 103 nodes, 341 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97495, time: 0.10791\n",
      "Epoch: 2, train_loss: 0.96815, time: 0.01013\n",
      "Epoch: 3, train_loss: 0.96728, time: 0.00918\n",
      "Epoch: 4, train_loss: 0.97171, time: 0.00887\n",
      "Epoch: 5, train_loss: 0.97886, time: 0.00876\n",
      "Epoch: 6, train_loss: 0.96898, time: 0.00883\n",
      "Epoch: 7, train_loss: 0.97614, time: 0.00891\n",
      "Epoch: 8, train_loss: 0.96618, time: 0.00892\n",
      "Epoch: 9, train_loss: 0.97217, time: 0.00877\n",
      "Epoch: 10, train_loss: 0.96945, time: 0.00875\n",
      "Epoch: 11, train_loss: 0.96675, time: 0.00872\n",
      "Epoch: 12, train_loss: 0.97014, time: 0.00874\n",
      "Epoch: 13, train_loss: 0.97631, time: 0.00874\n",
      "Epoch: 14, train_loss: 0.96922, time: 0.00866\n",
      "Epoch: 15, train_loss: 0.97300, time: 0.00869\n",
      "Epoch: 16, train_loss: 0.96951, time: 0.00875\n",
      "Epoch: 17, train_loss: 0.96900, time: 0.00873\n",
      "Epoch: 18, train_loss: 0.97205, time: 0.00873\n",
      "Epoch: 19, train_loss: 0.97238, time: 0.00880\n",
      "Epoch: 20, train_loss: 0.96423, time: 0.00882\n",
      "Epoch: 21, train_loss: 0.97918, time: 0.00877\n",
      "Epoch: 22, train_loss: 0.97206, time: 0.00874\n",
      "Epoch: 23, train_loss: 0.96918, time: 0.00883\n",
      "Epoch: 24, train_loss: 0.97135, time: 0.00875\n",
      "Epoch: 25, train_loss: 0.97222, time: 0.00866\n",
      "Epoch: 26, train_loss: 0.97569, time: 0.00873\n",
      "Epoch: 27, train_loss: 0.97145, time: 0.00881\n",
      "Epoch: 28, train_loss: 0.96806, time: 0.00882\n",
      "Epoch: 29, train_loss: 0.96409, time: 0.00875\n",
      "Epoch: 30, train_loss: 0.96957, time: 0.00892\n",
      "Epoch: 31, train_loss: 0.97805, time: 0.00887\n",
      "Epoch: 32, train_loss: 0.97328, time: 0.00867\n",
      "Epoch: 33, train_loss: 0.96960, time: 0.00872\n",
      "Epoch: 34, train_loss: 0.96693, time: 0.00884\n",
      "Epoch: 35, train_loss: 0.97295, time: 0.00876\n",
      "Epoch: 36, train_loss: 0.97119, time: 0.00870\n",
      "Epoch: 37, train_loss: 0.96700, time: 0.00877\n",
      "Epoch: 38, train_loss: 0.97511, time: 0.00875\n",
      "Epoch: 39, train_loss: 0.97202, time: 0.00873\n",
      "Epoch: 40, train_loss: 0.97596, time: 0.00869\n",
      "Epoch: 41, train_loss: 0.96703, time: 0.00869\n",
      "Epoch: 42, train_loss: 0.97327, time: 0.00883\n",
      "Epoch: 43, train_loss: 0.97470, time: 0.00874\n",
      "Epoch: 44, train_loss: 0.96584, time: 0.00873\n",
      "Epoch: 45, train_loss: 0.96825, time: 0.00873\n",
      "Epoch: 46, train_loss: 0.98169, time: 0.00873\n",
      "Epoch: 47, train_loss: 0.96873, time: 0.00872\n",
      "Epoch: 48, train_loss: 0.96773, time: 0.00883\n",
      "Epoch: 49, train_loss: 0.97105, time: 0.00866\n",
      "Epoch: 50, train_loss: 0.96900, time: 0.00863\n",
      "Epoch: 51, train_loss: 0.96951, time: 0.00868\n",
      "Epoch: 52, train_loss: 0.98282, time: 0.00869\n",
      "Epoch: 53, train_loss: 0.97316, time: 0.00904\n",
      "Epoch: 54, train_loss: 0.96678, time: 0.00883\n",
      "Epoch: 55, train_loss: 0.97052, time: 0.00868\n",
      "Epoch: 56, train_loss: 0.97104, time: 0.00881\n",
      "Epoch: 57, train_loss: 0.96877, time: 0.00885\n",
      "Epoch: 58, train_loss: 0.96935, time: 0.00878\n",
      "Epoch: 59, train_loss: 0.96835, time: 0.00868\n",
      "Epoch: 60, train_loss: 0.97239, time: 0.00872\n",
      "Epoch: 61, train_loss: 0.96440, time: 0.00866\n",
      "Epoch: 62, train_loss: 0.96892, time: 0.00865\n",
      "Epoch: 63, train_loss: 0.97215, time: 0.00866\n",
      "Epoch: 64, train_loss: 0.96971, time: 0.00870\n",
      "Epoch: 65, train_loss: 0.96163, time: 0.00860\n",
      "Epoch: 66, train_loss: 0.96977, time: 0.00866\n",
      "Epoch: 67, train_loss: 0.97344, time: 0.00869\n",
      "Epoch: 68, train_loss: 0.96720, time: 0.00869\n",
      "Epoch: 69, train_loss: 0.96813, time: 0.00873\n",
      "Epoch: 70, train_loss: 0.96896, time: 0.00871\n",
      "Epoch: 71, train_loss: 0.97715, time: 0.00879\n",
      "Epoch: 72, train_loss: 0.96836, time: 0.00872\n",
      "Epoch: 73, train_loss: 0.97066, time: 0.00869\n",
      "Epoch: 74, train_loss: 0.97637, time: 0.00876\n",
      "Epoch: 75, train_loss: 0.96966, time: 0.00877\n",
      "Epoch: 76, train_loss: 0.96941, time: 0.00907\n",
      "Epoch: 77, train_loss: 0.96643, time: 0.00883\n",
      "Epoch: 78, train_loss: 0.96854, time: 0.00883\n",
      "Epoch: 79, train_loss: 0.97069, time: 0.00880\n",
      "Epoch: 80, train_loss: 0.96957, time: 0.00873\n",
      "Epoch: 81, train_loss: 0.97114, time: 0.00877\n",
      "Epoch: 82, train_loss: 0.96685, time: 0.00875\n",
      "Epoch: 83, train_loss: 0.96847, time: 0.00872\n",
      "Epoch: 84, train_loss: 0.96904, time: 0.00871\n",
      "Epoch: 85, train_loss: 0.97495, time: 0.00866\n",
      "Epoch: 86, train_loss: 0.96458, time: 0.00866\n",
      "Epoch: 87, train_loss: 0.97408, time: 0.00867\n",
      "Epoch: 88, train_loss: 0.97231, time: 0.00857\n",
      "Epoch: 89, train_loss: 0.97291, time: 0.00867\n",
      "Epoch: 90, train_loss: 0.96201, time: 0.00860\n",
      "Epoch: 91, train_loss: 0.96475, time: 0.00863\n",
      "Epoch: 92, train_loss: 0.96831, time: 0.00860\n",
      "Epoch: 93, train_loss: 0.96574, time: 0.00868\n",
      "Epoch: 94, train_loss: 0.96765, time: 0.00862\n",
      "Epoch: 95, train_loss: 0.97073, time: 0.00858\n",
      "Epoch: 96, train_loss: 0.96663, time: 0.00859\n",
      "Epoch: 97, train_loss: 0.97038, time: 0.00855\n",
      "Epoch: 98, train_loss: 0.96925, time: 0.00856\n",
      "Epoch: 99, train_loss: 0.96657, time: 0.00858\n",
      "Epoch: 100, train_loss: 0.96642, time: 0.00914\n",
      "Epoch: 101, train_loss: 0.96953, time: 0.00879\n",
      "Epoch: 102, train_loss: 0.96735, time: 0.00858\n",
      "Epoch: 103, train_loss: 0.96950, time: 0.00854\n",
      "Epoch: 104, train_loss: 0.97397, time: 0.00859\n",
      "Epoch: 105, train_loss: 0.96695, time: 0.00859\n",
      "Epoch: 106, train_loss: 0.96995, time: 0.00866\n",
      "Epoch: 107, train_loss: 0.96609, time: 0.00863\n",
      "Epoch: 108, train_loss: 0.97056, time: 0.00868\n",
      "Epoch: 109, train_loss: 0.96964, time: 0.00867\n",
      "Epoch: 110, train_loss: 0.98014, time: 0.00866\n",
      "Epoch: 111, train_loss: 0.96686, time: 0.00868\n",
      "Epoch: 112, train_loss: 0.96508, time: 0.00860\n",
      "Epoch: 113, train_loss: 0.96729, time: 0.00864\n",
      "Epoch: 114, train_loss: 0.96877, time: 0.00861\n",
      "Epoch: 115, train_loss: 0.97030, time: 0.00855\n",
      "Epoch: 116, train_loss: 0.97144, time: 0.00864\n",
      "Epoch: 117, train_loss: 0.96966, time: 0.00867\n",
      "Epoch: 118, train_loss: 0.96477, time: 0.00864\n",
      "Epoch: 119, train_loss: 0.96897, time: 0.00857\n",
      "Epoch: 120, train_loss: 0.96274, time: 0.00868\n",
      "Epoch: 121, train_loss: 0.96543, time: 0.00873\n",
      "Epoch: 122, train_loss: 0.96781, time: 0.00858\n",
      "Epoch: 123, train_loss: 0.96864, time: 0.00859\n",
      "Epoch: 124, train_loss: 0.96905, time: 0.00915\n",
      "Epoch: 125, train_loss: 0.97119, time: 0.00878\n",
      "Epoch: 126, train_loss: 0.97220, time: 0.00865\n",
      "Epoch: 127, train_loss: 0.96597, time: 0.00858\n",
      "Epoch: 128, train_loss: 0.96845, time: 0.00870\n",
      "Epoch: 129, train_loss: 0.96329, time: 0.00861\n",
      "Epoch: 130, train_loss: 0.97237, time: 0.00860\n",
      "Epoch: 131, train_loss: 0.96488, time: 0.00869\n",
      "Epoch: 132, train_loss: 0.97599, time: 0.00860\n",
      "Epoch: 133, train_loss: 0.97123, time: 0.00861\n",
      "Epoch: 134, train_loss: 0.96958, time: 0.00867\n",
      "Epoch: 135, train_loss: 0.97248, time: 0.00869\n",
      "Epoch: 136, train_loss: 0.97313, time: 0.00864\n",
      "Epoch: 137, train_loss: 0.97529, time: 0.00867\n",
      "Epoch: 138, train_loss: 0.96716, time: 0.00855\n",
      "Epoch: 139, train_loss: 0.97115, time: 0.00880\n",
      "Epoch: 140, train_loss: 0.96678, time: 0.00880\n",
      "Epoch: 141, train_loss: 0.97103, time: 0.00869\n",
      "Epoch: 142, train_loss: 0.96974, time: 0.00871\n",
      "Epoch: 143, train_loss: 0.97070, time: 0.00865\n",
      "Epoch: 144, train_loss: 0.96733, time: 0.00853\n",
      "Epoch: 145, train_loss: 0.97342, time: 0.00856\n",
      "Epoch: 146, train_loss: 0.96366, time: 0.00872\n",
      "Epoch: 147, train_loss: 0.96737, time: 0.00861\n",
      "Epoch: 148, train_loss: 0.96513, time: 0.00921\n",
      "Epoch: 149, train_loss: 0.97142, time: 0.00894\n",
      "Epoch: 150, train_loss: 0.96689, time: 0.00868\n",
      "Epoch: 151, train_loss: 0.97111, time: 0.00867\n",
      "Epoch: 152, train_loss: 0.97151, time: 0.00868\n",
      "Epoch: 153, train_loss: 0.96991, time: 0.00858\n",
      "Epoch: 154, train_loss: 0.96975, time: 0.00867\n",
      "Epoch: 155, train_loss: 0.96433, time: 0.00868\n",
      "Epoch: 156, train_loss: 0.96766, time: 0.00862\n",
      "Epoch: 157, train_loss: 0.96965, time: 0.00868\n",
      "Epoch: 158, train_loss: 0.97163, time: 0.00872\n",
      "Epoch: 159, train_loss: 0.96290, time: 0.00869\n",
      "Epoch: 160, train_loss: 0.97538, time: 0.00861\n",
      "Epoch: 161, train_loss: 0.97044, time: 0.00858\n",
      "Epoch: 162, train_loss: 0.96721, time: 0.00862\n",
      "Epoch: 163, train_loss: 0.96185, time: 0.00869\n",
      "Epoch: 164, train_loss: 0.97028, time: 0.00863\n",
      "Epoch: 165, train_loss: 0.97470, time: 0.00857\n",
      "Epoch: 166, train_loss: 0.97183, time: 0.00873\n",
      "Epoch: 167, train_loss: 0.97998, time: 0.00865\n",
      "Epoch: 168, train_loss: 0.98138, time: 0.00862\n",
      "Epoch: 169, train_loss: 0.97318, time: 0.00860\n",
      "Epoch: 170, train_loss: 0.97075, time: 0.00856\n",
      "Epoch: 171, train_loss: 0.96795, time: 0.00870\n",
      "Epoch: 172, train_loss: 0.96797, time: 0.00908\n",
      "Epoch: 173, train_loss: 0.96962, time: 0.00878\n",
      "Epoch: 174, train_loss: 0.97631, time: 0.00858\n",
      "Epoch: 175, train_loss: 0.97118, time: 0.00858\n",
      "Epoch: 176, train_loss: 0.97369, time: 0.00857\n",
      "Epoch: 177, train_loss: 0.96731, time: 0.00863\n",
      "Epoch: 178, train_loss: 0.96775, time: 0.00868\n",
      "Epoch: 179, train_loss: 0.97766, time: 0.00868\n",
      "Epoch: 180, train_loss: 0.97014, time: 0.00858\n",
      "Epoch: 181, train_loss: 0.96193, time: 0.00854\n",
      "Epoch: 182, train_loss: 0.97123, time: 0.00865\n",
      "Epoch: 183, train_loss: 0.97792, time: 0.00863\n",
      "Epoch: 184, train_loss: 0.97544, time: 0.00877\n",
      "Epoch: 185, train_loss: 0.97037, time: 0.00871\n",
      "Epoch: 186, train_loss: 0.96666, time: 0.00858\n",
      "Epoch: 187, train_loss: 0.96973, time: 0.00871\n",
      "Epoch: 188, train_loss: 0.97167, time: 0.00871\n",
      "Epoch: 189, train_loss: 0.96787, time: 0.00865\n",
      "Epoch: 190, train_loss: 0.96521, time: 0.00868\n",
      "Epoch: 191, train_loss: 0.97221, time: 0.00864\n",
      "Epoch: 192, train_loss: 0.97185, time: 0.00868\n",
      "Epoch: 193, train_loss: 0.96652, time: 0.00861\n",
      "Epoch: 194, train_loss: 0.97545, time: 0.00875\n",
      "Epoch: 195, train_loss: 0.97835, time: 0.00872\n",
      "Epoch: 196, train_loss: 0.97611, time: 0.00921\n",
      "Epoch: 197, train_loss: 0.97239, time: 0.00886\n",
      "Epoch: 198, train_loss: 0.97541, time: 0.00869\n",
      "Epoch: 199, train_loss: 0.96590, time: 0.00870\n",
      "Epoch: 200, train_loss: 0.97559, time: 0.00876\n",
      "pairwise precision 0.19674 recall 0.87072 f1 0.32095\n",
      "average until now [0.39939175288421547, 0.8325778056118667, 0.5398261781756104]\n",
      "53 names 243.68620467185974 avg time 4.597852918336976\n",
      "Loading xue_qin dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 224 nodes, 7409 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.85398, time: 0.11749\n",
      "Epoch: 2, train_loss: 0.85082, time: 0.01614\n",
      "Epoch: 3, train_loss: 0.85434, time: 0.01501\n",
      "Epoch: 4, train_loss: 0.85399, time: 0.01443\n",
      "Epoch: 5, train_loss: 0.85641, time: 0.01439\n",
      "Epoch: 6, train_loss: 0.85261, time: 0.01647\n",
      "Epoch: 7, train_loss: 0.84989, time: 0.01422\n",
      "Epoch: 8, train_loss: 0.85274, time: 0.01442\n",
      "Epoch: 9, train_loss: 0.85402, time: 0.01470\n",
      "Epoch: 10, train_loss: 0.85246, time: 0.01495\n",
      "Epoch: 11, train_loss: 0.85078, time: 0.01465\n",
      "Epoch: 12, train_loss: 0.85494, time: 0.01493\n",
      "Epoch: 13, train_loss: 0.85176, time: 0.01494\n",
      "Epoch: 14, train_loss: 0.85413, time: 0.01462\n",
      "Epoch: 15, train_loss: 0.85313, time: 0.01575\n",
      "Epoch: 16, train_loss: 0.84969, time: 0.01532\n",
      "Epoch: 17, train_loss: 0.85331, time: 0.01499\n",
      "Epoch: 18, train_loss: 0.85154, time: 0.01474\n",
      "Epoch: 19, train_loss: 0.85145, time: 0.01481\n",
      "Epoch: 20, train_loss: 0.85191, time: 0.01471\n",
      "Epoch: 21, train_loss: 0.84825, time: 0.01505\n",
      "Epoch: 22, train_loss: 0.85199, time: 0.01490\n",
      "Epoch: 23, train_loss: 0.85155, time: 0.01492\n",
      "Epoch: 24, train_loss: 0.85114, time: 0.01459\n",
      "Epoch: 25, train_loss: 0.85039, time: 0.01493\n",
      "Epoch: 26, train_loss: 0.85332, time: 0.01512\n",
      "Epoch: 27, train_loss: 0.85034, time: 0.01472\n",
      "Epoch: 28, train_loss: 0.85182, time: 0.01521\n",
      "Epoch: 29, train_loss: 0.85001, time: 0.01476\n",
      "Epoch: 30, train_loss: 0.85132, time: 0.01490\n",
      "Epoch: 31, train_loss: 0.85238, time: 0.01483\n",
      "Epoch: 32, train_loss: 0.85243, time: 0.01478\n",
      "Epoch: 33, train_loss: 0.84846, time: 0.01537\n",
      "Epoch: 34, train_loss: 0.84951, time: 0.01473\n",
      "Epoch: 35, train_loss: 0.84919, time: 0.01512\n",
      "Epoch: 36, train_loss: 0.84870, time: 0.01495\n",
      "Epoch: 37, train_loss: 0.85022, time: 0.01417\n",
      "Epoch: 38, train_loss: 0.85137, time: 0.01424\n",
      "Epoch: 39, train_loss: 0.84812, time: 0.01436\n",
      "Epoch: 40, train_loss: 0.84587, time: 0.01491\n",
      "Epoch: 41, train_loss: 0.85278, time: 0.01526\n",
      "Epoch: 42, train_loss: 0.84225, time: 0.01459\n",
      "Epoch: 43, train_loss: 0.85046, time: 0.01541\n",
      "Epoch: 44, train_loss: 0.85150, time: 0.01533\n",
      "Epoch: 45, train_loss: 0.84821, time: 0.01474\n",
      "Epoch: 46, train_loss: 0.84620, time: 0.01506\n",
      "Epoch: 47, train_loss: 0.84496, time: 0.01496\n",
      "Epoch: 48, train_loss: 0.84801, time: 0.01448\n",
      "Epoch: 49, train_loss: 0.85048, time: 0.01475\n",
      "Epoch: 50, train_loss: 0.84659, time: 0.01446\n",
      "Epoch: 51, train_loss: 0.85029, time: 0.01410\n",
      "Epoch: 52, train_loss: 0.84517, time: 0.01430\n",
      "Epoch: 53, train_loss: 0.84516, time: 0.01379\n",
      "Epoch: 54, train_loss: 0.84441, time: 0.01440\n",
      "Epoch: 55, train_loss: 0.84792, time: 0.01437\n",
      "Epoch: 56, train_loss: 0.84673, time: 0.01436\n",
      "Epoch: 57, train_loss: 0.83012, time: 0.01447\n",
      "Epoch: 58, train_loss: 0.84095, time: 0.01516\n",
      "Epoch: 59, train_loss: 0.83055, time: 0.01486\n",
      "Epoch: 60, train_loss: 0.83849, time: 0.01513\n",
      "Epoch: 61, train_loss: 0.84609, time: 0.01528\n",
      "Epoch: 62, train_loss: 0.83552, time: 0.01522\n",
      "Epoch: 63, train_loss: 0.84121, time: 0.01489\n",
      "Epoch: 64, train_loss: 0.83857, time: 0.01481\n",
      "Epoch: 65, train_loss: 0.83781, time: 0.01498\n",
      "Epoch: 66, train_loss: 0.84254, time: 0.01485\n",
      "Epoch: 67, train_loss: 0.83770, time: 0.01495\n",
      "Epoch: 68, train_loss: 0.84146, time: 0.01476\n",
      "Epoch: 69, train_loss: 0.83837, time: 0.01498\n",
      "Epoch: 70, train_loss: 0.83711, time: 0.01498\n",
      "Epoch: 71, train_loss: 0.84307, time: 0.01560\n",
      "Epoch: 72, train_loss: 0.84414, time: 0.01512\n",
      "Epoch: 73, train_loss: 0.84247, time: 0.01524\n",
      "Epoch: 74, train_loss: 0.85456, time: 0.01486\n",
      "Epoch: 75, train_loss: 0.84612, time: 0.01453\n",
      "Epoch: 76, train_loss: 0.84218, time: 0.01495\n",
      "Epoch: 77, train_loss: 0.83572, time: 0.01518\n",
      "Epoch: 78, train_loss: 0.84137, time: 0.01494\n",
      "Epoch: 79, train_loss: 0.84089, time: 0.01473\n",
      "Epoch: 80, train_loss: 0.85520, time: 0.01489\n",
      "Epoch: 81, train_loss: 0.85021, time: 0.01490\n",
      "Epoch: 82, train_loss: 0.83882, time: 0.01505\n",
      "Epoch: 83, train_loss: 0.83530, time: 0.01486\n",
      "Epoch: 84, train_loss: 0.84400, time: 0.01489\n",
      "Epoch: 85, train_loss: 0.83782, time: 0.01459\n",
      "Epoch: 86, train_loss: 0.84098, time: 0.01490\n",
      "Epoch: 87, train_loss: 0.83767, time: 0.01490\n",
      "Epoch: 88, train_loss: 0.83901, time: 0.01510\n",
      "Epoch: 89, train_loss: 0.84390, time: 0.01479\n",
      "Epoch: 90, train_loss: 0.83680, time: 0.01497\n",
      "Epoch: 91, train_loss: 0.83796, time: 0.01419\n",
      "Epoch: 92, train_loss: 0.84720, time: 0.01452\n",
      "Epoch: 93, train_loss: 0.83988, time: 0.01422\n",
      "Epoch: 94, train_loss: 0.83933, time: 0.01518\n",
      "Epoch: 95, train_loss: 0.84136, time: 0.01515\n",
      "Epoch: 96, train_loss: 0.84055, time: 0.01530\n",
      "Epoch: 97, train_loss: 0.84530, time: 0.01553\n",
      "Epoch: 98, train_loss: 0.83485, time: 0.01517\n",
      "Epoch: 99, train_loss: 0.84496, time: 0.01512\n",
      "Epoch: 100, train_loss: 0.84882, time: 0.01512\n",
      "Epoch: 101, train_loss: 0.82866, time: 0.01502\n",
      "Epoch: 102, train_loss: 0.83939, time: 0.01494\n",
      "Epoch: 103, train_loss: 0.84021, time: 0.01478\n",
      "Epoch: 104, train_loss: 0.83605, time: 0.01511\n",
      "Epoch: 105, train_loss: 0.84149, time: 0.01515\n",
      "Epoch: 106, train_loss: 0.85232, time: 0.01485\n",
      "Epoch: 107, train_loss: 0.84046, time: 0.01482\n",
      "Epoch: 108, train_loss: 0.83663, time: 0.01456\n",
      "Epoch: 109, train_loss: 0.83825, time: 0.01442\n",
      "Epoch: 110, train_loss: 0.83117, time: 0.01540\n",
      "Epoch: 111, train_loss: 0.83761, time: 0.01500\n",
      "Epoch: 112, train_loss: 0.84404, time: 0.01485\n",
      "Epoch: 113, train_loss: 0.83047, time: 0.01506\n",
      "Epoch: 114, train_loss: 0.83861, time: 0.01525\n",
      "Epoch: 115, train_loss: 0.83241, time: 0.01500\n",
      "Epoch: 116, train_loss: 0.83894, time: 0.01493\n",
      "Epoch: 117, train_loss: 0.82495, time: 0.01524\n",
      "Epoch: 118, train_loss: 0.84082, time: 0.01492\n",
      "Epoch: 119, train_loss: 0.83458, time: 0.01512\n",
      "Epoch: 120, train_loss: 0.83186, time: 0.01501\n",
      "Epoch: 121, train_loss: 0.82877, time: 0.01496\n",
      "Epoch: 122, train_loss: 0.84517, time: 0.01535\n",
      "Epoch: 123, train_loss: 0.83850, time: 0.01503\n",
      "Epoch: 124, train_loss: 0.83328, time: 0.01499\n",
      "Epoch: 125, train_loss: 0.82684, time: 0.01495\n",
      "Epoch: 126, train_loss: 0.84493, time: 0.01522\n",
      "Epoch: 127, train_loss: 0.83489, time: 0.01490\n",
      "Epoch: 128, train_loss: 0.83307, time: 0.01515\n",
      "Epoch: 129, train_loss: 0.83253, time: 0.01522\n",
      "Epoch: 130, train_loss: 0.83133, time: 0.01524\n",
      "Epoch: 131, train_loss: 0.85284, time: 0.01523\n",
      "Epoch: 132, train_loss: 0.83443, time: 0.01511\n",
      "Epoch: 133, train_loss: 0.84212, time: 0.01505\n",
      "Epoch: 134, train_loss: 0.83543, time: 0.01470\n",
      "Epoch: 135, train_loss: 0.83188, time: 0.01479\n",
      "Epoch: 136, train_loss: 0.83811, time: 0.01490\n",
      "Epoch: 137, train_loss: 0.83812, time: 0.01517\n",
      "Epoch: 138, train_loss: 0.84156, time: 0.01474\n",
      "Epoch: 139, train_loss: 0.82990, time: 0.01529\n",
      "Epoch: 140, train_loss: 0.83168, time: 0.01542\n",
      "Epoch: 141, train_loss: 0.82944, time: 0.01568\n",
      "Epoch: 142, train_loss: 0.82578, time: 0.01552\n",
      "Epoch: 143, train_loss: 0.83917, time: 0.01498\n",
      "Epoch: 144, train_loss: 0.82865, time: 0.01502\n",
      "Epoch: 145, train_loss: 0.84450, time: 0.01529\n",
      "Epoch: 146, train_loss: 0.84000, time: 0.01480\n",
      "Epoch: 147, train_loss: 0.83644, time: 0.01514\n",
      "Epoch: 148, train_loss: 0.82965, time: 0.01481\n",
      "Epoch: 149, train_loss: 0.82676, time: 0.01531\n",
      "Epoch: 150, train_loss: 0.84801, time: 0.01491\n",
      "Epoch: 151, train_loss: 0.82972, time: 0.01505\n",
      "Epoch: 152, train_loss: 0.82012, time: 0.01534\n",
      "Epoch: 153, train_loss: 0.84658, time: 0.01545\n",
      "Epoch: 154, train_loss: 0.85352, time: 0.01509\n",
      "Epoch: 155, train_loss: 0.85021, time: 0.01538\n",
      "Epoch: 156, train_loss: 0.83308, time: 0.01561\n",
      "Epoch: 157, train_loss: 0.82431, time: 0.01459\n",
      "Epoch: 158, train_loss: 0.83229, time: 0.01507\n",
      "Epoch: 159, train_loss: 0.83233, time: 0.01502\n",
      "Epoch: 160, train_loss: 0.83726, time: 0.01518\n",
      "Epoch: 161, train_loss: 0.83060, time: 0.01469\n",
      "Epoch: 162, train_loss: 0.83042, time: 0.01524\n",
      "Epoch: 163, train_loss: 0.82732, time: 0.01503\n",
      "Epoch: 164, train_loss: 0.84017, time: 0.01509\n",
      "Epoch: 165, train_loss: 0.84207, time: 0.01468\n",
      "Epoch: 166, train_loss: 0.84916, time: 0.01469\n",
      "Epoch: 167, train_loss: 0.84330, time: 0.01456\n",
      "Epoch: 168, train_loss: 0.84087, time: 0.01496\n",
      "Epoch: 169, train_loss: 0.83388, time: 0.01566\n",
      "Epoch: 170, train_loss: 0.84498, time: 0.01547\n",
      "Epoch: 171, train_loss: 0.84067, time: 0.01486\n",
      "Epoch: 172, train_loss: 0.84219, time: 0.01498\n",
      "Epoch: 173, train_loss: 0.83006, time: 0.01550\n",
      "Epoch: 174, train_loss: 0.84249, time: 0.01516\n",
      "Epoch: 175, train_loss: 0.82614, time: 0.01538\n",
      "Epoch: 176, train_loss: 0.82666, time: 0.01481\n",
      "Epoch: 177, train_loss: 0.82806, time: 0.01483\n",
      "Epoch: 178, train_loss: 0.83913, time: 0.01466\n",
      "Epoch: 179, train_loss: 0.83104, time: 0.01469\n",
      "Epoch: 180, train_loss: 0.84202, time: 0.01499\n",
      "Epoch: 181, train_loss: 0.83410, time: 0.01509\n",
      "Epoch: 182, train_loss: 0.82712, time: 0.01511\n",
      "Epoch: 183, train_loss: 0.83070, time: 0.01540\n",
      "Epoch: 184, train_loss: 0.84294, time: 0.01523\n",
      "Epoch: 185, train_loss: 0.84325, time: 0.01520\n",
      "Epoch: 186, train_loss: 0.84007, time: 0.01476\n",
      "Epoch: 187, train_loss: 0.83097, time: 0.01536\n",
      "Epoch: 188, train_loss: 0.83657, time: 0.01515\n",
      "Epoch: 189, train_loss: 0.83784, time: 0.01519\n",
      "Epoch: 190, train_loss: 0.82842, time: 0.01531\n",
      "Epoch: 191, train_loss: 0.83715, time: 0.01477\n",
      "Epoch: 192, train_loss: 0.84165, time: 0.01482\n",
      "Epoch: 193, train_loss: 0.82964, time: 0.01452\n",
      "Epoch: 194, train_loss: 0.83875, time: 0.01440\n",
      "Epoch: 195, train_loss: 0.83948, time: 0.01470\n",
      "Epoch: 196, train_loss: 0.83873, time: 0.01487\n",
      "Epoch: 197, train_loss: 0.84030, time: 0.01561\n",
      "Epoch: 198, train_loss: 0.83900, time: 0.01550\n",
      "Epoch: 199, train_loss: 0.83216, time: 0.01512\n",
      "Epoch: 200, train_loss: 0.85004, time: 0.01485\n",
      "pairwise precision 0.53564 recall 0.37497 f1 0.44113\n",
      "average until now [0.4019149345781455, 0.8241036077398632, 0.5403173543593405]\n",
      "54 names 246.84526920318604 avg time 4.571208688947889\n",
      "Loading ming_shi dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 262 nodes, 3126 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.95822, time: 0.11248\n",
      "Epoch: 2, train_loss: 0.94906, time: 0.01467\n",
      "Epoch: 3, train_loss: 0.95360, time: 0.01301\n",
      "Epoch: 4, train_loss: 0.95354, time: 0.01276\n",
      "Epoch: 5, train_loss: 0.95429, time: 0.01285\n",
      "Epoch: 6, train_loss: 0.95202, time: 0.01269\n",
      "Epoch: 7, train_loss: 0.95366, time: 0.01264\n",
      "Epoch: 8, train_loss: 0.95682, time: 0.01284\n",
      "Epoch: 9, train_loss: 0.95302, time: 0.01262\n",
      "Epoch: 10, train_loss: 0.95616, time: 0.01240\n",
      "Epoch: 11, train_loss: 0.95535, time: 0.01261\n",
      "Epoch: 12, train_loss: 0.95459, time: 0.01271\n",
      "Epoch: 13, train_loss: 0.95333, time: 0.01267\n",
      "Epoch: 14, train_loss: 0.95407, time: 0.01257\n",
      "Epoch: 15, train_loss: 0.95322, time: 0.01265\n",
      "Epoch: 16, train_loss: 0.95717, time: 0.01257\n",
      "Epoch: 17, train_loss: 0.95232, time: 0.01299\n",
      "Epoch: 18, train_loss: 0.95705, time: 0.01376\n",
      "Epoch: 19, train_loss: 0.95498, time: 0.01264\n",
      "Epoch: 20, train_loss: 0.95666, time: 0.01313\n",
      "Epoch: 21, train_loss: 0.95329, time: 0.01274\n",
      "Epoch: 22, train_loss: 0.95405, time: 0.01288\n",
      "Epoch: 23, train_loss: 0.95332, time: 0.01256\n",
      "Epoch: 24, train_loss: 0.95325, time: 0.01246\n",
      "Epoch: 25, train_loss: 0.95625, time: 0.01249\n",
      "Epoch: 26, train_loss: 0.95474, time: 0.01255\n",
      "Epoch: 27, train_loss: 0.95292, time: 0.01260\n",
      "Epoch: 28, train_loss: 0.95347, time: 0.01275\n",
      "Epoch: 29, train_loss: 0.95646, time: 0.01229\n",
      "Epoch: 30, train_loss: 0.95794, time: 0.01267\n",
      "Epoch: 31, train_loss: 0.95784, time: 0.01256\n",
      "Epoch: 32, train_loss: 0.95354, time: 0.01315\n",
      "Epoch: 33, train_loss: 0.95293, time: 0.01318\n",
      "Epoch: 34, train_loss: 0.95301, time: 0.01310\n",
      "Epoch: 35, train_loss: 0.95394, time: 0.01278\n",
      "Epoch: 36, train_loss: 0.95461, time: 0.01278\n",
      "Epoch: 37, train_loss: 0.95422, time: 0.01278\n",
      "Epoch: 38, train_loss: 0.95365, time: 0.01278\n",
      "Epoch: 39, train_loss: 0.95316, time: 0.01274\n",
      "Epoch: 40, train_loss: 0.95432, time: 0.01226\n",
      "Epoch: 41, train_loss: 0.95821, time: 0.01249\n",
      "Epoch: 42, train_loss: 0.95453, time: 0.01225\n",
      "Epoch: 43, train_loss: 0.95330, time: 0.01250\n",
      "Epoch: 44, train_loss: 0.95361, time: 0.01227\n",
      "Epoch: 45, train_loss: 0.95190, time: 0.01245\n",
      "Epoch: 46, train_loss: 0.95486, time: 0.01242\n",
      "Epoch: 47, train_loss: 0.95562, time: 0.01230\n",
      "Epoch: 48, train_loss: 0.95528, time: 0.01255\n",
      "Epoch: 49, train_loss: 0.95225, time: 0.01250\n",
      "Epoch: 50, train_loss: 0.95341, time: 0.01267\n",
      "Epoch: 51, train_loss: 0.95681, time: 0.01264\n",
      "Epoch: 52, train_loss: 0.95482, time: 0.01275\n",
      "Epoch: 53, train_loss: 0.95433, time: 0.01281\n",
      "Epoch: 54, train_loss: 0.95485, time: 0.01271\n",
      "Epoch: 55, train_loss: 0.95123, time: 0.01245\n",
      "Epoch: 56, train_loss: 0.95276, time: 0.01331\n",
      "Epoch: 57, train_loss: 0.95608, time: 0.01299\n",
      "Epoch: 58, train_loss: 0.95445, time: 0.01309\n",
      "Epoch: 59, train_loss: 0.95376, time: 0.01272\n",
      "Epoch: 60, train_loss: 0.95225, time: 0.01247\n",
      "Epoch: 61, train_loss: 0.95398, time: 0.01255\n",
      "Epoch: 62, train_loss: 0.95526, time: 0.01264\n",
      "Epoch: 63, train_loss: 0.95414, time: 0.01229\n",
      "Epoch: 64, train_loss: 0.95530, time: 0.01279\n",
      "Epoch: 65, train_loss: 0.95404, time: 0.01317\n",
      "Epoch: 66, train_loss: 0.95631, time: 0.01306\n",
      "Epoch: 67, train_loss: 0.95709, time: 0.01248\n",
      "Epoch: 68, train_loss: 0.95578, time: 0.01269\n",
      "Epoch: 69, train_loss: 0.95436, time: 0.01300\n",
      "Epoch: 70, train_loss: 0.95316, time: 0.01273\n",
      "Epoch: 71, train_loss: 0.95601, time: 0.01216\n",
      "Epoch: 72, train_loss: 0.95346, time: 0.01253\n",
      "Epoch: 73, train_loss: 0.95541, time: 0.01254\n",
      "Epoch: 74, train_loss: 0.95482, time: 0.01260\n",
      "Epoch: 75, train_loss: 0.95203, time: 0.01255\n",
      "Epoch: 76, train_loss: 0.95630, time: 0.01252\n",
      "Epoch: 77, train_loss: 0.95471, time: 0.01278\n",
      "Epoch: 78, train_loss: 0.95304, time: 0.01294\n",
      "Epoch: 79, train_loss: 0.95635, time: 0.01257\n",
      "Epoch: 80, train_loss: 0.95520, time: 0.01258\n",
      "Epoch: 81, train_loss: 0.95495, time: 0.01329\n",
      "Epoch: 82, train_loss: 0.95542, time: 0.01274\n",
      "Epoch: 83, train_loss: 0.95101, time: 0.01276\n",
      "Epoch: 84, train_loss: 0.95301, time: 0.01255\n",
      "Epoch: 85, train_loss: 0.95264, time: 0.01234\n",
      "Epoch: 86, train_loss: 0.95353, time: 0.01254\n",
      "Epoch: 87, train_loss: 0.95449, time: 0.01235\n",
      "Epoch: 88, train_loss: 0.95168, time: 0.01253\n",
      "Epoch: 89, train_loss: 0.95532, time: 0.01270\n",
      "Epoch: 90, train_loss: 0.95345, time: 0.01292\n",
      "Epoch: 91, train_loss: 0.95485, time: 0.01256\n",
      "Epoch: 92, train_loss: 0.95635, time: 0.01281\n",
      "Epoch: 93, train_loss: 0.95506, time: 0.01279\n",
      "Epoch: 94, train_loss: 0.95349, time: 0.01277\n",
      "Epoch: 95, train_loss: 0.95403, time: 0.01274\n",
      "Epoch: 96, train_loss: 0.95632, time: 0.01269\n",
      "Epoch: 97, train_loss: 0.95243, time: 0.01302\n",
      "Epoch: 98, train_loss: 0.95309, time: 0.01301\n",
      "Epoch: 99, train_loss: 0.95314, time: 0.01250\n",
      "Epoch: 100, train_loss: 0.95385, time: 0.01263\n",
      "Epoch: 101, train_loss: 0.95348, time: 0.01263\n",
      "Epoch: 102, train_loss: 0.95355, time: 0.01308\n",
      "Epoch: 103, train_loss: 0.95422, time: 0.01241\n",
      "Epoch: 104, train_loss: 0.95231, time: 0.01285\n",
      "Epoch: 105, train_loss: 0.95416, time: 0.01272\n",
      "Epoch: 106, train_loss: 0.95373, time: 0.01280\n",
      "Epoch: 107, train_loss: 0.95391, time: 0.01294\n",
      "Epoch: 108, train_loss: 0.95260, time: 0.01249\n",
      "Epoch: 109, train_loss: 0.95577, time: 0.01256\n",
      "Epoch: 110, train_loss: 0.95391, time: 0.01256\n",
      "Epoch: 111, train_loss: 0.95538, time: 0.01312\n",
      "Epoch: 112, train_loss: 0.95067, time: 0.01286\n",
      "Epoch: 113, train_loss: 0.95249, time: 0.01293\n",
      "Epoch: 114, train_loss: 0.95634, time: 0.01334\n",
      "Epoch: 115, train_loss: 0.95557, time: 0.01293\n",
      "Epoch: 116, train_loss: 0.95638, time: 0.01272\n",
      "Epoch: 117, train_loss: 0.95516, time: 0.01273\n",
      "Epoch: 118, train_loss: 0.95502, time: 0.01303\n",
      "Epoch: 119, train_loss: 0.95382, time: 0.01349\n",
      "Epoch: 120, train_loss: 0.95592, time: 0.01263\n",
      "Epoch: 121, train_loss: 0.95312, time: 0.01274\n",
      "Epoch: 122, train_loss: 0.95542, time: 0.01264\n",
      "Epoch: 123, train_loss: 0.95644, time: 0.01283\n",
      "Epoch: 124, train_loss: 0.95483, time: 0.01262\n",
      "Epoch: 125, train_loss: 0.95500, time: 0.01239\n",
      "Epoch: 126, train_loss: 0.95261, time: 0.01266\n",
      "Epoch: 127, train_loss: 0.95271, time: 0.01284\n",
      "Epoch: 128, train_loss: 0.95384, time: 0.01284\n",
      "Epoch: 129, train_loss: 0.95394, time: 0.01311\n",
      "Epoch: 130, train_loss: 0.95572, time: 0.01391\n",
      "Epoch: 131, train_loss: 0.95533, time: 0.01294\n",
      "Epoch: 132, train_loss: 0.95771, time: 0.01260\n",
      "Epoch: 133, train_loss: 0.95745, time: 0.01252\n",
      "Epoch: 134, train_loss: 0.95438, time: 0.01280\n",
      "Epoch: 135, train_loss: 0.95434, time: 0.01275\n",
      "Epoch: 136, train_loss: 0.95378, time: 0.01252\n",
      "Epoch: 137, train_loss: 0.95421, time: 0.01259\n",
      "Epoch: 138, train_loss: 0.95494, time: 0.01266\n",
      "Epoch: 139, train_loss: 0.95408, time: 0.01227\n",
      "Epoch: 140, train_loss: 0.95455, time: 0.01281\n",
      "Epoch: 141, train_loss: 0.95336, time: 0.01275\n",
      "Epoch: 142, train_loss: 0.95311, time: 0.01254\n",
      "Epoch: 143, train_loss: 0.95362, time: 0.01267\n",
      "Epoch: 144, train_loss: 0.95236, time: 0.01262\n",
      "Epoch: 145, train_loss: 0.95617, time: 0.01302\n",
      "Epoch: 146, train_loss: 0.95256, time: 0.01293\n",
      "Epoch: 147, train_loss: 0.95502, time: 0.01257\n",
      "Epoch: 148, train_loss: 0.95391, time: 0.01250\n",
      "Epoch: 149, train_loss: 0.95359, time: 0.01305\n",
      "Epoch: 150, train_loss: 0.95419, time: 0.01259\n",
      "Epoch: 151, train_loss: 0.95423, time: 0.01242\n",
      "Epoch: 152, train_loss: 0.95522, time: 0.01267\n",
      "Epoch: 153, train_loss: 0.95171, time: 0.01315\n",
      "Epoch: 154, train_loss: 0.95331, time: 0.01268\n",
      "Epoch: 155, train_loss: 0.95540, time: 0.01262\n",
      "Epoch: 156, train_loss: 0.95475, time: 0.01275\n",
      "Epoch: 157, train_loss: 0.95430, time: 0.01277\n",
      "Epoch: 158, train_loss: 0.95392, time: 0.01256\n",
      "Epoch: 159, train_loss: 0.95556, time: 0.01291\n",
      "Epoch: 160, train_loss: 0.95569, time: 0.01298\n",
      "Epoch: 161, train_loss: 0.95586, time: 0.01319\n",
      "Epoch: 162, train_loss: 0.95423, time: 0.01292\n",
      "Epoch: 163, train_loss: 0.95482, time: 0.01261\n",
      "Epoch: 164, train_loss: 0.95478, time: 0.01234\n",
      "Epoch: 165, train_loss: 0.95430, time: 0.01258\n",
      "Epoch: 166, train_loss: 0.95438, time: 0.01275\n",
      "Epoch: 167, train_loss: 0.95421, time: 0.01257\n",
      "Epoch: 168, train_loss: 0.95475, time: 0.01256\n",
      "Epoch: 169, train_loss: 0.95391, time: 0.01246\n",
      "Epoch: 170, train_loss: 0.95063, time: 0.01305\n",
      "Epoch: 171, train_loss: 0.95403, time: 0.01297\n",
      "Epoch: 172, train_loss: 0.95522, time: 0.01250\n",
      "Epoch: 173, train_loss: 0.95229, time: 0.01301\n",
      "Epoch: 174, train_loss: 0.95463, time: 0.01278\n",
      "Epoch: 175, train_loss: 0.95519, time: 0.01265\n",
      "Epoch: 176, train_loss: 0.95684, time: 0.01289\n",
      "Epoch: 177, train_loss: 0.95475, time: 0.01286\n",
      "Epoch: 178, train_loss: 0.95698, time: 0.01267\n",
      "Epoch: 179, train_loss: 0.95434, time: 0.01238\n",
      "Epoch: 180, train_loss: 0.95113, time: 0.01241\n",
      "Epoch: 181, train_loss: 0.95488, time: 0.01252\n",
      "Epoch: 182, train_loss: 0.95411, time: 0.01274\n",
      "Epoch: 183, train_loss: 0.95441, time: 0.01235\n",
      "Epoch: 184, train_loss: 0.95271, time: 0.01275\n",
      "Epoch: 185, train_loss: 0.95447, time: 0.01235\n",
      "Epoch: 186, train_loss: 0.95365, time: 0.01226\n",
      "Epoch: 187, train_loss: 0.95036, time: 0.01231\n",
      "Epoch: 188, train_loss: 0.95582, time: 0.01328\n",
      "Epoch: 189, train_loss: 0.95383, time: 0.01269\n",
      "Epoch: 190, train_loss: 0.95474, time: 0.01281\n",
      "Epoch: 191, train_loss: 0.95187, time: 0.01260\n",
      "Epoch: 192, train_loss: 0.95412, time: 0.01307\n",
      "Epoch: 193, train_loss: 0.95486, time: 0.01309\n",
      "Epoch: 194, train_loss: 0.95439, time: 0.01268\n",
      "Epoch: 195, train_loss: 0.95504, time: 0.01288\n",
      "Epoch: 196, train_loss: 0.95416, time: 0.01215\n",
      "Epoch: 197, train_loss: 0.95310, time: 0.01278\n",
      "Epoch: 198, train_loss: 0.95647, time: 0.01219\n",
      "Epoch: 199, train_loss: 0.95451, time: 0.01234\n",
      "Epoch: 200, train_loss: 0.95292, time: 0.01223\n",
      "pairwise precision 0.56180 recall 0.95581 f1 0.70766\n",
      "average until now [0.4048220060858799, 0.8264981820938666, 0.5434567796637632]\n",
      "55 names 249.55167961120605 avg time 4.537303265658292\n",
      "Loading chunyan_liu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 222 nodes, 1139 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98323, time: 0.11073\n",
      "Epoch: 2, train_loss: 0.98087, time: 0.01276\n",
      "Epoch: 3, train_loss: 0.98291, time: 0.01122\n",
      "Epoch: 4, train_loss: 0.97771, time: 0.01117\n",
      "Epoch: 5, train_loss: 0.97954, time: 0.01072\n",
      "Epoch: 6, train_loss: 0.97592, time: 0.01065\n",
      "Epoch: 7, train_loss: 0.97693, time: 0.01132\n",
      "Epoch: 8, train_loss: 0.97707, time: 0.01094\n",
      "Epoch: 9, train_loss: 0.97454, time: 0.01074\n",
      "Epoch: 10, train_loss: 0.97514, time: 0.01106\n",
      "Epoch: 11, train_loss: 0.97376, time: 0.01066\n",
      "Epoch: 12, train_loss: 0.97867, time: 0.01087\n",
      "Epoch: 13, train_loss: 0.97777, time: 0.01120\n",
      "Epoch: 14, train_loss: 0.97850, time: 0.01074\n",
      "Epoch: 15, train_loss: 0.97561, time: 0.01098\n",
      "Epoch: 16, train_loss: 0.97747, time: 0.01071\n",
      "Epoch: 17, train_loss: 0.97793, time: 0.01099\n",
      "Epoch: 18, train_loss: 0.97690, time: 0.01076\n",
      "Epoch: 19, train_loss: 0.97811, time: 0.01103\n",
      "Epoch: 20, train_loss: 0.97557, time: 0.01132\n",
      "Epoch: 21, train_loss: 0.97494, time: 0.01129\n",
      "Epoch: 22, train_loss: 0.97723, time: 0.01056\n",
      "Epoch: 23, train_loss: 0.97816, time: 0.01150\n",
      "Epoch: 24, train_loss: 0.97752, time: 0.01058\n",
      "Epoch: 25, train_loss: 0.97787, time: 0.01075\n",
      "Epoch: 26, train_loss: 0.97638, time: 0.01079\n",
      "Epoch: 27, train_loss: 0.97869, time: 0.01225\n",
      "Epoch: 28, train_loss: 0.97714, time: 0.01087\n",
      "Epoch: 29, train_loss: 0.97708, time: 0.01108\n",
      "Epoch: 30, train_loss: 0.97813, time: 0.01113\n",
      "Epoch: 31, train_loss: 0.97481, time: 0.01102\n",
      "Epoch: 32, train_loss: 0.97849, time: 0.01119\n",
      "Epoch: 33, train_loss: 0.97764, time: 0.01113\n",
      "Epoch: 34, train_loss: 0.97897, time: 0.01085\n",
      "Epoch: 35, train_loss: 0.97536, time: 0.01116\n",
      "Epoch: 36, train_loss: 0.97860, time: 0.01085\n",
      "Epoch: 37, train_loss: 0.97778, time: 0.01089\n",
      "Epoch: 38, train_loss: 0.98047, time: 0.01086\n",
      "Epoch: 39, train_loss: 0.97701, time: 0.01132\n",
      "Epoch: 40, train_loss: 0.97654, time: 0.01092\n",
      "Epoch: 41, train_loss: 0.97833, time: 0.01074\n",
      "Epoch: 42, train_loss: 0.97509, time: 0.01093\n",
      "Epoch: 43, train_loss: 0.97542, time: 0.01088\n",
      "Epoch: 44, train_loss: 0.97701, time: 0.01073\n",
      "Epoch: 45, train_loss: 0.97593, time: 0.01116\n",
      "Epoch: 46, train_loss: 0.97450, time: 0.01076\n",
      "Epoch: 47, train_loss: 0.97653, time: 0.01130\n",
      "Epoch: 48, train_loss: 0.97498, time: 0.01089\n",
      "Epoch: 49, train_loss: 0.97775, time: 0.01104\n",
      "Epoch: 50, train_loss: 0.97593, time: 0.01078\n",
      "Epoch: 51, train_loss: 0.97675, time: 0.01110\n",
      "Epoch: 52, train_loss: 0.97711, time: 0.01091\n",
      "Epoch: 53, train_loss: 0.97729, time: 0.01131\n",
      "Epoch: 54, train_loss: 0.97551, time: 0.01069\n",
      "Epoch: 55, train_loss: 0.97824, time: 0.01105\n",
      "Epoch: 56, train_loss: 0.97518, time: 0.01068\n",
      "Epoch: 57, train_loss: 0.97563, time: 0.01118\n",
      "Epoch: 58, train_loss: 0.97506, time: 0.01120\n",
      "Epoch: 59, train_loss: 0.97462, time: 0.01101\n",
      "Epoch: 60, train_loss: 0.97853, time: 0.01097\n",
      "Epoch: 61, train_loss: 0.97942, time: 0.01100\n",
      "Epoch: 62, train_loss: 0.97520, time: 0.01092\n",
      "Epoch: 63, train_loss: 0.97784, time: 0.01099\n",
      "Epoch: 64, train_loss: 0.97475, time: 0.01107\n",
      "Epoch: 65, train_loss: 0.97414, time: 0.01102\n",
      "Epoch: 66, train_loss: 0.97334, time: 0.01096\n",
      "Epoch: 67, train_loss: 0.97498, time: 0.01109\n",
      "Epoch: 68, train_loss: 0.97605, time: 0.01099\n",
      "Epoch: 69, train_loss: 0.97606, time: 0.01082\n",
      "Epoch: 70, train_loss: 0.97960, time: 0.01133\n",
      "Epoch: 71, train_loss: 0.97099, time: 0.01095\n",
      "Epoch: 72, train_loss: 0.97250, time: 0.01086\n",
      "Epoch: 73, train_loss: 0.97741, time: 0.01114\n",
      "Epoch: 74, train_loss: 0.97666, time: 0.01071\n",
      "Epoch: 75, train_loss: 0.97516, time: 0.01102\n",
      "Epoch: 76, train_loss: 0.97974, time: 0.01072\n",
      "Epoch: 77, train_loss: 0.97843, time: 0.01140\n",
      "Epoch: 78, train_loss: 0.97732, time: 0.01102\n",
      "Epoch: 79, train_loss: 0.97657, time: 0.01113\n",
      "Epoch: 80, train_loss: 0.97771, time: 0.01102\n",
      "Epoch: 81, train_loss: 0.97832, time: 0.01099\n",
      "Epoch: 82, train_loss: 0.97541, time: 0.01085\n",
      "Epoch: 83, train_loss: 0.97596, time: 0.01093\n",
      "Epoch: 84, train_loss: 0.97639, time: 0.01084\n",
      "Epoch: 85, train_loss: 0.97849, time: 0.01095\n",
      "Epoch: 86, train_loss: 0.97820, time: 0.01096\n",
      "Epoch: 87, train_loss: 0.97768, time: 0.01136\n",
      "Epoch: 88, train_loss: 0.97566, time: 0.01087\n",
      "Epoch: 89, train_loss: 0.97905, time: 0.01089\n",
      "Epoch: 90, train_loss: 0.97532, time: 0.01102\n",
      "Epoch: 91, train_loss: 0.97592, time: 0.01093\n",
      "Epoch: 92, train_loss: 0.97675, time: 0.01106\n",
      "Epoch: 93, train_loss: 0.97424, time: 0.01097\n",
      "Epoch: 94, train_loss: 0.97874, time: 0.01071\n",
      "Epoch: 95, train_loss: 0.97900, time: 0.01102\n",
      "Epoch: 96, train_loss: 0.97805, time: 0.01143\n",
      "Epoch: 97, train_loss: 0.97470, time: 0.01105\n",
      "Epoch: 98, train_loss: 0.97576, time: 0.01061\n",
      "Epoch: 99, train_loss: 0.97651, time: 0.01079\n",
      "Epoch: 100, train_loss: 0.97777, time: 0.01073\n",
      "Epoch: 101, train_loss: 0.97627, time: 0.01063\n",
      "Epoch: 102, train_loss: 0.97857, time: 0.01095\n",
      "Epoch: 103, train_loss: 0.97733, time: 0.01097\n",
      "Epoch: 104, train_loss: 0.97548, time: 0.01111\n",
      "Epoch: 105, train_loss: 0.97757, time: 0.01087\n",
      "Epoch: 106, train_loss: 0.97925, time: 0.01113\n",
      "Epoch: 107, train_loss: 0.97431, time: 0.01093\n",
      "Epoch: 108, train_loss: 0.97459, time: 0.01088\n",
      "Epoch: 109, train_loss: 0.97454, time: 0.01099\n",
      "Epoch: 110, train_loss: 0.97567, time: 0.01076\n",
      "Epoch: 111, train_loss: 0.97744, time: 0.01104\n",
      "Epoch: 112, train_loss: 0.97610, time: 0.01094\n",
      "Epoch: 113, train_loss: 0.97313, time: 0.01090\n",
      "Epoch: 114, train_loss: 0.97408, time: 0.01061\n",
      "Epoch: 115, train_loss: 0.97570, time: 0.01156\n",
      "Epoch: 116, train_loss: 0.97409, time: 0.01115\n",
      "Epoch: 117, train_loss: 0.97580, time: 0.01098\n",
      "Epoch: 118, train_loss: 0.97724, time: 0.01083\n",
      "Epoch: 119, train_loss: 0.97798, time: 0.01103\n",
      "Epoch: 120, train_loss: 0.97350, time: 0.01070\n",
      "Epoch: 121, train_loss: 0.97830, time: 0.01102\n",
      "Epoch: 122, train_loss: 0.97478, time: 0.01117\n",
      "Epoch: 123, train_loss: 0.97528, time: 0.01074\n",
      "Epoch: 124, train_loss: 0.97592, time: 0.01098\n",
      "Epoch: 125, train_loss: 0.97694, time: 0.01101\n",
      "Epoch: 126, train_loss: 0.97520, time: 0.01072\n",
      "Epoch: 127, train_loss: 0.97536, time: 0.01079\n",
      "Epoch: 128, train_loss: 0.97567, time: 0.01086\n",
      "Epoch: 129, train_loss: 0.97359, time: 0.01075\n",
      "Epoch: 130, train_loss: 0.97933, time: 0.01097\n",
      "Epoch: 131, train_loss: 0.97508, time: 0.01088\n",
      "Epoch: 132, train_loss: 0.97808, time: 0.01088\n",
      "Epoch: 133, train_loss: 0.97424, time: 0.01075\n",
      "Epoch: 134, train_loss: 0.97657, time: 0.01124\n",
      "Epoch: 135, train_loss: 0.97601, time: 0.01096\n",
      "Epoch: 136, train_loss: 0.97710, time: 0.01088\n",
      "Epoch: 137, train_loss: 0.97374, time: 0.01102\n",
      "Epoch: 138, train_loss: 0.97456, time: 0.01107\n",
      "Epoch: 139, train_loss: 0.97604, time: 0.01095\n",
      "Epoch: 140, train_loss: 0.97856, time: 0.01081\n",
      "Epoch: 141, train_loss: 0.97350, time: 0.01081\n",
      "Epoch: 142, train_loss: 0.97506, time: 0.01107\n",
      "Epoch: 143, train_loss: 0.97655, time: 0.01076\n",
      "Epoch: 144, train_loss: 0.97466, time: 0.01101\n",
      "Epoch: 145, train_loss: 0.97183, time: 0.01061\n",
      "Epoch: 146, train_loss: 0.97744, time: 0.01102\n",
      "Epoch: 147, train_loss: 0.97617, time: 0.01093\n",
      "Epoch: 148, train_loss: 0.97554, time: 0.01083\n",
      "Epoch: 149, train_loss: 0.98095, time: 0.01060\n",
      "Epoch: 150, train_loss: 0.97814, time: 0.01076\n",
      "Epoch: 151, train_loss: 0.97789, time: 0.01082\n",
      "Epoch: 152, train_loss: 0.97614, time: 0.01099\n",
      "Epoch: 153, train_loss: 0.97531, time: 0.01135\n",
      "Epoch: 154, train_loss: 0.97497, time: 0.01103\n",
      "Epoch: 155, train_loss: 0.97685, time: 0.01100\n",
      "Epoch: 156, train_loss: 0.97534, time: 0.01090\n",
      "Epoch: 157, train_loss: 0.97162, time: 0.01120\n",
      "Epoch: 158, train_loss: 0.97556, time: 0.01106\n",
      "Epoch: 159, train_loss: 0.97816, time: 0.01113\n",
      "Epoch: 160, train_loss: 0.97632, time: 0.01067\n",
      "Epoch: 161, train_loss: 0.97483, time: 0.01079\n",
      "Epoch: 162, train_loss: 0.97470, time: 0.01067\n",
      "Epoch: 163, train_loss: 0.97474, time: 0.01090\n",
      "Epoch: 164, train_loss: 0.97665, time: 0.01102\n",
      "Epoch: 165, train_loss: 0.97619, time: 0.01073\n",
      "Epoch: 166, train_loss: 0.97495, time: 0.01098\n",
      "Epoch: 167, train_loss: 0.97588, time: 0.01063\n",
      "Epoch: 168, train_loss: 0.97368, time: 0.01085\n",
      "Epoch: 169, train_loss: 0.97564, time: 0.01064\n",
      "Epoch: 170, train_loss: 0.97504, time: 0.01074\n",
      "Epoch: 171, train_loss: 0.97501, time: 0.01060\n",
      "Epoch: 172, train_loss: 0.97472, time: 0.01096\n",
      "Epoch: 173, train_loss: 0.97735, time: 0.01072\n",
      "Epoch: 174, train_loss: 0.97350, time: 0.01101\n",
      "Epoch: 175, train_loss: 0.97791, time: 0.01083\n",
      "Epoch: 176, train_loss: 0.97750, time: 0.01099\n",
      "Epoch: 177, train_loss: 0.97948, time: 0.01098\n",
      "Epoch: 178, train_loss: 0.97351, time: 0.01076\n",
      "Epoch: 179, train_loss: 0.97490, time: 0.01100\n",
      "Epoch: 180, train_loss: 0.97689, time: 0.01075\n",
      "Epoch: 181, train_loss: 0.97785, time: 0.01078\n",
      "Epoch: 182, train_loss: 0.97603, time: 0.01085\n",
      "Epoch: 183, train_loss: 0.97725, time: 0.01084\n",
      "Epoch: 184, train_loss: 0.97612, time: 0.01093\n",
      "Epoch: 185, train_loss: 0.97752, time: 0.01088\n",
      "Epoch: 186, train_loss: 0.97558, time: 0.01073\n",
      "Epoch: 187, train_loss: 0.97383, time: 0.01100\n",
      "Epoch: 188, train_loss: 0.97360, time: 0.01075\n",
      "Epoch: 189, train_loss: 0.97465, time: 0.01118\n",
      "Epoch: 190, train_loss: 0.97449, time: 0.01077\n",
      "Epoch: 191, train_loss: 0.97716, time: 0.01147\n",
      "Epoch: 192, train_loss: 0.97678, time: 0.01102\n",
      "Epoch: 193, train_loss: 0.97275, time: 0.01141\n",
      "Epoch: 194, train_loss: 0.97468, time: 0.01097\n",
      "Epoch: 195, train_loss: 0.97450, time: 0.01120\n",
      "Epoch: 196, train_loss: 0.97533, time: 0.01090\n",
      "Epoch: 197, train_loss: 0.97479, time: 0.01098\n",
      "Epoch: 198, train_loss: 0.97540, time: 0.01068\n",
      "Epoch: 199, train_loss: 0.97755, time: 0.01101\n",
      "Epoch: 200, train_loss: 0.97715, time: 0.01097\n",
      "pairwise precision 0.39137 recall 0.96720 f1 0.55726\n",
      "average until now [0.40458186744409413, 0.8290106496951173, 0.5437819573719771]\n",
      "56 names 251.9035747051239 avg time 4.498278119734356\n",
      "Loading shuai_yuan dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 310 nodes, 2162 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98147, time: 0.11370\n",
      "Epoch: 2, train_loss: 0.97746, time: 0.01531\n",
      "Epoch: 3, train_loss: 0.98004, time: 0.01365\n",
      "Epoch: 4, train_loss: 0.97436, time: 0.01363\n",
      "Epoch: 5, train_loss: 0.97799, time: 0.01394\n",
      "Epoch: 6, train_loss: 0.97544, time: 0.01374\n",
      "Epoch: 7, train_loss: 0.97683, time: 0.01381\n",
      "Epoch: 8, train_loss: 0.97773, time: 0.01355\n",
      "Epoch: 9, train_loss: 0.97692, time: 0.01323\n",
      "Epoch: 10, train_loss: 0.97633, time: 0.01325\n",
      "Epoch: 11, train_loss: 0.97415, time: 0.01294\n",
      "Epoch: 12, train_loss: 0.97669, time: 0.01374\n",
      "Epoch: 13, train_loss: 0.97602, time: 0.01338\n",
      "Epoch: 14, train_loss: 0.97723, time: 0.01333\n",
      "Epoch: 15, train_loss: 0.97638, time: 0.01329\n",
      "Epoch: 16, train_loss: 0.97461, time: 0.01365\n",
      "Epoch: 17, train_loss: 0.97580, time: 0.01349\n",
      "Epoch: 18, train_loss: 0.97821, time: 0.01358\n",
      "Epoch: 19, train_loss: 0.97740, time: 0.01351\n",
      "Epoch: 20, train_loss: 0.97528, time: 0.01367\n",
      "Epoch: 21, train_loss: 0.97680, time: 0.01346\n",
      "Epoch: 22, train_loss: 0.97460, time: 0.01361\n",
      "Epoch: 23, train_loss: 0.97654, time: 0.01363\n",
      "Epoch: 24, train_loss: 0.97620, time: 0.01336\n",
      "Epoch: 25, train_loss: 0.97907, time: 0.01355\n",
      "Epoch: 26, train_loss: 0.97636, time: 0.01372\n",
      "Epoch: 27, train_loss: 0.97558, time: 0.01353\n",
      "Epoch: 28, train_loss: 0.97767, time: 0.01324\n",
      "Epoch: 29, train_loss: 0.97599, time: 0.01368\n",
      "Epoch: 30, train_loss: 0.97591, time: 0.01378\n",
      "Epoch: 31, train_loss: 0.97476, time: 0.01397\n",
      "Epoch: 32, train_loss: 0.97622, time: 0.01395\n",
      "Epoch: 33, train_loss: 0.98006, time: 0.01337\n",
      "Epoch: 34, train_loss: 0.97473, time: 0.01310\n",
      "Epoch: 35, train_loss: 0.97643, time: 0.01356\n",
      "Epoch: 36, train_loss: 0.97665, time: 0.01340\n",
      "Epoch: 37, train_loss: 0.97649, time: 0.01365\n",
      "Epoch: 38, train_loss: 0.97617, time: 0.01363\n",
      "Epoch: 39, train_loss: 0.97676, time: 0.01239\n",
      "Epoch: 40, train_loss: 0.97531, time: 0.01365\n",
      "Epoch: 41, train_loss: 0.97603, time: 0.01353\n",
      "Epoch: 42, train_loss: 0.97648, time: 0.01346\n",
      "Epoch: 43, train_loss: 0.97494, time: 0.01269\n",
      "Epoch: 44, train_loss: 0.97631, time: 0.01317\n",
      "Epoch: 45, train_loss: 0.97539, time: 0.01356\n",
      "Epoch: 46, train_loss: 0.97599, time: 0.01329\n",
      "Epoch: 47, train_loss: 0.97734, time: 0.01392\n",
      "Epoch: 48, train_loss: 0.97484, time: 0.01335\n",
      "Epoch: 49, train_loss: 0.97503, time: 0.01343\n",
      "Epoch: 50, train_loss: 0.97568, time: 0.01314\n",
      "Epoch: 51, train_loss: 0.97523, time: 0.01322\n",
      "Epoch: 52, train_loss: 0.97723, time: 0.01320\n",
      "Epoch: 53, train_loss: 0.97650, time: 0.01326\n",
      "Epoch: 54, train_loss: 0.97581, time: 0.01342\n",
      "Epoch: 55, train_loss: 0.97538, time: 0.01368\n",
      "Epoch: 56, train_loss: 0.97520, time: 0.01275\n",
      "Epoch: 57, train_loss: 0.97509, time: 0.01384\n",
      "Epoch: 58, train_loss: 0.97569, time: 0.01306\n",
      "Epoch: 59, train_loss: 0.97696, time: 0.01370\n",
      "Epoch: 60, train_loss: 0.97717, time: 0.01364\n",
      "Epoch: 61, train_loss: 0.97565, time: 0.01330\n",
      "Epoch: 62, train_loss: 0.97466, time: 0.01406\n",
      "Epoch: 63, train_loss: 0.97549, time: 0.01364\n",
      "Epoch: 64, train_loss: 0.97538, time: 0.01323\n",
      "Epoch: 65, train_loss: 0.97757, time: 0.01367\n",
      "Epoch: 66, train_loss: 0.97340, time: 0.01226\n",
      "Epoch: 67, train_loss: 0.97902, time: 0.01314\n",
      "Epoch: 68, train_loss: 0.97706, time: 0.01314\n",
      "Epoch: 69, train_loss: 0.97464, time: 0.01323\n",
      "Epoch: 70, train_loss: 0.97525, time: 0.01335\n",
      "Epoch: 71, train_loss: 0.97679, time: 0.01349\n",
      "Epoch: 72, train_loss: 0.97400, time: 0.01318\n",
      "Epoch: 73, train_loss: 0.97470, time: 0.01319\n",
      "Epoch: 74, train_loss: 0.97575, time: 0.01350\n",
      "Epoch: 75, train_loss: 0.97565, time: 0.01338\n",
      "Epoch: 76, train_loss: 0.97464, time: 0.01366\n",
      "Epoch: 77, train_loss: 0.97827, time: 0.01350\n",
      "Epoch: 78, train_loss: 0.97533, time: 0.01329\n",
      "Epoch: 79, train_loss: 0.97613, time: 0.01277\n",
      "Epoch: 80, train_loss: 0.97491, time: 0.01321\n",
      "Epoch: 81, train_loss: 0.97618, time: 0.01347\n",
      "Epoch: 82, train_loss: 0.97512, time: 0.01356\n",
      "Epoch: 83, train_loss: 0.97715, time: 0.01346\n",
      "Epoch: 84, train_loss: 0.97482, time: 0.01346\n",
      "Epoch: 85, train_loss: 0.97487, time: 0.01334\n",
      "Epoch: 86, train_loss: 0.97470, time: 0.01364\n",
      "Epoch: 87, train_loss: 0.97686, time: 0.01276\n",
      "Epoch: 88, train_loss: 0.97587, time: 0.01358\n",
      "Epoch: 89, train_loss: 0.97500, time: 0.01324\n",
      "Epoch: 90, train_loss: 0.97569, time: 0.01363\n",
      "Epoch: 91, train_loss: 0.97810, time: 0.01347\n",
      "Epoch: 92, train_loss: 0.97532, time: 0.01372\n",
      "Epoch: 93, train_loss: 0.97679, time: 0.01377\n",
      "Epoch: 94, train_loss: 0.97648, time: 0.01365\n",
      "Epoch: 95, train_loss: 0.97524, time: 0.01360\n",
      "Epoch: 96, train_loss: 0.97683, time: 0.01373\n",
      "Epoch: 97, train_loss: 0.97493, time: 0.01316\n",
      "Epoch: 98, train_loss: 0.97377, time: 0.01395\n",
      "Epoch: 99, train_loss: 0.97673, time: 0.01333\n",
      "Epoch: 100, train_loss: 0.97830, time: 0.01386\n",
      "Epoch: 101, train_loss: 0.97597, time: 0.01319\n",
      "Epoch: 102, train_loss: 0.97924, time: 0.01406\n",
      "Epoch: 103, train_loss: 0.97690, time: 0.01396\n",
      "Epoch: 104, train_loss: 0.97609, time: 0.01356\n",
      "Epoch: 105, train_loss: 0.97438, time: 0.01336\n",
      "Epoch: 106, train_loss: 0.97770, time: 0.01367\n",
      "Epoch: 107, train_loss: 0.97579, time: 0.01345\n",
      "Epoch: 108, train_loss: 0.97647, time: 0.01310\n",
      "Epoch: 109, train_loss: 0.97598, time: 0.01271\n",
      "Epoch: 110, train_loss: 0.97790, time: 0.01340\n",
      "Epoch: 111, train_loss: 0.97664, time: 0.01377\n",
      "Epoch: 112, train_loss: 0.97833, time: 0.01359\n",
      "Epoch: 113, train_loss: 0.97549, time: 0.01277\n",
      "Epoch: 114, train_loss: 0.97468, time: 0.01290\n",
      "Epoch: 115, train_loss: 0.97427, time: 0.01319\n",
      "Epoch: 116, train_loss: 0.97401, time: 0.01302\n",
      "Epoch: 117, train_loss: 0.97735, time: 0.01340\n",
      "Epoch: 118, train_loss: 0.97620, time: 0.01277\n",
      "Epoch: 119, train_loss: 0.97570, time: 0.01332\n",
      "Epoch: 120, train_loss: 0.97594, time: 0.01291\n",
      "Epoch: 121, train_loss: 0.97400, time: 0.01303\n",
      "Epoch: 122, train_loss: 0.97403, time: 0.01350\n",
      "Epoch: 123, train_loss: 0.97517, time: 0.01324\n",
      "Epoch: 124, train_loss: 0.97453, time: 0.01399\n",
      "Epoch: 125, train_loss: 0.97392, time: 0.01378\n",
      "Epoch: 126, train_loss: 0.97635, time: 0.01357\n",
      "Epoch: 127, train_loss: 0.97731, time: 0.01362\n",
      "Epoch: 128, train_loss: 0.97583, time: 0.01300\n",
      "Epoch: 129, train_loss: 0.97678, time: 0.01359\n",
      "Epoch: 130, train_loss: 0.97458, time: 0.01371\n",
      "Epoch: 131, train_loss: 0.97779, time: 0.01367\n",
      "Epoch: 132, train_loss: 0.97715, time: 0.01299\n",
      "Epoch: 133, train_loss: 0.97513, time: 0.01350\n",
      "Epoch: 134, train_loss: 0.97664, time: 0.01356\n",
      "Epoch: 135, train_loss: 0.97515, time: 0.01353\n",
      "Epoch: 136, train_loss: 0.97555, time: 0.01369\n",
      "Epoch: 137, train_loss: 0.97669, time: 0.01293\n",
      "Epoch: 138, train_loss: 0.97637, time: 0.01370\n",
      "Epoch: 139, train_loss: 0.97402, time: 0.01383\n",
      "Epoch: 140, train_loss: 0.97602, time: 0.01326\n",
      "Epoch: 141, train_loss: 0.97609, time: 0.01344\n",
      "Epoch: 142, train_loss: 0.97584, time: 0.01340\n",
      "Epoch: 143, train_loss: 0.97551, time: 0.01382\n",
      "Epoch: 144, train_loss: 0.97575, time: 0.01326\n",
      "Epoch: 145, train_loss: 0.97590, time: 0.01351\n",
      "Epoch: 146, train_loss: 0.97739, time: 0.01322\n",
      "Epoch: 147, train_loss: 0.97554, time: 0.01357\n",
      "Epoch: 148, train_loss: 0.97651, time: 0.01374\n",
      "Epoch: 149, train_loss: 0.97675, time: 0.01330\n",
      "Epoch: 150, train_loss: 0.97727, time: 0.01319\n",
      "Epoch: 151, train_loss: 0.97567, time: 0.01352\n",
      "Epoch: 152, train_loss: 0.97495, time: 0.01395\n",
      "Epoch: 153, train_loss: 0.97774, time: 0.01328\n",
      "Epoch: 154, train_loss: 0.97578, time: 0.01411\n",
      "Epoch: 155, train_loss: 0.97740, time: 0.01357\n",
      "Epoch: 156, train_loss: 0.97571, time: 0.01335\n",
      "Epoch: 157, train_loss: 0.97605, time: 0.01332\n",
      "Epoch: 158, train_loss: 0.97851, time: 0.01374\n",
      "Epoch: 159, train_loss: 0.97627, time: 0.01315\n",
      "Epoch: 160, train_loss: 0.97586, time: 0.01391\n",
      "Epoch: 161, train_loss: 0.97715, time: 0.01336\n",
      "Epoch: 162, train_loss: 0.97601, time: 0.01322\n",
      "Epoch: 163, train_loss: 0.97661, time: 0.01372\n",
      "Epoch: 164, train_loss: 0.97695, time: 0.01363\n",
      "Epoch: 165, train_loss: 0.97471, time: 0.01349\n",
      "Epoch: 166, train_loss: 0.97596, time: 0.01378\n",
      "Epoch: 167, train_loss: 0.97638, time: 0.01349\n",
      "Epoch: 168, train_loss: 0.97484, time: 0.01294\n",
      "Epoch: 169, train_loss: 0.97702, time: 0.01416\n",
      "Epoch: 170, train_loss: 0.97598, time: 0.01370\n",
      "Epoch: 171, train_loss: 0.97636, time: 0.01351\n",
      "Epoch: 172, train_loss: 0.97576, time: 0.01307\n",
      "Epoch: 173, train_loss: 0.97472, time: 0.01392\n",
      "Epoch: 174, train_loss: 0.97525, time: 0.01368\n",
      "Epoch: 175, train_loss: 0.97567, time: 0.01338\n",
      "Epoch: 176, train_loss: 0.97638, time: 0.01336\n",
      "Epoch: 177, train_loss: 0.97549, time: 0.01340\n",
      "Epoch: 178, train_loss: 0.97439, time: 0.01350\n",
      "Epoch: 179, train_loss: 0.97754, time: 0.01351\n",
      "Epoch: 180, train_loss: 0.97737, time: 0.01360\n",
      "Epoch: 181, train_loss: 0.97747, time: 0.01389\n",
      "Epoch: 182, train_loss: 0.97799, time: 0.01354\n",
      "Epoch: 183, train_loss: 0.97654, time: 0.01353\n",
      "Epoch: 184, train_loss: 0.97598, time: 0.01391\n",
      "Epoch: 185, train_loss: 0.97635, time: 0.01342\n",
      "Epoch: 186, train_loss: 0.97749, time: 0.01367\n",
      "Epoch: 187, train_loss: 0.97517, time: 0.01366\n",
      "Epoch: 188, train_loss: 0.97527, time: 0.01353\n",
      "Epoch: 189, train_loss: 0.97392, time: 0.01369\n",
      "Epoch: 190, train_loss: 0.97496, time: 0.01347\n",
      "Epoch: 191, train_loss: 0.97645, time: 0.01358\n",
      "Epoch: 192, train_loss: 0.97373, time: 0.01345\n",
      "Epoch: 193, train_loss: 0.97750, time: 0.01339\n",
      "Epoch: 194, train_loss: 0.97612, time: 0.01326\n",
      "Epoch: 195, train_loss: 0.97588, time: 0.01253\n",
      "Epoch: 196, train_loss: 0.97751, time: 0.01362\n",
      "Epoch: 197, train_loss: 0.97398, time: 0.01347\n",
      "Epoch: 198, train_loss: 0.97618, time: 0.01356\n",
      "Epoch: 199, train_loss: 0.97460, time: 0.01388\n",
      "Epoch: 200, train_loss: 0.97715, time: 0.01360\n",
      "pairwise precision 0.13573 recall 0.90028 f1 0.23590\n",
      "average until now [0.39986519574802054, 0.8302609069557564, 0.5397697672654601]\n",
      "57 names 254.7664704322815 avg time 4.469587200566342\n",
      "Loading wen_chang_chen dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 301 nodes, 14021 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.84483, time: 0.12170\n",
      "Epoch: 2, train_loss: 0.84161, time: 0.02121\n",
      "Epoch: 3, train_loss: 0.84566, time: 0.02066\n",
      "Epoch: 4, train_loss: 0.84294, time: 0.02119\n",
      "Epoch: 5, train_loss: 0.84505, time: 0.02238\n",
      "Epoch: 6, train_loss: 0.84336, time: 0.02016\n",
      "Epoch: 7, train_loss: 0.84472, time: 0.02091\n",
      "Epoch: 8, train_loss: 0.84445, time: 0.02096\n",
      "Epoch: 9, train_loss: 0.84497, time: 0.02063\n",
      "Epoch: 10, train_loss: 0.84405, time: 0.02072\n",
      "Epoch: 11, train_loss: 0.84260, time: 0.02129\n",
      "Epoch: 12, train_loss: 0.84577, time: 0.02031\n",
      "Epoch: 13, train_loss: 0.84369, time: 0.02141\n",
      "Epoch: 14, train_loss: 0.84479, time: 0.02116\n",
      "Epoch: 15, train_loss: 0.84473, time: 0.02137\n",
      "Epoch: 16, train_loss: 0.84329, time: 0.02106\n",
      "Epoch: 17, train_loss: 0.84219, time: 0.02152\n",
      "Epoch: 18, train_loss: 0.84498, time: 0.02190\n",
      "Epoch: 19, train_loss: 0.84346, time: 0.02156\n",
      "Epoch: 20, train_loss: 0.84376, time: 0.02207\n",
      "Epoch: 21, train_loss: 0.84565, time: 0.02167\n",
      "Epoch: 22, train_loss: 0.84459, time: 0.02151\n",
      "Epoch: 23, train_loss: 0.84372, time: 0.02174\n",
      "Epoch: 24, train_loss: 0.84397, time: 0.02224\n",
      "Epoch: 25, train_loss: 0.84425, time: 0.02156\n",
      "Epoch: 26, train_loss: 0.84427, time: 0.02158\n",
      "Epoch: 27, train_loss: 0.84457, time: 0.02032\n",
      "Epoch: 28, train_loss: 0.84431, time: 0.02085\n",
      "Epoch: 29, train_loss: 0.84205, time: 0.02065\n",
      "Epoch: 30, train_loss: 0.84508, time: 0.02061\n",
      "Epoch: 31, train_loss: 0.84207, time: 0.01963\n",
      "Epoch: 32, train_loss: 0.84260, time: 0.02106\n",
      "Epoch: 33, train_loss: 0.84485, time: 0.02217\n",
      "Epoch: 34, train_loss: 0.84267, time: 0.02135\n",
      "Epoch: 35, train_loss: 0.84449, time: 0.02177\n",
      "Epoch: 36, train_loss: 0.84318, time: 0.02182\n",
      "Epoch: 37, train_loss: 0.84521, time: 0.02142\n",
      "Epoch: 38, train_loss: 0.84430, time: 0.02039\n",
      "Epoch: 39, train_loss: 0.84354, time: 0.02161\n",
      "Epoch: 40, train_loss: 0.84185, time: 0.02126\n",
      "Epoch: 41, train_loss: 0.84317, time: 0.02173\n",
      "Epoch: 42, train_loss: 0.84345, time: 0.02142\n",
      "Epoch: 43, train_loss: 0.84220, time: 0.02073\n",
      "Epoch: 44, train_loss: 0.84426, time: 0.02064\n",
      "Epoch: 45, train_loss: 0.84320, time: 0.02253\n",
      "Epoch: 46, train_loss: 0.84189, time: 0.02214\n",
      "Epoch: 47, train_loss: 0.84393, time: 0.02154\n",
      "Epoch: 48, train_loss: 0.84214, time: 0.02168\n",
      "Epoch: 49, train_loss: 0.84456, time: 0.02222\n",
      "Epoch: 50, train_loss: 0.84453, time: 0.02057\n",
      "Epoch: 51, train_loss: 0.84472, time: 0.02081\n",
      "Epoch: 52, train_loss: 0.84493, time: 0.02184\n",
      "Epoch: 53, train_loss: 0.84384, time: 0.02170\n",
      "Epoch: 54, train_loss: 0.84558, time: 0.02129\n",
      "Epoch: 55, train_loss: 0.84490, time: 0.02127\n",
      "Epoch: 56, train_loss: 0.84477, time: 0.02127\n",
      "Epoch: 57, train_loss: 0.84368, time: 0.02156\n",
      "Epoch: 58, train_loss: 0.84480, time: 0.02185\n",
      "Epoch: 59, train_loss: 0.84502, time: 0.02043\n",
      "Epoch: 60, train_loss: 0.84314, time: 0.01966\n",
      "Epoch: 61, train_loss: 0.84445, time: 0.02132\n",
      "Epoch: 62, train_loss: 0.84405, time: 0.02144\n",
      "Epoch: 63, train_loss: 0.84281, time: 0.02126\n",
      "Epoch: 64, train_loss: 0.84387, time: 0.02099\n",
      "Epoch: 65, train_loss: 0.84404, time: 0.01989\n",
      "Epoch: 66, train_loss: 0.84294, time: 0.02036\n",
      "Epoch: 67, train_loss: 0.84469, time: 0.02082\n",
      "Epoch: 68, train_loss: 0.84347, time: 0.02077\n",
      "Epoch: 69, train_loss: 0.84257, time: 0.02070\n",
      "Epoch: 70, train_loss: 0.84227, time: 0.01992\n",
      "Epoch: 71, train_loss: 0.84428, time: 0.02134\n",
      "Epoch: 72, train_loss: 0.84502, time: 0.02091\n",
      "Epoch: 73, train_loss: 0.84497, time: 0.02051\n",
      "Epoch: 74, train_loss: 0.84650, time: 0.02052\n",
      "Epoch: 75, train_loss: 0.84327, time: 0.02027\n",
      "Epoch: 76, train_loss: 0.84507, time: 0.02145\n",
      "Epoch: 77, train_loss: 0.84494, time: 0.02113\n",
      "Epoch: 78, train_loss: 0.84375, time: 0.02048\n",
      "Epoch: 79, train_loss: 0.84631, time: 0.02110\n",
      "Epoch: 80, train_loss: 0.84647, time: 0.02089\n",
      "Epoch: 81, train_loss: 0.84396, time: 0.02226\n",
      "Epoch: 82, train_loss: 0.84554, time: 0.02134\n",
      "Epoch: 83, train_loss: 0.84202, time: 0.02206\n",
      "Epoch: 84, train_loss: 0.84521, time: 0.02251\n",
      "Epoch: 85, train_loss: 0.84245, time: 0.02203\n",
      "Epoch: 86, train_loss: 0.84248, time: 0.02221\n",
      "Epoch: 87, train_loss: 0.84490, time: 0.02202\n",
      "Epoch: 88, train_loss: 0.84318, time: 0.02096\n",
      "Epoch: 89, train_loss: 0.84225, time: 0.02060\n",
      "Epoch: 90, train_loss: 0.84460, time: 0.02032\n",
      "Epoch: 91, train_loss: 0.84322, time: 0.02122\n",
      "Epoch: 92, train_loss: 0.84448, time: 0.02070\n",
      "Epoch: 93, train_loss: 0.84391, time: 0.02069\n",
      "Epoch: 94, train_loss: 0.84505, time: 0.02044\n",
      "Epoch: 95, train_loss: 0.84124, time: 0.02065\n",
      "Epoch: 96, train_loss: 0.84359, time: 0.02008\n",
      "Epoch: 97, train_loss: 0.84441, time: 0.02078\n",
      "Epoch: 98, train_loss: 0.84382, time: 0.02163\n",
      "Epoch: 99, train_loss: 0.84508, time: 0.02064\n",
      "Epoch: 100, train_loss: 0.84403, time: 0.02045\n",
      "Epoch: 101, train_loss: 0.84357, time: 0.02052\n",
      "Epoch: 102, train_loss: 0.84282, time: 0.02058\n",
      "Epoch: 103, train_loss: 0.84312, time: 0.02069\n",
      "Epoch: 104, train_loss: 0.84675, time: 0.02103\n",
      "Epoch: 105, train_loss: 0.84472, time: 0.02163\n",
      "Epoch: 106, train_loss: 0.84288, time: 0.02137\n",
      "Epoch: 107, train_loss: 0.84437, time: 0.02141\n",
      "Epoch: 108, train_loss: 0.84373, time: 0.02122\n",
      "Epoch: 109, train_loss: 0.84212, time: 0.02163\n",
      "Epoch: 110, train_loss: 0.84485, time: 0.02094\n",
      "Epoch: 111, train_loss: 0.84301, time: 0.02152\n",
      "Epoch: 112, train_loss: 0.84482, time: 0.02074\n",
      "Epoch: 113, train_loss: 0.84407, time: 0.02017\n",
      "Epoch: 114, train_loss: 0.84369, time: 0.02022\n",
      "Epoch: 115, train_loss: 0.84416, time: 0.01974\n",
      "Epoch: 116, train_loss: 0.84514, time: 0.01981\n",
      "Epoch: 117, train_loss: 0.84376, time: 0.02080\n",
      "Epoch: 118, train_loss: 0.84421, time: 0.02114\n",
      "Epoch: 119, train_loss: 0.84530, time: 0.02139\n",
      "Epoch: 120, train_loss: 0.84512, time: 0.02072\n",
      "Epoch: 121, train_loss: 0.84362, time: 0.02105\n",
      "Epoch: 122, train_loss: 0.84248, time: 0.02091\n",
      "Epoch: 123, train_loss: 0.84410, time: 0.02114\n",
      "Epoch: 124, train_loss: 0.84385, time: 0.02116\n",
      "Epoch: 125, train_loss: 0.84263, time: 0.02143\n",
      "Epoch: 126, train_loss: 0.84310, time: 0.02118\n",
      "Epoch: 127, train_loss: 0.84304, time: 0.02124\n",
      "Epoch: 128, train_loss: 0.84581, time: 0.02052\n",
      "Epoch: 129, train_loss: 0.84515, time: 0.02086\n",
      "Epoch: 130, train_loss: 0.84418, time: 0.02133\n",
      "Epoch: 131, train_loss: 0.84177, time: 0.02234\n",
      "Epoch: 132, train_loss: 0.84365, time: 0.02212\n",
      "Epoch: 133, train_loss: 0.84329, time: 0.02125\n",
      "Epoch: 134, train_loss: 0.84336, time: 0.02041\n",
      "Epoch: 135, train_loss: 0.84296, time: 0.02092\n",
      "Epoch: 136, train_loss: 0.84420, time: 0.02112\n",
      "Epoch: 137, train_loss: 0.84332, time: 0.02068\n",
      "Epoch: 138, train_loss: 0.84532, time: 0.02059\n",
      "Epoch: 139, train_loss: 0.84297, time: 0.02082\n",
      "Epoch: 140, train_loss: 0.84379, time: 0.02014\n",
      "Epoch: 141, train_loss: 0.84425, time: 0.02122\n",
      "Epoch: 142, train_loss: 0.84364, time: 0.02143\n",
      "Epoch: 143, train_loss: 0.84437, time: 0.02126\n",
      "Epoch: 144, train_loss: 0.84362, time: 0.02179\n",
      "Epoch: 145, train_loss: 0.84440, time: 0.02217\n",
      "Epoch: 146, train_loss: 0.84311, time: 0.02150\n",
      "Epoch: 147, train_loss: 0.84434, time: 0.02331\n",
      "Epoch: 148, train_loss: 0.84214, time: 0.02237\n",
      "Epoch: 149, train_loss: 0.84341, time: 0.02214\n",
      "Epoch: 150, train_loss: 0.84118, time: 0.02234\n",
      "Epoch: 151, train_loss: 0.84284, time: 0.02220\n",
      "Epoch: 152, train_loss: 0.84525, time: 0.02230\n",
      "Epoch: 153, train_loss: 0.84380, time: 0.02211\n",
      "Epoch: 154, train_loss: 0.84308, time: 0.02095\n",
      "Epoch: 155, train_loss: 0.84204, time: 0.02006\n",
      "Epoch: 156, train_loss: 0.84311, time: 0.02049\n",
      "Epoch: 157, train_loss: 0.84257, time: 0.02069\n",
      "Epoch: 158, train_loss: 0.84496, time: 0.02146\n",
      "Epoch: 159, train_loss: 0.84317, time: 0.02070\n",
      "Epoch: 160, train_loss: 0.84239, time: 0.02003\n",
      "Epoch: 161, train_loss: 0.84327, time: 0.02101\n",
      "Epoch: 162, train_loss: 0.84321, time: 0.02052\n",
      "Epoch: 163, train_loss: 0.84289, time: 0.02241\n",
      "Epoch: 164, train_loss: 0.84277, time: 0.02175\n",
      "Epoch: 165, train_loss: 0.84420, time: 0.02127\n",
      "Epoch: 166, train_loss: 0.84579, time: 0.02137\n",
      "Epoch: 167, train_loss: 0.84529, time: 0.02104\n",
      "Epoch: 168, train_loss: 0.84564, time: 0.02192\n",
      "Epoch: 169, train_loss: 0.84313, time: 0.02097\n",
      "Epoch: 170, train_loss: 0.84337, time: 0.02133\n",
      "Epoch: 171, train_loss: 0.84486, time: 0.02111\n",
      "Epoch: 172, train_loss: 0.84252, time: 0.02055\n",
      "Epoch: 173, train_loss: 0.84197, time: 0.02179\n",
      "Epoch: 174, train_loss: 0.84409, time: 0.02220\n",
      "Epoch: 175, train_loss: 0.84241, time: 0.02163\n",
      "Epoch: 176, train_loss: 0.84486, time: 0.02141\n",
      "Epoch: 177, train_loss: 0.84266, time: 0.02185\n",
      "Epoch: 178, train_loss: 0.84211, time: 0.02136\n",
      "Epoch: 179, train_loss: 0.84296, time: 0.02046\n",
      "Epoch: 180, train_loss: 0.84350, time: 0.02058\n",
      "Epoch: 181, train_loss: 0.84261, time: 0.02127\n",
      "Epoch: 182, train_loss: 0.84386, time: 0.02056\n",
      "Epoch: 183, train_loss: 0.84377, time: 0.02204\n",
      "Epoch: 184, train_loss: 0.84194, time: 0.02123\n",
      "Epoch: 185, train_loss: 0.84491, time: 0.02068\n",
      "Epoch: 186, train_loss: 0.84423, time: 0.02095\n",
      "Epoch: 187, train_loss: 0.84239, time: 0.02254\n",
      "Epoch: 188, train_loss: 0.84498, time: 0.02123\n",
      "Epoch: 189, train_loss: 0.84257, time: 0.02120\n",
      "Epoch: 190, train_loss: 0.84525, time: 0.02113\n",
      "Epoch: 191, train_loss: 0.84200, time: 0.02123\n",
      "Epoch: 192, train_loss: 0.84320, time: 0.02113\n",
      "Epoch: 193, train_loss: 0.84524, time: 0.02186\n",
      "Epoch: 194, train_loss: 0.84314, time: 0.02108\n",
      "Epoch: 195, train_loss: 0.84499, time: 0.02007\n",
      "Epoch: 196, train_loss: 0.84267, time: 0.02047\n",
      "Epoch: 197, train_loss: 0.84349, time: 0.02008\n",
      "Epoch: 198, train_loss: 0.84449, time: 0.02019\n",
      "Epoch: 199, train_loss: 0.84313, time: 0.02057\n",
      "Epoch: 200, train_loss: 0.84481, time: 0.02099\n",
      "pairwise precision 1.00000 recall 1.00000 f1 1.00000\n",
      "average until now [0.4102123475454685, 0.8331874430427261, 0.5497568514054212]\n",
      "58 names 259.19796204566956 avg time 4.468930380097751\n",
      "Loading minghui_li dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 173 nodes, 1474 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.95490, time: 0.11028\n",
      "Epoch: 2, train_loss: 0.95543, time: 0.01219\n",
      "Epoch: 3, train_loss: 0.96299, time: 0.01055\n",
      "Epoch: 4, train_loss: 0.95045, time: 0.01056\n",
      "Epoch: 5, train_loss: 0.95334, time: 0.01008\n",
      "Epoch: 6, train_loss: 0.95024, time: 0.01045\n",
      "Epoch: 7, train_loss: 0.95231, time: 0.01010\n",
      "Epoch: 8, train_loss: 0.94648, time: 0.01047\n",
      "Epoch: 9, train_loss: 0.95050, time: 0.01072\n",
      "Epoch: 10, train_loss: 0.95049, time: 0.01029\n",
      "Epoch: 11, train_loss: 0.95122, time: 0.01036\n",
      "Epoch: 12, train_loss: 0.94956, time: 0.01035\n",
      "Epoch: 13, train_loss: 0.95222, time: 0.01021\n",
      "Epoch: 14, train_loss: 0.94821, time: 0.01044\n",
      "Epoch: 15, train_loss: 0.94985, time: 0.01019\n",
      "Epoch: 16, train_loss: 0.95070, time: 0.01052\n",
      "Epoch: 17, train_loss: 0.95087, time: 0.01014\n",
      "Epoch: 18, train_loss: 0.95490, time: 0.01047\n",
      "Epoch: 19, train_loss: 0.95515, time: 0.01022\n",
      "Epoch: 20, train_loss: 0.95063, time: 0.01064\n",
      "Epoch: 21, train_loss: 0.95376, time: 0.01029\n",
      "Epoch: 22, train_loss: 0.94847, time: 0.01033\n",
      "Epoch: 23, train_loss: 0.95261, time: 0.01065\n",
      "Epoch: 24, train_loss: 0.94938, time: 0.01031\n",
      "Epoch: 25, train_loss: 0.95010, time: 0.01033\n",
      "Epoch: 26, train_loss: 0.94753, time: 0.00998\n",
      "Epoch: 27, train_loss: 0.95409, time: 0.01045\n",
      "Epoch: 28, train_loss: 0.94831, time: 0.01007\n",
      "Epoch: 29, train_loss: 0.94967, time: 0.01071\n",
      "Epoch: 30, train_loss: 0.94872, time: 0.01003\n",
      "Epoch: 31, train_loss: 0.94890, time: 0.01034\n",
      "Epoch: 32, train_loss: 0.95055, time: 0.00984\n",
      "Epoch: 33, train_loss: 0.95177, time: 0.01011\n",
      "Epoch: 34, train_loss: 0.95262, time: 0.01007\n",
      "Epoch: 35, train_loss: 0.95039, time: 0.01065\n",
      "Epoch: 36, train_loss: 0.94945, time: 0.01022\n",
      "Epoch: 37, train_loss: 0.94921, time: 0.01038\n",
      "Epoch: 38, train_loss: 0.95301, time: 0.01011\n",
      "Epoch: 39, train_loss: 0.95091, time: 0.01045\n",
      "Epoch: 40, train_loss: 0.95233, time: 0.01020\n",
      "Epoch: 41, train_loss: 0.95349, time: 0.01052\n",
      "Epoch: 42, train_loss: 0.95489, time: 0.01013\n",
      "Epoch: 43, train_loss: 0.94743, time: 0.01018\n",
      "Epoch: 44, train_loss: 0.95097, time: 0.01005\n",
      "Epoch: 45, train_loss: 0.95150, time: 0.01012\n",
      "Epoch: 46, train_loss: 0.95050, time: 0.01011\n",
      "Epoch: 47, train_loss: 0.94719, time: 0.01010\n",
      "Epoch: 48, train_loss: 0.95088, time: 0.01060\n",
      "Epoch: 49, train_loss: 0.94898, time: 0.01059\n",
      "Epoch: 50, train_loss: 0.95273, time: 0.01061\n",
      "Epoch: 51, train_loss: 0.95034, time: 0.01014\n",
      "Epoch: 52, train_loss: 0.95230, time: 0.01152\n",
      "Epoch: 53, train_loss: 0.95918, time: 0.01067\n",
      "Epoch: 54, train_loss: 0.95014, time: 0.01040\n",
      "Epoch: 55, train_loss: 0.94956, time: 0.01034\n",
      "Epoch: 56, train_loss: 0.95016, time: 0.01017\n",
      "Epoch: 57, train_loss: 0.95202, time: 0.01066\n",
      "Epoch: 58, train_loss: 0.95068, time: 0.01021\n",
      "Epoch: 59, train_loss: 0.95052, time: 0.01063\n",
      "Epoch: 60, train_loss: 0.95036, time: 0.01011\n",
      "Epoch: 61, train_loss: 0.94841, time: 0.01036\n",
      "Epoch: 62, train_loss: 0.95108, time: 0.01021\n",
      "Epoch: 63, train_loss: 0.95291, time: 0.01032\n",
      "Epoch: 64, train_loss: 0.94788, time: 0.01036\n",
      "Epoch: 65, train_loss: 0.95688, time: 0.01039\n",
      "Epoch: 66, train_loss: 0.95288, time: 0.01019\n",
      "Epoch: 67, train_loss: 0.94954, time: 0.01044\n",
      "Epoch: 68, train_loss: 0.94590, time: 0.01007\n",
      "Epoch: 69, train_loss: 0.94929, time: 0.01080\n",
      "Epoch: 70, train_loss: 0.94758, time: 0.01055\n",
      "Epoch: 71, train_loss: 0.95418, time: 0.01029\n",
      "Epoch: 72, train_loss: 0.94890, time: 0.00996\n",
      "Epoch: 73, train_loss: 0.94643, time: 0.01020\n",
      "Epoch: 74, train_loss: 0.95002, time: 0.01050\n",
      "Epoch: 75, train_loss: 0.95393, time: 0.01039\n",
      "Epoch: 76, train_loss: 0.95593, time: 0.01040\n",
      "Epoch: 77, train_loss: 0.95026, time: 0.01040\n",
      "Epoch: 78, train_loss: 0.95017, time: 0.01047\n",
      "Epoch: 79, train_loss: 0.95136, time: 0.01048\n",
      "Epoch: 80, train_loss: 0.95003, time: 0.00993\n",
      "Epoch: 81, train_loss: 0.94990, time: 0.01038\n",
      "Epoch: 82, train_loss: 0.94835, time: 0.01041\n",
      "Epoch: 83, train_loss: 0.94955, time: 0.01029\n",
      "Epoch: 84, train_loss: 0.94649, time: 0.01041\n",
      "Epoch: 85, train_loss: 0.94619, time: 0.01032\n",
      "Epoch: 86, train_loss: 0.95196, time: 0.01048\n",
      "Epoch: 87, train_loss: 0.94977, time: 0.01019\n",
      "Epoch: 88, train_loss: 0.95025, time: 0.01048\n",
      "Epoch: 89, train_loss: 0.94845, time: 0.01090\n",
      "Epoch: 90, train_loss: 0.94977, time: 0.01061\n",
      "Epoch: 91, train_loss: 0.95062, time: 0.01027\n",
      "Epoch: 92, train_loss: 0.94900, time: 0.01026\n",
      "Epoch: 93, train_loss: 0.95097, time: 0.01014\n",
      "Epoch: 94, train_loss: 0.94792, time: 0.01048\n",
      "Epoch: 95, train_loss: 0.95366, time: 0.01036\n",
      "Epoch: 96, train_loss: 0.94979, time: 0.01032\n",
      "Epoch: 97, train_loss: 0.95207, time: 0.01051\n",
      "Epoch: 98, train_loss: 0.95094, time: 0.01044\n",
      "Epoch: 99, train_loss: 0.94822, time: 0.01021\n",
      "Epoch: 100, train_loss: 0.95131, time: 0.01044\n",
      "Epoch: 101, train_loss: 0.94679, time: 0.01049\n",
      "Epoch: 102, train_loss: 0.95006, time: 0.01045\n",
      "Epoch: 103, train_loss: 0.94959, time: 0.01032\n",
      "Epoch: 104, train_loss: 0.95081, time: 0.01021\n",
      "Epoch: 105, train_loss: 0.94848, time: 0.01048\n",
      "Epoch: 106, train_loss: 0.95267, time: 0.01036\n",
      "Epoch: 107, train_loss: 0.95304, time: 0.01027\n",
      "Epoch: 108, train_loss: 0.95170, time: 0.01029\n",
      "Epoch: 109, train_loss: 0.95281, time: 0.01067\n",
      "Epoch: 110, train_loss: 0.94656, time: 0.01063\n",
      "Epoch: 111, train_loss: 0.95243, time: 0.01018\n",
      "Epoch: 112, train_loss: 0.95029, time: 0.01015\n",
      "Epoch: 113, train_loss: 0.95359, time: 0.01053\n",
      "Epoch: 114, train_loss: 0.94933, time: 0.01005\n",
      "Epoch: 115, train_loss: 0.95039, time: 0.01038\n",
      "Epoch: 116, train_loss: 0.94977, time: 0.01030\n",
      "Epoch: 117, train_loss: 0.95017, time: 0.01040\n",
      "Epoch: 118, train_loss: 0.94773, time: 0.01038\n",
      "Epoch: 119, train_loss: 0.94887, time: 0.01013\n",
      "Epoch: 120, train_loss: 0.95225, time: 0.01040\n",
      "Epoch: 121, train_loss: 0.94680, time: 0.01030\n",
      "Epoch: 122, train_loss: 0.95017, time: 0.01036\n",
      "Epoch: 123, train_loss: 0.95048, time: 0.01019\n",
      "Epoch: 124, train_loss: 0.95298, time: 0.01039\n",
      "Epoch: 125, train_loss: 0.95118, time: 0.01018\n",
      "Epoch: 126, train_loss: 0.94791, time: 0.01044\n",
      "Epoch: 127, train_loss: 0.94868, time: 0.01007\n",
      "Epoch: 128, train_loss: 0.94779, time: 0.01044\n",
      "Epoch: 129, train_loss: 0.95420, time: 0.01062\n",
      "Epoch: 130, train_loss: 0.94777, time: 0.01065\n",
      "Epoch: 131, train_loss: 0.94899, time: 0.01021\n",
      "Epoch: 132, train_loss: 0.95139, time: 0.01041\n",
      "Epoch: 133, train_loss: 0.95033, time: 0.01028\n",
      "Epoch: 134, train_loss: 0.94847, time: 0.01024\n",
      "Epoch: 135, train_loss: 0.94774, time: 0.01003\n",
      "Epoch: 136, train_loss: 0.94775, time: 0.01011\n",
      "Epoch: 137, train_loss: 0.94921, time: 0.01040\n",
      "Epoch: 138, train_loss: 0.94902, time: 0.01033\n",
      "Epoch: 139, train_loss: 0.94819, time: 0.01028\n",
      "Epoch: 140, train_loss: 0.95103, time: 0.01039\n",
      "Epoch: 141, train_loss: 0.95155, time: 0.01014\n",
      "Epoch: 142, train_loss: 0.95228, time: 0.01011\n",
      "Epoch: 143, train_loss: 0.95235, time: 0.01013\n",
      "Epoch: 144, train_loss: 0.95210, time: 0.01030\n",
      "Epoch: 145, train_loss: 0.95465, time: 0.01017\n",
      "Epoch: 146, train_loss: 0.94987, time: 0.01036\n",
      "Epoch: 147, train_loss: 0.94976, time: 0.01033\n",
      "Epoch: 148, train_loss: 0.94814, time: 0.01029\n",
      "Epoch: 149, train_loss: 0.94615, time: 0.01058\n",
      "Epoch: 150, train_loss: 0.95143, time: 0.01040\n",
      "Epoch: 151, train_loss: 0.95293, time: 0.01020\n",
      "Epoch: 152, train_loss: 0.94931, time: 0.01027\n",
      "Epoch: 153, train_loss: 0.95166, time: 0.01034\n",
      "Epoch: 154, train_loss: 0.95355, time: 0.01023\n",
      "Epoch: 155, train_loss: 0.94971, time: 0.01070\n",
      "Epoch: 156, train_loss: 0.95041, time: 0.01011\n",
      "Epoch: 157, train_loss: 0.95425, time: 0.01018\n",
      "Epoch: 158, train_loss: 0.94540, time: 0.01029\n",
      "Epoch: 159, train_loss: 0.95005, time: 0.01058\n",
      "Epoch: 160, train_loss: 0.95197, time: 0.01052\n",
      "Epoch: 161, train_loss: 0.95084, time: 0.01050\n",
      "Epoch: 162, train_loss: 0.95222, time: 0.01026\n",
      "Epoch: 163, train_loss: 0.95286, time: 0.01042\n",
      "Epoch: 164, train_loss: 0.94651, time: 0.01039\n",
      "Epoch: 165, train_loss: 0.95059, time: 0.01040\n",
      "Epoch: 166, train_loss: 0.95059, time: 0.01019\n",
      "Epoch: 167, train_loss: 0.95485, time: 0.01044\n",
      "Epoch: 168, train_loss: 0.94858, time: 0.01007\n",
      "Epoch: 169, train_loss: 0.94516, time: 0.01086\n",
      "Epoch: 170, train_loss: 0.95409, time: 0.01043\n",
      "Epoch: 171, train_loss: 0.94966, time: 0.01035\n",
      "Epoch: 172, train_loss: 0.94806, time: 0.01028\n",
      "Epoch: 173, train_loss: 0.95057, time: 0.01033\n",
      "Epoch: 174, train_loss: 0.95039, time: 0.01030\n",
      "Epoch: 175, train_loss: 0.95132, time: 0.01015\n",
      "Epoch: 176, train_loss: 0.94820, time: 0.01040\n",
      "Epoch: 177, train_loss: 0.95013, time: 0.01045\n",
      "Epoch: 178, train_loss: 0.94808, time: 0.01040\n",
      "Epoch: 179, train_loss: 0.95201, time: 0.01031\n",
      "Epoch: 180, train_loss: 0.94805, time: 0.01034\n",
      "Epoch: 181, train_loss: 0.95289, time: 0.01039\n",
      "Epoch: 182, train_loss: 0.95205, time: 0.01028\n",
      "Epoch: 183, train_loss: 0.94813, time: 0.01033\n",
      "Epoch: 184, train_loss: 0.94848, time: 0.01022\n",
      "Epoch: 185, train_loss: 0.95220, time: 0.01023\n",
      "Epoch: 186, train_loss: 0.95233, time: 0.01042\n",
      "Epoch: 187, train_loss: 0.94427, time: 0.01035\n",
      "Epoch: 188, train_loss: 0.95139, time: 0.01026\n",
      "Epoch: 189, train_loss: 0.95043, time: 0.01070\n",
      "Epoch: 190, train_loss: 0.94511, time: 0.01033\n",
      "Epoch: 191, train_loss: 0.94776, time: 0.01034\n",
      "Epoch: 192, train_loss: 0.95093, time: 0.01008\n",
      "Epoch: 193, train_loss: 0.95085, time: 0.01031\n",
      "Epoch: 194, train_loss: 0.94948, time: 0.01039\n",
      "Epoch: 195, train_loss: 0.94924, time: 0.01045\n",
      "Epoch: 196, train_loss: 0.94753, time: 0.01026\n",
      "Epoch: 197, train_loss: 0.95037, time: 0.01035\n",
      "Epoch: 198, train_loss: 0.95270, time: 0.01043\n",
      "Epoch: 199, train_loss: 0.94797, time: 0.01037\n",
      "Epoch: 200, train_loss: 0.94964, time: 0.01048\n",
      "pairwise precision 0.31027 recall 0.66527 f1 0.42318\n",
      "average until now [0.40851837650108114, 0.8303413232263778, 0.5476159881233094]\n",
      "59 names 261.4130344390869 avg time 4.43072939727266\n",
      "Loading kwok_fai_so dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 229 nodes, 7284 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.86200, time: 0.11558\n",
      "Epoch: 2, train_loss: 0.86017, time: 0.01653\n",
      "Epoch: 3, train_loss: 0.86128, time: 0.01517\n",
      "Epoch: 4, train_loss: 0.86412, time: 0.01527\n",
      "Epoch: 5, train_loss: 0.86483, time: 0.01458\n",
      "Epoch: 6, train_loss: 0.85843, time: 0.01425\n",
      "Epoch: 7, train_loss: 0.85849, time: 0.01468\n",
      "Epoch: 8, train_loss: 0.86494, time: 0.01501\n",
      "Epoch: 9, train_loss: 0.86074, time: 0.01482\n",
      "Epoch: 10, train_loss: 0.86177, time: 0.01466\n",
      "Epoch: 11, train_loss: 0.86021, time: 0.01499\n",
      "Epoch: 12, train_loss: 0.85965, time: 0.01484\n",
      "Epoch: 13, train_loss: 0.85935, time: 0.01504\n",
      "Epoch: 14, train_loss: 0.86240, time: 0.01465\n",
      "Epoch: 15, train_loss: 0.86080, time: 0.01540\n",
      "Epoch: 16, train_loss: 0.86043, time: 0.01491\n",
      "Epoch: 17, train_loss: 0.86183, time: 0.01504\n",
      "Epoch: 18, train_loss: 0.85765, time: 0.01539\n",
      "Epoch: 19, train_loss: 0.85912, time: 0.01524\n",
      "Epoch: 20, train_loss: 0.85599, time: 0.01516\n",
      "Epoch: 21, train_loss: 0.85956, time: 0.01503\n",
      "Epoch: 22, train_loss: 0.86124, time: 0.01448\n",
      "Epoch: 23, train_loss: 0.86119, time: 0.01460\n",
      "Epoch: 24, train_loss: 0.85932, time: 0.01478\n",
      "Epoch: 25, train_loss: 0.85835, time: 0.01500\n",
      "Epoch: 26, train_loss: 0.86184, time: 0.01472\n",
      "Epoch: 27, train_loss: 0.85794, time: 0.01528\n",
      "Epoch: 28, train_loss: 0.85733, time: 0.01491\n",
      "Epoch: 29, train_loss: 0.86046, time: 0.01530\n",
      "Epoch: 30, train_loss: 0.85881, time: 0.01525\n",
      "Epoch: 31, train_loss: 0.86041, time: 0.01444\n",
      "Epoch: 32, train_loss: 0.85906, time: 0.01508\n",
      "Epoch: 33, train_loss: 0.85705, time: 0.01503\n",
      "Epoch: 34, train_loss: 0.86002, time: 0.01529\n",
      "Epoch: 35, train_loss: 0.86082, time: 0.01485\n",
      "Epoch: 36, train_loss: 0.85850, time: 0.01538\n",
      "Epoch: 37, train_loss: 0.85978, time: 0.01529\n",
      "Epoch: 38, train_loss: 0.85995, time: 0.01506\n",
      "Epoch: 39, train_loss: 0.85895, time: 0.01525\n",
      "Epoch: 40, train_loss: 0.86094, time: 0.01519\n",
      "Epoch: 41, train_loss: 0.85978, time: 0.01538\n",
      "Epoch: 42, train_loss: 0.86169, time: 0.01509\n",
      "Epoch: 43, train_loss: 0.85847, time: 0.01555\n",
      "Epoch: 44, train_loss: 0.86043, time: 0.01552\n",
      "Epoch: 45, train_loss: 0.86018, time: 0.01620\n",
      "Epoch: 46, train_loss: 0.86244, time: 0.01561\n",
      "Epoch: 47, train_loss: 0.85757, time: 0.01502\n",
      "Epoch: 48, train_loss: 0.86085, time: 0.01494\n",
      "Epoch: 49, train_loss: 0.85937, time: 0.01503\n",
      "Epoch: 50, train_loss: 0.85984, time: 0.01536\n",
      "Epoch: 51, train_loss: 0.86269, time: 0.01508\n",
      "Epoch: 52, train_loss: 0.85932, time: 0.01478\n",
      "Epoch: 53, train_loss: 0.85940, time: 0.01472\n",
      "Epoch: 54, train_loss: 0.85848, time: 0.01541\n",
      "Epoch: 55, train_loss: 0.85978, time: 0.01479\n",
      "Epoch: 56, train_loss: 0.86057, time: 0.01500\n",
      "Epoch: 57, train_loss: 0.86318, time: 0.01543\n",
      "Epoch: 58, train_loss: 0.85994, time: 0.01484\n",
      "Epoch: 59, train_loss: 0.86060, time: 0.01512\n",
      "Epoch: 60, train_loss: 0.86184, time: 0.01566\n",
      "Epoch: 61, train_loss: 0.86034, time: 0.01502\n",
      "Epoch: 62, train_loss: 0.85868, time: 0.01490\n",
      "Epoch: 63, train_loss: 0.86104, time: 0.01429\n",
      "Epoch: 64, train_loss: 0.85795, time: 0.01510\n",
      "Epoch: 65, train_loss: 0.85802, time: 0.01480\n",
      "Epoch: 66, train_loss: 0.85900, time: 0.01512\n",
      "Epoch: 67, train_loss: 0.85988, time: 0.01481\n",
      "Epoch: 68, train_loss: 0.85778, time: 0.01537\n",
      "Epoch: 69, train_loss: 0.86039, time: 0.01603\n",
      "Epoch: 70, train_loss: 0.85856, time: 0.01630\n",
      "Epoch: 71, train_loss: 0.85956, time: 0.01566\n",
      "Epoch: 72, train_loss: 0.86125, time: 0.01536\n",
      "Epoch: 73, train_loss: 0.85918, time: 0.01535\n",
      "Epoch: 74, train_loss: 0.85894, time: 0.01477\n",
      "Epoch: 75, train_loss: 0.86090, time: 0.01499\n",
      "Epoch: 76, train_loss: 0.85816, time: 0.01495\n",
      "Epoch: 77, train_loss: 0.85977, time: 0.01478\n",
      "Epoch: 78, train_loss: 0.86279, time: 0.01480\n",
      "Epoch: 79, train_loss: 0.85964, time: 0.01528\n",
      "Epoch: 80, train_loss: 0.86077, time: 0.01490\n",
      "Epoch: 81, train_loss: 0.85908, time: 0.01496\n",
      "Epoch: 82, train_loss: 0.85861, time: 0.01508\n",
      "Epoch: 83, train_loss: 0.86013, time: 0.01491\n",
      "Epoch: 84, train_loss: 0.86090, time: 0.01459\n",
      "Epoch: 85, train_loss: 0.86166, time: 0.01515\n",
      "Epoch: 86, train_loss: 0.86177, time: 0.01464\n",
      "Epoch: 87, train_loss: 0.85686, time: 0.01513\n",
      "Epoch: 88, train_loss: 0.86002, time: 0.01477\n",
      "Epoch: 89, train_loss: 0.85972, time: 0.01468\n",
      "Epoch: 90, train_loss: 0.86263, time: 0.01407\n",
      "Epoch: 91, train_loss: 0.85977, time: 0.01435\n",
      "Epoch: 92, train_loss: 0.85960, time: 0.01501\n",
      "Epoch: 93, train_loss: 0.85931, time: 0.01499\n",
      "Epoch: 94, train_loss: 0.86162, time: 0.01535\n",
      "Epoch: 95, train_loss: 0.86032, time: 0.01492\n",
      "Epoch: 96, train_loss: 0.85905, time: 0.01490\n",
      "Epoch: 97, train_loss: 0.85906, time: 0.01504\n",
      "Epoch: 98, train_loss: 0.86035, time: 0.01511\n",
      "Epoch: 99, train_loss: 0.86260, time: 0.01487\n",
      "Epoch: 100, train_loss: 0.85855, time: 0.01495\n",
      "Epoch: 101, train_loss: 0.85947, time: 0.01486\n",
      "Epoch: 102, train_loss: 0.85900, time: 0.01466\n",
      "Epoch: 103, train_loss: 0.85994, time: 0.01488\n",
      "Epoch: 104, train_loss: 0.85972, time: 0.01472\n",
      "Epoch: 105, train_loss: 0.85949, time: 0.01518\n",
      "Epoch: 106, train_loss: 0.86076, time: 0.01535\n",
      "Epoch: 107, train_loss: 0.85845, time: 0.01470\n",
      "Epoch: 108, train_loss: 0.86058, time: 0.01488\n",
      "Epoch: 109, train_loss: 0.85836, time: 0.01462\n",
      "Epoch: 110, train_loss: 0.85785, time: 0.01529\n",
      "Epoch: 111, train_loss: 0.86066, time: 0.01523\n",
      "Epoch: 112, train_loss: 0.85977, time: 0.01479\n",
      "Epoch: 113, train_loss: 0.85723, time: 0.01570\n",
      "Epoch: 114, train_loss: 0.85864, time: 0.01486\n",
      "Epoch: 115, train_loss: 0.86082, time: 0.01490\n",
      "Epoch: 116, train_loss: 0.85886, time: 0.01468\n",
      "Epoch: 117, train_loss: 0.86198, time: 0.01499\n",
      "Epoch: 118, train_loss: 0.85944, time: 0.01486\n",
      "Epoch: 119, train_loss: 0.86150, time: 0.01496\n",
      "Epoch: 120, train_loss: 0.85849, time: 0.01473\n",
      "Epoch: 121, train_loss: 0.86206, time: 0.01485\n",
      "Epoch: 122, train_loss: 0.86173, time: 0.01488\n",
      "Epoch: 123, train_loss: 0.85938, time: 0.01472\n",
      "Epoch: 124, train_loss: 0.85956, time: 0.01505\n",
      "Epoch: 125, train_loss: 0.85985, time: 0.01485\n",
      "Epoch: 126, train_loss: 0.86197, time: 0.01444\n",
      "Epoch: 127, train_loss: 0.85835, time: 0.01521\n",
      "Epoch: 128, train_loss: 0.85858, time: 0.01482\n",
      "Epoch: 129, train_loss: 0.85793, time: 0.01621\n",
      "Epoch: 130, train_loss: 0.86063, time: 0.01488\n",
      "Epoch: 131, train_loss: 0.86145, time: 0.01486\n",
      "Epoch: 132, train_loss: 0.86159, time: 0.01504\n",
      "Epoch: 133, train_loss: 0.85896, time: 0.01471\n",
      "Epoch: 134, train_loss: 0.86166, time: 0.01502\n",
      "Epoch: 135, train_loss: 0.85862, time: 0.01505\n",
      "Epoch: 136, train_loss: 0.85868, time: 0.01476\n",
      "Epoch: 137, train_loss: 0.86151, time: 0.01477\n",
      "Epoch: 138, train_loss: 0.85880, time: 0.01462\n",
      "Epoch: 139, train_loss: 0.86016, time: 0.01492\n",
      "Epoch: 140, train_loss: 0.85984, time: 0.01483\n",
      "Epoch: 141, train_loss: 0.86117, time: 0.01561\n",
      "Epoch: 142, train_loss: 0.86033, time: 0.01505\n",
      "Epoch: 143, train_loss: 0.85970, time: 0.01494\n",
      "Epoch: 144, train_loss: 0.85867, time: 0.01478\n",
      "Epoch: 145, train_loss: 0.86193, time: 0.01522\n",
      "Epoch: 146, train_loss: 0.86123, time: 0.01472\n",
      "Epoch: 147, train_loss: 0.85913, time: 0.01501\n",
      "Epoch: 148, train_loss: 0.85952, time: 0.01569\n",
      "Epoch: 149, train_loss: 0.86000, time: 0.01506\n",
      "Epoch: 150, train_loss: 0.86160, time: 0.01494\n",
      "Epoch: 151, train_loss: 0.86139, time: 0.01546\n",
      "Epoch: 152, train_loss: 0.85961, time: 0.01500\n",
      "Epoch: 153, train_loss: 0.85934, time: 0.01501\n",
      "Epoch: 154, train_loss: 0.86212, time: 0.01508\n",
      "Epoch: 155, train_loss: 0.86222, time: 0.01532\n",
      "Epoch: 156, train_loss: 0.86302, time: 0.01527\n",
      "Epoch: 157, train_loss: 0.86212, time: 0.01500\n",
      "Epoch: 158, train_loss: 0.86189, time: 0.01538\n",
      "Epoch: 159, train_loss: 0.85743, time: 0.01511\n",
      "Epoch: 160, train_loss: 0.86002, time: 0.01472\n",
      "Epoch: 161, train_loss: 0.86177, time: 0.01516\n",
      "Epoch: 162, train_loss: 0.86105, time: 0.01536\n",
      "Epoch: 163, train_loss: 0.85837, time: 0.01487\n",
      "Epoch: 164, train_loss: 0.85831, time: 0.01489\n",
      "Epoch: 165, train_loss: 0.85957, time: 0.01449\n",
      "Epoch: 166, train_loss: 0.86092, time: 0.01477\n",
      "Epoch: 167, train_loss: 0.86004, time: 0.01492\n",
      "Epoch: 168, train_loss: 0.85959, time: 0.01520\n",
      "Epoch: 169, train_loss: 0.85796, time: 0.01522\n",
      "Epoch: 170, train_loss: 0.85949, time: 0.01522\n",
      "Epoch: 171, train_loss: 0.85880, time: 0.01476\n",
      "Epoch: 172, train_loss: 0.85921, time: 0.01492\n",
      "Epoch: 173, train_loss: 0.86157, time: 0.01481\n",
      "Epoch: 174, train_loss: 0.85971, time: 0.01512\n",
      "Epoch: 175, train_loss: 0.86198, time: 0.01526\n",
      "Epoch: 176, train_loss: 0.85927, time: 0.01532\n",
      "Epoch: 177, train_loss: 0.86079, time: 0.01502\n",
      "Epoch: 178, train_loss: 0.86153, time: 0.01494\n",
      "Epoch: 179, train_loss: 0.86055, time: 0.01522\n",
      "Epoch: 180, train_loss: 0.85935, time: 0.01472\n",
      "Epoch: 181, train_loss: 0.85804, time: 0.01510\n",
      "Epoch: 182, train_loss: 0.86086, time: 0.01497\n",
      "Epoch: 183, train_loss: 0.86022, time: 0.01504\n",
      "Epoch: 184, train_loss: 0.85944, time: 0.01472\n",
      "Epoch: 185, train_loss: 0.85803, time: 0.01520\n",
      "Epoch: 186, train_loss: 0.86192, time: 0.01532\n",
      "Epoch: 187, train_loss: 0.86338, time: 0.01489\n",
      "Epoch: 188, train_loss: 0.86156, time: 0.01526\n",
      "Epoch: 189, train_loss: 0.86027, time: 0.01508\n",
      "Epoch: 190, train_loss: 0.85754, time: 0.01475\n",
      "Epoch: 191, train_loss: 0.85999, time: 0.01505\n",
      "Epoch: 192, train_loss: 0.86109, time: 0.01486\n",
      "Epoch: 193, train_loss: 0.86165, time: 0.01513\n",
      "Epoch: 194, train_loss: 0.85915, time: 0.01511\n",
      "Epoch: 195, train_loss: 0.85783, time: 0.01470\n",
      "Epoch: 196, train_loss: 0.86496, time: 0.01505\n",
      "Epoch: 197, train_loss: 0.86036, time: 0.01526\n",
      "Epoch: 198, train_loss: 0.85756, time: 0.01510\n",
      "Epoch: 199, train_loss: 0.85747, time: 0.01480\n",
      "Epoch: 200, train_loss: 0.85770, time: 0.01477\n",
      "pairwise precision 0.87654 recall 0.99070 f1 0.93013\n",
      "average until now [0.41631867112257753, 0.8330139189224302, 0.5551752199786978]\n",
      "60 names 264.5829472541809 avg time 4.409715787569682\n",
      "Loading yin_shi dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 169 nodes, 3451 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.88167, time: 0.11062\n",
      "Epoch: 2, train_loss: 0.88080, time: 0.01298\n",
      "Epoch: 3, train_loss: 0.88245, time: 0.01149\n",
      "Epoch: 4, train_loss: 0.88018, time: 0.01155\n",
      "Epoch: 5, train_loss: 0.87990, time: 0.01160\n",
      "Epoch: 6, train_loss: 0.88099, time: 0.01154\n",
      "Epoch: 7, train_loss: 0.88230, time: 0.01163\n",
      "Epoch: 8, train_loss: 0.88237, time: 0.01144\n",
      "Epoch: 9, train_loss: 0.87648, time: 0.01164\n",
      "Epoch: 10, train_loss: 0.87356, time: 0.01125\n",
      "Epoch: 11, train_loss: 0.88068, time: 0.01168\n",
      "Epoch: 12, train_loss: 0.87770, time: 0.01162\n",
      "Epoch: 13, train_loss: 0.88212, time: 0.01138\n",
      "Epoch: 14, train_loss: 0.87931, time: 0.01156\n",
      "Epoch: 15, train_loss: 0.87651, time: 0.01173\n",
      "Epoch: 16, train_loss: 0.87776, time: 0.01172\n",
      "Epoch: 17, train_loss: 0.88034, time: 0.01164\n",
      "Epoch: 18, train_loss: 0.88252, time: 0.01141\n",
      "Epoch: 19, train_loss: 0.87861, time: 0.01182\n",
      "Epoch: 20, train_loss: 0.87817, time: 0.01178\n",
      "Epoch: 21, train_loss: 0.88203, time: 0.01166\n",
      "Epoch: 22, train_loss: 0.87764, time: 0.01174\n",
      "Epoch: 23, train_loss: 0.87940, time: 0.01166\n",
      "Epoch: 24, train_loss: 0.87590, time: 0.01157\n",
      "Epoch: 25, train_loss: 0.87738, time: 0.01151\n",
      "Epoch: 26, train_loss: 0.88297, time: 0.01173\n",
      "Epoch: 27, train_loss: 0.87803, time: 0.01170\n",
      "Epoch: 28, train_loss: 0.87767, time: 0.01175\n",
      "Epoch: 29, train_loss: 0.88162, time: 0.01198\n",
      "Epoch: 30, train_loss: 0.87632, time: 0.01148\n",
      "Epoch: 31, train_loss: 0.87807, time: 0.01152\n",
      "Epoch: 32, train_loss: 0.87622, time: 0.01144\n",
      "Epoch: 33, train_loss: 0.87707, time: 0.01164\n",
      "Epoch: 34, train_loss: 0.87358, time: 0.01140\n",
      "Epoch: 35, train_loss: 0.87488, time: 0.01146\n",
      "Epoch: 36, train_loss: 0.87905, time: 0.01146\n",
      "Epoch: 37, train_loss: 0.88087, time: 0.01197\n",
      "Epoch: 38, train_loss: 0.87794, time: 0.01159\n",
      "Epoch: 39, train_loss: 0.87702, time: 0.01138\n",
      "Epoch: 40, train_loss: 0.88009, time: 0.01161\n",
      "Epoch: 41, train_loss: 0.87801, time: 0.01179\n",
      "Epoch: 42, train_loss: 0.88002, time: 0.01145\n",
      "Epoch: 43, train_loss: 0.88864, time: 0.01194\n",
      "Epoch: 44, train_loss: 0.87377, time: 0.01137\n",
      "Epoch: 45, train_loss: 0.88585, time: 0.01186\n",
      "Epoch: 46, train_loss: 0.87866, time: 0.01163\n",
      "Epoch: 47, train_loss: 0.87579, time: 0.01134\n",
      "Epoch: 48, train_loss: 0.87933, time: 0.01130\n",
      "Epoch: 49, train_loss: 0.87527, time: 0.01154\n",
      "Epoch: 50, train_loss: 0.87914, time: 0.01142\n",
      "Epoch: 51, train_loss: 0.88174, time: 0.01178\n",
      "Epoch: 52, train_loss: 0.87997, time: 0.01115\n",
      "Epoch: 53, train_loss: 0.87673, time: 0.01180\n",
      "Epoch: 54, train_loss: 0.88599, time: 0.01165\n",
      "Epoch: 55, train_loss: 0.87575, time: 0.01175\n",
      "Epoch: 56, train_loss: 0.87962, time: 0.01160\n",
      "Epoch: 57, train_loss: 0.87643, time: 0.01164\n",
      "Epoch: 58, train_loss: 0.87668, time: 0.01171\n",
      "Epoch: 59, train_loss: 0.87669, time: 0.01136\n",
      "Epoch: 60, train_loss: 0.87982, time: 0.01190\n",
      "Epoch: 61, train_loss: 0.88381, time: 0.01170\n",
      "Epoch: 62, train_loss: 0.88488, time: 0.01150\n",
      "Epoch: 63, train_loss: 0.87648, time: 0.01193\n",
      "Epoch: 64, train_loss: 0.87619, time: 0.01146\n",
      "Epoch: 65, train_loss: 0.87906, time: 0.01121\n",
      "Epoch: 66, train_loss: 0.87888, time: 0.01136\n",
      "Epoch: 67, train_loss: 0.87747, time: 0.01159\n",
      "Epoch: 68, train_loss: 0.87814, time: 0.01163\n",
      "Epoch: 69, train_loss: 0.87599, time: 0.01118\n",
      "Epoch: 70, train_loss: 0.87688, time: 0.01189\n",
      "Epoch: 71, train_loss: 0.88315, time: 0.01134\n",
      "Epoch: 72, train_loss: 0.87908, time: 0.01139\n",
      "Epoch: 73, train_loss: 0.87818, time: 0.01207\n",
      "Epoch: 74, train_loss: 0.88112, time: 0.01178\n",
      "Epoch: 75, train_loss: 0.88172, time: 0.01157\n",
      "Epoch: 76, train_loss: 0.87551, time: 0.01189\n",
      "Epoch: 77, train_loss: 0.87818, time: 0.01160\n",
      "Epoch: 78, train_loss: 0.87798, time: 0.01155\n",
      "Epoch: 79, train_loss: 0.87893, time: 0.01152\n",
      "Epoch: 80, train_loss: 0.87643, time: 0.01139\n",
      "Epoch: 81, train_loss: 0.87462, time: 0.01142\n",
      "Epoch: 82, train_loss: 0.88287, time: 0.01134\n",
      "Epoch: 83, train_loss: 0.88360, time: 0.01141\n",
      "Epoch: 84, train_loss: 0.88035, time: 0.01149\n",
      "Epoch: 85, train_loss: 0.88034, time: 0.01162\n",
      "Epoch: 86, train_loss: 0.87319, time: 0.01157\n",
      "Epoch: 87, train_loss: 0.87729, time: 0.01150\n",
      "Epoch: 88, train_loss: 0.87378, time: 0.01150\n",
      "Epoch: 89, train_loss: 0.87721, time: 0.01158\n",
      "Epoch: 90, train_loss: 0.87476, time: 0.01145\n",
      "Epoch: 91, train_loss: 0.88202, time: 0.01190\n",
      "Epoch: 92, train_loss: 0.87519, time: 0.01163\n",
      "Epoch: 93, train_loss: 0.87493, time: 0.01196\n",
      "Epoch: 94, train_loss: 0.88241, time: 0.01173\n",
      "Epoch: 95, train_loss: 0.87714, time: 0.01167\n",
      "Epoch: 96, train_loss: 0.88495, time: 0.01167\n",
      "Epoch: 97, train_loss: 0.87315, time: 0.01146\n",
      "Epoch: 98, train_loss: 0.87099, time: 0.01162\n",
      "Epoch: 99, train_loss: 0.87901, time: 0.01187\n",
      "Epoch: 100, train_loss: 0.87477, time: 0.01157\n",
      "Epoch: 101, train_loss: 0.87565, time: 0.01165\n",
      "Epoch: 102, train_loss: 0.88887, time: 0.01158\n",
      "Epoch: 103, train_loss: 0.87370, time: 0.01159\n",
      "Epoch: 104, train_loss: 0.87566, time: 0.01139\n",
      "Epoch: 105, train_loss: 0.88021, time: 0.01164\n",
      "Epoch: 106, train_loss: 0.87459, time: 0.01152\n",
      "Epoch: 107, train_loss: 0.88306, time: 0.01172\n",
      "Epoch: 108, train_loss: 0.87343, time: 0.01150\n",
      "Epoch: 109, train_loss: 0.88667, time: 0.01211\n",
      "Epoch: 110, train_loss: 0.87636, time: 0.01144\n",
      "Epoch: 111, train_loss: 0.88040, time: 0.01122\n",
      "Epoch: 112, train_loss: 0.87766, time: 0.01169\n",
      "Epoch: 113, train_loss: 0.87589, time: 0.01153\n",
      "Epoch: 114, train_loss: 0.88140, time: 0.01151\n",
      "Epoch: 115, train_loss: 0.87219, time: 0.01161\n",
      "Epoch: 116, train_loss: 0.87885, time: 0.01141\n",
      "Epoch: 117, train_loss: 0.87779, time: 0.01169\n",
      "Epoch: 118, train_loss: 0.87568, time: 0.01150\n",
      "Epoch: 119, train_loss: 0.87723, time: 0.01173\n",
      "Epoch: 120, train_loss: 0.87828, time: 0.01153\n",
      "Epoch: 121, train_loss: 0.87749, time: 0.01180\n",
      "Epoch: 122, train_loss: 0.87847, time: 0.01166\n",
      "Epoch: 123, train_loss: 0.87222, time: 0.01176\n",
      "Epoch: 124, train_loss: 0.87987, time: 0.01160\n",
      "Epoch: 125, train_loss: 0.88320, time: 0.01153\n",
      "Epoch: 126, train_loss: 0.87674, time: 0.01146\n",
      "Epoch: 127, train_loss: 0.87716, time: 0.01195\n",
      "Epoch: 128, train_loss: 0.88251, time: 0.01163\n",
      "Epoch: 129, train_loss: 0.87902, time: 0.01159\n",
      "Epoch: 130, train_loss: 0.87694, time: 0.01116\n",
      "Epoch: 131, train_loss: 0.88110, time: 0.01138\n",
      "Epoch: 132, train_loss: 0.87856, time: 0.01150\n",
      "Epoch: 133, train_loss: 0.87526, time: 0.01177\n",
      "Epoch: 134, train_loss: 0.88228, time: 0.01173\n",
      "Epoch: 135, train_loss: 0.88010, time: 0.01156\n",
      "Epoch: 136, train_loss: 0.87749, time: 0.01136\n",
      "Epoch: 137, train_loss: 0.87589, time: 0.01131\n",
      "Epoch: 138, train_loss: 0.87747, time: 0.01143\n",
      "Epoch: 139, train_loss: 0.87753, time: 0.01173\n",
      "Epoch: 140, train_loss: 0.87369, time: 0.01154\n",
      "Epoch: 141, train_loss: 0.87770, time: 0.01159\n",
      "Epoch: 142, train_loss: 0.87766, time: 0.01156\n",
      "Epoch: 143, train_loss: 0.88247, time: 0.01129\n",
      "Epoch: 144, train_loss: 0.87729, time: 0.01159\n",
      "Epoch: 145, train_loss: 0.87850, time: 0.01195\n",
      "Epoch: 146, train_loss: 0.87489, time: 0.01156\n",
      "Epoch: 147, train_loss: 0.87908, time: 0.01142\n",
      "Epoch: 148, train_loss: 0.87903, time: 0.01160\n",
      "Epoch: 149, train_loss: 0.87564, time: 0.01154\n",
      "Epoch: 150, train_loss: 0.87677, time: 0.01169\n",
      "Epoch: 151, train_loss: 0.87313, time: 0.01163\n",
      "Epoch: 152, train_loss: 0.87599, time: 0.01133\n",
      "Epoch: 153, train_loss: 0.87986, time: 0.01173\n",
      "Epoch: 154, train_loss: 0.87912, time: 0.01155\n",
      "Epoch: 155, train_loss: 0.87999, time: 0.01104\n",
      "Epoch: 156, train_loss: 0.87468, time: 0.01101\n",
      "Epoch: 157, train_loss: 0.87365, time: 0.01124\n",
      "Epoch: 158, train_loss: 0.87918, time: 0.01120\n",
      "Epoch: 159, train_loss: 0.87574, time: 0.01115\n",
      "Epoch: 160, train_loss: 0.87596, time: 0.01131\n",
      "Epoch: 161, train_loss: 0.87563, time: 0.01163\n",
      "Epoch: 162, train_loss: 0.88188, time: 0.01165\n",
      "Epoch: 163, train_loss: 0.87388, time: 0.01170\n",
      "Epoch: 164, train_loss: 0.87536, time: 0.01166\n",
      "Epoch: 165, train_loss: 0.87630, time: 0.01152\n",
      "Epoch: 166, train_loss: 0.87815, time: 0.01141\n",
      "Epoch: 167, train_loss: 0.87713, time: 0.01140\n",
      "Epoch: 168, train_loss: 0.87764, time: 0.01145\n",
      "Epoch: 169, train_loss: 0.87956, time: 0.01140\n",
      "Epoch: 170, train_loss: 0.88494, time: 0.01130\n",
      "Epoch: 171, train_loss: 0.87615, time: 0.01171\n",
      "Epoch: 172, train_loss: 0.87883, time: 0.01137\n",
      "Epoch: 173, train_loss: 0.87534, time: 0.01136\n",
      "Epoch: 174, train_loss: 0.87869, time: 0.01160\n",
      "Epoch: 175, train_loss: 0.87648, time: 0.01150\n",
      "Epoch: 176, train_loss: 0.87826, time: 0.01146\n",
      "Epoch: 177, train_loss: 0.87619, time: 0.01127\n",
      "Epoch: 178, train_loss: 0.87883, time: 0.01143\n",
      "Epoch: 179, train_loss: 0.87679, time: 0.01153\n",
      "Epoch: 180, train_loss: 0.88249, time: 0.01153\n",
      "Epoch: 181, train_loss: 0.88015, time: 0.01184\n",
      "Epoch: 182, train_loss: 0.88073, time: 0.01149\n",
      "Epoch: 183, train_loss: 0.87465, time: 0.01146\n",
      "Epoch: 184, train_loss: 0.87851, time: 0.01144\n",
      "Epoch: 185, train_loss: 0.87446, time: 0.01160\n",
      "Epoch: 186, train_loss: 0.87717, time: 0.01156\n",
      "Epoch: 187, train_loss: 0.88203, time: 0.01142\n",
      "Epoch: 188, train_loss: 0.87974, time: 0.01134\n",
      "Epoch: 189, train_loss: 0.87736, time: 0.01136\n",
      "Epoch: 190, train_loss: 0.87399, time: 0.01170\n",
      "Epoch: 191, train_loss: 0.87381, time: 0.01161\n",
      "Epoch: 192, train_loss: 0.87773, time: 0.01141\n",
      "Epoch: 193, train_loss: 0.87658, time: 0.01151\n",
      "Epoch: 194, train_loss: 0.87583, time: 0.01177\n",
      "Epoch: 195, train_loss: 0.87751, time: 0.01146\n",
      "Epoch: 196, train_loss: 0.88119, time: 0.01168\n",
      "Epoch: 197, train_loss: 0.88132, time: 0.01127\n",
      "Epoch: 198, train_loss: 0.87923, time: 0.01156\n",
      "Epoch: 199, train_loss: 0.87939, time: 0.01203\n",
      "Epoch: 200, train_loss: 0.87790, time: 0.01128\n",
      "pairwise precision 0.98345 recall 0.99256 f1 0.98798\n",
      "average until now [0.4256158422391174, 0.8356294444418945, 0.5639777346274337]\n",
      "61 names 267.0412874221802 avg time 4.3777260233144295\n",
      "Loading rong_lu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 205 nodes, 1082 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98405, time: 0.11040\n",
      "Epoch: 2, train_loss: 0.97609, time: 0.01276\n",
      "Epoch: 3, train_loss: 0.97561, time: 0.01103\n",
      "Epoch: 4, train_loss: 0.97547, time: 0.01145\n",
      "Epoch: 5, train_loss: 0.97303, time: 0.01077\n",
      "Epoch: 6, train_loss: 0.97490, time: 0.01055\n",
      "Epoch: 7, train_loss: 0.97647, time: 0.01070\n",
      "Epoch: 8, train_loss: 0.97276, time: 0.01059\n",
      "Epoch: 9, train_loss: 0.97547, time: 0.01070\n",
      "Epoch: 10, train_loss: 0.97406, time: 0.01052\n",
      "Epoch: 11, train_loss: 0.97376, time: 0.01069\n",
      "Epoch: 12, train_loss: 0.97807, time: 0.01078\n",
      "Epoch: 13, train_loss: 0.97465, time: 0.01063\n",
      "Epoch: 14, train_loss: 0.97201, time: 0.01077\n",
      "Epoch: 15, train_loss: 0.97218, time: 0.01085\n",
      "Epoch: 16, train_loss: 0.97532, time: 0.01068\n",
      "Epoch: 17, train_loss: 0.97477, time: 0.01042\n",
      "Epoch: 18, train_loss: 0.97440, time: 0.01062\n",
      "Epoch: 19, train_loss: 0.97103, time: 0.01053\n",
      "Epoch: 20, train_loss: 0.97706, time: 0.01105\n",
      "Epoch: 21, train_loss: 0.97402, time: 0.01062\n",
      "Epoch: 22, train_loss: 0.97121, time: 0.01115\n",
      "Epoch: 23, train_loss: 0.97232, time: 0.01154\n",
      "Epoch: 24, train_loss: 0.97171, time: 0.01076\n",
      "Epoch: 25, train_loss: 0.97084, time: 0.01061\n",
      "Epoch: 26, train_loss: 0.97255, time: 0.01122\n",
      "Epoch: 27, train_loss: 0.97632, time: 0.01051\n",
      "Epoch: 28, train_loss: 0.97812, time: 0.01048\n",
      "Epoch: 29, train_loss: 0.97113, time: 0.01068\n",
      "Epoch: 30, train_loss: 0.97645, time: 0.01046\n",
      "Epoch: 31, train_loss: 0.97206, time: 0.01051\n",
      "Epoch: 32, train_loss: 0.97305, time: 0.01067\n",
      "Epoch: 33, train_loss: 0.97417, time: 0.01059\n",
      "Epoch: 34, train_loss: 0.97299, time: 0.01069\n",
      "Epoch: 35, train_loss: 0.97309, time: 0.01074\n",
      "Epoch: 36, train_loss: 0.97554, time: 0.01039\n",
      "Epoch: 37, train_loss: 0.97531, time: 0.01075\n",
      "Epoch: 38, train_loss: 0.97261, time: 0.01048\n",
      "Epoch: 39, train_loss: 0.97479, time: 0.01087\n",
      "Epoch: 40, train_loss: 0.97812, time: 0.01048\n",
      "Epoch: 41, train_loss: 0.97270, time: 0.01065\n",
      "Epoch: 42, train_loss: 0.97491, time: 0.01111\n",
      "Epoch: 43, train_loss: 0.97253, time: 0.01041\n",
      "Epoch: 44, train_loss: 0.97016, time: 0.01077\n",
      "Epoch: 45, train_loss: 0.97125, time: 0.01079\n",
      "Epoch: 46, train_loss: 0.97298, time: 0.01049\n",
      "Epoch: 47, train_loss: 0.97532, time: 0.01064\n",
      "Epoch: 48, train_loss: 0.97443, time: 0.01048\n",
      "Epoch: 49, train_loss: 0.97398, time: 0.01078\n",
      "Epoch: 50, train_loss: 0.97356, time: 0.01036\n",
      "Epoch: 51, train_loss: 0.97510, time: 0.01068\n",
      "Epoch: 52, train_loss: 0.97404, time: 0.01037\n",
      "Epoch: 53, train_loss: 0.97422, time: 0.01069\n",
      "Epoch: 54, train_loss: 0.97578, time: 0.01067\n",
      "Epoch: 55, train_loss: 0.97318, time: 0.01059\n",
      "Epoch: 56, train_loss: 0.97114, time: 0.01050\n",
      "Epoch: 57, train_loss: 0.97236, time: 0.01072\n",
      "Epoch: 58, train_loss: 0.97671, time: 0.01034\n",
      "Epoch: 59, train_loss: 0.97097, time: 0.01022\n",
      "Epoch: 60, train_loss: 0.97101, time: 0.01072\n",
      "Epoch: 61, train_loss: 0.97344, time: 0.01096\n",
      "Epoch: 62, train_loss: 0.97023, time: 0.01077\n",
      "Epoch: 63, train_loss: 0.97386, time: 0.01065\n",
      "Epoch: 64, train_loss: 0.97508, time: 0.01027\n",
      "Epoch: 65, train_loss: 0.97119, time: 0.01060\n",
      "Epoch: 66, train_loss: 0.97339, time: 0.01061\n",
      "Epoch: 67, train_loss: 0.97271, time: 0.01044\n",
      "Epoch: 68, train_loss: 0.97509, time: 0.01062\n",
      "Epoch: 69, train_loss: 0.97325, time: 0.01053\n",
      "Epoch: 70, train_loss: 0.97156, time: 0.01061\n",
      "Epoch: 71, train_loss: 0.97413, time: 0.01040\n",
      "Epoch: 72, train_loss: 0.97159, time: 0.01040\n",
      "Epoch: 73, train_loss: 0.97118, time: 0.01072\n",
      "Epoch: 74, train_loss: 0.97280, time: 0.01076\n",
      "Epoch: 75, train_loss: 0.97349, time: 0.01052\n",
      "Epoch: 76, train_loss: 0.97395, time: 0.01027\n",
      "Epoch: 77, train_loss: 0.97345, time: 0.01068\n",
      "Epoch: 78, train_loss: 0.97426, time: 0.01066\n",
      "Epoch: 79, train_loss: 0.96905, time: 0.01036\n",
      "Epoch: 80, train_loss: 0.97195, time: 0.01103\n",
      "Epoch: 81, train_loss: 0.97123, time: 0.01059\n",
      "Epoch: 82, train_loss: 0.97786, time: 0.01077\n",
      "Epoch: 83, train_loss: 0.97493, time: 0.01038\n",
      "Epoch: 84, train_loss: 0.97478, time: 0.01057\n",
      "Epoch: 85, train_loss: 0.97201, time: 0.01067\n",
      "Epoch: 86, train_loss: 0.97412, time: 0.01060\n",
      "Epoch: 87, train_loss: 0.97487, time: 0.01050\n",
      "Epoch: 88, train_loss: 0.97060, time: 0.01051\n",
      "Epoch: 89, train_loss: 0.97342, time: 0.01053\n",
      "Epoch: 90, train_loss: 0.97008, time: 0.01043\n",
      "Epoch: 91, train_loss: 0.97146, time: 0.01086\n",
      "Epoch: 92, train_loss: 0.97468, time: 0.01067\n",
      "Epoch: 93, train_loss: 0.97535, time: 0.01060\n",
      "Epoch: 94, train_loss: 0.97255, time: 0.01056\n",
      "Epoch: 95, train_loss: 0.97170, time: 0.01033\n",
      "Epoch: 96, train_loss: 0.97472, time: 0.01039\n",
      "Epoch: 97, train_loss: 0.97851, time: 0.01022\n",
      "Epoch: 98, train_loss: 0.97564, time: 0.01037\n",
      "Epoch: 99, train_loss: 0.97430, time: 0.01055\n",
      "Epoch: 100, train_loss: 0.97096, time: 0.01069\n",
      "Epoch: 101, train_loss: 0.97165, time: 0.01052\n",
      "Epoch: 102, train_loss: 0.97383, time: 0.01027\n",
      "Epoch: 103, train_loss: 0.97344, time: 0.01037\n",
      "Epoch: 104, train_loss: 0.97194, time: 0.01046\n",
      "Epoch: 105, train_loss: 0.97306, time: 0.01027\n",
      "Epoch: 106, train_loss: 0.97408, time: 0.01030\n",
      "Epoch: 107, train_loss: 0.97300, time: 0.01054\n",
      "Epoch: 108, train_loss: 0.97614, time: 0.01059\n",
      "Epoch: 109, train_loss: 0.97342, time: 0.01058\n",
      "Epoch: 110, train_loss: 0.97436, time: 0.01033\n",
      "Epoch: 111, train_loss: 0.97119, time: 0.01044\n",
      "Epoch: 112, train_loss: 0.97303, time: 0.01070\n",
      "Epoch: 113, train_loss: 0.97320, time: 0.01048\n",
      "Epoch: 114, train_loss: 0.97244, time: 0.01040\n",
      "Epoch: 115, train_loss: 0.97256, time: 0.01051\n",
      "Epoch: 116, train_loss: 0.96881, time: 0.01057\n",
      "Epoch: 117, train_loss: 0.97030, time: 0.01034\n",
      "Epoch: 118, train_loss: 0.97107, time: 0.01045\n",
      "Epoch: 119, train_loss: 0.97195, time: 0.01029\n",
      "Epoch: 120, train_loss: 0.96929, time: 0.01069\n",
      "Epoch: 121, train_loss: 0.97304, time: 0.01022\n",
      "Epoch: 122, train_loss: 0.97182, time: 0.01076\n",
      "Epoch: 123, train_loss: 0.97294, time: 0.01068\n",
      "Epoch: 124, train_loss: 0.97139, time: 0.01100\n",
      "Epoch: 125, train_loss: 0.97149, time: 0.01067\n",
      "Epoch: 126, train_loss: 0.97428, time: 0.01050\n",
      "Epoch: 127, train_loss: 0.97667, time: 0.00999\n",
      "Epoch: 128, train_loss: 0.97094, time: 0.01039\n",
      "Epoch: 129, train_loss: 0.97389, time: 0.01050\n",
      "Epoch: 130, train_loss: 0.97256, time: 0.01050\n",
      "Epoch: 131, train_loss: 0.97609, time: 0.01060\n",
      "Epoch: 132, train_loss: 0.97148, time: 0.01078\n",
      "Epoch: 133, train_loss: 0.97311, time: 0.01040\n",
      "Epoch: 134, train_loss: 0.97314, time: 0.01044\n",
      "Epoch: 135, train_loss: 0.97306, time: 0.01069\n",
      "Epoch: 136, train_loss: 0.97420, time: 0.01083\n",
      "Epoch: 137, train_loss: 0.97428, time: 0.01077\n",
      "Epoch: 138, train_loss: 0.97566, time: 0.01085\n",
      "Epoch: 139, train_loss: 0.97079, time: 0.01074\n",
      "Epoch: 140, train_loss: 0.97317, time: 0.01050\n",
      "Epoch: 141, train_loss: 0.97332, time: 0.01051\n",
      "Epoch: 142, train_loss: 0.97314, time: 0.01067\n",
      "Epoch: 143, train_loss: 0.97343, time: 0.01044\n",
      "Epoch: 144, train_loss: 0.97267, time: 0.01078\n",
      "Epoch: 145, train_loss: 0.97019, time: 0.01061\n",
      "Epoch: 146, train_loss: 0.97146, time: 0.01053\n",
      "Epoch: 147, train_loss: 0.97148, time: 0.01058\n",
      "Epoch: 148, train_loss: 0.97517, time: 0.01038\n",
      "Epoch: 149, train_loss: 0.97151, time: 0.01044\n",
      "Epoch: 150, train_loss: 0.97116, time: 0.01065\n",
      "Epoch: 151, train_loss: 0.96981, time: 0.01045\n",
      "Epoch: 152, train_loss: 0.97393, time: 0.01059\n",
      "Epoch: 153, train_loss: 0.97392, time: 0.01045\n",
      "Epoch: 154, train_loss: 0.97585, time: 0.01048\n",
      "Epoch: 155, train_loss: 0.97416, time: 0.01044\n",
      "Epoch: 156, train_loss: 0.97444, time: 0.01060\n",
      "Epoch: 157, train_loss: 0.97260, time: 0.01051\n",
      "Epoch: 158, train_loss: 0.97025, time: 0.01055\n",
      "Epoch: 159, train_loss: 0.97030, time: 0.01091\n",
      "Epoch: 160, train_loss: 0.97053, time: 0.01035\n",
      "Epoch: 161, train_loss: 0.97223, time: 0.01083\n",
      "Epoch: 162, train_loss: 0.97230, time: 0.01071\n",
      "Epoch: 163, train_loss: 0.97467, time: 0.01062\n",
      "Epoch: 164, train_loss: 0.97072, time: 0.01072\n",
      "Epoch: 165, train_loss: 0.97637, time: 0.01031\n",
      "Epoch: 166, train_loss: 0.97466, time: 0.01060\n",
      "Epoch: 167, train_loss: 0.97482, time: 0.01061\n",
      "Epoch: 168, train_loss: 0.97394, time: 0.01070\n",
      "Epoch: 169, train_loss: 0.97146, time: 0.01066\n",
      "Epoch: 170, train_loss: 0.97271, time: 0.01061\n",
      "Epoch: 171, train_loss: 0.97286, time: 0.01059\n",
      "Epoch: 172, train_loss: 0.97185, time: 0.01059\n",
      "Epoch: 173, train_loss: 0.97006, time: 0.01048\n",
      "Epoch: 174, train_loss: 0.97202, time: 0.01050\n",
      "Epoch: 175, train_loss: 0.97155, time: 0.01061\n",
      "Epoch: 176, train_loss: 0.97378, time: 0.01054\n",
      "Epoch: 177, train_loss: 0.97474, time: 0.01080\n",
      "Epoch: 178, train_loss: 0.97515, time: 0.01098\n",
      "Epoch: 179, train_loss: 0.97487, time: 0.01085\n",
      "Epoch: 180, train_loss: 0.97433, time: 0.01068\n",
      "Epoch: 181, train_loss: 0.97674, time: 0.01083\n",
      "Epoch: 182, train_loss: 0.97559, time: 0.01042\n",
      "Epoch: 183, train_loss: 0.97540, time: 0.01078\n",
      "Epoch: 184, train_loss: 0.97862, time: 0.01048\n",
      "Epoch: 185, train_loss: 0.97580, time: 0.01060\n",
      "Epoch: 186, train_loss: 0.97449, time: 0.01058\n",
      "Epoch: 187, train_loss: 0.97269, time: 0.01050\n",
      "Epoch: 188, train_loss: 0.97474, time: 0.01071\n",
      "Epoch: 189, train_loss: 0.97418, time: 0.01033\n",
      "Epoch: 190, train_loss: 0.97204, time: 0.01086\n",
      "Epoch: 191, train_loss: 0.97422, time: 0.01064\n",
      "Epoch: 192, train_loss: 0.97416, time: 0.01077\n",
      "Epoch: 193, train_loss: 0.97451, time: 0.01093\n",
      "Epoch: 194, train_loss: 0.97333, time: 0.01046\n",
      "Epoch: 195, train_loss: 0.97599, time: 0.01049\n",
      "Epoch: 196, train_loss: 0.97507, time: 0.01051\n",
      "Epoch: 197, train_loss: 0.97042, time: 0.01107\n",
      "Epoch: 198, train_loss: 0.97476, time: 0.01059\n",
      "Epoch: 199, train_loss: 0.97435, time: 0.01073\n",
      "Epoch: 200, train_loss: 0.97503, time: 0.01070\n",
      "pairwise precision 0.09516 recall 0.87306 f1 0.17162\n",
      "average until now [0.42028597069578105, 0.8362330886073588, 0.5594137753361123]\n",
      "62 names 269.31280875205994 avg time 4.343754979871934\n",
      "Loading s_lin dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 71 nodes, 283 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.95852, time: 0.10680\n",
      "Epoch: 2, train_loss: 0.94264, time: 0.00940\n",
      "Epoch: 3, train_loss: 0.95007, time: 0.00858\n",
      "Epoch: 4, train_loss: 0.95082, time: 0.00859\n",
      "Epoch: 5, train_loss: 0.95796, time: 0.00850\n",
      "Epoch: 6, train_loss: 0.95237, time: 0.00936\n",
      "Epoch: 7, train_loss: 0.94805, time: 0.00857\n",
      "Epoch: 8, train_loss: 0.94562, time: 0.00832\n",
      "Epoch: 9, train_loss: 0.94746, time: 0.00828\n",
      "Epoch: 10, train_loss: 0.94524, time: 0.00833\n",
      "Epoch: 11, train_loss: 0.93341, time: 0.00834\n",
      "Epoch: 12, train_loss: 0.95162, time: 0.00831\n",
      "Epoch: 13, train_loss: 0.95495, time: 0.00844\n",
      "Epoch: 14, train_loss: 0.94972, time: 0.00840\n",
      "Epoch: 15, train_loss: 0.94876, time: 0.00823\n",
      "Epoch: 16, train_loss: 0.95056, time: 0.00834\n",
      "Epoch: 17, train_loss: 0.94553, time: 0.00832\n",
      "Epoch: 18, train_loss: 0.94763, time: 0.00840\n",
      "Epoch: 19, train_loss: 0.94396, time: 0.00821\n",
      "Epoch: 20, train_loss: 0.95544, time: 0.00828\n",
      "Epoch: 21, train_loss: 0.94663, time: 0.00818\n",
      "Epoch: 22, train_loss: 0.94752, time: 0.00821\n",
      "Epoch: 23, train_loss: 0.96013, time: 0.00819\n",
      "Epoch: 24, train_loss: 0.94470, time: 0.00815\n",
      "Epoch: 25, train_loss: 0.95412, time: 0.00815\n",
      "Epoch: 26, train_loss: 0.94833, time: 0.00827\n",
      "Epoch: 27, train_loss: 0.95564, time: 0.00825\n",
      "Epoch: 28, train_loss: 0.94564, time: 0.00822\n",
      "Epoch: 29, train_loss: 0.94593, time: 0.00816\n",
      "Epoch: 30, train_loss: 0.94227, time: 0.00823\n",
      "Epoch: 31, train_loss: 0.95554, time: 0.00879\n",
      "Epoch: 32, train_loss: 0.95730, time: 0.00844\n",
      "Epoch: 33, train_loss: 0.95540, time: 0.00821\n",
      "Epoch: 34, train_loss: 0.94410, time: 0.00821\n",
      "Epoch: 35, train_loss: 0.94864, time: 0.00816\n",
      "Epoch: 36, train_loss: 0.94366, time: 0.00817\n",
      "Epoch: 37, train_loss: 0.94825, time: 0.00826\n",
      "Epoch: 38, train_loss: 0.95362, time: 0.00838\n",
      "Epoch: 39, train_loss: 0.94753, time: 0.00832\n",
      "Epoch: 40, train_loss: 0.95262, time: 0.00835\n",
      "Epoch: 41, train_loss: 0.95051, time: 0.00839\n",
      "Epoch: 42, train_loss: 0.95498, time: 0.00833\n",
      "Epoch: 43, train_loss: 0.94180, time: 0.00820\n",
      "Epoch: 44, train_loss: 0.94036, time: 0.00826\n",
      "Epoch: 45, train_loss: 0.94842, time: 0.00836\n",
      "Epoch: 46, train_loss: 0.94657, time: 0.00833\n",
      "Epoch: 47, train_loss: 0.94853, time: 0.00831\n",
      "Epoch: 48, train_loss: 0.93743, time: 0.00824\n",
      "Epoch: 49, train_loss: 0.94956, time: 0.00829\n",
      "Epoch: 50, train_loss: 0.92827, time: 0.00823\n",
      "Epoch: 51, train_loss: 0.95264, time: 0.00818\n",
      "Epoch: 52, train_loss: 0.94994, time: 0.00830\n",
      "Epoch: 53, train_loss: 0.95289, time: 0.00830\n",
      "Epoch: 54, train_loss: 0.93664, time: 0.00816\n",
      "Epoch: 55, train_loss: 0.94786, time: 0.00824\n",
      "Epoch: 56, train_loss: 0.95478, time: 0.00876\n",
      "Epoch: 57, train_loss: 0.94230, time: 0.00840\n",
      "Epoch: 58, train_loss: 0.95057, time: 0.00812\n",
      "Epoch: 59, train_loss: 0.94576, time: 0.00821\n",
      "Epoch: 60, train_loss: 0.94293, time: 0.00808\n",
      "Epoch: 61, train_loss: 0.94455, time: 0.00811\n",
      "Epoch: 62, train_loss: 0.96242, time: 0.00814\n",
      "Epoch: 63, train_loss: 0.94836, time: 0.00806\n",
      "Epoch: 64, train_loss: 0.95445, time: 0.00821\n",
      "Epoch: 65, train_loss: 0.94547, time: 0.00821\n",
      "Epoch: 66, train_loss: 0.95461, time: 0.00835\n",
      "Epoch: 67, train_loss: 0.94841, time: 0.00818\n",
      "Epoch: 68, train_loss: 0.95271, time: 0.00825\n",
      "Epoch: 69, train_loss: 0.94455, time: 0.00814\n",
      "Epoch: 70, train_loss: 0.93976, time: 0.00819\n",
      "Epoch: 71, train_loss: 0.95298, time: 0.00827\n",
      "Epoch: 72, train_loss: 0.95684, time: 0.00812\n",
      "Epoch: 73, train_loss: 0.95056, time: 0.00824\n",
      "Epoch: 74, train_loss: 0.94014, time: 0.00827\n",
      "Epoch: 75, train_loss: 0.95202, time: 0.00825\n",
      "Epoch: 76, train_loss: 0.95315, time: 0.00823\n",
      "Epoch: 77, train_loss: 0.93773, time: 0.00815\n",
      "Epoch: 78, train_loss: 0.94874, time: 0.00814\n",
      "Epoch: 79, train_loss: 0.95101, time: 0.00823\n",
      "Epoch: 80, train_loss: 0.93997, time: 0.00837\n",
      "Epoch: 81, train_loss: 0.95271, time: 0.00871\n",
      "Epoch: 82, train_loss: 0.95509, time: 0.00840\n",
      "Epoch: 83, train_loss: 0.95130, time: 0.00810\n",
      "Epoch: 84, train_loss: 0.94527, time: 0.00809\n",
      "Epoch: 85, train_loss: 0.94572, time: 0.00819\n",
      "Epoch: 86, train_loss: 0.95209, time: 0.00822\n",
      "Epoch: 87, train_loss: 0.95202, time: 0.00809\n",
      "Epoch: 88, train_loss: 0.94988, time: 0.00812\n",
      "Epoch: 89, train_loss: 0.94653, time: 0.00819\n",
      "Epoch: 90, train_loss: 0.94849, time: 0.00821\n",
      "Epoch: 91, train_loss: 0.94899, time: 0.00821\n",
      "Epoch: 92, train_loss: 0.95344, time: 0.00814\n",
      "Epoch: 93, train_loss: 0.94999, time: 0.00811\n",
      "Epoch: 94, train_loss: 0.95105, time: 0.00810\n",
      "Epoch: 95, train_loss: 0.95132, time: 0.00828\n",
      "Epoch: 96, train_loss: 0.94794, time: 0.00820\n",
      "Epoch: 97, train_loss: 0.94919, time: 0.00819\n",
      "Epoch: 98, train_loss: 0.95439, time: 0.00833\n",
      "Epoch: 99, train_loss: 0.95372, time: 0.00819\n",
      "Epoch: 100, train_loss: 0.94958, time: 0.00830\n",
      "Epoch: 101, train_loss: 0.94303, time: 0.00825\n",
      "Epoch: 102, train_loss: 0.94545, time: 0.00824\n",
      "Epoch: 103, train_loss: 0.95264, time: 0.00817\n",
      "Epoch: 104, train_loss: 0.94414, time: 0.00804\n",
      "Epoch: 105, train_loss: 0.94827, time: 0.00825\n",
      "Epoch: 106, train_loss: 0.95131, time: 0.00867\n",
      "Epoch: 107, train_loss: 0.94724, time: 0.00840\n",
      "Epoch: 108, train_loss: 0.95847, time: 0.00824\n",
      "Epoch: 109, train_loss: 0.95193, time: 0.00831\n",
      "Epoch: 110, train_loss: 0.95155, time: 0.00828\n",
      "Epoch: 111, train_loss: 0.95556, time: 0.00831\n",
      "Epoch: 112, train_loss: 0.94741, time: 0.00836\n",
      "Epoch: 113, train_loss: 0.94294, time: 0.00824\n",
      "Epoch: 114, train_loss: 0.94881, time: 0.00828\n",
      "Epoch: 115, train_loss: 0.95414, time: 0.00822\n",
      "Epoch: 116, train_loss: 0.94681, time: 0.00824\n",
      "Epoch: 117, train_loss: 0.94874, time: 0.00815\n",
      "Epoch: 118, train_loss: 0.95188, time: 0.00818\n",
      "Epoch: 119, train_loss: 0.94993, time: 0.00819\n",
      "Epoch: 120, train_loss: 0.94627, time: 0.00814\n",
      "Epoch: 121, train_loss: 0.93985, time: 0.00812\n",
      "Epoch: 122, train_loss: 0.95214, time: 0.00829\n",
      "Epoch: 123, train_loss: 0.96213, time: 0.00830\n",
      "Epoch: 124, train_loss: 0.94648, time: 0.00842\n",
      "Epoch: 125, train_loss: 0.94648, time: 0.00816\n",
      "Epoch: 126, train_loss: 0.95212, time: 0.00835\n",
      "Epoch: 127, train_loss: 0.94950, time: 0.00821\n",
      "Epoch: 128, train_loss: 0.94453, time: 0.00822\n",
      "Epoch: 129, train_loss: 0.94485, time: 0.00818\n",
      "Epoch: 130, train_loss: 0.94093, time: 0.00829\n",
      "Epoch: 131, train_loss: 0.94736, time: 0.00867\n",
      "Epoch: 132, train_loss: 0.94923, time: 0.00842\n",
      "Epoch: 133, train_loss: 0.94753, time: 0.00820\n",
      "Epoch: 134, train_loss: 0.95650, time: 0.00834\n",
      "Epoch: 135, train_loss: 0.94816, time: 0.00829\n",
      "Epoch: 136, train_loss: 0.93911, time: 0.00821\n",
      "Epoch: 137, train_loss: 0.95028, time: 0.00823\n",
      "Epoch: 138, train_loss: 0.95117, time: 0.00826\n",
      "Epoch: 139, train_loss: 0.95088, time: 0.00830\n",
      "Epoch: 140, train_loss: 0.95647, time: 0.00826\n",
      "Epoch: 141, train_loss: 0.94789, time: 0.00822\n",
      "Epoch: 142, train_loss: 0.94044, time: 0.00830\n",
      "Epoch: 143, train_loss: 0.94621, time: 0.00827\n",
      "Epoch: 144, train_loss: 0.95240, time: 0.00821\n",
      "Epoch: 145, train_loss: 0.94825, time: 0.00822\n",
      "Epoch: 146, train_loss: 0.93330, time: 0.00839\n",
      "Epoch: 147, train_loss: 0.95084, time: 0.00829\n",
      "Epoch: 148, train_loss: 0.94524, time: 0.00821\n",
      "Epoch: 149, train_loss: 0.95408, time: 0.00821\n",
      "Epoch: 150, train_loss: 0.94283, time: 0.00824\n",
      "Epoch: 151, train_loss: 0.93743, time: 0.00840\n",
      "Epoch: 152, train_loss: 0.95106, time: 0.00828\n",
      "Epoch: 153, train_loss: 0.94679, time: 0.00831\n",
      "Epoch: 154, train_loss: 0.94889, time: 0.00824\n",
      "Epoch: 155, train_loss: 0.94731, time: 0.00823\n",
      "Epoch: 156, train_loss: 0.95379, time: 0.00866\n",
      "Epoch: 157, train_loss: 0.95252, time: 0.00835\n",
      "Epoch: 158, train_loss: 0.95346, time: 0.00834\n",
      "Epoch: 159, train_loss: 0.94083, time: 0.00836\n",
      "Epoch: 160, train_loss: 0.95357, time: 0.00827\n",
      "Epoch: 161, train_loss: 0.94610, time: 0.00819\n",
      "Epoch: 162, train_loss: 0.95219, time: 0.00821\n",
      "Epoch: 163, train_loss: 0.95435, time: 0.00822\n",
      "Epoch: 164, train_loss: 0.94534, time: 0.00837\n",
      "Epoch: 165, train_loss: 0.94097, time: 0.00820\n",
      "Epoch: 166, train_loss: 0.95161, time: 0.00819\n",
      "Epoch: 167, train_loss: 0.94984, time: 0.00824\n",
      "Epoch: 168, train_loss: 0.93956, time: 0.00826\n",
      "Epoch: 169, train_loss: 0.94095, time: 0.00817\n",
      "Epoch: 170, train_loss: 0.94834, time: 0.00816\n",
      "Epoch: 171, train_loss: 0.94375, time: 0.00824\n",
      "Epoch: 172, train_loss: 0.95474, time: 0.00818\n",
      "Epoch: 173, train_loss: 0.94480, time: 0.00821\n",
      "Epoch: 174, train_loss: 0.94543, time: 0.00814\n",
      "Epoch: 175, train_loss: 0.94280, time: 0.00820\n",
      "Epoch: 176, train_loss: 0.94665, time: 0.00819\n",
      "Epoch: 177, train_loss: 0.94727, time: 0.00825\n",
      "Epoch: 178, train_loss: 0.94701, time: 0.00820\n",
      "Epoch: 179, train_loss: 0.93954, time: 0.00813\n",
      "Epoch: 180, train_loss: 0.94200, time: 0.00809\n",
      "Epoch: 181, train_loss: 0.94563, time: 0.00865\n",
      "Epoch: 182, train_loss: 0.95147, time: 0.00828\n",
      "Epoch: 183, train_loss: 0.94802, time: 0.00818\n",
      "Epoch: 184, train_loss: 0.95154, time: 0.00816\n",
      "Epoch: 185, train_loss: 0.94374, time: 0.00821\n",
      "Epoch: 186, train_loss: 0.94566, time: 0.00811\n",
      "Epoch: 187, train_loss: 0.94226, time: 0.00820\n",
      "Epoch: 188, train_loss: 0.95261, time: 0.00822\n",
      "Epoch: 189, train_loss: 0.93636, time: 0.00813\n",
      "Epoch: 190, train_loss: 0.94280, time: 0.00814\n",
      "Epoch: 191, train_loss: 0.94761, time: 0.00817\n",
      "Epoch: 192, train_loss: 0.94330, time: 0.00824\n",
      "Epoch: 193, train_loss: 0.93943, time: 0.00822\n",
      "Epoch: 194, train_loss: 0.94401, time: 0.00814\n",
      "Epoch: 195, train_loss: 0.95145, time: 0.00810\n",
      "Epoch: 196, train_loss: 0.94360, time: 0.00822\n",
      "Epoch: 197, train_loss: 0.94700, time: 0.00823\n",
      "Epoch: 198, train_loss: 0.93535, time: 0.00804\n",
      "Epoch: 199, train_loss: 0.95097, time: 0.00822\n",
      "Epoch: 200, train_loss: 0.94498, time: 0.00812\n",
      "pairwise precision 0.25599 recall 0.89389 f1 0.39800\n",
      "average until now [0.4176780230186256, 0.837148288395197, 0.5573017379215921]\n",
      "63 names 271.08851861953735 avg time 4.302992359040275\n",
      "Loading shengkai_gong dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 238 nodes, 9417 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.83319, time: 0.11582\n",
      "Epoch: 2, train_loss: 0.83429, time: 0.01769\n",
      "Epoch: 3, train_loss: 0.82998, time: 0.01631\n",
      "Epoch: 4, train_loss: 0.83493, time: 0.01641\n",
      "Epoch: 5, train_loss: 0.83352, time: 0.01613\n",
      "Epoch: 6, train_loss: 0.83130, time: 0.01645\n",
      "Epoch: 7, train_loss: 0.83132, time: 0.01601\n",
      "Epoch: 8, train_loss: 0.83301, time: 0.01635\n",
      "Epoch: 9, train_loss: 0.83500, time: 0.01659\n",
      "Epoch: 10, train_loss: 0.83102, time: 0.01603\n",
      "Epoch: 11, train_loss: 0.83364, time: 0.01653\n",
      "Epoch: 12, train_loss: 0.83041, time: 0.01539\n",
      "Epoch: 13, train_loss: 0.83495, time: 0.01583\n",
      "Epoch: 14, train_loss: 0.83544, time: 0.01538\n",
      "Epoch: 15, train_loss: 0.83386, time: 0.01616\n",
      "Epoch: 16, train_loss: 0.83587, time: 0.01615\n",
      "Epoch: 17, train_loss: 0.83469, time: 0.01609\n",
      "Epoch: 18, train_loss: 0.83110, time: 0.01556\n",
      "Epoch: 19, train_loss: 0.83396, time: 0.01691\n",
      "Epoch: 20, train_loss: 0.83312, time: 0.01623\n",
      "Epoch: 21, train_loss: 0.83046, time: 0.01610\n",
      "Epoch: 22, train_loss: 0.83281, time: 0.01623\n",
      "Epoch: 23, train_loss: 0.83393, time: 0.01677\n",
      "Epoch: 24, train_loss: 0.83434, time: 0.01625\n",
      "Epoch: 25, train_loss: 0.83484, time: 0.01580\n",
      "Epoch: 26, train_loss: 0.83169, time: 0.01639\n",
      "Epoch: 27, train_loss: 0.83298, time: 0.01620\n",
      "Epoch: 28, train_loss: 0.83395, time: 0.01601\n",
      "Epoch: 29, train_loss: 0.82934, time: 0.01592\n",
      "Epoch: 30, train_loss: 0.83400, time: 0.01630\n",
      "Epoch: 31, train_loss: 0.83183, time: 0.01599\n",
      "Epoch: 32, train_loss: 0.83136, time: 0.01641\n",
      "Epoch: 33, train_loss: 0.83142, time: 0.01674\n",
      "Epoch: 34, train_loss: 0.83170, time: 0.01619\n",
      "Epoch: 35, train_loss: 0.83091, time: 0.01622\n",
      "Epoch: 36, train_loss: 0.83248, time: 0.01594\n",
      "Epoch: 37, train_loss: 0.83202, time: 0.01637\n",
      "Epoch: 38, train_loss: 0.83152, time: 0.01607\n",
      "Epoch: 39, train_loss: 0.83249, time: 0.01625\n",
      "Epoch: 40, train_loss: 0.83158, time: 0.01636\n",
      "Epoch: 41, train_loss: 0.83382, time: 0.01639\n",
      "Epoch: 42, train_loss: 0.83271, time: 0.01600\n",
      "Epoch: 43, train_loss: 0.83266, time: 0.01618\n",
      "Epoch: 44, train_loss: 0.83141, time: 0.01611\n",
      "Epoch: 45, train_loss: 0.83169, time: 0.01705\n",
      "Epoch: 46, train_loss: 0.83339, time: 0.01600\n",
      "Epoch: 47, train_loss: 0.83293, time: 0.01607\n",
      "Epoch: 48, train_loss: 0.83187, time: 0.01637\n",
      "Epoch: 49, train_loss: 0.83373, time: 0.01624\n",
      "Epoch: 50, train_loss: 0.83100, time: 0.01612\n",
      "Epoch: 51, train_loss: 0.83201, time: 0.01633\n",
      "Epoch: 52, train_loss: 0.83417, time: 0.01620\n",
      "Epoch: 53, train_loss: 0.83137, time: 0.01637\n",
      "Epoch: 54, train_loss: 0.83194, time: 0.01634\n",
      "Epoch: 55, train_loss: 0.83219, time: 0.01622\n",
      "Epoch: 56, train_loss: 0.83069, time: 0.01604\n",
      "Epoch: 57, train_loss: 0.83369, time: 0.01647\n",
      "Epoch: 58, train_loss: 0.83368, time: 0.01650\n",
      "Epoch: 59, train_loss: 0.83240, time: 0.01631\n",
      "Epoch: 60, train_loss: 0.83327, time: 0.01756\n",
      "Epoch: 61, train_loss: 0.82899, time: 0.01684\n",
      "Epoch: 62, train_loss: 0.83273, time: 0.01530\n",
      "Epoch: 63, train_loss: 0.83282, time: 0.01582\n",
      "Epoch: 64, train_loss: 0.83344, time: 0.01584\n",
      "Epoch: 65, train_loss: 0.83586, time: 0.01529\n",
      "Epoch: 66, train_loss: 0.83103, time: 0.01616\n",
      "Epoch: 67, train_loss: 0.83465, time: 0.01604\n",
      "Epoch: 68, train_loss: 0.83351, time: 0.01609\n",
      "Epoch: 69, train_loss: 0.83534, time: 0.01576\n",
      "Epoch: 70, train_loss: 0.83471, time: 0.01578\n",
      "Epoch: 71, train_loss: 0.83402, time: 0.01642\n",
      "Epoch: 72, train_loss: 0.83276, time: 0.01615\n",
      "Epoch: 73, train_loss: 0.83160, time: 0.01619\n",
      "Epoch: 74, train_loss: 0.83164, time: 0.01654\n",
      "Epoch: 75, train_loss: 0.83498, time: 0.01613\n",
      "Epoch: 76, train_loss: 0.83100, time: 0.01615\n",
      "Epoch: 77, train_loss: 0.83250, time: 0.01571\n",
      "Epoch: 78, train_loss: 0.83346, time: 0.01595\n",
      "Epoch: 79, train_loss: 0.83216, time: 0.01634\n",
      "Epoch: 80, train_loss: 0.82857, time: 0.01611\n",
      "Epoch: 81, train_loss: 0.83017, time: 0.01623\n",
      "Epoch: 82, train_loss: 0.83524, time: 0.01653\n",
      "Epoch: 83, train_loss: 0.83277, time: 0.01627\n",
      "Epoch: 84, train_loss: 0.83482, time: 0.01633\n",
      "Epoch: 85, train_loss: 0.83344, time: 0.01609\n",
      "Epoch: 86, train_loss: 0.83385, time: 0.01627\n",
      "Epoch: 87, train_loss: 0.83224, time: 0.01617\n",
      "Epoch: 88, train_loss: 0.83283, time: 0.01599\n",
      "Epoch: 89, train_loss: 0.83285, time: 0.01643\n",
      "Epoch: 90, train_loss: 0.83659, time: 0.01607\n",
      "Epoch: 91, train_loss: 0.83255, time: 0.01598\n",
      "Epoch: 92, train_loss: 0.83320, time: 0.01607\n",
      "Epoch: 93, train_loss: 0.83435, time: 0.01634\n",
      "Epoch: 94, train_loss: 0.83119, time: 0.01651\n",
      "Epoch: 95, train_loss: 0.83334, time: 0.01624\n",
      "Epoch: 96, train_loss: 0.83216, time: 0.01576\n",
      "Epoch: 97, train_loss: 0.83463, time: 0.01656\n",
      "Epoch: 98, train_loss: 0.83135, time: 0.01690\n",
      "Epoch: 99, train_loss: 0.82948, time: 0.01580\n",
      "Epoch: 100, train_loss: 0.83275, time: 0.01582\n",
      "Epoch: 101, train_loss: 0.83320, time: 0.01644\n",
      "Epoch: 102, train_loss: 0.83382, time: 0.01616\n",
      "Epoch: 103, train_loss: 0.82964, time: 0.01586\n",
      "Epoch: 104, train_loss: 0.83079, time: 0.01626\n",
      "Epoch: 105, train_loss: 0.83371, time: 0.01618\n",
      "Epoch: 106, train_loss: 0.82875, time: 0.01677\n",
      "Epoch: 107, train_loss: 0.83141, time: 0.01643\n",
      "Epoch: 108, train_loss: 0.82847, time: 0.01577\n",
      "Epoch: 109, train_loss: 0.82821, time: 0.01613\n",
      "Epoch: 110, train_loss: 0.83158, time: 0.01618\n",
      "Epoch: 111, train_loss: 0.83072, time: 0.01604\n",
      "Epoch: 112, train_loss: 0.82968, time: 0.01610\n",
      "Epoch: 113, train_loss: 0.82582, time: 0.01676\n",
      "Epoch: 114, train_loss: 0.82299, time: 0.01659\n",
      "Epoch: 115, train_loss: 0.82620, time: 0.01635\n",
      "Epoch: 116, train_loss: 0.83831, time: 0.01626\n",
      "Epoch: 117, train_loss: 0.82660, time: 0.01626\n",
      "Epoch: 118, train_loss: 0.82265, time: 0.01672\n",
      "Epoch: 119, train_loss: 0.83004, time: 0.01639\n",
      "Epoch: 120, train_loss: 0.82214, time: 0.01608\n",
      "Epoch: 121, train_loss: 0.82677, time: 0.01605\n",
      "Epoch: 122, train_loss: 0.83336, time: 0.01637\n",
      "Epoch: 123, train_loss: 0.82371, time: 0.01686\n",
      "Epoch: 124, train_loss: 0.81836, time: 0.01621\n",
      "Epoch: 125, train_loss: 0.82518, time: 0.01557\n",
      "Epoch: 126, train_loss: 0.82434, time: 0.01605\n",
      "Epoch: 127, train_loss: 0.83054, time: 0.01641\n",
      "Epoch: 128, train_loss: 0.82804, time: 0.01606\n",
      "Epoch: 129, train_loss: 0.82790, time: 0.01627\n",
      "Epoch: 130, train_loss: 0.81820, time: 0.01615\n",
      "Epoch: 131, train_loss: 0.82312, time: 0.01617\n",
      "Epoch: 132, train_loss: 0.82734, time: 0.01558\n",
      "Epoch: 133, train_loss: 0.82748, time: 0.01560\n",
      "Epoch: 134, train_loss: 0.82677, time: 0.01587\n",
      "Epoch: 135, train_loss: 0.81860, time: 0.01613\n",
      "Epoch: 136, train_loss: 0.82034, time: 0.01624\n",
      "Epoch: 137, train_loss: 0.84777, time: 0.01612\n",
      "Epoch: 138, train_loss: 0.82045, time: 0.01582\n",
      "Epoch: 139, train_loss: 0.83030, time: 0.01631\n",
      "Epoch: 140, train_loss: 0.81566, time: 0.01612\n",
      "Epoch: 141, train_loss: 0.82560, time: 0.01643\n",
      "Epoch: 142, train_loss: 0.81614, time: 0.01592\n",
      "Epoch: 143, train_loss: 0.82784, time: 0.01613\n",
      "Epoch: 144, train_loss: 0.82693, time: 0.01626\n",
      "Epoch: 145, train_loss: 0.82233, time: 0.01568\n",
      "Epoch: 146, train_loss: 0.82410, time: 0.01601\n",
      "Epoch: 147, train_loss: 0.82005, time: 0.01595\n",
      "Epoch: 148, train_loss: 0.82813, time: 0.01657\n",
      "Epoch: 149, train_loss: 0.82930, time: 0.01671\n",
      "Epoch: 150, train_loss: 0.83024, time: 0.01649\n",
      "Epoch: 151, train_loss: 0.82882, time: 0.01635\n",
      "Epoch: 152, train_loss: 0.81588, time: 0.01639\n",
      "Epoch: 153, train_loss: 0.83263, time: 0.01609\n",
      "Epoch: 154, train_loss: 0.81907, time: 0.01616\n",
      "Epoch: 155, train_loss: 0.82684, time: 0.01648\n",
      "Epoch: 156, train_loss: 0.81823, time: 0.01595\n",
      "Epoch: 157, train_loss: 0.81610, time: 0.01585\n",
      "Epoch: 158, train_loss: 0.82266, time: 0.01592\n",
      "Epoch: 159, train_loss: 0.82132, time: 0.01644\n",
      "Epoch: 160, train_loss: 0.82126, time: 0.01609\n",
      "Epoch: 161, train_loss: 0.82494, time: 0.01606\n",
      "Epoch: 162, train_loss: 0.82124, time: 0.01619\n",
      "Epoch: 163, train_loss: 0.82035, time: 0.01593\n",
      "Epoch: 164, train_loss: 0.80943, time: 0.01591\n",
      "Epoch: 165, train_loss: 0.81939, time: 0.01554\n",
      "Epoch: 166, train_loss: 0.82350, time: 0.01549\n",
      "Epoch: 167, train_loss: 0.81718, time: 0.01557\n",
      "Epoch: 168, train_loss: 0.82080, time: 0.01583\n",
      "Epoch: 169, train_loss: 0.80872, time: 0.01647\n",
      "Epoch: 170, train_loss: 0.80892, time: 0.01639\n",
      "Epoch: 171, train_loss: 0.82261, time: 0.01648\n",
      "Epoch: 172, train_loss: 0.82430, time: 0.01656\n",
      "Epoch: 173, train_loss: 0.82133, time: 0.01596\n",
      "Epoch: 174, train_loss: 0.81937, time: 0.01626\n",
      "Epoch: 175, train_loss: 0.82207, time: 0.01666\n",
      "Epoch: 176, train_loss: 0.83259, time: 0.01640\n",
      "Epoch: 177, train_loss: 0.83039, time: 0.01653\n",
      "Epoch: 178, train_loss: 0.81923, time: 0.01595\n",
      "Epoch: 179, train_loss: 0.82260, time: 0.01583\n",
      "Epoch: 180, train_loss: 0.82611, time: 0.01529\n",
      "Epoch: 181, train_loss: 0.80695, time: 0.01571\n",
      "Epoch: 182, train_loss: 0.82646, time: 0.01573\n",
      "Epoch: 183, train_loss: 0.82645, time: 0.01581\n",
      "Epoch: 184, train_loss: 0.81157, time: 0.01560\n",
      "Epoch: 185, train_loss: 0.82366, time: 0.01566\n",
      "Epoch: 186, train_loss: 0.82562, time: 0.01545\n",
      "Epoch: 187, train_loss: 0.81919, time: 0.01534\n",
      "Epoch: 188, train_loss: 0.82966, time: 0.01569\n",
      "Epoch: 189, train_loss: 0.82601, time: 0.01567\n",
      "Epoch: 190, train_loss: 0.82257, time: 0.01623\n",
      "Epoch: 191, train_loss: 0.82155, time: 0.01586\n",
      "Epoch: 192, train_loss: 0.82759, time: 0.01642\n",
      "Epoch: 193, train_loss: 0.81181, time: 0.01601\n",
      "Epoch: 194, train_loss: 0.83279, time: 0.01551\n",
      "Epoch: 195, train_loss: 0.82666, time: 0.01571\n",
      "Epoch: 196, train_loss: 0.82891, time: 0.01554\n",
      "Epoch: 197, train_loss: 0.83423, time: 0.01530\n",
      "Epoch: 198, train_loss: 0.82955, time: 0.01601\n",
      "Epoch: 199, train_loss: 0.81982, time: 0.01605\n",
      "Epoch: 200, train_loss: 0.82236, time: 0.01640\n",
      "pairwise precision 1.00000 recall 1.00000 f1 1.00000\n",
      "average until now [0.4267768039089596, 0.839692846389022, 0.5659218587082709]\n",
      "64 names 274.48646211624146 avg time 4.288850970566273\n",
      "Loading yong_qing_huang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 196 nodes, 7547 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.80444, time: 0.11449\n",
      "Epoch: 2, train_loss: 0.80480, time: 0.01601\n",
      "Epoch: 3, train_loss: 0.80549, time: 0.01496\n",
      "Epoch: 4, train_loss: 0.80288, time: 0.01486\n",
      "Epoch: 5, train_loss: 0.80696, time: 0.01528\n",
      "Epoch: 6, train_loss: 0.80255, time: 0.01583\n",
      "Epoch: 7, train_loss: 0.80438, time: 0.01544\n",
      "Epoch: 8, train_loss: 0.80319, time: 0.01508\n",
      "Epoch: 9, train_loss: 0.80548, time: 0.01483\n",
      "Epoch: 10, train_loss: 0.80527, time: 0.01516\n",
      "Epoch: 11, train_loss: 0.80482, time: 0.01528\n",
      "Epoch: 12, train_loss: 0.80597, time: 0.01526\n",
      "Epoch: 13, train_loss: 0.80513, time: 0.01512\n",
      "Epoch: 14, train_loss: 0.80360, time: 0.01581\n",
      "Epoch: 15, train_loss: 0.80569, time: 0.01511\n",
      "Epoch: 16, train_loss: 0.80128, time: 0.01542\n",
      "Epoch: 17, train_loss: 0.80323, time: 0.01485\n",
      "Epoch: 18, train_loss: 0.80389, time: 0.01466\n",
      "Epoch: 19, train_loss: 0.80562, time: 0.01444\n",
      "Epoch: 20, train_loss: 0.80408, time: 0.01473\n",
      "Epoch: 21, train_loss: 0.80506, time: 0.01503\n",
      "Epoch: 22, train_loss: 0.80318, time: 0.01452\n",
      "Epoch: 23, train_loss: 0.80432, time: 0.01475\n",
      "Epoch: 24, train_loss: 0.80556, time: 0.01470\n",
      "Epoch: 25, train_loss: 0.80218, time: 0.01415\n",
      "Epoch: 26, train_loss: 0.80564, time: 0.01477\n",
      "Epoch: 27, train_loss: 0.80320, time: 0.01463\n",
      "Epoch: 28, train_loss: 0.80276, time: 0.01415\n",
      "Epoch: 29, train_loss: 0.80608, time: 0.01455\n",
      "Epoch: 30, train_loss: 0.80558, time: 0.01480\n",
      "Epoch: 31, train_loss: 0.80254, time: 0.01449\n",
      "Epoch: 32, train_loss: 0.80300, time: 0.01449\n",
      "Epoch: 33, train_loss: 0.80378, time: 0.01486\n",
      "Epoch: 34, train_loss: 0.80319, time: 0.01438\n",
      "Epoch: 35, train_loss: 0.80225, time: 0.01517\n",
      "Epoch: 36, train_loss: 0.80297, time: 0.01575\n",
      "Epoch: 37, train_loss: 0.80390, time: 0.01514\n",
      "Epoch: 38, train_loss: 0.80633, time: 0.01508\n",
      "Epoch: 39, train_loss: 0.79508, time: 0.01513\n",
      "Epoch: 40, train_loss: 0.80108, time: 0.01514\n",
      "Epoch: 41, train_loss: 0.80145, time: 0.01486\n",
      "Epoch: 42, train_loss: 0.80252, time: 0.01517\n",
      "Epoch: 43, train_loss: 0.79897, time: 0.01487\n",
      "Epoch: 44, train_loss: 0.79888, time: 0.01460\n",
      "Epoch: 45, train_loss: 0.80546, time: 0.01622\n",
      "Epoch: 46, train_loss: 0.79436, time: 0.01545\n",
      "Epoch: 47, train_loss: 0.79494, time: 0.01501\n",
      "Epoch: 48, train_loss: 0.79649, time: 0.01500\n",
      "Epoch: 49, train_loss: 0.79289, time: 0.01528\n",
      "Epoch: 50, train_loss: 0.78574, time: 0.01520\n",
      "Epoch: 51, train_loss: 0.78152, time: 0.01439\n",
      "Epoch: 52, train_loss: 0.80020, time: 0.01466\n",
      "Epoch: 53, train_loss: 0.79061, time: 0.01456\n",
      "Epoch: 54, train_loss: 0.77017, time: 0.01512\n",
      "Epoch: 55, train_loss: 0.78347, time: 0.01596\n",
      "Epoch: 56, train_loss: 0.77523, time: 0.01532\n",
      "Epoch: 57, train_loss: 0.76687, time: 0.01468\n",
      "Epoch: 58, train_loss: 0.77995, time: 0.01493\n",
      "Epoch: 59, train_loss: 0.77838, time: 0.01536\n",
      "Epoch: 60, train_loss: 0.76397, time: 0.01480\n",
      "Epoch: 61, train_loss: 0.76836, time: 0.01530\n",
      "Epoch: 62, train_loss: 0.76544, time: 0.01513\n",
      "Epoch: 63, train_loss: 0.76682, time: 0.01524\n",
      "Epoch: 64, train_loss: 0.79184, time: 0.01514\n",
      "Epoch: 65, train_loss: 0.75379, time: 0.01465\n",
      "Epoch: 66, train_loss: 0.76027, time: 0.01485\n",
      "Epoch: 67, train_loss: 0.77164, time: 0.01480\n",
      "Epoch: 68, train_loss: 0.77205, time: 0.01514\n",
      "Epoch: 69, train_loss: 0.76133, time: 0.01504\n",
      "Epoch: 70, train_loss: 0.75602, time: 0.01432\n",
      "Epoch: 71, train_loss: 0.75860, time: 0.01404\n",
      "Epoch: 72, train_loss: 0.76180, time: 0.01465\n",
      "Epoch: 73, train_loss: 0.78935, time: 0.01488\n",
      "Epoch: 74, train_loss: 0.77455, time: 0.01493\n",
      "Epoch: 75, train_loss: 0.74754, time: 0.01477\n",
      "Epoch: 76, train_loss: 0.77664, time: 0.01468\n",
      "Epoch: 77, train_loss: 0.76685, time: 0.01457\n",
      "Epoch: 78, train_loss: 0.74263, time: 0.01452\n",
      "Epoch: 79, train_loss: 0.74652, time: 0.01452\n",
      "Epoch: 80, train_loss: 0.77174, time: 0.01462\n",
      "Epoch: 81, train_loss: 0.74953, time: 0.01454\n",
      "Epoch: 82, train_loss: 0.77391, time: 0.01482\n",
      "Epoch: 83, train_loss: 0.76189, time: 0.01551\n",
      "Epoch: 84, train_loss: 0.75249, time: 0.01584\n",
      "Epoch: 85, train_loss: 0.77760, time: 0.01464\n",
      "Epoch: 86, train_loss: 0.75616, time: 0.01466\n",
      "Epoch: 87, train_loss: 0.72987, time: 0.01478\n",
      "Epoch: 88, train_loss: 0.76175, time: 0.01473\n",
      "Epoch: 89, train_loss: 0.80047, time: 0.01492\n",
      "Epoch: 90, train_loss: 0.76322, time: 0.01455\n",
      "Epoch: 91, train_loss: 0.75713, time: 0.01499\n",
      "Epoch: 92, train_loss: 0.75817, time: 0.01441\n",
      "Epoch: 93, train_loss: 0.76817, time: 0.01487\n",
      "Epoch: 94, train_loss: 0.76674, time: 0.01507\n",
      "Epoch: 95, train_loss: 0.75658, time: 0.01453\n",
      "Epoch: 96, train_loss: 0.75212, time: 0.01503\n",
      "Epoch: 97, train_loss: 0.76760, time: 0.01538\n",
      "Epoch: 98, train_loss: 0.75156, time: 0.01532\n",
      "Epoch: 99, train_loss: 0.75275, time: 0.01496\n",
      "Epoch: 100, train_loss: 0.75345, time: 0.01545\n",
      "Epoch: 101, train_loss: 0.76042, time: 0.01542\n",
      "Epoch: 102, train_loss: 0.75774, time: 0.01535\n",
      "Epoch: 103, train_loss: 0.75482, time: 0.01474\n",
      "Epoch: 104, train_loss: 0.76005, time: 0.01444\n",
      "Epoch: 105, train_loss: 0.76665, time: 0.01495\n",
      "Epoch: 106, train_loss: 0.76498, time: 0.01505\n",
      "Epoch: 107, train_loss: 0.76088, time: 0.01510\n",
      "Epoch: 108, train_loss: 0.74415, time: 0.01473\n",
      "Epoch: 109, train_loss: 0.74751, time: 0.01377\n",
      "Epoch: 110, train_loss: 0.76178, time: 0.01450\n",
      "Epoch: 111, train_loss: 0.76231, time: 0.01473\n",
      "Epoch: 112, train_loss: 0.75818, time: 0.01457\n",
      "Epoch: 113, train_loss: 0.74473, time: 0.01486\n",
      "Epoch: 114, train_loss: 0.75350, time: 0.01458\n",
      "Epoch: 115, train_loss: 0.74822, time: 0.01437\n",
      "Epoch: 116, train_loss: 0.75953, time: 0.01448\n",
      "Epoch: 117, train_loss: 0.74339, time: 0.01446\n",
      "Epoch: 118, train_loss: 0.75791, time: 0.01435\n",
      "Epoch: 119, train_loss: 0.75416, time: 0.01500\n",
      "Epoch: 120, train_loss: 0.76136, time: 0.01535\n",
      "Epoch: 121, train_loss: 0.71998, time: 0.01490\n",
      "Epoch: 122, train_loss: 0.74153, time: 0.01530\n",
      "Epoch: 123, train_loss: 0.74071, time: 0.01526\n",
      "Epoch: 124, train_loss: 0.74240, time: 0.01457\n",
      "Epoch: 125, train_loss: 0.75328, time: 0.01470\n",
      "Epoch: 126, train_loss: 0.75344, time: 0.01528\n",
      "Epoch: 127, train_loss: 0.76396, time: 0.01564\n",
      "Epoch: 128, train_loss: 0.74263, time: 0.01488\n",
      "Epoch: 129, train_loss: 0.76038, time: 0.01478\n",
      "Epoch: 130, train_loss: 0.74874, time: 0.01473\n",
      "Epoch: 131, train_loss: 0.75008, time: 0.01465\n",
      "Epoch: 132, train_loss: 0.75256, time: 0.01499\n",
      "Epoch: 133, train_loss: 0.78913, time: 0.01508\n",
      "Epoch: 134, train_loss: 0.74212, time: 0.01523\n",
      "Epoch: 135, train_loss: 0.73250, time: 0.01466\n",
      "Epoch: 136, train_loss: 0.73759, time: 0.01521\n",
      "Epoch: 137, train_loss: 0.75334, time: 0.01454\n",
      "Epoch: 138, train_loss: 0.75379, time: 0.01389\n",
      "Epoch: 139, train_loss: 0.74855, time: 0.01420\n",
      "Epoch: 140, train_loss: 0.75437, time: 0.01425\n",
      "Epoch: 141, train_loss: 0.74956, time: 0.01464\n",
      "Epoch: 142, train_loss: 0.76622, time: 0.01443\n",
      "Epoch: 143, train_loss: 0.73604, time: 0.01478\n",
      "Epoch: 144, train_loss: 0.79448, time: 0.01546\n",
      "Epoch: 145, train_loss: 0.75204, time: 0.01559\n",
      "Epoch: 146, train_loss: 0.74430, time: 0.01563\n",
      "Epoch: 147, train_loss: 0.77973, time: 0.01481\n",
      "Epoch: 148, train_loss: 0.73439, time: 0.01460\n",
      "Epoch: 149, train_loss: 0.76181, time: 0.01497\n",
      "Epoch: 150, train_loss: 0.73337, time: 0.01488\n",
      "Epoch: 151, train_loss: 0.75673, time: 0.01494\n",
      "Epoch: 152, train_loss: 0.75477, time: 0.01462\n",
      "Epoch: 153, train_loss: 0.75976, time: 0.01453\n",
      "Epoch: 154, train_loss: 0.74460, time: 0.01602\n",
      "Epoch: 155, train_loss: 0.75918, time: 0.01527\n",
      "Epoch: 156, train_loss: 0.75383, time: 0.01416\n",
      "Epoch: 157, train_loss: 0.74822, time: 0.01551\n",
      "Epoch: 158, train_loss: 0.76346, time: 0.01504\n",
      "Epoch: 159, train_loss: 0.76642, time: 0.01484\n",
      "Epoch: 160, train_loss: 0.74724, time: 0.01495\n",
      "Epoch: 161, train_loss: 0.75999, time: 0.01601\n",
      "Epoch: 162, train_loss: 0.74429, time: 0.01674\n",
      "Epoch: 163, train_loss: 0.78721, time: 0.01505\n",
      "Epoch: 164, train_loss: 0.73370, time: 0.01507\n",
      "Epoch: 165, train_loss: 0.74756, time: 0.01624\n",
      "Epoch: 166, train_loss: 0.74500, time: 0.01684\n",
      "Epoch: 167, train_loss: 0.74655, time: 0.01631\n",
      "Epoch: 168, train_loss: 0.74645, time: 0.01587\n",
      "Epoch: 169, train_loss: 0.75967, time: 0.01561\n",
      "Epoch: 170, train_loss: 0.75816, time: 0.01485\n",
      "Epoch: 171, train_loss: 0.73993, time: 0.01469\n",
      "Epoch: 172, train_loss: 0.74915, time: 0.01536\n",
      "Epoch: 173, train_loss: 0.75532, time: 0.01606\n",
      "Epoch: 174, train_loss: 0.73475, time: 0.01513\n",
      "Epoch: 175, train_loss: 0.75142, time: 0.01503\n",
      "Epoch: 176, train_loss: 0.75656, time: 0.01537\n",
      "Epoch: 177, train_loss: 0.74298, time: 0.01532\n",
      "Epoch: 178, train_loss: 0.74720, time: 0.01572\n",
      "Epoch: 179, train_loss: 0.76380, time: 0.01596\n",
      "Epoch: 180, train_loss: 0.80616, time: 0.01498\n",
      "Epoch: 181, train_loss: 0.74092, time: 0.01497\n",
      "Epoch: 182, train_loss: 0.78141, time: 0.01497\n",
      "Epoch: 183, train_loss: 0.75331, time: 0.01541\n",
      "Epoch: 184, train_loss: 0.74813, time: 0.01573\n",
      "Epoch: 185, train_loss: 0.75257, time: 0.01543\n",
      "Epoch: 186, train_loss: 0.74552, time: 0.01504\n",
      "Epoch: 187, train_loss: 0.75814, time: 0.01483\n",
      "Epoch: 188, train_loss: 0.75049, time: 0.01523\n",
      "Epoch: 189, train_loss: 0.74536, time: 0.01495\n",
      "Epoch: 190, train_loss: 0.75139, time: 0.01489\n",
      "Epoch: 191, train_loss: 0.73205, time: 0.01504\n",
      "Epoch: 192, train_loss: 0.74839, time: 0.01476\n",
      "Epoch: 193, train_loss: 0.76574, time: 0.01467\n",
      "Epoch: 194, train_loss: 0.76295, time: 0.01490\n",
      "Epoch: 195, train_loss: 0.74616, time: 0.01489\n",
      "Epoch: 196, train_loss: 0.75413, time: 0.01481\n",
      "Epoch: 197, train_loss: 0.74752, time: 0.01472\n",
      "Epoch: 198, train_loss: 0.75918, time: 0.01476\n",
      "Epoch: 199, train_loss: 0.73544, time: 0.01519\n",
      "Epoch: 200, train_loss: 0.76355, time: 0.01477\n",
      "pairwise precision 0.55050 recall 0.45854 f1 0.50033\n",
      "average until now [0.4286803059505319, 0.833828957098776, 0.5662470175881094]\n",
      "65 names 277.64236760139465 avg time 4.2714210400214565\n",
      "Loading jie_jiang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 558 nodes, 3515 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99178, time: 0.11978\n",
      "Epoch: 2, train_loss: 0.98927, time: 0.02084\n",
      "Epoch: 3, train_loss: 0.98748, time: 0.01927\n",
      "Epoch: 4, train_loss: 0.98937, time: 0.02029\n",
      "Epoch: 5, train_loss: 0.98837, time: 0.01901\n",
      "Epoch: 6, train_loss: 0.98644, time: 0.02018\n",
      "Epoch: 7, train_loss: 0.98641, time: 0.01956\n",
      "Epoch: 8, train_loss: 0.98762, time: 0.01941\n",
      "Epoch: 9, train_loss: 0.98784, time: 0.01931\n",
      "Epoch: 10, train_loss: 0.98614, time: 0.01947\n",
      "Epoch: 11, train_loss: 0.98635, time: 0.01917\n",
      "Epoch: 12, train_loss: 0.98767, time: 0.01975\n",
      "Epoch: 13, train_loss: 0.98593, time: 0.01950\n",
      "Epoch: 14, train_loss: 0.98749, time: 0.02015\n",
      "Epoch: 15, train_loss: 0.98685, time: 0.01978\n",
      "Epoch: 16, train_loss: 0.98556, time: 0.01928\n",
      "Epoch: 17, train_loss: 0.98660, time: 0.01939\n",
      "Epoch: 18, train_loss: 0.98624, time: 0.01993\n",
      "Epoch: 19, train_loss: 0.98624, time: 0.01884\n",
      "Epoch: 20, train_loss: 0.98628, time: 0.02012\n",
      "Epoch: 21, train_loss: 0.98648, time: 0.01959\n",
      "Epoch: 22, train_loss: 0.98757, time: 0.01905\n",
      "Epoch: 23, train_loss: 0.98752, time: 0.01911\n",
      "Epoch: 24, train_loss: 0.98791, time: 0.01914\n",
      "Epoch: 25, train_loss: 0.98644, time: 0.01948\n",
      "Epoch: 26, train_loss: 0.98676, time: 0.02021\n",
      "Epoch: 27, train_loss: 0.98810, time: 0.02003\n",
      "Epoch: 28, train_loss: 0.98737, time: 0.01932\n",
      "Epoch: 29, train_loss: 0.98720, time: 0.01997\n",
      "Epoch: 30, train_loss: 0.98661, time: 0.01999\n",
      "Epoch: 31, train_loss: 0.98705, time: 0.01969\n",
      "Epoch: 32, train_loss: 0.98614, time: 0.01999\n",
      "Epoch: 33, train_loss: 0.98647, time: 0.01969\n",
      "Epoch: 34, train_loss: 0.98683, time: 0.02026\n",
      "Epoch: 35, train_loss: 0.98640, time: 0.01903\n",
      "Epoch: 36, train_loss: 0.98625, time: 0.02003\n",
      "Epoch: 37, train_loss: 0.98744, time: 0.01748\n",
      "Epoch: 38, train_loss: 0.98589, time: 0.02008\n",
      "Epoch: 39, train_loss: 0.98723, time: 0.02004\n",
      "Epoch: 40, train_loss: 0.98583, time: 0.01968\n",
      "Epoch: 41, train_loss: 0.98596, time: 0.01980\n",
      "Epoch: 42, train_loss: 0.98686, time: 0.01948\n",
      "Epoch: 43, train_loss: 0.98729, time: 0.01954\n",
      "Epoch: 44, train_loss: 0.98753, time: 0.01995\n",
      "Epoch: 45, train_loss: 0.98684, time: 0.01958\n",
      "Epoch: 46, train_loss: 0.98685, time: 0.01909\n",
      "Epoch: 47, train_loss: 0.98705, time: 0.02007\n",
      "Epoch: 48, train_loss: 0.98640, time: 0.01838\n",
      "Epoch: 49, train_loss: 0.98808, time: 0.01944\n",
      "Epoch: 50, train_loss: 0.98721, time: 0.02006\n",
      "Epoch: 51, train_loss: 0.98688, time: 0.02063\n",
      "Epoch: 52, train_loss: 0.98743, time: 0.01949\n",
      "Epoch: 53, train_loss: 0.98721, time: 0.02010\n",
      "Epoch: 54, train_loss: 0.98676, time: 0.01997\n",
      "Epoch: 55, train_loss: 0.98698, time: 0.01937\n",
      "Epoch: 56, train_loss: 0.98741, time: 0.01997\n",
      "Epoch: 57, train_loss: 0.98664, time: 0.01956\n",
      "Epoch: 58, train_loss: 0.98684, time: 0.01972\n",
      "Epoch: 59, train_loss: 0.98710, time: 0.01957\n",
      "Epoch: 60, train_loss: 0.98743, time: 0.01913\n",
      "Epoch: 61, train_loss: 0.98682, time: 0.01984\n",
      "Epoch: 62, train_loss: 0.98596, time: 0.01958\n",
      "Epoch: 63, train_loss: 0.98744, time: 0.01934\n",
      "Epoch: 64, train_loss: 0.98625, time: 0.01965\n",
      "Epoch: 65, train_loss: 0.98543, time: 0.02000\n",
      "Epoch: 66, train_loss: 0.98630, time: 0.01957\n",
      "Epoch: 67, train_loss: 0.98692, time: 0.01849\n",
      "Epoch: 68, train_loss: 0.98694, time: 0.01963\n",
      "Epoch: 69, train_loss: 0.98708, time: 0.01987\n",
      "Epoch: 70, train_loss: 0.98723, time: 0.01878\n",
      "Epoch: 71, train_loss: 0.98755, time: 0.02029\n",
      "Epoch: 72, train_loss: 0.98690, time: 0.01998\n",
      "Epoch: 73, train_loss: 0.98643, time: 0.01951\n",
      "Epoch: 74, train_loss: 0.98722, time: 0.01982\n",
      "Epoch: 75, train_loss: 0.98618, time: 0.02068\n",
      "Epoch: 76, train_loss: 0.98642, time: 0.01900\n",
      "Epoch: 77, train_loss: 0.98771, time: 0.01902\n",
      "Epoch: 78, train_loss: 0.98695, time: 0.01908\n",
      "Epoch: 79, train_loss: 0.98597, time: 0.01869\n",
      "Epoch: 80, train_loss: 0.98660, time: 0.01807\n",
      "Epoch: 81, train_loss: 0.98803, time: 0.01842\n",
      "Epoch: 82, train_loss: 0.98662, time: 0.01988\n",
      "Epoch: 83, train_loss: 0.98579, time: 0.01939\n",
      "Epoch: 84, train_loss: 0.98659, time: 0.01961\n",
      "Epoch: 85, train_loss: 0.98696, time: 0.01964\n",
      "Epoch: 86, train_loss: 0.98644, time: 0.01973\n",
      "Epoch: 87, train_loss: 0.98683, time: 0.02043\n",
      "Epoch: 88, train_loss: 0.98543, time: 0.01952\n",
      "Epoch: 89, train_loss: 0.98691, time: 0.01955\n",
      "Epoch: 90, train_loss: 0.98676, time: 0.02024\n",
      "Epoch: 91, train_loss: 0.98690, time: 0.01965\n",
      "Epoch: 92, train_loss: 0.98702, time: 0.01963\n",
      "Epoch: 93, train_loss: 0.98628, time: 0.01965\n",
      "Epoch: 94, train_loss: 0.98784, time: 0.01921\n",
      "Epoch: 95, train_loss: 0.98666, time: 0.01971\n",
      "Epoch: 96, train_loss: 0.98580, time: 0.01943\n",
      "Epoch: 97, train_loss: 0.98640, time: 0.01967\n",
      "Epoch: 98, train_loss: 0.98705, time: 0.01948\n",
      "Epoch: 99, train_loss: 0.98732, time: 0.01988\n",
      "Epoch: 100, train_loss: 0.98745, time: 0.02060\n",
      "Epoch: 101, train_loss: 0.98763, time: 0.01982\n",
      "Epoch: 102, train_loss: 0.98661, time: 0.02047\n",
      "Epoch: 103, train_loss: 0.98674, time: 0.02052\n",
      "Epoch: 104, train_loss: 0.98694, time: 0.01944\n",
      "Epoch: 105, train_loss: 0.98656, time: 0.01858\n",
      "Epoch: 106, train_loss: 0.98755, time: 0.01854\n",
      "Epoch: 107, train_loss: 0.98669, time: 0.01829\n",
      "Epoch: 108, train_loss: 0.98676, time: 0.01801\n",
      "Epoch: 109, train_loss: 0.98585, time: 0.01769\n",
      "Epoch: 110, train_loss: 0.98656, time: 0.01804\n",
      "Epoch: 111, train_loss: 0.98817, time: 0.01830\n",
      "Epoch: 112, train_loss: 0.98593, time: 0.01811\n",
      "Epoch: 113, train_loss: 0.98754, time: 0.01790\n",
      "Epoch: 114, train_loss: 0.98727, time: 0.01843\n",
      "Epoch: 115, train_loss: 0.98669, time: 0.01835\n",
      "Epoch: 116, train_loss: 0.98664, time: 0.01942\n",
      "Epoch: 117, train_loss: 0.98649, time: 0.01942\n",
      "Epoch: 118, train_loss: 0.98703, time: 0.01971\n",
      "Epoch: 119, train_loss: 0.98626, time: 0.01965\n",
      "Epoch: 120, train_loss: 0.98670, time: 0.01963\n",
      "Epoch: 121, train_loss: 0.98620, time: 0.01962\n",
      "Epoch: 122, train_loss: 0.98571, time: 0.01987\n",
      "Epoch: 123, train_loss: 0.98693, time: 0.01976\n",
      "Epoch: 124, train_loss: 0.98550, time: 0.01887\n",
      "Epoch: 125, train_loss: 0.98709, time: 0.01891\n",
      "Epoch: 126, train_loss: 0.98677, time: 0.01880\n",
      "Epoch: 127, train_loss: 0.98642, time: 0.01967\n",
      "Epoch: 128, train_loss: 0.98699, time: 0.01997\n",
      "Epoch: 129, train_loss: 0.98656, time: 0.01988\n",
      "Epoch: 130, train_loss: 0.98670, time: 0.01941\n",
      "Epoch: 131, train_loss: 0.98702, time: 0.01922\n",
      "Epoch: 132, train_loss: 0.98783, time: 0.01966\n",
      "Epoch: 133, train_loss: 0.98850, time: 0.01977\n",
      "Epoch: 134, train_loss: 0.98678, time: 0.01926\n",
      "Epoch: 135, train_loss: 0.98700, time: 0.01922\n",
      "Epoch: 136, train_loss: 0.98813, time: 0.02018\n",
      "Epoch: 137, train_loss: 0.98644, time: 0.02013\n",
      "Epoch: 138, train_loss: 0.98675, time: 0.01965\n",
      "Epoch: 139, train_loss: 0.98565, time: 0.01965\n",
      "Epoch: 140, train_loss: 0.98737, time: 0.01987\n",
      "Epoch: 141, train_loss: 0.98711, time: 0.02014\n",
      "Epoch: 142, train_loss: 0.98598, time: 0.01959\n",
      "Epoch: 143, train_loss: 0.98673, time: 0.02020\n",
      "Epoch: 144, train_loss: 0.98643, time: 0.01912\n",
      "Epoch: 145, train_loss: 0.98730, time: 0.01988\n",
      "Epoch: 146, train_loss: 0.98815, time: 0.01966\n",
      "Epoch: 147, train_loss: 0.98649, time: 0.01953\n",
      "Epoch: 148, train_loss: 0.98693, time: 0.01971\n",
      "Epoch: 149, train_loss: 0.98715, time: 0.02084\n",
      "Epoch: 150, train_loss: 0.98744, time: 0.02046\n",
      "Epoch: 151, train_loss: 0.98740, time: 0.01980\n",
      "Epoch: 152, train_loss: 0.98672, time: 0.01982\n",
      "Epoch: 153, train_loss: 0.98733, time: 0.01950\n",
      "Epoch: 154, train_loss: 0.98737, time: 0.01988\n",
      "Epoch: 155, train_loss: 0.98571, time: 0.01982\n",
      "Epoch: 156, train_loss: 0.98717, time: 0.02035\n",
      "Epoch: 157, train_loss: 0.98681, time: 0.01972\n",
      "Epoch: 158, train_loss: 0.98918, time: 0.01932\n",
      "Epoch: 159, train_loss: 0.98671, time: 0.01904\n",
      "Epoch: 160, train_loss: 0.98826, time: 0.01946\n",
      "Epoch: 161, train_loss: 0.98565, time: 0.01971\n",
      "Epoch: 162, train_loss: 0.98578, time: 0.02008\n",
      "Epoch: 163, train_loss: 0.98781, time: 0.01961\n",
      "Epoch: 164, train_loss: 0.98717, time: 0.02001\n",
      "Epoch: 165, train_loss: 0.98719, time: 0.02023\n",
      "Epoch: 166, train_loss: 0.98630, time: 0.01950\n",
      "Epoch: 167, train_loss: 0.98601, time: 0.01968\n",
      "Epoch: 168, train_loss: 0.98742, time: 0.01991\n",
      "Epoch: 169, train_loss: 0.98654, time: 0.02005\n",
      "Epoch: 170, train_loss: 0.98687, time: 0.01854\n",
      "Epoch: 171, train_loss: 0.98602, time: 0.02024\n",
      "Epoch: 172, train_loss: 0.98708, time: 0.01979\n",
      "Epoch: 173, train_loss: 0.98593, time: 0.01960\n",
      "Epoch: 174, train_loss: 0.98644, time: 0.02006\n",
      "Epoch: 175, train_loss: 0.98755, time: 0.02006\n",
      "Epoch: 176, train_loss: 0.98671, time: 0.02002\n",
      "Epoch: 177, train_loss: 0.98616, time: 0.01927\n",
      "Epoch: 178, train_loss: 0.98721, time: 0.01993\n",
      "Epoch: 179, train_loss: 0.98703, time: 0.02047\n",
      "Epoch: 180, train_loss: 0.98746, time: 0.01919\n",
      "Epoch: 181, train_loss: 0.98821, time: 0.02004\n",
      "Epoch: 182, train_loss: 0.98700, time: 0.02006\n",
      "Epoch: 183, train_loss: 0.98596, time: 0.02083\n",
      "Epoch: 184, train_loss: 0.98580, time: 0.01870\n",
      "Epoch: 185, train_loss: 0.98742, time: 0.01999\n",
      "Epoch: 186, train_loss: 0.98659, time: 0.01998\n",
      "Epoch: 187, train_loss: 0.98615, time: 0.02034\n",
      "Epoch: 188, train_loss: 0.98558, time: 0.01974\n",
      "Epoch: 189, train_loss: 0.98607, time: 0.01945\n",
      "Epoch: 190, train_loss: 0.98628, time: 0.01938\n",
      "Epoch: 191, train_loss: 0.98688, time: 0.01924\n",
      "Epoch: 192, train_loss: 0.98577, time: 0.01994\n",
      "Epoch: 193, train_loss: 0.98645, time: 0.01965\n",
      "Epoch: 194, train_loss: 0.98566, time: 0.02024\n",
      "Epoch: 195, train_loss: 0.98650, time: 0.01948\n",
      "Epoch: 196, train_loss: 0.98672, time: 0.01986\n",
      "Epoch: 197, train_loss: 0.98603, time: 0.01964\n",
      "Epoch: 198, train_loss: 0.98660, time: 0.01961\n",
      "Epoch: 199, train_loss: 0.98691, time: 0.01940\n",
      "Epoch: 200, train_loss: 0.98521, time: 0.01936\n",
      "pairwise precision 0.06309 recall 0.90647 f1 0.11797\n",
      "average until now [0.423141090186471, 0.8349296297341164, 0.5616425661301161]\n",
      "66 names 281.78081464767456 avg time 4.2694062825405235\n",
      "Loading xiao_ning_zhang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 186 nodes, 2332 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.94054, time: 0.11101\n",
      "Epoch: 2, train_loss: 0.93314, time: 0.01284\n",
      "Epoch: 3, train_loss: 0.93379, time: 0.01116\n",
      "Epoch: 4, train_loss: 0.92967, time: 0.01131\n",
      "Epoch: 5, train_loss: 0.93685, time: 0.01119\n",
      "Epoch: 6, train_loss: 0.93607, time: 0.01108\n",
      "Epoch: 7, train_loss: 0.93200, time: 0.01059\n",
      "Epoch: 8, train_loss: 0.93443, time: 0.01115\n",
      "Epoch: 9, train_loss: 0.93548, time: 0.01084\n",
      "Epoch: 10, train_loss: 0.93330, time: 0.01097\n",
      "Epoch: 11, train_loss: 0.93105, time: 0.01097\n",
      "Epoch: 12, train_loss: 0.93289, time: 0.01107\n",
      "Epoch: 13, train_loss: 0.93092, time: 0.01084\n",
      "Epoch: 14, train_loss: 0.93138, time: 0.01118\n",
      "Epoch: 15, train_loss: 0.93873, time: 0.01085\n",
      "Epoch: 16, train_loss: 0.93339, time: 0.01111\n",
      "Epoch: 17, train_loss: 0.93048, time: 0.01073\n",
      "Epoch: 18, train_loss: 0.93257, time: 0.01119\n",
      "Epoch: 19, train_loss: 0.93648, time: 0.01084\n",
      "Epoch: 20, train_loss: 0.93555, time: 0.01137\n",
      "Epoch: 21, train_loss: 0.93588, time: 0.01087\n",
      "Epoch: 22, train_loss: 0.93382, time: 0.01113\n",
      "Epoch: 23, train_loss: 0.93422, time: 0.01119\n",
      "Epoch: 24, train_loss: 0.93193, time: 0.01097\n",
      "Epoch: 25, train_loss: 0.93360, time: 0.01107\n",
      "Epoch: 26, train_loss: 0.93242, time: 0.01120\n",
      "Epoch: 27, train_loss: 0.93451, time: 0.01112\n",
      "Epoch: 28, train_loss: 0.93559, time: 0.01103\n",
      "Epoch: 29, train_loss: 0.93710, time: 0.01091\n",
      "Epoch: 30, train_loss: 0.93294, time: 0.01095\n",
      "Epoch: 31, train_loss: 0.93033, time: 0.01080\n",
      "Epoch: 32, train_loss: 0.93160, time: 0.01084\n",
      "Epoch: 33, train_loss: 0.93230, time: 0.01100\n",
      "Epoch: 34, train_loss: 0.93214, time: 0.01095\n",
      "Epoch: 35, train_loss: 0.93304, time: 0.01100\n",
      "Epoch: 36, train_loss: 0.93348, time: 0.01083\n",
      "Epoch: 37, train_loss: 0.93249, time: 0.01106\n",
      "Epoch: 38, train_loss: 0.93281, time: 0.01089\n",
      "Epoch: 39, train_loss: 0.93218, time: 0.01166\n",
      "Epoch: 40, train_loss: 0.93538, time: 0.01103\n",
      "Epoch: 41, train_loss: 0.93110, time: 0.01107\n",
      "Epoch: 42, train_loss: 0.93085, time: 0.01121\n",
      "Epoch: 43, train_loss: 0.93211, time: 0.01146\n",
      "Epoch: 44, train_loss: 0.93325, time: 0.01117\n",
      "Epoch: 45, train_loss: 0.93216, time: 0.01098\n",
      "Epoch: 46, train_loss: 0.93271, time: 0.01100\n",
      "Epoch: 47, train_loss: 0.93386, time: 0.01099\n",
      "Epoch: 48, train_loss: 0.93188, time: 0.01107\n",
      "Epoch: 49, train_loss: 0.93267, time: 0.01109\n",
      "Epoch: 50, train_loss: 0.93696, time: 0.01074\n",
      "Epoch: 51, train_loss: 0.93413, time: 0.01116\n",
      "Epoch: 52, train_loss: 0.93450, time: 0.01099\n",
      "Epoch: 53, train_loss: 0.93383, time: 0.01131\n",
      "Epoch: 54, train_loss: 0.93284, time: 0.01097\n",
      "Epoch: 55, train_loss: 0.93453, time: 0.01073\n",
      "Epoch: 56, train_loss: 0.93099, time: 0.01092\n",
      "Epoch: 57, train_loss: 0.93353, time: 0.01098\n",
      "Epoch: 58, train_loss: 0.93398, time: 0.01087\n",
      "Epoch: 59, train_loss: 0.93156, time: 0.01118\n",
      "Epoch: 60, train_loss: 0.93056, time: 0.01123\n",
      "Epoch: 61, train_loss: 0.93302, time: 0.01108\n",
      "Epoch: 62, train_loss: 0.93147, time: 0.01086\n",
      "Epoch: 63, train_loss: 0.93339, time: 0.01081\n",
      "Epoch: 64, train_loss: 0.93303, time: 0.01125\n",
      "Epoch: 65, train_loss: 0.93357, time: 0.01094\n",
      "Epoch: 66, train_loss: 0.93204, time: 0.01088\n",
      "Epoch: 67, train_loss: 0.93136, time: 0.01125\n",
      "Epoch: 68, train_loss: 0.93502, time: 0.01078\n",
      "Epoch: 69, train_loss: 0.93488, time: 0.01107\n",
      "Epoch: 70, train_loss: 0.93083, time: 0.01130\n",
      "Epoch: 71, train_loss: 0.93308, time: 0.01080\n",
      "Epoch: 72, train_loss: 0.93172, time: 0.01141\n",
      "Epoch: 73, train_loss: 0.93448, time: 0.01085\n",
      "Epoch: 74, train_loss: 0.93210, time: 0.01075\n",
      "Epoch: 75, train_loss: 0.93309, time: 0.01119\n",
      "Epoch: 76, train_loss: 0.93228, time: 0.01071\n",
      "Epoch: 77, train_loss: 0.93036, time: 0.01144\n",
      "Epoch: 78, train_loss: 0.93010, time: 0.01109\n",
      "Epoch: 79, train_loss: 0.92942, time: 0.01096\n",
      "Epoch: 80, train_loss: 0.93251, time: 0.01114\n",
      "Epoch: 81, train_loss: 0.93318, time: 0.01094\n",
      "Epoch: 82, train_loss: 0.93463, time: 0.01109\n",
      "Epoch: 83, train_loss: 0.93379, time: 0.01097\n",
      "Epoch: 84, train_loss: 0.93256, time: 0.01151\n",
      "Epoch: 85, train_loss: 0.93465, time: 0.01087\n",
      "Epoch: 86, train_loss: 0.93502, time: 0.01107\n",
      "Epoch: 87, train_loss: 0.93391, time: 0.01116\n",
      "Epoch: 88, train_loss: 0.93798, time: 0.01088\n",
      "Epoch: 89, train_loss: 0.93192, time: 0.01101\n",
      "Epoch: 90, train_loss: 0.92965, time: 0.01066\n",
      "Epoch: 91, train_loss: 0.93479, time: 0.01102\n",
      "Epoch: 92, train_loss: 0.93369, time: 0.01081\n",
      "Epoch: 93, train_loss: 0.93498, time: 0.01089\n",
      "Epoch: 94, train_loss: 0.93590, time: 0.01088\n",
      "Epoch: 95, train_loss: 0.93045, time: 0.01105\n",
      "Epoch: 96, train_loss: 0.93159, time: 0.01135\n",
      "Epoch: 97, train_loss: 0.93317, time: 0.01186\n",
      "Epoch: 98, train_loss: 0.92966, time: 0.01069\n",
      "Epoch: 99, train_loss: 0.93228, time: 0.01129\n",
      "Epoch: 100, train_loss: 0.93192, time: 0.01073\n",
      "Epoch: 101, train_loss: 0.93395, time: 0.01129\n",
      "Epoch: 102, train_loss: 0.93011, time: 0.01098\n",
      "Epoch: 103, train_loss: 0.93326, time: 0.01121\n",
      "Epoch: 104, train_loss: 0.93461, time: 0.01112\n",
      "Epoch: 105, train_loss: 0.93178, time: 0.01075\n",
      "Epoch: 106, train_loss: 0.93579, time: 0.01111\n",
      "Epoch: 107, train_loss: 0.93132, time: 0.01086\n",
      "Epoch: 108, train_loss: 0.93283, time: 0.01117\n",
      "Epoch: 109, train_loss: 0.93212, time: 0.01072\n",
      "Epoch: 110, train_loss: 0.93176, time: 0.01121\n",
      "Epoch: 111, train_loss: 0.93419, time: 0.01079\n",
      "Epoch: 112, train_loss: 0.93485, time: 0.01106\n",
      "Epoch: 113, train_loss: 0.93411, time: 0.01092\n",
      "Epoch: 114, train_loss: 0.93549, time: 0.01099\n",
      "Epoch: 115, train_loss: 0.93053, time: 0.01150\n",
      "Epoch: 116, train_loss: 0.93003, time: 0.01094\n",
      "Epoch: 117, train_loss: 0.93001, time: 0.01118\n",
      "Epoch: 118, train_loss: 0.93284, time: 0.01079\n",
      "Epoch: 119, train_loss: 0.93444, time: 0.01104\n",
      "Epoch: 120, train_loss: 0.93191, time: 0.01075\n",
      "Epoch: 121, train_loss: 0.92989, time: 0.01106\n",
      "Epoch: 122, train_loss: 0.93422, time: 0.01076\n",
      "Epoch: 123, train_loss: 0.93385, time: 0.01100\n",
      "Epoch: 124, train_loss: 0.93266, time: 0.01087\n",
      "Epoch: 125, train_loss: 0.93748, time: 0.01094\n",
      "Epoch: 126, train_loss: 0.93232, time: 0.01076\n",
      "Epoch: 127, train_loss: 0.93296, time: 0.01114\n",
      "Epoch: 128, train_loss: 0.93072, time: 0.01100\n",
      "Epoch: 129, train_loss: 0.93383, time: 0.01124\n",
      "Epoch: 130, train_loss: 0.93193, time: 0.01117\n",
      "Epoch: 131, train_loss: 0.93431, time: 0.01122\n",
      "Epoch: 132, train_loss: 0.93188, time: 0.01077\n",
      "Epoch: 133, train_loss: 0.93493, time: 0.01080\n",
      "Epoch: 134, train_loss: 0.93571, time: 0.01136\n",
      "Epoch: 135, train_loss: 0.93050, time: 0.01107\n",
      "Epoch: 136, train_loss: 0.93512, time: 0.01097\n",
      "Epoch: 137, train_loss: 0.92874, time: 0.01095\n",
      "Epoch: 138, train_loss: 0.93331, time: 0.01100\n",
      "Epoch: 139, train_loss: 0.93160, time: 0.01118\n",
      "Epoch: 140, train_loss: 0.93238, time: 0.01113\n",
      "Epoch: 141, train_loss: 0.93454, time: 0.01086\n",
      "Epoch: 142, train_loss: 0.93718, time: 0.01112\n",
      "Epoch: 143, train_loss: 0.93752, time: 0.01088\n",
      "Epoch: 144, train_loss: 0.93711, time: 0.01121\n",
      "Epoch: 145, train_loss: 0.93429, time: 0.01246\n",
      "Epoch: 146, train_loss: 0.93209, time: 0.01150\n",
      "Epoch: 147, train_loss: 0.93276, time: 0.01099\n",
      "Epoch: 148, train_loss: 0.93662, time: 0.01136\n",
      "Epoch: 149, train_loss: 0.93094, time: 0.01167\n",
      "Epoch: 150, train_loss: 0.93426, time: 0.01116\n",
      "Epoch: 151, train_loss: 0.93231, time: 0.01129\n",
      "Epoch: 152, train_loss: 0.92946, time: 0.01125\n",
      "Epoch: 153, train_loss: 0.93219, time: 0.01092\n",
      "Epoch: 154, train_loss: 0.93252, time: 0.01116\n",
      "Epoch: 155, train_loss: 0.93327, time: 0.01101\n",
      "Epoch: 156, train_loss: 0.93220, time: 0.01111\n",
      "Epoch: 157, train_loss: 0.93047, time: 0.01109\n",
      "Epoch: 158, train_loss: 0.92929, time: 0.01095\n",
      "Epoch: 159, train_loss: 0.93416, time: 0.01135\n",
      "Epoch: 160, train_loss: 0.93505, time: 0.01089\n",
      "Epoch: 161, train_loss: 0.93101, time: 0.01113\n",
      "Epoch: 162, train_loss: 0.93559, time: 0.01084\n",
      "Epoch: 163, train_loss: 0.93635, time: 0.01104\n",
      "Epoch: 164, train_loss: 0.93404, time: 0.01076\n",
      "Epoch: 165, train_loss: 0.92879, time: 0.01112\n",
      "Epoch: 166, train_loss: 0.93458, time: 0.01081\n",
      "Epoch: 167, train_loss: 0.93352, time: 0.01108\n",
      "Epoch: 168, train_loss: 0.93186, time: 0.01078\n",
      "Epoch: 169, train_loss: 0.93315, time: 0.01129\n",
      "Epoch: 170, train_loss: 0.93201, time: 0.01087\n",
      "Epoch: 171, train_loss: 0.93133, time: 0.01122\n",
      "Epoch: 172, train_loss: 0.92938, time: 0.01102\n",
      "Epoch: 173, train_loss: 0.93472, time: 0.01117\n",
      "Epoch: 174, train_loss: 0.93183, time: 0.01079\n",
      "Epoch: 175, train_loss: 0.93676, time: 0.01094\n",
      "Epoch: 176, train_loss: 0.93550, time: 0.01107\n",
      "Epoch: 177, train_loss: 0.93324, time: 0.01100\n",
      "Epoch: 178, train_loss: 0.93218, time: 0.01131\n",
      "Epoch: 179, train_loss: 0.93399, time: 0.01080\n",
      "Epoch: 180, train_loss: 0.92923, time: 0.01131\n",
      "Epoch: 181, train_loss: 0.93214, time: 0.01135\n",
      "Epoch: 182, train_loss: 0.93160, time: 0.01081\n",
      "Epoch: 183, train_loss: 0.93270, time: 0.01117\n",
      "Epoch: 184, train_loss: 0.93345, time: 0.01136\n",
      "Epoch: 185, train_loss: 0.93361, time: 0.01101\n",
      "Epoch: 186, train_loss: 0.93035, time: 0.01113\n",
      "Epoch: 187, train_loss: 0.93566, time: 0.01064\n",
      "Epoch: 188, train_loss: 0.93324, time: 0.01111\n",
      "Epoch: 189, train_loss: 0.93116, time: 0.01133\n",
      "Epoch: 190, train_loss: 0.93346, time: 0.01118\n",
      "Epoch: 191, train_loss: 0.93176, time: 0.01095\n",
      "Epoch: 192, train_loss: 0.93115, time: 0.01079\n",
      "Epoch: 193, train_loss: 0.93258, time: 0.01099\n",
      "Epoch: 194, train_loss: 0.93276, time: 0.01105\n",
      "Epoch: 195, train_loss: 0.93392, time: 0.01119\n",
      "Epoch: 196, train_loss: 0.93482, time: 0.01114\n",
      "Epoch: 197, train_loss: 0.93251, time: 0.01133\n",
      "Epoch: 198, train_loss: 0.93350, time: 0.01090\n",
      "Epoch: 199, train_loss: 0.93464, time: 0.01108\n",
      "Epoch: 200, train_loss: 0.93414, time: 0.01095\n",
      "pairwise precision 0.78667 recall 0.99646 f1 0.87922\n",
      "average until now [0.4285668286991507, 0.8373405224210462, 0.5669551913380187]\n",
      "67 names 284.1403896808624 avg time 4.2409013385203345\n",
      "Loading chun_hung_lin dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 217 nodes, 2197 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.95750, time: 0.11168\n",
      "Epoch: 2, train_loss: 0.95339, time: 0.01282\n",
      "Epoch: 3, train_loss: 0.95570, time: 0.01154\n",
      "Epoch: 4, train_loss: 0.95712, time: 0.01126\n",
      "Epoch: 5, train_loss: 0.95335, time: 0.01157\n",
      "Epoch: 6, train_loss: 0.95570, time: 0.01169\n",
      "Epoch: 7, train_loss: 0.95199, time: 0.01127\n",
      "Epoch: 8, train_loss: 0.95424, time: 0.01123\n",
      "Epoch: 9, train_loss: 0.95420, time: 0.01141\n",
      "Epoch: 10, train_loss: 0.95306, time: 0.01133\n",
      "Epoch: 11, train_loss: 0.94862, time: 0.01136\n",
      "Epoch: 12, train_loss: 0.95749, time: 0.01162\n",
      "Epoch: 13, train_loss: 0.94927, time: 0.01131\n",
      "Epoch: 14, train_loss: 0.95208, time: 0.01122\n",
      "Epoch: 15, train_loss: 0.95187, time: 0.01137\n",
      "Epoch: 16, train_loss: 0.95414, time: 0.01138\n",
      "Epoch: 17, train_loss: 0.95267, time: 0.01147\n",
      "Epoch: 18, train_loss: 0.95240, time: 0.01137\n",
      "Epoch: 19, train_loss: 0.95674, time: 0.01178\n",
      "Epoch: 20, train_loss: 0.95462, time: 0.01150\n",
      "Epoch: 21, train_loss: 0.95393, time: 0.01119\n",
      "Epoch: 22, train_loss: 0.95546, time: 0.01140\n",
      "Epoch: 23, train_loss: 0.95589, time: 0.01144\n",
      "Epoch: 24, train_loss: 0.95149, time: 0.01123\n",
      "Epoch: 25, train_loss: 0.95339, time: 0.01099\n",
      "Epoch: 26, train_loss: 0.95122, time: 0.01152\n",
      "Epoch: 27, train_loss: 0.95111, time: 0.01134\n",
      "Epoch: 28, train_loss: 0.95142, time: 0.01130\n",
      "Epoch: 29, train_loss: 0.95260, time: 0.01113\n",
      "Epoch: 30, train_loss: 0.95194, time: 0.01121\n",
      "Epoch: 31, train_loss: 0.95498, time: 0.01134\n",
      "Epoch: 32, train_loss: 0.95178, time: 0.01133\n",
      "Epoch: 33, train_loss: 0.95489, time: 0.01130\n",
      "Epoch: 34, train_loss: 0.95404, time: 0.01131\n",
      "Epoch: 35, train_loss: 0.95302, time: 0.01134\n",
      "Epoch: 36, train_loss: 0.94915, time: 0.01133\n",
      "Epoch: 37, train_loss: 0.95187, time: 0.01155\n",
      "Epoch: 38, train_loss: 0.95273, time: 0.01161\n",
      "Epoch: 39, train_loss: 0.95075, time: 0.01150\n",
      "Epoch: 40, train_loss: 0.95037, time: 0.01138\n",
      "Epoch: 41, train_loss: 0.95588, time: 0.01134\n",
      "Epoch: 42, train_loss: 0.95129, time: 0.01101\n",
      "Epoch: 43, train_loss: 0.95375, time: 0.01146\n",
      "Epoch: 44, train_loss: 0.95123, time: 0.01125\n",
      "Epoch: 45, train_loss: 0.95098, time: 0.01146\n",
      "Epoch: 46, train_loss: 0.95254, time: 0.01131\n",
      "Epoch: 47, train_loss: 0.95484, time: 0.01145\n",
      "Epoch: 48, train_loss: 0.95564, time: 0.01131\n",
      "Epoch: 49, train_loss: 0.95284, time: 0.01151\n",
      "Epoch: 50, train_loss: 0.95488, time: 0.01172\n",
      "Epoch: 51, train_loss: 0.94986, time: 0.01111\n",
      "Epoch: 52, train_loss: 0.95675, time: 0.01126\n",
      "Epoch: 53, train_loss: 0.95324, time: 0.01125\n",
      "Epoch: 54, train_loss: 0.94929, time: 0.01166\n",
      "Epoch: 55, train_loss: 0.95167, time: 0.01161\n",
      "Epoch: 56, train_loss: 0.95150, time: 0.01171\n",
      "Epoch: 57, train_loss: 0.95109, time: 0.01133\n",
      "Epoch: 58, train_loss: 0.95165, time: 0.01112\n",
      "Epoch: 59, train_loss: 0.95135, time: 0.01135\n",
      "Epoch: 60, train_loss: 0.95387, time: 0.01159\n",
      "Epoch: 61, train_loss: 0.95558, time: 0.01120\n",
      "Epoch: 62, train_loss: 0.95572, time: 0.01141\n",
      "Epoch: 63, train_loss: 0.95537, time: 0.01157\n",
      "Epoch: 64, train_loss: 0.95035, time: 0.01133\n",
      "Epoch: 65, train_loss: 0.95346, time: 0.01136\n",
      "Epoch: 66, train_loss: 0.95196, time: 0.01145\n",
      "Epoch: 67, train_loss: 0.95005, time: 0.01127\n",
      "Epoch: 68, train_loss: 0.95163, time: 0.01140\n",
      "Epoch: 69, train_loss: 0.95603, time: 0.01127\n",
      "Epoch: 70, train_loss: 0.95352, time: 0.01157\n",
      "Epoch: 71, train_loss: 0.95181, time: 0.01158\n",
      "Epoch: 72, train_loss: 0.95496, time: 0.01159\n",
      "Epoch: 73, train_loss: 0.95215, time: 0.01148\n",
      "Epoch: 74, train_loss: 0.95215, time: 0.01135\n",
      "Epoch: 75, train_loss: 0.95362, time: 0.01115\n",
      "Epoch: 76, train_loss: 0.95147, time: 0.01146\n",
      "Epoch: 77, train_loss: 0.95293, time: 0.01120\n",
      "Epoch: 78, train_loss: 0.94912, time: 0.01170\n",
      "Epoch: 79, train_loss: 0.94970, time: 0.01101\n",
      "Epoch: 80, train_loss: 0.95661, time: 0.01158\n",
      "Epoch: 81, train_loss: 0.95407, time: 0.01133\n",
      "Epoch: 82, train_loss: 0.95407, time: 0.01132\n",
      "Epoch: 83, train_loss: 0.95253, time: 0.01121\n",
      "Epoch: 84, train_loss: 0.95395, time: 0.01137\n",
      "Epoch: 85, train_loss: 0.95166, time: 0.01115\n",
      "Epoch: 86, train_loss: 0.94933, time: 0.01153\n",
      "Epoch: 87, train_loss: 0.95225, time: 0.01140\n",
      "Epoch: 88, train_loss: 0.95099, time: 0.01138\n",
      "Epoch: 89, train_loss: 0.95410, time: 0.01158\n",
      "Epoch: 90, train_loss: 0.95203, time: 0.01153\n",
      "Epoch: 91, train_loss: 0.95191, time: 0.01153\n",
      "Epoch: 92, train_loss: 0.95228, time: 0.01162\n",
      "Epoch: 93, train_loss: 0.94983, time: 0.01151\n",
      "Epoch: 94, train_loss: 0.95284, time: 0.01141\n",
      "Epoch: 95, train_loss: 0.95174, time: 0.01130\n",
      "Epoch: 96, train_loss: 0.95262, time: 0.01132\n",
      "Epoch: 97, train_loss: 0.95445, time: 0.01155\n",
      "Epoch: 98, train_loss: 0.94881, time: 0.01146\n",
      "Epoch: 99, train_loss: 0.95646, time: 0.01129\n",
      "Epoch: 100, train_loss: 0.95418, time: 0.01150\n",
      "Epoch: 101, train_loss: 0.95013, time: 0.01156\n",
      "Epoch: 102, train_loss: 0.95549, time: 0.01141\n",
      "Epoch: 103, train_loss: 0.95319, time: 0.01161\n",
      "Epoch: 104, train_loss: 0.95145, time: 0.01137\n",
      "Epoch: 105, train_loss: 0.95352, time: 0.01150\n",
      "Epoch: 106, train_loss: 0.95324, time: 0.01126\n",
      "Epoch: 107, train_loss: 0.95319, time: 0.01151\n",
      "Epoch: 108, train_loss: 0.95287, time: 0.01169\n",
      "Epoch: 109, train_loss: 0.95415, time: 0.01202\n",
      "Epoch: 110, train_loss: 0.95126, time: 0.01135\n",
      "Epoch: 111, train_loss: 0.95129, time: 0.01165\n",
      "Epoch: 112, train_loss: 0.95302, time: 0.01133\n",
      "Epoch: 113, train_loss: 0.95173, time: 0.01138\n",
      "Epoch: 114, train_loss: 0.95234, time: 0.01140\n",
      "Epoch: 115, train_loss: 0.94764, time: 0.01138\n",
      "Epoch: 116, train_loss: 0.95237, time: 0.01172\n",
      "Epoch: 117, train_loss: 0.95249, time: 0.01144\n",
      "Epoch: 118, train_loss: 0.95056, time: 0.01159\n",
      "Epoch: 119, train_loss: 0.95469, time: 0.01158\n",
      "Epoch: 120, train_loss: 0.95238, time: 0.01133\n",
      "Epoch: 121, train_loss: 0.95222, time: 0.01120\n",
      "Epoch: 122, train_loss: 0.95238, time: 0.01159\n",
      "Epoch: 123, train_loss: 0.95537, time: 0.01163\n",
      "Epoch: 124, train_loss: 0.95162, time: 0.01159\n",
      "Epoch: 125, train_loss: 0.95212, time: 0.01170\n",
      "Epoch: 126, train_loss: 0.94982, time: 0.01138\n",
      "Epoch: 127, train_loss: 0.95195, time: 0.01143\n",
      "Epoch: 128, train_loss: 0.95310, time: 0.01156\n",
      "Epoch: 129, train_loss: 0.95063, time: 0.01136\n",
      "Epoch: 130, train_loss: 0.95369, time: 0.01168\n",
      "Epoch: 131, train_loss: 0.95238, time: 0.01116\n",
      "Epoch: 132, train_loss: 0.95282, time: 0.01173\n",
      "Epoch: 133, train_loss: 0.95444, time: 0.01110\n",
      "Epoch: 134, train_loss: 0.95073, time: 0.01139\n",
      "Epoch: 135, train_loss: 0.95194, time: 0.01119\n",
      "Epoch: 136, train_loss: 0.95107, time: 0.01141\n",
      "Epoch: 137, train_loss: 0.95340, time: 0.01121\n",
      "Epoch: 138, train_loss: 0.95354, time: 0.01156\n",
      "Epoch: 139, train_loss: 0.95469, time: 0.01122\n",
      "Epoch: 140, train_loss: 0.95217, time: 0.01162\n",
      "Epoch: 141, train_loss: 0.95323, time: 0.01110\n",
      "Epoch: 142, train_loss: 0.95516, time: 0.01150\n",
      "Epoch: 143, train_loss: 0.95131, time: 0.01126\n",
      "Epoch: 144, train_loss: 0.95091, time: 0.01154\n",
      "Epoch: 145, train_loss: 0.95617, time: 0.01185\n",
      "Epoch: 146, train_loss: 0.95592, time: 0.01148\n",
      "Epoch: 147, train_loss: 0.95373, time: 0.01112\n",
      "Epoch: 148, train_loss: 0.95303, time: 0.01158\n",
      "Epoch: 149, train_loss: 0.94976, time: 0.01135\n",
      "Epoch: 150, train_loss: 0.95225, time: 0.01112\n",
      "Epoch: 151, train_loss: 0.95300, time: 0.01108\n",
      "Epoch: 152, train_loss: 0.95318, time: 0.01160\n",
      "Epoch: 153, train_loss: 0.95427, time: 0.01144\n",
      "Epoch: 154, train_loss: 0.95301, time: 0.01130\n",
      "Epoch: 155, train_loss: 0.95257, time: 0.01143\n",
      "Epoch: 156, train_loss: 0.95076, time: 0.01136\n",
      "Epoch: 157, train_loss: 0.95413, time: 0.01138\n",
      "Epoch: 158, train_loss: 0.95309, time: 0.01151\n",
      "Epoch: 159, train_loss: 0.95115, time: 0.01171\n",
      "Epoch: 160, train_loss: 0.95039, time: 0.01150\n",
      "Epoch: 161, train_loss: 0.95504, time: 0.01119\n",
      "Epoch: 162, train_loss: 0.95236, time: 0.01155\n",
      "Epoch: 163, train_loss: 0.95106, time: 0.01144\n",
      "Epoch: 164, train_loss: 0.95170, time: 0.01159\n",
      "Epoch: 165, train_loss: 0.95216, time: 0.01139\n",
      "Epoch: 166, train_loss: 0.95378, time: 0.01151\n",
      "Epoch: 167, train_loss: 0.95241, time: 0.01143\n",
      "Epoch: 168, train_loss: 0.95005, time: 0.01165\n",
      "Epoch: 169, train_loss: 0.95296, time: 0.01126\n",
      "Epoch: 170, train_loss: 0.95338, time: 0.01121\n",
      "Epoch: 171, train_loss: 0.95068, time: 0.01121\n",
      "Epoch: 172, train_loss: 0.95490, time: 0.01146\n",
      "Epoch: 173, train_loss: 0.94957, time: 0.01128\n",
      "Epoch: 174, train_loss: 0.95377, time: 0.01140\n",
      "Epoch: 175, train_loss: 0.95230, time: 0.01142\n",
      "Epoch: 176, train_loss: 0.95016, time: 0.01117\n",
      "Epoch: 177, train_loss: 0.95309, time: 0.01138\n",
      "Epoch: 178, train_loss: 0.94879, time: 0.01062\n",
      "Epoch: 179, train_loss: 0.95282, time: 0.01106\n",
      "Epoch: 180, train_loss: 0.95240, time: 0.01104\n",
      "Epoch: 181, train_loss: 0.95324, time: 0.01135\n",
      "Epoch: 182, train_loss: 0.95179, time: 0.01126\n",
      "Epoch: 183, train_loss: 0.95124, time: 0.01161\n",
      "Epoch: 184, train_loss: 0.95446, time: 0.01132\n",
      "Epoch: 185, train_loss: 0.95083, time: 0.01133\n",
      "Epoch: 186, train_loss: 0.95324, time: 0.01130\n",
      "Epoch: 187, train_loss: 0.95290, time: 0.01144\n",
      "Epoch: 188, train_loss: 0.95074, time: 0.01139\n",
      "Epoch: 189, train_loss: 0.95152, time: 0.01153\n",
      "Epoch: 190, train_loss: 0.95205, time: 0.01115\n",
      "Epoch: 191, train_loss: 0.95055, time: 0.01144\n",
      "Epoch: 192, train_loss: 0.95285, time: 0.01109\n",
      "Epoch: 193, train_loss: 0.95378, time: 0.01126\n",
      "Epoch: 194, train_loss: 0.95064, time: 0.01137\n",
      "Epoch: 195, train_loss: 0.95221, time: 0.01113\n",
      "Epoch: 196, train_loss: 0.95188, time: 0.01121\n",
      "Epoch: 197, train_loss: 0.95366, time: 0.01136\n",
      "Epoch: 198, train_loss: 0.95147, time: 0.01140\n",
      "Epoch: 199, train_loss: 0.95229, time: 0.01169\n",
      "Epoch: 200, train_loss: 0.95121, time: 0.01109\n",
      "pairwise precision 0.44455 recall 0.96761 f1 0.60921\n",
      "average until now [0.4288018428342214, 0.8392562237623771, 0.5675995837087834]\n",
      "68 names 286.57336926460266 avg time 4.2143142538912155\n",
      "Loading xiao_juan_li dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 100 nodes, 248 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98267, time: 0.10684\n",
      "Epoch: 2, train_loss: 0.98610, time: 0.00953\n",
      "Epoch: 3, train_loss: 0.99118, time: 0.00867\n",
      "Epoch: 4, train_loss: 0.97790, time: 0.00853\n",
      "Epoch: 5, train_loss: 0.97101, time: 0.00841\n",
      "Epoch: 6, train_loss: 0.98538, time: 0.00842\n",
      "Epoch: 7, train_loss: 0.97473, time: 0.00840\n",
      "Epoch: 8, train_loss: 0.97735, time: 0.00863\n",
      "Epoch: 9, train_loss: 0.97619, time: 0.00847\n",
      "Epoch: 10, train_loss: 0.97819, time: 0.00835\n",
      "Epoch: 11, train_loss: 0.97801, time: 0.00847\n",
      "Epoch: 12, train_loss: 0.97730, time: 0.00852\n",
      "Epoch: 13, train_loss: 0.98264, time: 0.00833\n",
      "Epoch: 14, train_loss: 0.98457, time: 0.00832\n",
      "Epoch: 15, train_loss: 0.97477, time: 0.00835\n",
      "Epoch: 16, train_loss: 0.98392, time: 0.00835\n",
      "Epoch: 17, train_loss: 0.98209, time: 0.00844\n",
      "Epoch: 18, train_loss: 0.97003, time: 0.00846\n",
      "Epoch: 19, train_loss: 0.97568, time: 0.00838\n",
      "Epoch: 20, train_loss: 0.97160, time: 0.00832\n",
      "Epoch: 21, train_loss: 0.97692, time: 0.00841\n",
      "Epoch: 22, train_loss: 0.97793, time: 0.00841\n",
      "Epoch: 23, train_loss: 0.98129, time: 0.00844\n",
      "Epoch: 24, train_loss: 0.98369, time: 0.00846\n",
      "Epoch: 25, train_loss: 0.98280, time: 0.00845\n",
      "Epoch: 26, train_loss: 0.97831, time: 0.00845\n",
      "Epoch: 27, train_loss: 0.97538, time: 0.00843\n",
      "Epoch: 28, train_loss: 0.97566, time: 0.00840\n",
      "Epoch: 29, train_loss: 0.97526, time: 0.00842\n",
      "Epoch: 30, train_loss: 0.97482, time: 0.00837\n",
      "Epoch: 31, train_loss: 0.97222, time: 0.00842\n",
      "Epoch: 32, train_loss: 0.98104, time: 0.00866\n",
      "Epoch: 33, train_loss: 0.97440, time: 0.00843\n",
      "Epoch: 34, train_loss: 0.98613, time: 0.00838\n",
      "Epoch: 35, train_loss: 0.96996, time: 0.00851\n",
      "Epoch: 36, train_loss: 0.98321, time: 0.00841\n",
      "Epoch: 37, train_loss: 0.97488, time: 0.00838\n",
      "Epoch: 38, train_loss: 0.97937, time: 0.00839\n",
      "Epoch: 39, train_loss: 0.97774, time: 0.00845\n",
      "Epoch: 40, train_loss: 0.97720, time: 0.00834\n",
      "Epoch: 41, train_loss: 0.97448, time: 0.00829\n",
      "Epoch: 42, train_loss: 0.97661, time: 0.00833\n",
      "Epoch: 43, train_loss: 0.97506, time: 0.00849\n",
      "Epoch: 44, train_loss: 0.97268, time: 0.00852\n",
      "Epoch: 45, train_loss: 0.98371, time: 0.00838\n",
      "Epoch: 46, train_loss: 0.97570, time: 0.00849\n",
      "Epoch: 47, train_loss: 0.97625, time: 0.00831\n",
      "Epoch: 48, train_loss: 0.98248, time: 0.00837\n",
      "Epoch: 49, train_loss: 0.97512, time: 0.00842\n",
      "Epoch: 50, train_loss: 0.98081, time: 0.00836\n",
      "Epoch: 51, train_loss: 0.98374, time: 0.00843\n",
      "Epoch: 52, train_loss: 0.97753, time: 0.00844\n",
      "Epoch: 53, train_loss: 0.97690, time: 0.00830\n",
      "Epoch: 54, train_loss: 0.98394, time: 0.00825\n",
      "Epoch: 55, train_loss: 0.97836, time: 0.00832\n",
      "Epoch: 56, train_loss: 0.97360, time: 0.00855\n",
      "Epoch: 57, train_loss: 0.97338, time: 0.00841\n",
      "Epoch: 58, train_loss: 0.98044, time: 0.00842\n",
      "Epoch: 59, train_loss: 0.97760, time: 0.00841\n",
      "Epoch: 60, train_loss: 0.97714, time: 0.00841\n",
      "Epoch: 61, train_loss: 0.98294, time: 0.00837\n",
      "Epoch: 62, train_loss: 0.97798, time: 0.00831\n",
      "Epoch: 63, train_loss: 0.97056, time: 0.00837\n",
      "Epoch: 64, train_loss: 0.97736, time: 0.00841\n",
      "Epoch: 65, train_loss: 0.98224, time: 0.00849\n",
      "Epoch: 66, train_loss: 0.97769, time: 0.00842\n",
      "Epoch: 67, train_loss: 0.97822, time: 0.00841\n",
      "Epoch: 68, train_loss: 0.97959, time: 0.00844\n",
      "Epoch: 69, train_loss: 0.97499, time: 0.00846\n",
      "Epoch: 70, train_loss: 0.97745, time: 0.00846\n",
      "Epoch: 71, train_loss: 0.97597, time: 0.00839\n",
      "Epoch: 72, train_loss: 0.97824, time: 0.00838\n",
      "Epoch: 73, train_loss: 0.98695, time: 0.00835\n",
      "Epoch: 74, train_loss: 0.98318, time: 0.00827\n",
      "Epoch: 75, train_loss: 0.96826, time: 0.00839\n",
      "Epoch: 76, train_loss: 0.97789, time: 0.00837\n",
      "Epoch: 77, train_loss: 0.97941, time: 0.00831\n",
      "Epoch: 78, train_loss: 0.97651, time: 0.00833\n",
      "Epoch: 79, train_loss: 0.98492, time: 0.00837\n",
      "Epoch: 80, train_loss: 0.97943, time: 0.00841\n",
      "Epoch: 81, train_loss: 0.97857, time: 0.00845\n",
      "Epoch: 82, train_loss: 0.98296, time: 0.00839\n",
      "Epoch: 83, train_loss: 0.97529, time: 0.00850\n",
      "Epoch: 84, train_loss: 0.97852, time: 0.00845\n",
      "Epoch: 85, train_loss: 0.98246, time: 0.00844\n",
      "Epoch: 86, train_loss: 0.98320, time: 0.00840\n",
      "Epoch: 87, train_loss: 0.96956, time: 0.00836\n",
      "Epoch: 88, train_loss: 0.98159, time: 0.00840\n",
      "Epoch: 89, train_loss: 0.97504, time: 0.00834\n",
      "Epoch: 90, train_loss: 0.97861, time: 0.00844\n",
      "Epoch: 91, train_loss: 0.98401, time: 0.00845\n",
      "Epoch: 92, train_loss: 0.97514, time: 0.00843\n",
      "Epoch: 93, train_loss: 0.97317, time: 0.00836\n",
      "Epoch: 94, train_loss: 0.98349, time: 0.00842\n",
      "Epoch: 95, train_loss: 0.97798, time: 0.00831\n",
      "Epoch: 96, train_loss: 0.97535, time: 0.00825\n",
      "Epoch: 97, train_loss: 0.97330, time: 0.00832\n",
      "Epoch: 98, train_loss: 0.97738, time: 0.00837\n",
      "Epoch: 99, train_loss: 0.98161, time: 0.00837\n",
      "Epoch: 100, train_loss: 0.96877, time: 0.00840\n",
      "Epoch: 101, train_loss: 0.98271, time: 0.00841\n",
      "Epoch: 102, train_loss: 0.97663, time: 0.00839\n",
      "Epoch: 103, train_loss: 0.97306, time: 0.00839\n",
      "Epoch: 104, train_loss: 0.98296, time: 0.00855\n",
      "Epoch: 105, train_loss: 0.98968, time: 0.00837\n",
      "Epoch: 106, train_loss: 0.97535, time: 0.00831\n",
      "Epoch: 107, train_loss: 0.97715, time: 0.00836\n",
      "Epoch: 108, train_loss: 0.97633, time: 0.00840\n",
      "Epoch: 109, train_loss: 0.98304, time: 0.00838\n",
      "Epoch: 110, train_loss: 0.97984, time: 0.00831\n",
      "Epoch: 111, train_loss: 0.97607, time: 0.00837\n",
      "Epoch: 112, train_loss: 0.97219, time: 0.00845\n",
      "Epoch: 113, train_loss: 0.98166, time: 0.00836\n",
      "Epoch: 114, train_loss: 0.97792, time: 0.00843\n",
      "Epoch: 115, train_loss: 0.97700, time: 0.00846\n",
      "Epoch: 116, train_loss: 0.98037, time: 0.00840\n",
      "Epoch: 117, train_loss: 0.98565, time: 0.00838\n",
      "Epoch: 118, train_loss: 0.98040, time: 0.00838\n",
      "Epoch: 119, train_loss: 0.98069, time: 0.00840\n",
      "Epoch: 120, train_loss: 0.97768, time: 0.00836\n",
      "Epoch: 121, train_loss: 0.97424, time: 0.00838\n",
      "Epoch: 122, train_loss: 0.98392, time: 0.00843\n",
      "Epoch: 123, train_loss: 0.97607, time: 0.00839\n",
      "Epoch: 124, train_loss: 0.98207, time: 0.00846\n",
      "Epoch: 125, train_loss: 0.97887, time: 0.00840\n",
      "Epoch: 126, train_loss: 0.97137, time: 0.00853\n",
      "Epoch: 127, train_loss: 0.98283, time: 0.00843\n",
      "Epoch: 128, train_loss: 0.97826, time: 0.00857\n",
      "Epoch: 129, train_loss: 0.97262, time: 0.00843\n",
      "Epoch: 130, train_loss: 0.97360, time: 0.00837\n",
      "Epoch: 131, train_loss: 0.97046, time: 0.00846\n",
      "Epoch: 132, train_loss: 0.98084, time: 0.00838\n",
      "Epoch: 133, train_loss: 0.97679, time: 0.00823\n",
      "Epoch: 134, train_loss: 0.97902, time: 0.00839\n",
      "Epoch: 135, train_loss: 0.97468, time: 0.00843\n",
      "Epoch: 136, train_loss: 0.97845, time: 0.00835\n",
      "Epoch: 137, train_loss: 0.97905, time: 0.00836\n",
      "Epoch: 138, train_loss: 0.97791, time: 0.00848\n",
      "Epoch: 139, train_loss: 0.97661, time: 0.00835\n",
      "Epoch: 140, train_loss: 0.98166, time: 0.00877\n",
      "Epoch: 141, train_loss: 0.97554, time: 0.00885\n",
      "Epoch: 142, train_loss: 0.97184, time: 0.00892\n",
      "Epoch: 143, train_loss: 0.97742, time: 0.00890\n",
      "Epoch: 144, train_loss: 0.97049, time: 0.00897\n",
      "Epoch: 145, train_loss: 0.97586, time: 0.00883\n",
      "Epoch: 146, train_loss: 0.97541, time: 0.00893\n",
      "Epoch: 147, train_loss: 0.98145, time: 0.00891\n",
      "Epoch: 148, train_loss: 0.97495, time: 0.00893\n",
      "Epoch: 149, train_loss: 0.97613, time: 0.00892\n",
      "Epoch: 150, train_loss: 0.98176, time: 0.00881\n",
      "Epoch: 151, train_loss: 0.97860, time: 0.00892\n",
      "Epoch: 152, train_loss: 0.97965, time: 0.00906\n",
      "Epoch: 153, train_loss: 0.97779, time: 0.00893\n",
      "Epoch: 154, train_loss: 0.98404, time: 0.00889\n",
      "Epoch: 155, train_loss: 0.98203, time: 0.00837\n",
      "Epoch: 156, train_loss: 0.97579, time: 0.00833\n",
      "Epoch: 157, train_loss: 0.97627, time: 0.00833\n",
      "Epoch: 158, train_loss: 0.98075, time: 0.00843\n",
      "Epoch: 159, train_loss: 0.97479, time: 0.00853\n",
      "Epoch: 160, train_loss: 0.97027, time: 0.00851\n",
      "Epoch: 161, train_loss: 0.97401, time: 0.00844\n",
      "Epoch: 162, train_loss: 0.97706, time: 0.00842\n",
      "Epoch: 163, train_loss: 0.98400, time: 0.00847\n",
      "Epoch: 164, train_loss: 0.97443, time: 0.00842\n",
      "Epoch: 165, train_loss: 0.97724, time: 0.00852\n",
      "Epoch: 166, train_loss: 0.97985, time: 0.00847\n",
      "Epoch: 167, train_loss: 0.97131, time: 0.00835\n",
      "Epoch: 168, train_loss: 0.97654, time: 0.00839\n",
      "Epoch: 169, train_loss: 0.97843, time: 0.00837\n",
      "Epoch: 170, train_loss: 0.97814, time: 0.00845\n",
      "Epoch: 171, train_loss: 0.97310, time: 0.00849\n",
      "Epoch: 172, train_loss: 0.97567, time: 0.00852\n",
      "Epoch: 173, train_loss: 0.98101, time: 0.00842\n",
      "Epoch: 174, train_loss: 0.98032, time: 0.00836\n",
      "Epoch: 175, train_loss: 0.98019, time: 0.00838\n",
      "Epoch: 176, train_loss: 0.98167, time: 0.00855\n",
      "Epoch: 177, train_loss: 0.97775, time: 0.00850\n",
      "Epoch: 178, train_loss: 0.97294, time: 0.00838\n",
      "Epoch: 179, train_loss: 0.98179, time: 0.00845\n",
      "Epoch: 180, train_loss: 0.98385, time: 0.00843\n",
      "Epoch: 181, train_loss: 0.97425, time: 0.00836\n",
      "Epoch: 182, train_loss: 0.97732, time: 0.00834\n",
      "Epoch: 183, train_loss: 0.97873, time: 0.00848\n",
      "Epoch: 184, train_loss: 0.97346, time: 0.00842\n",
      "Epoch: 185, train_loss: 0.98096, time: 0.00845\n",
      "Epoch: 186, train_loss: 0.97652, time: 0.00844\n",
      "Epoch: 187, train_loss: 0.97731, time: 0.00841\n",
      "Epoch: 188, train_loss: 0.97580, time: 0.00842\n",
      "Epoch: 189, train_loss: 0.97947, time: 0.00843\n",
      "Epoch: 190, train_loss: 0.98046, time: 0.00845\n",
      "Epoch: 191, train_loss: 0.97853, time: 0.00845\n",
      "Epoch: 192, train_loss: 0.96866, time: 0.00847\n",
      "Epoch: 193, train_loss: 0.98064, time: 0.00844\n",
      "Epoch: 194, train_loss: 0.96933, time: 0.00834\n",
      "Epoch: 195, train_loss: 0.97914, time: 0.00839\n",
      "Epoch: 196, train_loss: 0.97953, time: 0.00836\n",
      "Epoch: 197, train_loss: 0.96844, time: 0.00836\n",
      "Epoch: 198, train_loss: 0.98252, time: 0.00831\n",
      "Epoch: 199, train_loss: 0.98770, time: 0.00838\n",
      "Epoch: 200, train_loss: 0.98002, time: 0.00849\n",
      "pairwise precision 0.32040 recall 0.86068 f1 0.46697\n",
      "average until now [0.42723079495247135, 0.8395667294259815, 0.5662921727042325]\n",
      "69 names 288.39042139053345 avg time 4.179571324500484\n",
      "Loading fei_sun dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 357 nodes, 2003 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98550, time: 0.11303\n",
      "Epoch: 2, train_loss: 0.98712, time: 0.01560\n",
      "Epoch: 3, train_loss: 0.98714, time: 0.01441\n",
      "Epoch: 4, train_loss: 0.98582, time: 0.01458\n",
      "Epoch: 5, train_loss: 0.98268, time: 0.01433\n",
      "Epoch: 6, train_loss: 0.98287, time: 0.01409\n",
      "Epoch: 7, train_loss: 0.98472, time: 0.01351\n",
      "Epoch: 8, train_loss: 0.98294, time: 0.01463\n",
      "Epoch: 9, train_loss: 0.98350, time: 0.01317\n",
      "Epoch: 10, train_loss: 0.98285, time: 0.01410\n",
      "Epoch: 11, train_loss: 0.98497, time: 0.01394\n",
      "Epoch: 12, train_loss: 0.98361, time: 0.01414\n",
      "Epoch: 13, train_loss: 0.98221, time: 0.01392\n",
      "Epoch: 14, train_loss: 0.98277, time: 0.01381\n",
      "Epoch: 15, train_loss: 0.98357, time: 0.01482\n",
      "Epoch: 16, train_loss: 0.98056, time: 0.01427\n",
      "Epoch: 17, train_loss: 0.98265, time: 0.01482\n",
      "Epoch: 18, train_loss: 0.98383, time: 0.01458\n",
      "Epoch: 19, train_loss: 0.98366, time: 0.01431\n",
      "Epoch: 20, train_loss: 0.98077, time: 0.01412\n",
      "Epoch: 21, train_loss: 0.98169, time: 0.01389\n",
      "Epoch: 22, train_loss: 0.98368, time: 0.01436\n",
      "Epoch: 23, train_loss: 0.98267, time: 0.01425\n",
      "Epoch: 24, train_loss: 0.98101, time: 0.01435\n",
      "Epoch: 25, train_loss: 0.98525, time: 0.01438\n",
      "Epoch: 26, train_loss: 0.98442, time: 0.01429\n",
      "Epoch: 27, train_loss: 0.98346, time: 0.01447\n",
      "Epoch: 28, train_loss: 0.98090, time: 0.01417\n",
      "Epoch: 29, train_loss: 0.98187, time: 0.01397\n",
      "Epoch: 30, train_loss: 0.98093, time: 0.01396\n",
      "Epoch: 31, train_loss: 0.98325, time: 0.01390\n",
      "Epoch: 32, train_loss: 0.98261, time: 0.01424\n",
      "Epoch: 33, train_loss: 0.98294, time: 0.01483\n",
      "Epoch: 34, train_loss: 0.98158, time: 0.01407\n",
      "Epoch: 35, train_loss: 0.98395, time: 0.01422\n",
      "Epoch: 36, train_loss: 0.98281, time: 0.01407\n",
      "Epoch: 37, train_loss: 0.98250, time: 0.01407\n",
      "Epoch: 38, train_loss: 0.98256, time: 0.01445\n",
      "Epoch: 39, train_loss: 0.98425, time: 0.01406\n",
      "Epoch: 40, train_loss: 0.98249, time: 0.01417\n",
      "Epoch: 41, train_loss: 0.98063, time: 0.01420\n",
      "Epoch: 42, train_loss: 0.98306, time: 0.01307\n",
      "Epoch: 43, train_loss: 0.98508, time: 0.01364\n",
      "Epoch: 44, train_loss: 0.98251, time: 0.01374\n",
      "Epoch: 45, train_loss: 0.98224, time: 0.01410\n",
      "Epoch: 46, train_loss: 0.98348, time: 0.01406\n",
      "Epoch: 47, train_loss: 0.98174, time: 0.01438\n",
      "Epoch: 48, train_loss: 0.98202, time: 0.01453\n",
      "Epoch: 49, train_loss: 0.98216, time: 0.01449\n",
      "Epoch: 50, train_loss: 0.98255, time: 0.01471\n",
      "Epoch: 51, train_loss: 0.98351, time: 0.01379\n",
      "Epoch: 52, train_loss: 0.98195, time: 0.01422\n",
      "Epoch: 53, train_loss: 0.98293, time: 0.01441\n",
      "Epoch: 54, train_loss: 0.98292, time: 0.01435\n",
      "Epoch: 55, train_loss: 0.98298, time: 0.01451\n",
      "Epoch: 56, train_loss: 0.98183, time: 0.01456\n",
      "Epoch: 57, train_loss: 0.98139, time: 0.01408\n",
      "Epoch: 58, train_loss: 0.98318, time: 0.01394\n",
      "Epoch: 59, train_loss: 0.98292, time: 0.01428\n",
      "Epoch: 60, train_loss: 0.98133, time: 0.01422\n",
      "Epoch: 61, train_loss: 0.98294, time: 0.01458\n",
      "Epoch: 62, train_loss: 0.98226, time: 0.01403\n",
      "Epoch: 63, train_loss: 0.98280, time: 0.01448\n",
      "Epoch: 64, train_loss: 0.98221, time: 0.01398\n",
      "Epoch: 65, train_loss: 0.98330, time: 0.01425\n",
      "Epoch: 66, train_loss: 0.98229, time: 0.01415\n",
      "Epoch: 67, train_loss: 0.98340, time: 0.01401\n",
      "Epoch: 68, train_loss: 0.98353, time: 0.01442\n",
      "Epoch: 69, train_loss: 0.98393, time: 0.01421\n",
      "Epoch: 70, train_loss: 0.98385, time: 0.01394\n",
      "Epoch: 71, train_loss: 0.98126, time: 0.01339\n",
      "Epoch: 72, train_loss: 0.98274, time: 0.01428\n",
      "Epoch: 73, train_loss: 0.98393, time: 0.01439\n",
      "Epoch: 74, train_loss: 0.98412, time: 0.01381\n",
      "Epoch: 75, train_loss: 0.98412, time: 0.01415\n",
      "Epoch: 76, train_loss: 0.98265, time: 0.01422\n",
      "Epoch: 77, train_loss: 0.98535, time: 0.01379\n",
      "Epoch: 78, train_loss: 0.98166, time: 0.01417\n",
      "Epoch: 79, train_loss: 0.98196, time: 0.01414\n",
      "Epoch: 80, train_loss: 0.98437, time: 0.01437\n",
      "Epoch: 81, train_loss: 0.98320, time: 0.01464\n",
      "Epoch: 82, train_loss: 0.98351, time: 0.01371\n",
      "Epoch: 83, train_loss: 0.98312, time: 0.01462\n",
      "Epoch: 84, train_loss: 0.98341, time: 0.01427\n",
      "Epoch: 85, train_loss: 0.98208, time: 0.01466\n",
      "Epoch: 86, train_loss: 0.98336, time: 0.01435\n",
      "Epoch: 87, train_loss: 0.98223, time: 0.01402\n",
      "Epoch: 88, train_loss: 0.98191, time: 0.01379\n",
      "Epoch: 89, train_loss: 0.98322, time: 0.01407\n",
      "Epoch: 90, train_loss: 0.98234, time: 0.01382\n",
      "Epoch: 91, train_loss: 0.98353, time: 0.01402\n",
      "Epoch: 92, train_loss: 0.98326, time: 0.01405\n",
      "Epoch: 93, train_loss: 0.98300, time: 0.01408\n",
      "Epoch: 94, train_loss: 0.98298, time: 0.01395\n",
      "Epoch: 95, train_loss: 0.98228, time: 0.01287\n",
      "Epoch: 96, train_loss: 0.98201, time: 0.01425\n",
      "Epoch: 97, train_loss: 0.98167, time: 0.01394\n",
      "Epoch: 98, train_loss: 0.98297, time: 0.01371\n",
      "Epoch: 99, train_loss: 0.98255, time: 0.01404\n",
      "Epoch: 100, train_loss: 0.98104, time: 0.01465\n",
      "Epoch: 101, train_loss: 0.98056, time: 0.01435\n",
      "Epoch: 102, train_loss: 0.98170, time: 0.01389\n",
      "Epoch: 103, train_loss: 0.98284, time: 0.01437\n",
      "Epoch: 104, train_loss: 0.98296, time: 0.01338\n",
      "Epoch: 105, train_loss: 0.98368, time: 0.01408\n",
      "Epoch: 106, train_loss: 0.98253, time: 0.01450\n",
      "Epoch: 107, train_loss: 0.98312, time: 0.01414\n",
      "Epoch: 108, train_loss: 0.98093, time: 0.01456\n",
      "Epoch: 109, train_loss: 0.98184, time: 0.01397\n",
      "Epoch: 110, train_loss: 0.98116, time: 0.01345\n",
      "Epoch: 111, train_loss: 0.98169, time: 0.01372\n",
      "Epoch: 112, train_loss: 0.98411, time: 0.01385\n",
      "Epoch: 113, train_loss: 0.98480, time: 0.01403\n",
      "Epoch: 114, train_loss: 0.98157, time: 0.01428\n",
      "Epoch: 115, train_loss: 0.98438, time: 0.01377\n",
      "Epoch: 116, train_loss: 0.98335, time: 0.01424\n",
      "Epoch: 117, train_loss: 0.98166, time: 0.01438\n",
      "Epoch: 118, train_loss: 0.98426, time: 0.01481\n",
      "Epoch: 119, train_loss: 0.98298, time: 0.01438\n",
      "Epoch: 120, train_loss: 0.98234, time: 0.01444\n",
      "Epoch: 121, train_loss: 0.98329, time: 0.01391\n",
      "Epoch: 122, train_loss: 0.98220, time: 0.01373\n",
      "Epoch: 123, train_loss: 0.98013, time: 0.01451\n",
      "Epoch: 124, train_loss: 0.98111, time: 0.01442\n",
      "Epoch: 125, train_loss: 0.98195, time: 0.01414\n",
      "Epoch: 126, train_loss: 0.98085, time: 0.01414\n",
      "Epoch: 127, train_loss: 0.98306, time: 0.01393\n",
      "Epoch: 128, train_loss: 0.98322, time: 0.01372\n",
      "Epoch: 129, train_loss: 0.98324, time: 0.01348\n",
      "Epoch: 130, train_loss: 0.98323, time: 0.01386\n",
      "Epoch: 131, train_loss: 0.98527, time: 0.01458\n",
      "Epoch: 132, train_loss: 0.98198, time: 0.01441\n",
      "Epoch: 133, train_loss: 0.98159, time: 0.01445\n",
      "Epoch: 134, train_loss: 0.98331, time: 0.01387\n",
      "Epoch: 135, train_loss: 0.98398, time: 0.01335\n",
      "Epoch: 136, train_loss: 0.98079, time: 0.01320\n",
      "Epoch: 137, train_loss: 0.98189, time: 0.01397\n",
      "Epoch: 138, train_loss: 0.98418, time: 0.01406\n",
      "Epoch: 139, train_loss: 0.98177, time: 0.01405\n",
      "Epoch: 140, train_loss: 0.98163, time: 0.01401\n",
      "Epoch: 141, train_loss: 0.98270, time: 0.01426\n",
      "Epoch: 142, train_loss: 0.98217, time: 0.01441\n",
      "Epoch: 143, train_loss: 0.98236, time: 0.01390\n",
      "Epoch: 144, train_loss: 0.98210, time: 0.01434\n",
      "Epoch: 145, train_loss: 0.98036, time: 0.01430\n",
      "Epoch: 146, train_loss: 0.98153, time: 0.01375\n",
      "Epoch: 147, train_loss: 0.98206, time: 0.01418\n",
      "Epoch: 148, train_loss: 0.98193, time: 0.01427\n",
      "Epoch: 149, train_loss: 0.98284, time: 0.01430\n",
      "Epoch: 150, train_loss: 0.98251, time: 0.01407\n",
      "Epoch: 151, train_loss: 0.98256, time: 0.01442\n",
      "Epoch: 152, train_loss: 0.98065, time: 0.01370\n",
      "Epoch: 153, train_loss: 0.98220, time: 0.01359\n",
      "Epoch: 154, train_loss: 0.98404, time: 0.01422\n",
      "Epoch: 155, train_loss: 0.98185, time: 0.01342\n",
      "Epoch: 156, train_loss: 0.98267, time: 0.01402\n",
      "Epoch: 157, train_loss: 0.98168, time: 0.01422\n",
      "Epoch: 158, train_loss: 0.98274, time: 0.01433\n",
      "Epoch: 159, train_loss: 0.98156, time: 0.01458\n",
      "Epoch: 160, train_loss: 0.98207, time: 0.01440\n",
      "Epoch: 161, train_loss: 0.98070, time: 0.01418\n",
      "Epoch: 162, train_loss: 0.98236, time: 0.01410\n",
      "Epoch: 163, train_loss: 0.98245, time: 0.01411\n",
      "Epoch: 164, train_loss: 0.98311, time: 0.01473\n",
      "Epoch: 165, train_loss: 0.98288, time: 0.01357\n",
      "Epoch: 166, train_loss: 0.98357, time: 0.01412\n",
      "Epoch: 167, train_loss: 0.98225, time: 0.01447\n",
      "Epoch: 168, train_loss: 0.98261, time: 0.01457\n",
      "Epoch: 169, train_loss: 0.98183, time: 0.01389\n",
      "Epoch: 170, train_loss: 0.98341, time: 0.01405\n",
      "Epoch: 171, train_loss: 0.98429, time: 0.01468\n",
      "Epoch: 172, train_loss: 0.98304, time: 0.01496\n",
      "Epoch: 173, train_loss: 0.98303, time: 0.01404\n",
      "Epoch: 174, train_loss: 0.98311, time: 0.01426\n",
      "Epoch: 175, train_loss: 0.98437, time: 0.01405\n",
      "Epoch: 176, train_loss: 0.98395, time: 0.01466\n",
      "Epoch: 177, train_loss: 0.98279, time: 0.01397\n",
      "Epoch: 178, train_loss: 0.98216, time: 0.01382\n",
      "Epoch: 179, train_loss: 0.98195, time: 0.01462\n",
      "Epoch: 180, train_loss: 0.98331, time: 0.01444\n",
      "Epoch: 181, train_loss: 0.98280, time: 0.01420\n",
      "Epoch: 182, train_loss: 0.98185, time: 0.01401\n",
      "Epoch: 183, train_loss: 0.98246, time: 0.01456\n",
      "Epoch: 184, train_loss: 0.98305, time: 0.01438\n",
      "Epoch: 185, train_loss: 0.98448, time: 0.01461\n",
      "Epoch: 186, train_loss: 0.98314, time: 0.01391\n",
      "Epoch: 187, train_loss: 0.98332, time: 0.01448\n",
      "Epoch: 188, train_loss: 0.98275, time: 0.01421\n",
      "Epoch: 189, train_loss: 0.98281, time: 0.01427\n",
      "Epoch: 190, train_loss: 0.98129, time: 0.01416\n",
      "Epoch: 191, train_loss: 0.97963, time: 0.01428\n",
      "Epoch: 192, train_loss: 0.98421, time: 0.01442\n",
      "Epoch: 193, train_loss: 0.98235, time: 0.01361\n",
      "Epoch: 194, train_loss: 0.98419, time: 0.01409\n",
      "Epoch: 195, train_loss: 0.98518, time: 0.01416\n",
      "Epoch: 196, train_loss: 0.98180, time: 0.01327\n",
      "Epoch: 197, train_loss: 0.98296, time: 0.01411\n",
      "Epoch: 198, train_loss: 0.98462, time: 0.01402\n",
      "Epoch: 199, train_loss: 0.98099, time: 0.01467\n",
      "Epoch: 200, train_loss: 0.98460, time: 0.01402\n",
      "pairwise precision 0.12597 recall 0.88311 f1 0.22049\n",
      "average until now [0.4229270901221788, 0.8401887825872986, 0.5626381627375395]\n",
      "70 names 291.39724469184875 avg time 4.162817781312125\n",
      "Loading yongqing_huang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 352 nodes, 26848 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.78128, time: 0.13470\n",
      "Epoch: 2, train_loss: 0.78163, time: 0.03443\n",
      "Epoch: 3, train_loss: 0.78167, time: 0.03452\n",
      "Epoch: 4, train_loss: 0.78293, time: 0.03481\n",
      "Epoch: 5, train_loss: 0.78172, time: 0.03405\n",
      "Epoch: 6, train_loss: 0.78256, time: 0.03341\n",
      "Epoch: 7, train_loss: 0.78213, time: 0.03381\n",
      "Epoch: 8, train_loss: 0.78088, time: 0.03340\n",
      "Epoch: 9, train_loss: 0.78001, time: 0.03320\n",
      "Epoch: 10, train_loss: 0.78244, time: 0.03437\n",
      "Epoch: 11, train_loss: 0.78214, time: 0.03306\n",
      "Epoch: 12, train_loss: 0.78072, time: 0.03310\n",
      "Epoch: 13, train_loss: 0.78028, time: 0.03296\n",
      "Epoch: 14, train_loss: 0.77996, time: 0.03413\n",
      "Epoch: 15, train_loss: 0.78092, time: 0.03325\n",
      "Epoch: 16, train_loss: 0.78223, time: 0.03383\n",
      "Epoch: 17, train_loss: 0.78188, time: 0.03267\n",
      "Epoch: 18, train_loss: 0.78182, time: 0.03309\n",
      "Epoch: 19, train_loss: 0.78332, time: 0.03365\n",
      "Epoch: 20, train_loss: 0.77979, time: 0.03503\n",
      "Epoch: 21, train_loss: 0.78319, time: 0.03501\n",
      "Epoch: 22, train_loss: 0.78134, time: 0.03479\n",
      "Epoch: 23, train_loss: 0.77988, time: 0.03420\n",
      "Epoch: 24, train_loss: 0.78120, time: 0.03318\n",
      "Epoch: 25, train_loss: 0.77997, time: 0.03326\n",
      "Epoch: 26, train_loss: 0.77818, time: 0.03373\n",
      "Epoch: 27, train_loss: 0.78041, time: 0.03435\n",
      "Epoch: 28, train_loss: 0.77932, time: 0.03388\n",
      "Epoch: 29, train_loss: 0.77781, time: 0.03356\n",
      "Epoch: 30, train_loss: 0.77873, time: 0.03411\n",
      "Epoch: 31, train_loss: 0.77743, time: 0.03456\n",
      "Epoch: 32, train_loss: 0.78174, time: 0.03455\n",
      "Epoch: 33, train_loss: 0.77606, time: 0.03443\n",
      "Epoch: 34, train_loss: 0.77930, time: 0.03414\n",
      "Epoch: 35, train_loss: 0.77738, time: 0.03456\n",
      "Epoch: 36, train_loss: 0.77234, time: 0.03579\n",
      "Epoch: 37, train_loss: 0.77492, time: 0.03426\n",
      "Epoch: 38, train_loss: 0.77452, time: 0.03378\n",
      "Epoch: 39, train_loss: 0.77445, time: 0.03450\n",
      "Epoch: 40, train_loss: 0.77823, time: 0.03367\n",
      "Epoch: 41, train_loss: 0.77656, time: 0.03368\n",
      "Epoch: 42, train_loss: 0.76715, time: 0.03311\n",
      "Epoch: 43, train_loss: 0.78005, time: 0.03286\n",
      "Epoch: 44, train_loss: 0.76849, time: 0.03379\n",
      "Epoch: 45, train_loss: 0.77442, time: 0.03361\n",
      "Epoch: 46, train_loss: 0.77670, time: 0.03315\n",
      "Epoch: 47, train_loss: 0.77479, time: 0.03370\n",
      "Epoch: 48, train_loss: 0.76667, time: 0.03363\n",
      "Epoch: 49, train_loss: 0.76987, time: 0.03306\n",
      "Epoch: 50, train_loss: 0.77075, time: 0.03316\n",
      "Epoch: 51, train_loss: 0.77944, time: 0.03395\n",
      "Epoch: 52, train_loss: 0.76487, time: 0.03412\n",
      "Epoch: 53, train_loss: 0.77573, time: 0.03345\n",
      "Epoch: 54, train_loss: 0.76865, time: 0.03278\n",
      "Epoch: 55, train_loss: 0.76771, time: 0.03380\n",
      "Epoch: 56, train_loss: 0.76763, time: 0.03441\n",
      "Epoch: 57, train_loss: 0.76922, time: 0.03385\n",
      "Epoch: 58, train_loss: 0.76368, time: 0.03271\n",
      "Epoch: 59, train_loss: 0.76583, time: 0.03271\n",
      "Epoch: 60, train_loss: 0.77339, time: 0.03317\n",
      "Epoch: 61, train_loss: 0.77310, time: 0.03374\n",
      "Epoch: 62, train_loss: 0.77285, time: 0.03351\n",
      "Epoch: 63, train_loss: 0.76853, time: 0.03350\n",
      "Epoch: 64, train_loss: 0.76929, time: 0.03368\n",
      "Epoch: 65, train_loss: 0.77000, time: 0.03486\n",
      "Epoch: 66, train_loss: 0.77316, time: 0.03463\n",
      "Epoch: 67, train_loss: 0.77217, time: 0.03343\n",
      "Epoch: 68, train_loss: 0.76543, time: 0.03356\n",
      "Epoch: 69, train_loss: 0.76468, time: 0.03462\n",
      "Epoch: 70, train_loss: 0.77171, time: 0.03533\n",
      "Epoch: 71, train_loss: 0.78075, time: 0.03415\n",
      "Epoch: 72, train_loss: 0.76494, time: 0.03375\n",
      "Epoch: 73, train_loss: 0.77407, time: 0.03369\n",
      "Epoch: 74, train_loss: 0.76140, time: 0.03399\n",
      "Epoch: 75, train_loss: 0.76418, time: 0.03365\n",
      "Epoch: 76, train_loss: 0.77507, time: 0.03387\n",
      "Epoch: 77, train_loss: 0.77544, time: 0.03369\n",
      "Epoch: 78, train_loss: 0.76115, time: 0.03346\n",
      "Epoch: 79, train_loss: 0.76354, time: 0.03441\n",
      "Epoch: 80, train_loss: 0.78011, time: 0.03476\n",
      "Epoch: 81, train_loss: 0.76861, time: 0.03482\n",
      "Epoch: 82, train_loss: 0.77750, time: 0.03386\n",
      "Epoch: 83, train_loss: 0.76791, time: 0.03551\n",
      "Epoch: 84, train_loss: 0.76867, time: 0.03286\n",
      "Epoch: 85, train_loss: 0.76392, time: 0.03395\n",
      "Epoch: 86, train_loss: 0.76184, time: 0.03352\n",
      "Epoch: 87, train_loss: 0.77737, time: 0.03295\n",
      "Epoch: 88, train_loss: 0.76611, time: 0.03376\n",
      "Epoch: 89, train_loss: 0.77026, time: 0.03454\n",
      "Epoch: 90, train_loss: 0.77363, time: 0.03504\n",
      "Epoch: 91, train_loss: 0.76558, time: 0.03578\n",
      "Epoch: 92, train_loss: 0.76589, time: 0.03507\n",
      "Epoch: 93, train_loss: 0.76581, time: 0.03486\n",
      "Epoch: 94, train_loss: 0.76489, time: 0.03457\n",
      "Epoch: 95, train_loss: 0.77243, time: 0.03393\n",
      "Epoch: 96, train_loss: 0.76373, time: 0.03459\n",
      "Epoch: 97, train_loss: 0.77956, time: 0.03477\n",
      "Epoch: 98, train_loss: 0.76604, time: 0.03409\n",
      "Epoch: 99, train_loss: 0.76252, time: 0.03458\n",
      "Epoch: 100, train_loss: 0.77260, time: 0.03344\n",
      "Epoch: 101, train_loss: 0.76759, time: 0.03379\n",
      "Epoch: 102, train_loss: 0.78121, time: 0.03370\n",
      "Epoch: 103, train_loss: 0.76341, time: 0.03358\n",
      "Epoch: 104, train_loss: 0.76197, time: 0.03377\n",
      "Epoch: 105, train_loss: 0.76160, time: 0.03417\n",
      "Epoch: 106, train_loss: 0.77145, time: 0.03355\n",
      "Epoch: 107, train_loss: 0.77111, time: 0.03390\n",
      "Epoch: 108, train_loss: 0.77311, time: 0.03431\n",
      "Epoch: 109, train_loss: 0.76882, time: 0.03443\n",
      "Epoch: 110, train_loss: 0.77658, time: 0.03478\n",
      "Epoch: 111, train_loss: 0.76836, time: 0.03445\n",
      "Epoch: 112, train_loss: 0.77279, time: 0.03418\n",
      "Epoch: 113, train_loss: 0.76913, time: 0.03433\n",
      "Epoch: 114, train_loss: 0.76445, time: 0.03371\n",
      "Epoch: 115, train_loss: 0.77459, time: 0.03296\n",
      "Epoch: 116, train_loss: 0.76423, time: 0.03332\n",
      "Epoch: 117, train_loss: 0.77476, time: 0.03306\n",
      "Epoch: 118, train_loss: 0.77490, time: 0.03411\n",
      "Epoch: 119, train_loss: 0.77058, time: 0.03475\n",
      "Epoch: 120, train_loss: 0.77108, time: 0.03384\n",
      "Epoch: 121, train_loss: 0.76695, time: 0.03460\n",
      "Epoch: 122, train_loss: 0.76849, time: 0.03356\n",
      "Epoch: 123, train_loss: 0.77047, time: 0.03417\n",
      "Epoch: 124, train_loss: 0.77496, time: 0.03357\n",
      "Epoch: 125, train_loss: 0.76796, time: 0.03307\n",
      "Epoch: 126, train_loss: 0.76526, time: 0.03229\n",
      "Epoch: 127, train_loss: 0.76220, time: 0.03357\n",
      "Epoch: 128, train_loss: 0.77337, time: 0.03381\n",
      "Epoch: 129, train_loss: 0.76775, time: 0.03301\n",
      "Epoch: 130, train_loss: 0.76589, time: 0.03406\n",
      "Epoch: 131, train_loss: 0.76446, time: 0.03452\n",
      "Epoch: 132, train_loss: 0.76450, time: 0.03488\n",
      "Epoch: 133, train_loss: 0.76659, time: 0.03336\n",
      "Epoch: 134, train_loss: 0.77050, time: 0.03370\n",
      "Epoch: 135, train_loss: 0.76042, time: 0.03377\n",
      "Epoch: 136, train_loss: 0.76923, time: 0.03504\n",
      "Epoch: 137, train_loss: 0.76547, time: 0.03382\n",
      "Epoch: 138, train_loss: 0.77121, time: 0.03346\n",
      "Epoch: 139, train_loss: 0.77022, time: 0.03356\n",
      "Epoch: 140, train_loss: 0.77241, time: 0.03337\n",
      "Epoch: 141, train_loss: 0.78100, time: 0.03494\n",
      "Epoch: 142, train_loss: 0.78443, time: 0.03391\n",
      "Epoch: 143, train_loss: 0.77718, time: 0.03349\n",
      "Epoch: 144, train_loss: 0.77725, time: 0.03289\n",
      "Epoch: 145, train_loss: 0.76905, time: 0.03362\n",
      "Epoch: 146, train_loss: 0.76609, time: 0.03360\n",
      "Epoch: 147, train_loss: 0.77463, time: 0.03298\n",
      "Epoch: 148, train_loss: 0.76256, time: 0.03240\n",
      "Epoch: 149, train_loss: 0.76693, time: 0.03366\n",
      "Epoch: 150, train_loss: 0.76836, time: 0.03434\n",
      "Epoch: 151, train_loss: 0.77463, time: 0.03378\n",
      "Epoch: 152, train_loss: 0.76507, time: 0.03363\n",
      "Epoch: 153, train_loss: 0.76602, time: 0.03373\n",
      "Epoch: 154, train_loss: 0.77470, time: 0.03255\n",
      "Epoch: 155, train_loss: 0.76568, time: 0.03327\n",
      "Epoch: 156, train_loss: 0.76619, time: 0.03263\n",
      "Epoch: 157, train_loss: 0.76501, time: 0.03678\n",
      "Epoch: 158, train_loss: 0.77127, time: 0.03465\n",
      "Epoch: 159, train_loss: 0.76354, time: 0.03443\n",
      "Epoch: 160, train_loss: 0.77046, time: 0.03376\n",
      "Epoch: 161, train_loss: 0.77588, time: 0.03402\n",
      "Epoch: 162, train_loss: 0.76594, time: 0.03285\n",
      "Epoch: 163, train_loss: 0.76696, time: 0.03413\n",
      "Epoch: 164, train_loss: 0.76413, time: 0.03291\n",
      "Epoch: 165, train_loss: 0.76125, time: 0.03323\n",
      "Epoch: 166, train_loss: 0.76599, time: 0.03326\n",
      "Epoch: 167, train_loss: 0.76687, time: 0.03408\n",
      "Epoch: 168, train_loss: 0.76621, time: 0.03392\n",
      "Epoch: 169, train_loss: 0.77301, time: 0.03420\n",
      "Epoch: 170, train_loss: 0.76988, time: 0.03308\n",
      "Epoch: 171, train_loss: 0.76331, time: 0.03364\n",
      "Epoch: 172, train_loss: 0.76633, time: 0.03389\n",
      "Epoch: 173, train_loss: 0.76609, time: 0.03432\n",
      "Epoch: 174, train_loss: 0.76650, time: 0.03366\n",
      "Epoch: 175, train_loss: 0.76804, time: 0.03388\n",
      "Epoch: 176, train_loss: 0.77422, time: 0.03409\n",
      "Epoch: 177, train_loss: 0.76135, time: 0.03331\n",
      "Epoch: 178, train_loss: 0.77179, time: 0.03329\n",
      "Epoch: 179, train_loss: 0.77221, time: 0.03459\n",
      "Epoch: 180, train_loss: 0.76620, time: 0.03406\n",
      "Epoch: 181, train_loss: 0.76869, time: 0.03478\n",
      "Epoch: 182, train_loss: 0.76891, time: 0.03469\n",
      "Epoch: 183, train_loss: 0.76728, time: 0.03454\n",
      "Epoch: 184, train_loss: 0.77029, time: 0.03383\n",
      "Epoch: 185, train_loss: 0.76690, time: 0.03329\n",
      "Epoch: 186, train_loss: 0.76491, time: 0.03255\n",
      "Epoch: 187, train_loss: 0.76848, time: 0.03332\n",
      "Epoch: 188, train_loss: 0.76669, time: 0.03284\n",
      "Epoch: 189, train_loss: 0.77604, time: 0.03461\n",
      "Epoch: 190, train_loss: 0.76695, time: 0.03359\n",
      "Epoch: 191, train_loss: 0.77247, time: 0.03369\n",
      "Epoch: 192, train_loss: 0.76239, time: 0.03391\n",
      "Epoch: 193, train_loss: 0.76228, time: 0.03423\n",
      "Epoch: 194, train_loss: 0.77624, time: 0.03407\n",
      "Epoch: 195, train_loss: 0.77942, time: 0.03375\n",
      "Epoch: 196, train_loss: 0.76863, time: 0.03254\n",
      "Epoch: 197, train_loss: 0.76458, time: 0.03242\n",
      "Epoch: 198, train_loss: 0.78065, time: 0.03387\n",
      "Epoch: 199, train_loss: 0.76834, time: 0.03283\n",
      "Epoch: 200, train_loss: 0.76610, time: 0.03398\n",
      "pairwise precision 0.96630 recall 0.99422 f1 0.98006\n",
      "average until now [0.43058023485708286, 0.8423582519244864, 0.5698669931246809]\n",
      "71 names 298.39458084106445 avg time 4.20274057522626\n",
      "Loading zhifeng_liu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 340 nodes, 4268 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.96750, time: 0.11777\n",
      "Epoch: 2, train_loss: 0.96036, time: 0.01695\n",
      "Epoch: 3, train_loss: 0.96380, time: 0.01613\n",
      "Epoch: 4, train_loss: 0.96329, time: 0.01589\n",
      "Epoch: 5, train_loss: 0.96213, time: 0.01542\n",
      "Epoch: 6, train_loss: 0.96325, time: 0.01566\n",
      "Epoch: 7, train_loss: 0.96204, time: 0.01551\n",
      "Epoch: 8, train_loss: 0.96070, time: 0.01583\n",
      "Epoch: 9, train_loss: 0.96359, time: 0.01547\n",
      "Epoch: 10, train_loss: 0.96252, time: 0.01583\n",
      "Epoch: 11, train_loss: 0.96203, time: 0.01606\n",
      "Epoch: 12, train_loss: 0.96258, time: 0.01576\n",
      "Epoch: 13, train_loss: 0.96317, time: 0.01604\n",
      "Epoch: 14, train_loss: 0.96023, time: 0.01526\n",
      "Epoch: 15, train_loss: 0.96274, time: 0.01527\n",
      "Epoch: 16, train_loss: 0.96096, time: 0.01575\n",
      "Epoch: 17, train_loss: 0.96295, time: 0.01581\n",
      "Epoch: 18, train_loss: 0.96211, time: 0.01574\n",
      "Epoch: 19, train_loss: 0.96010, time: 0.01634\n",
      "Epoch: 20, train_loss: 0.96277, time: 0.01546\n",
      "Epoch: 21, train_loss: 0.96201, time: 0.01573\n",
      "Epoch: 22, train_loss: 0.96176, time: 0.01578\n",
      "Epoch: 23, train_loss: 0.96174, time: 0.01611\n",
      "Epoch: 24, train_loss: 0.95915, time: 0.01577\n",
      "Epoch: 25, train_loss: 0.96159, time: 0.01564\n",
      "Epoch: 26, train_loss: 0.96269, time: 0.01610\n",
      "Epoch: 27, train_loss: 0.95919, time: 0.01537\n",
      "Epoch: 28, train_loss: 0.95975, time: 0.01591\n",
      "Epoch: 29, train_loss: 0.96137, time: 0.01565\n",
      "Epoch: 30, train_loss: 0.96524, time: 0.01542\n",
      "Epoch: 31, train_loss: 0.96250, time: 0.01567\n",
      "Epoch: 32, train_loss: 0.96213, time: 0.01624\n",
      "Epoch: 33, train_loss: 0.96317, time: 0.01571\n",
      "Epoch: 34, train_loss: 0.96008, time: 0.01513\n",
      "Epoch: 35, train_loss: 0.96150, time: 0.01559\n",
      "Epoch: 36, train_loss: 0.96030, time: 0.01551\n",
      "Epoch: 37, train_loss: 0.96299, time: 0.01532\n",
      "Epoch: 38, train_loss: 0.96134, time: 0.01541\n",
      "Epoch: 39, train_loss: 0.96287, time: 0.01588\n",
      "Epoch: 40, train_loss: 0.96290, time: 0.01574\n",
      "Epoch: 41, train_loss: 0.96072, time: 0.01566\n",
      "Epoch: 42, train_loss: 0.96326, time: 0.01591\n",
      "Epoch: 43, train_loss: 0.96336, time: 0.01599\n",
      "Epoch: 44, train_loss: 0.96216, time: 0.01631\n",
      "Epoch: 45, train_loss: 0.96116, time: 0.01575\n",
      "Epoch: 46, train_loss: 0.96039, time: 0.01582\n",
      "Epoch: 47, train_loss: 0.96053, time: 0.01542\n",
      "Epoch: 48, train_loss: 0.96035, time: 0.01564\n",
      "Epoch: 49, train_loss: 0.96148, time: 0.01587\n",
      "Epoch: 50, train_loss: 0.96312, time: 0.01558\n",
      "Epoch: 51, train_loss: 0.96337, time: 0.01519\n",
      "Epoch: 52, train_loss: 0.96193, time: 0.01500\n",
      "Epoch: 53, train_loss: 0.96219, time: 0.01584\n",
      "Epoch: 54, train_loss: 0.96071, time: 0.01551\n",
      "Epoch: 55, train_loss: 0.96296, time: 0.01551\n",
      "Epoch: 56, train_loss: 0.96350, time: 0.01528\n",
      "Epoch: 57, train_loss: 0.96150, time: 0.01591\n",
      "Epoch: 58, train_loss: 0.96162, time: 0.01616\n",
      "Epoch: 59, train_loss: 0.96293, time: 0.01562\n",
      "Epoch: 60, train_loss: 0.96226, time: 0.01608\n",
      "Epoch: 61, train_loss: 0.96056, time: 0.01542\n",
      "Epoch: 62, train_loss: 0.96197, time: 0.01488\n",
      "Epoch: 63, train_loss: 0.95928, time: 0.01592\n",
      "Epoch: 64, train_loss: 0.96096, time: 0.01569\n",
      "Epoch: 65, train_loss: 0.96058, time: 0.01550\n",
      "Epoch: 66, train_loss: 0.95983, time: 0.01579\n",
      "Epoch: 67, train_loss: 0.96062, time: 0.01614\n",
      "Epoch: 68, train_loss: 0.96060, time: 0.01591\n",
      "Epoch: 69, train_loss: 0.95940, time: 0.01566\n",
      "Epoch: 70, train_loss: 0.96215, time: 0.01601\n",
      "Epoch: 71, train_loss: 0.96244, time: 0.01527\n",
      "Epoch: 72, train_loss: 0.96300, time: 0.01609\n",
      "Epoch: 73, train_loss: 0.96368, time: 0.01570\n",
      "Epoch: 74, train_loss: 0.96079, time: 0.01574\n",
      "Epoch: 75, train_loss: 0.96081, time: 0.01609\n",
      "Epoch: 76, train_loss: 0.95988, time: 0.01565\n",
      "Epoch: 77, train_loss: 0.96061, time: 0.01590\n",
      "Epoch: 78, train_loss: 0.96228, time: 0.01650\n",
      "Epoch: 79, train_loss: 0.96072, time: 0.01591\n",
      "Epoch: 80, train_loss: 0.96034, time: 0.01590\n",
      "Epoch: 81, train_loss: 0.96195, time: 0.01556\n",
      "Epoch: 82, train_loss: 0.96222, time: 0.01577\n",
      "Epoch: 83, train_loss: 0.96162, time: 0.01587\n",
      "Epoch: 84, train_loss: 0.96257, time: 0.01602\n",
      "Epoch: 85, train_loss: 0.96141, time: 0.01587\n",
      "Epoch: 86, train_loss: 0.96226, time: 0.01566\n",
      "Epoch: 87, train_loss: 0.96258, time: 0.01601\n",
      "Epoch: 88, train_loss: 0.96089, time: 0.01583\n",
      "Epoch: 89, train_loss: 0.96001, time: 0.01618\n",
      "Epoch: 90, train_loss: 0.96134, time: 0.01627\n",
      "Epoch: 91, train_loss: 0.95981, time: 0.01564\n",
      "Epoch: 92, train_loss: 0.96231, time: 0.01518\n",
      "Epoch: 93, train_loss: 0.96169, time: 0.01564\n",
      "Epoch: 94, train_loss: 0.96103, time: 0.01519\n",
      "Epoch: 95, train_loss: 0.95963, time: 0.01493\n",
      "Epoch: 96, train_loss: 0.96133, time: 0.01556\n",
      "Epoch: 97, train_loss: 0.96213, time: 0.01606\n",
      "Epoch: 98, train_loss: 0.96153, time: 0.01597\n",
      "Epoch: 99, train_loss: 0.96018, time: 0.01581\n",
      "Epoch: 100, train_loss: 0.96211, time: 0.01570\n",
      "Epoch: 101, train_loss: 0.95957, time: 0.01580\n",
      "Epoch: 102, train_loss: 0.96041, time: 0.01554\n",
      "Epoch: 103, train_loss: 0.96169, time: 0.01589\n",
      "Epoch: 104, train_loss: 0.96068, time: 0.01576\n",
      "Epoch: 105, train_loss: 0.96128, time: 0.01597\n",
      "Epoch: 106, train_loss: 0.96098, time: 0.01567\n",
      "Epoch: 107, train_loss: 0.96124, time: 0.01549\n",
      "Epoch: 108, train_loss: 0.96127, time: 0.01614\n",
      "Epoch: 109, train_loss: 0.96434, time: 0.01586\n",
      "Epoch: 110, train_loss: 0.96310, time: 0.01581\n",
      "Epoch: 111, train_loss: 0.96171, time: 0.01571\n",
      "Epoch: 112, train_loss: 0.96177, time: 0.01544\n",
      "Epoch: 113, train_loss: 0.96172, time: 0.01597\n",
      "Epoch: 114, train_loss: 0.96165, time: 0.01598\n",
      "Epoch: 115, train_loss: 0.96052, time: 0.01544\n",
      "Epoch: 116, train_loss: 0.96253, time: 0.01560\n",
      "Epoch: 117, train_loss: 0.96025, time: 0.01506\n",
      "Epoch: 118, train_loss: 0.95991, time: 0.01531\n",
      "Epoch: 119, train_loss: 0.96149, time: 0.01595\n",
      "Epoch: 120, train_loss: 0.96150, time: 0.01430\n",
      "Epoch: 121, train_loss: 0.96146, time: 0.01500\n",
      "Epoch: 122, train_loss: 0.96347, time: 0.01522\n",
      "Epoch: 123, train_loss: 0.96149, time: 0.01517\n",
      "Epoch: 124, train_loss: 0.96013, time: 0.01522\n",
      "Epoch: 125, train_loss: 0.96258, time: 0.01519\n",
      "Epoch: 126, train_loss: 0.96188, time: 0.01532\n",
      "Epoch: 127, train_loss: 0.96091, time: 0.01495\n",
      "Epoch: 128, train_loss: 0.96112, time: 0.01527\n",
      "Epoch: 129, train_loss: 0.96408, time: 0.01522\n",
      "Epoch: 130, train_loss: 0.96347, time: 0.01462\n",
      "Epoch: 131, train_loss: 0.96008, time: 0.01522\n",
      "Epoch: 132, train_loss: 0.96109, time: 0.01508\n",
      "Epoch: 133, train_loss: 0.96107, time: 0.01456\n",
      "Epoch: 134, train_loss: 0.96115, time: 0.01488\n",
      "Epoch: 135, train_loss: 0.96092, time: 0.01518\n",
      "Epoch: 136, train_loss: 0.96101, time: 0.01593\n",
      "Epoch: 137, train_loss: 0.96127, time: 0.01574\n",
      "Epoch: 138, train_loss: 0.96150, time: 0.01628\n",
      "Epoch: 139, train_loss: 0.96240, time: 0.01617\n",
      "Epoch: 140, train_loss: 0.96198, time: 0.01635\n",
      "Epoch: 141, train_loss: 0.96138, time: 0.01575\n",
      "Epoch: 142, train_loss: 0.96070, time: 0.01480\n",
      "Epoch: 143, train_loss: 0.96137, time: 0.01566\n",
      "Epoch: 144, train_loss: 0.96317, time: 0.01588\n",
      "Epoch: 145, train_loss: 0.96186, time: 0.01520\n",
      "Epoch: 146, train_loss: 0.96223, time: 0.01568\n",
      "Epoch: 147, train_loss: 0.96190, time: 0.01592\n",
      "Epoch: 148, train_loss: 0.96197, time: 0.01583\n",
      "Epoch: 149, train_loss: 0.95978, time: 0.01584\n",
      "Epoch: 150, train_loss: 0.96012, time: 0.01589\n",
      "Epoch: 151, train_loss: 0.96277, time: 0.01585\n",
      "Epoch: 152, train_loss: 0.96423, time: 0.01594\n",
      "Epoch: 153, train_loss: 0.96215, time: 0.01610\n",
      "Epoch: 154, train_loss: 0.96324, time: 0.01572\n",
      "Epoch: 155, train_loss: 0.96309, time: 0.01581\n",
      "Epoch: 156, train_loss: 0.96109, time: 0.01549\n",
      "Epoch: 157, train_loss: 0.96159, time: 0.01587\n",
      "Epoch: 158, train_loss: 0.96281, time: 0.01564\n",
      "Epoch: 159, train_loss: 0.96207, time: 0.01589\n",
      "Epoch: 160, train_loss: 0.96096, time: 0.01562\n",
      "Epoch: 161, train_loss: 0.96081, time: 0.01585\n",
      "Epoch: 162, train_loss: 0.96109, time: 0.01606\n",
      "Epoch: 163, train_loss: 0.96063, time: 0.01537\n",
      "Epoch: 164, train_loss: 0.95831, time: 0.01578\n",
      "Epoch: 165, train_loss: 0.96341, time: 0.01535\n",
      "Epoch: 166, train_loss: 0.96162, time: 0.01554\n",
      "Epoch: 167, train_loss: 0.96255, time: 0.01542\n",
      "Epoch: 168, train_loss: 0.96145, time: 0.01559\n",
      "Epoch: 169, train_loss: 0.96099, time: 0.01557\n",
      "Epoch: 170, train_loss: 0.96184, time: 0.01576\n",
      "Epoch: 171, train_loss: 0.96218, time: 0.01622\n",
      "Epoch: 172, train_loss: 0.96159, time: 0.01569\n",
      "Epoch: 173, train_loss: 0.96098, time: 0.01593\n",
      "Epoch: 174, train_loss: 0.96337, time: 0.01549\n",
      "Epoch: 175, train_loss: 0.96211, time: 0.01525\n",
      "Epoch: 176, train_loss: 0.95875, time: 0.01493\n",
      "Epoch: 177, train_loss: 0.96141, time: 0.01544\n",
      "Epoch: 178, train_loss: 0.96225, time: 0.01561\n",
      "Epoch: 179, train_loss: 0.96286, time: 0.01581\n",
      "Epoch: 180, train_loss: 0.96389, time: 0.01577\n",
      "Epoch: 181, train_loss: 0.96014, time: 0.01573\n",
      "Epoch: 182, train_loss: 0.96129, time: 0.01606\n",
      "Epoch: 183, train_loss: 0.95933, time: 0.01541\n",
      "Epoch: 184, train_loss: 0.96100, time: 0.01555\n",
      "Epoch: 185, train_loss: 0.96262, time: 0.01577\n",
      "Epoch: 186, train_loss: 0.96172, time: 0.01546\n",
      "Epoch: 187, train_loss: 0.96178, time: 0.01567\n",
      "Epoch: 188, train_loss: 0.96256, time: 0.01587\n",
      "Epoch: 189, train_loss: 0.96052, time: 0.01559\n",
      "Epoch: 190, train_loss: 0.96115, time: 0.01620\n",
      "Epoch: 191, train_loss: 0.96150, time: 0.01542\n",
      "Epoch: 192, train_loss: 0.96219, time: 0.01581\n",
      "Epoch: 193, train_loss: 0.96162, time: 0.01573\n",
      "Epoch: 194, train_loss: 0.96343, time: 0.01570\n",
      "Epoch: 195, train_loss: 0.96324, time: 0.01566\n",
      "Epoch: 196, train_loss: 0.96118, time: 0.01539\n",
      "Epoch: 197, train_loss: 0.96013, time: 0.01568\n",
      "Epoch: 198, train_loss: 0.96110, time: 0.01561\n",
      "Epoch: 199, train_loss: 0.96352, time: 0.01553\n",
      "Epoch: 200, train_loss: 0.95959, time: 0.01590\n",
      "pairwise precision 0.28362 recall 0.94891 f1 0.43671\n",
      "average until now [0.42853914431248286, 0.843838131494772, 0.568412651946471]\n",
      "72 names 302.20999002456665 avg time 4.197360972563426\n",
      "Loading su_zeng dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 253 nodes, 9252 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.85531, time: 0.11809\n",
      "Epoch: 2, train_loss: 0.85853, time: 0.01869\n",
      "Epoch: 3, train_loss: 0.85456, time: 0.01768\n",
      "Epoch: 4, train_loss: 0.85400, time: 0.01697\n",
      "Epoch: 5, train_loss: 0.85474, time: 0.01676\n",
      "Epoch: 6, train_loss: 0.85498, time: 0.01686\n",
      "Epoch: 7, train_loss: 0.85768, time: 0.01672\n",
      "Epoch: 8, train_loss: 0.85422, time: 0.01673\n",
      "Epoch: 9, train_loss: 0.85288, time: 0.01570\n",
      "Epoch: 10, train_loss: 0.85482, time: 0.01614\n",
      "Epoch: 11, train_loss: 0.85448, time: 0.01555\n",
      "Epoch: 12, train_loss: 0.85474, time: 0.01588\n",
      "Epoch: 13, train_loss: 0.85455, time: 0.01580\n",
      "Epoch: 14, train_loss: 0.85302, time: 0.01615\n",
      "Epoch: 15, train_loss: 0.85127, time: 0.01630\n",
      "Epoch: 16, train_loss: 0.85592, time: 0.01637\n",
      "Epoch: 17, train_loss: 0.85163, time: 0.01608\n",
      "Epoch: 18, train_loss: 0.85549, time: 0.01640\n",
      "Epoch: 19, train_loss: 0.85340, time: 0.01647\n",
      "Epoch: 20, train_loss: 0.85372, time: 0.01622\n",
      "Epoch: 21, train_loss: 0.85093, time: 0.01650\n",
      "Epoch: 22, train_loss: 0.85210, time: 0.01630\n",
      "Epoch: 23, train_loss: 0.85568, time: 0.01670\n",
      "Epoch: 24, train_loss: 0.84931, time: 0.01640\n",
      "Epoch: 25, train_loss: 0.85441, time: 0.01655\n",
      "Epoch: 26, train_loss: 0.85110, time: 0.01660\n",
      "Epoch: 27, train_loss: 0.83908, time: 0.01703\n",
      "Epoch: 28, train_loss: 0.85034, time: 0.01646\n",
      "Epoch: 29, train_loss: 0.84865, time: 0.01618\n",
      "Epoch: 30, train_loss: 0.84433, time: 0.01602\n",
      "Epoch: 31, train_loss: 0.84188, time: 0.01608\n",
      "Epoch: 32, train_loss: 0.84249, time: 0.01660\n",
      "Epoch: 33, train_loss: 0.84291, time: 0.01677\n",
      "Epoch: 34, train_loss: 0.84010, time: 0.01661\n",
      "Epoch: 35, train_loss: 0.85370, time: 0.01687\n",
      "Epoch: 36, train_loss: 0.84169, time: 0.01650\n",
      "Epoch: 37, train_loss: 0.84650, time: 0.01605\n",
      "Epoch: 38, train_loss: 0.83898, time: 0.01719\n",
      "Epoch: 39, train_loss: 0.84975, time: 0.01650\n",
      "Epoch: 40, train_loss: 0.83639, time: 0.01653\n",
      "Epoch: 41, train_loss: 0.84060, time: 0.01654\n",
      "Epoch: 42, train_loss: 0.84555, time: 0.01685\n",
      "Epoch: 43, train_loss: 0.83624, time: 0.01604\n",
      "Epoch: 44, train_loss: 0.83511, time: 0.01633\n",
      "Epoch: 45, train_loss: 0.83951, time: 0.01638\n",
      "Epoch: 46, train_loss: 0.83585, time: 0.01633\n",
      "Epoch: 47, train_loss: 0.83459, time: 0.01594\n",
      "Epoch: 48, train_loss: 0.84245, time: 0.01625\n",
      "Epoch: 49, train_loss: 0.84841, time: 0.01593\n",
      "Epoch: 50, train_loss: 0.84745, time: 0.01639\n",
      "Epoch: 51, train_loss: 0.83647, time: 0.01722\n",
      "Epoch: 52, train_loss: 0.84407, time: 0.01698\n",
      "Epoch: 53, train_loss: 0.84046, time: 0.01644\n",
      "Epoch: 54, train_loss: 0.84405, time: 0.01614\n",
      "Epoch: 55, train_loss: 0.83887, time: 0.01643\n",
      "Epoch: 56, train_loss: 0.82364, time: 0.01611\n",
      "Epoch: 57, train_loss: 0.83485, time: 0.01543\n",
      "Epoch: 58, train_loss: 0.83359, time: 0.01662\n",
      "Epoch: 59, train_loss: 0.83206, time: 0.01630\n",
      "Epoch: 60, train_loss: 0.83750, time: 0.01617\n",
      "Epoch: 61, train_loss: 0.83163, time: 0.01673\n",
      "Epoch: 62, train_loss: 0.82872, time: 0.01665\n",
      "Epoch: 63, train_loss: 0.83034, time: 0.01697\n",
      "Epoch: 64, train_loss: 0.82988, time: 0.01651\n",
      "Epoch: 65, train_loss: 0.83478, time: 0.01629\n",
      "Epoch: 66, train_loss: 0.85501, time: 0.01667\n",
      "Epoch: 67, train_loss: 0.84567, time: 0.01625\n",
      "Epoch: 68, train_loss: 0.83649, time: 0.01637\n",
      "Epoch: 69, train_loss: 0.83424, time: 0.01667\n",
      "Epoch: 70, train_loss: 0.83437, time: 0.01638\n",
      "Epoch: 71, train_loss: 0.83623, time: 0.01584\n",
      "Epoch: 72, train_loss: 0.84012, time: 0.01665\n",
      "Epoch: 73, train_loss: 0.84401, time: 0.01765\n",
      "Epoch: 74, train_loss: 0.82503, time: 0.01671\n",
      "Epoch: 75, train_loss: 0.85135, time: 0.01664\n",
      "Epoch: 76, train_loss: 0.83267, time: 0.01694\n",
      "Epoch: 77, train_loss: 0.83897, time: 0.01603\n",
      "Epoch: 78, train_loss: 0.83188, time: 0.01637\n",
      "Epoch: 79, train_loss: 0.84049, time: 0.01643\n",
      "Epoch: 80, train_loss: 0.83859, time: 0.01658\n",
      "Epoch: 81, train_loss: 0.83382, time: 0.01672\n",
      "Epoch: 82, train_loss: 0.83715, time: 0.01675\n",
      "Epoch: 83, train_loss: 0.83073, time: 0.01741\n",
      "Epoch: 84, train_loss: 0.82292, time: 0.01695\n",
      "Epoch: 85, train_loss: 0.82411, time: 0.01697\n",
      "Epoch: 86, train_loss: 0.83949, time: 0.01716\n",
      "Epoch: 87, train_loss: 0.85199, time: 0.01679\n",
      "Epoch: 88, train_loss: 0.84052, time: 0.01638\n",
      "Epoch: 89, train_loss: 0.83553, time: 0.01745\n",
      "Epoch: 90, train_loss: 0.82643, time: 0.01722\n",
      "Epoch: 91, train_loss: 0.84066, time: 0.01680\n",
      "Epoch: 92, train_loss: 0.83803, time: 0.01663\n",
      "Epoch: 93, train_loss: 0.83375, time: 0.01610\n",
      "Epoch: 94, train_loss: 0.84500, time: 0.01659\n",
      "Epoch: 95, train_loss: 0.83410, time: 0.01620\n",
      "Epoch: 96, train_loss: 0.83976, time: 0.01627\n",
      "Epoch: 97, train_loss: 0.83293, time: 0.01656\n",
      "Epoch: 98, train_loss: 0.83612, time: 0.01683\n",
      "Epoch: 99, train_loss: 0.84025, time: 0.01653\n",
      "Epoch: 100, train_loss: 0.83083, time: 0.01687\n",
      "Epoch: 101, train_loss: 0.83988, time: 0.01624\n",
      "Epoch: 102, train_loss: 0.82082, time: 0.01681\n",
      "Epoch: 103, train_loss: 0.83885, time: 0.01655\n",
      "Epoch: 104, train_loss: 0.83835, time: 0.01661\n",
      "Epoch: 105, train_loss: 0.83344, time: 0.01618\n",
      "Epoch: 106, train_loss: 0.83167, time: 0.01579\n",
      "Epoch: 107, train_loss: 0.84686, time: 0.01607\n",
      "Epoch: 108, train_loss: 0.84264, time: 0.01693\n",
      "Epoch: 109, train_loss: 0.82630, time: 0.01727\n",
      "Epoch: 110, train_loss: 0.82505, time: 0.01724\n",
      "Epoch: 111, train_loss: 0.83429, time: 0.01747\n",
      "Epoch: 112, train_loss: 0.83784, time: 0.01741\n",
      "Epoch: 113, train_loss: 0.83082, time: 0.01723\n",
      "Epoch: 114, train_loss: 0.82851, time: 0.01658\n",
      "Epoch: 115, train_loss: 0.82491, time: 0.01692\n",
      "Epoch: 116, train_loss: 0.83988, time: 0.01683\n",
      "Epoch: 117, train_loss: 0.83518, time: 0.01691\n",
      "Epoch: 118, train_loss: 0.83716, time: 0.01679\n",
      "Epoch: 119, train_loss: 0.84000, time: 0.01697\n",
      "Epoch: 120, train_loss: 0.83509, time: 0.01663\n",
      "Epoch: 121, train_loss: 0.82941, time: 0.01708\n",
      "Epoch: 122, train_loss: 0.83183, time: 0.01753\n",
      "Epoch: 123, train_loss: 0.82618, time: 0.01703\n",
      "Epoch: 124, train_loss: 0.83274, time: 0.01706\n",
      "Epoch: 125, train_loss: 0.83072, time: 0.01685\n",
      "Epoch: 126, train_loss: 0.84296, time: 0.01624\n",
      "Epoch: 127, train_loss: 0.83209, time: 0.01660\n",
      "Epoch: 128, train_loss: 0.83963, time: 0.01671\n",
      "Epoch: 129, train_loss: 0.82965, time: 0.01739\n",
      "Epoch: 130, train_loss: 0.82767, time: 0.01670\n",
      "Epoch: 131, train_loss: 0.82954, time: 0.01682\n",
      "Epoch: 132, train_loss: 0.83514, time: 0.01637\n",
      "Epoch: 133, train_loss: 0.83500, time: 0.01601\n",
      "Epoch: 134, train_loss: 0.83118, time: 0.01626\n",
      "Epoch: 135, train_loss: 0.81807, time: 0.01648\n",
      "Epoch: 136, train_loss: 0.83644, time: 0.01577\n",
      "Epoch: 137, train_loss: 0.83181, time: 0.01664\n",
      "Epoch: 138, train_loss: 0.82929, time: 0.01669\n",
      "Epoch: 139, train_loss: 0.84688, time: 0.01728\n",
      "Epoch: 140, train_loss: 0.82898, time: 0.01610\n",
      "Epoch: 141, train_loss: 0.83573, time: 0.01679\n",
      "Epoch: 142, train_loss: 0.83680, time: 0.01628\n",
      "Epoch: 143, train_loss: 0.83506, time: 0.01616\n",
      "Epoch: 144, train_loss: 0.83022, time: 0.01650\n",
      "Epoch: 145, train_loss: 0.84408, time: 0.01651\n",
      "Epoch: 146, train_loss: 0.82932, time: 0.01629\n",
      "Epoch: 147, train_loss: 0.83925, time: 0.01686\n",
      "Epoch: 148, train_loss: 0.83775, time: 0.01643\n",
      "Epoch: 149, train_loss: 0.83094, time: 0.01657\n",
      "Epoch: 150, train_loss: 0.83497, time: 0.01656\n",
      "Epoch: 151, train_loss: 0.84286, time: 0.01612\n",
      "Epoch: 152, train_loss: 0.84237, time: 0.01648\n",
      "Epoch: 153, train_loss: 0.84135, time: 0.01685\n",
      "Epoch: 154, train_loss: 0.83809, time: 0.01657\n",
      "Epoch: 155, train_loss: 0.83781, time: 0.01652\n",
      "Epoch: 156, train_loss: 0.82502, time: 0.01638\n",
      "Epoch: 157, train_loss: 0.83483, time: 0.01678\n",
      "Epoch: 158, train_loss: 0.83692, time: 0.01723\n",
      "Epoch: 159, train_loss: 0.83227, time: 0.01673\n",
      "Epoch: 160, train_loss: 0.83747, time: 0.01680\n",
      "Epoch: 161, train_loss: 0.83256, time: 0.01701\n",
      "Epoch: 162, train_loss: 0.82991, time: 0.01687\n",
      "Epoch: 163, train_loss: 0.84404, time: 0.01666\n",
      "Epoch: 164, train_loss: 0.83919, time: 0.01705\n",
      "Epoch: 165, train_loss: 0.85100, time: 0.01647\n",
      "Epoch: 166, train_loss: 0.83320, time: 0.01807\n",
      "Epoch: 167, train_loss: 0.83294, time: 0.01697\n",
      "Epoch: 168, train_loss: 0.82194, time: 0.01667\n",
      "Epoch: 169, train_loss: 0.82365, time: 0.01668\n",
      "Epoch: 170, train_loss: 0.83265, time: 0.01639\n",
      "Epoch: 171, train_loss: 0.82928, time: 0.01672\n",
      "Epoch: 172, train_loss: 0.83370, time: 0.01669\n",
      "Epoch: 173, train_loss: 0.83528, time: 0.01679\n",
      "Epoch: 174, train_loss: 0.83421, time: 0.01621\n",
      "Epoch: 175, train_loss: 0.83302, time: 0.01642\n",
      "Epoch: 176, train_loss: 0.83048, time: 0.01636\n",
      "Epoch: 177, train_loss: 0.83905, time: 0.01651\n",
      "Epoch: 178, train_loss: 0.84441, time: 0.01643\n",
      "Epoch: 179, train_loss: 0.83151, time: 0.01669\n",
      "Epoch: 180, train_loss: 0.84484, time: 0.01636\n",
      "Epoch: 181, train_loss: 0.82926, time: 0.01678\n",
      "Epoch: 182, train_loss: 0.82923, time: 0.01673\n",
      "Epoch: 183, train_loss: 0.82546, time: 0.01659\n",
      "Epoch: 184, train_loss: 0.82905, time: 0.01686\n",
      "Epoch: 185, train_loss: 0.82331, time: 0.01668\n",
      "Epoch: 186, train_loss: 0.83435, time: 0.01757\n",
      "Epoch: 187, train_loss: 0.83016, time: 0.01712\n",
      "Epoch: 188, train_loss: 0.83512, time: 0.01715\n",
      "Epoch: 189, train_loss: 0.83279, time: 0.01654\n",
      "Epoch: 190, train_loss: 0.83157, time: 0.01655\n",
      "Epoch: 191, train_loss: 0.83245, time: 0.01648\n",
      "Epoch: 192, train_loss: 0.82155, time: 0.01659\n",
      "Epoch: 193, train_loss: 0.84038, time: 0.01668\n",
      "Epoch: 194, train_loss: 0.83403, time: 0.01728\n",
      "Epoch: 195, train_loss: 0.84563, time: 0.01665\n",
      "Epoch: 196, train_loss: 0.83984, time: 0.01626\n",
      "Epoch: 197, train_loss: 0.83031, time: 0.01655\n",
      "Epoch: 198, train_loss: 0.81784, time: 0.01663\n",
      "Epoch: 199, train_loss: 0.84097, time: 0.01691\n",
      "Epoch: 200, train_loss: 0.85234, time: 0.01672\n",
      "pairwise precision 0.94462 recall 0.55218 f1 0.69695\n",
      "average until now [0.4356087387841942, 0.8398427861064481, 0.5736679908147839]\n",
      "73 names 305.7079710960388 avg time 4.187780425973134\n",
      "Loading lei_song dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 700 nodes, 37347 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.92545, time: 0.15225\n",
      "Epoch: 2, train_loss: 0.92449, time: 0.05365\n",
      "Epoch: 3, train_loss: 0.92276, time: 0.05439\n",
      "Epoch: 4, train_loss: 0.92282, time: 0.05261\n",
      "Epoch: 5, train_loss: 0.92163, time: 0.05261\n",
      "Epoch: 6, train_loss: 0.92185, time: 0.05377\n",
      "Epoch: 7, train_loss: 0.92148, time: 0.05335\n",
      "Epoch: 8, train_loss: 0.92275, time: 0.05167\n",
      "Epoch: 9, train_loss: 0.92219, time: 0.05265\n",
      "Epoch: 10, train_loss: 0.92218, time: 0.05330\n",
      "Epoch: 11, train_loss: 0.92188, time: 0.05245\n",
      "Epoch: 12, train_loss: 0.92091, time: 0.05277\n",
      "Epoch: 13, train_loss: 0.92179, time: 0.05147\n",
      "Epoch: 14, train_loss: 0.92195, time: 0.05220\n",
      "Epoch: 15, train_loss: 0.92170, time: 0.05238\n",
      "Epoch: 16, train_loss: 0.92220, time: 0.05227\n",
      "Epoch: 17, train_loss: 0.92114, time: 0.05217\n",
      "Epoch: 18, train_loss: 0.92135, time: 0.05191\n",
      "Epoch: 19, train_loss: 0.92161, time: 0.05054\n",
      "Epoch: 20, train_loss: 0.92178, time: 0.05143\n",
      "Epoch: 21, train_loss: 0.92193, time: 0.05013\n",
      "Epoch: 22, train_loss: 0.92206, time: 0.05171\n",
      "Epoch: 23, train_loss: 0.92303, time: 0.05324\n",
      "Epoch: 24, train_loss: 0.92207, time: 0.05024\n",
      "Epoch: 25, train_loss: 0.91992, time: 0.04993\n",
      "Epoch: 26, train_loss: 0.92165, time: 0.05110\n",
      "Epoch: 27, train_loss: 0.92051, time: 0.05133\n",
      "Epoch: 28, train_loss: 0.92061, time: 0.04987\n",
      "Epoch: 29, train_loss: 0.92138, time: 0.05189\n",
      "Epoch: 30, train_loss: 0.92065, time: 0.05101\n",
      "Epoch: 31, train_loss: 0.92104, time: 0.05023\n",
      "Epoch: 32, train_loss: 0.91960, time: 0.05144\n",
      "Epoch: 33, train_loss: 0.92123, time: 0.05282\n",
      "Epoch: 34, train_loss: 0.92390, time: 0.05109\n",
      "Epoch: 35, train_loss: 0.92023, time: 0.05246\n",
      "Epoch: 36, train_loss: 0.92077, time: 0.05021\n",
      "Epoch: 37, train_loss: 0.92106, time: 0.05175\n",
      "Epoch: 38, train_loss: 0.91922, time: 0.05189\n",
      "Epoch: 39, train_loss: 0.92069, time: 0.05062\n",
      "Epoch: 40, train_loss: 0.92045, time: 0.05052\n",
      "Epoch: 41, train_loss: 0.91578, time: 0.05147\n",
      "Epoch: 42, train_loss: 0.92138, time: 0.05041\n",
      "Epoch: 43, train_loss: 0.91869, time: 0.05148\n",
      "Epoch: 44, train_loss: 0.91956, time: 0.05126\n",
      "Epoch: 45, train_loss: 0.91671, time: 0.05166\n",
      "Epoch: 46, train_loss: 0.91859, time: 0.05187\n",
      "Epoch: 47, train_loss: 0.92258, time: 0.05085\n",
      "Epoch: 48, train_loss: 0.91993, time: 0.05093\n",
      "Epoch: 49, train_loss: 0.91778, time: 0.05255\n",
      "Epoch: 50, train_loss: 0.91653, time: 0.05076\n",
      "Epoch: 51, train_loss: 0.91565, time: 0.05114\n",
      "Epoch: 52, train_loss: 0.91659, time: 0.04932\n",
      "Epoch: 53, train_loss: 0.91418, time: 0.05154\n",
      "Epoch: 54, train_loss: 0.91399, time: 0.05087\n",
      "Epoch: 55, train_loss: 0.91282, time: 0.05048\n",
      "Epoch: 56, train_loss: 0.91242, time: 0.04992\n",
      "Epoch: 57, train_loss: 0.91765, time: 0.05164\n",
      "Epoch: 58, train_loss: 0.91813, time: 0.05118\n",
      "Epoch: 59, train_loss: 0.91437, time: 0.05062\n",
      "Epoch: 60, train_loss: 0.91528, time: 0.05172\n",
      "Epoch: 61, train_loss: 0.91216, time: 0.05287\n",
      "Epoch: 62, train_loss: 0.91622, time: 0.05194\n",
      "Epoch: 63, train_loss: 0.91558, time: 0.05040\n",
      "Epoch: 64, train_loss: 0.91266, time: 0.05180\n",
      "Epoch: 65, train_loss: 0.91606, time: 0.05177\n",
      "Epoch: 66, train_loss: 0.91364, time: 0.05172\n",
      "Epoch: 67, train_loss: 0.91349, time: 0.05040\n",
      "Epoch: 68, train_loss: 0.91347, time: 0.05316\n",
      "Epoch: 69, train_loss: 0.91362, time: 0.05120\n",
      "Epoch: 70, train_loss: 0.91686, time: 0.05194\n",
      "Epoch: 71, train_loss: 0.91359, time: 0.05226\n",
      "Epoch: 72, train_loss: 0.90962, time: 0.05218\n",
      "Epoch: 73, train_loss: 0.91765, time: 0.05046\n",
      "Epoch: 74, train_loss: 0.90982, time: 0.05181\n",
      "Epoch: 75, train_loss: 0.91543, time: 0.05129\n",
      "Epoch: 76, train_loss: 0.90918, time: 0.04965\n",
      "Epoch: 77, train_loss: 0.91615, time: 0.05278\n",
      "Epoch: 78, train_loss: 0.90825, time: 0.05271\n",
      "Epoch: 79, train_loss: 0.91653, time: 0.05102\n",
      "Epoch: 80, train_loss: 0.90451, time: 0.05192\n",
      "Epoch: 81, train_loss: 0.91227, time: 0.05130\n",
      "Epoch: 82, train_loss: 0.91227, time: 0.05151\n",
      "Epoch: 83, train_loss: 0.90832, time: 0.05068\n",
      "Epoch: 84, train_loss: 0.91092, time: 0.05178\n",
      "Epoch: 85, train_loss: 0.91264, time: 0.05358\n",
      "Epoch: 86, train_loss: 0.90676, time: 0.05285\n",
      "Epoch: 87, train_loss: 0.90859, time: 0.05091\n",
      "Epoch: 88, train_loss: 0.91321, time: 0.05234\n",
      "Epoch: 89, train_loss: 0.90837, time: 0.05209\n",
      "Epoch: 90, train_loss: 0.91248, time: 0.05367\n",
      "Epoch: 91, train_loss: 0.90928, time: 0.05203\n",
      "Epoch: 92, train_loss: 0.91398, time: 0.05102\n",
      "Epoch: 93, train_loss: 0.90703, time: 0.05166\n",
      "Epoch: 94, train_loss: 0.90797, time: 0.05324\n",
      "Epoch: 95, train_loss: 0.90693, time: 0.05067\n",
      "Epoch: 96, train_loss: 0.90933, time: 0.05010\n",
      "Epoch: 97, train_loss: 0.91178, time: 0.05212\n",
      "Epoch: 98, train_loss: 0.90870, time: 0.04973\n",
      "Epoch: 99, train_loss: 0.90491, time: 0.05115\n",
      "Epoch: 100, train_loss: 0.91482, time: 0.05171\n",
      "Epoch: 101, train_loss: 0.90827, time: 0.05168\n",
      "Epoch: 102, train_loss: 0.91172, time: 0.05118\n",
      "Epoch: 103, train_loss: 0.91027, time: 0.05088\n",
      "Epoch: 104, train_loss: 0.90948, time: 0.05183\n",
      "Epoch: 105, train_loss: 0.90948, time: 0.05079\n",
      "Epoch: 106, train_loss: 0.90698, time: 0.05130\n",
      "Epoch: 107, train_loss: 0.90823, time: 0.05163\n",
      "Epoch: 108, train_loss: 0.91050, time: 0.05246\n",
      "Epoch: 109, train_loss: 0.91140, time: 0.05094\n",
      "Epoch: 110, train_loss: 0.90707, time: 0.05158\n",
      "Epoch: 111, train_loss: 0.90586, time: 0.05280\n",
      "Epoch: 112, train_loss: 0.90801, time: 0.05061\n",
      "Epoch: 113, train_loss: 0.90897, time: 0.05100\n",
      "Epoch: 114, train_loss: 0.90679, time: 0.05087\n",
      "Epoch: 115, train_loss: 0.91020, time: 0.05271\n",
      "Epoch: 116, train_loss: 0.90753, time: 0.05100\n",
      "Epoch: 117, train_loss: 0.91072, time: 0.04872\n",
      "Epoch: 118, train_loss: 0.91030, time: 0.05129\n",
      "Epoch: 119, train_loss: 0.90663, time: 0.05139\n",
      "Epoch: 120, train_loss: 0.91200, time: 0.05053\n",
      "Epoch: 121, train_loss: 0.90761, time: 0.05234\n",
      "Epoch: 122, train_loss: 0.91692, time: 0.05048\n",
      "Epoch: 123, train_loss: 0.90932, time: 0.05226\n",
      "Epoch: 124, train_loss: 0.91066, time: 0.05016\n",
      "Epoch: 125, train_loss: 0.90711, time: 0.05090\n",
      "Epoch: 126, train_loss: 0.91137, time: 0.05090\n",
      "Epoch: 127, train_loss: 0.91252, time: 0.05052\n",
      "Epoch: 128, train_loss: 0.90987, time: 0.05056\n",
      "Epoch: 129, train_loss: 0.90529, time: 0.05221\n",
      "Epoch: 130, train_loss: 0.90734, time: 0.05288\n",
      "Epoch: 131, train_loss: 0.90940, time: 0.05104\n",
      "Epoch: 132, train_loss: 0.90684, time: 0.05015\n",
      "Epoch: 133, train_loss: 0.90503, time: 0.05080\n",
      "Epoch: 134, train_loss: 0.91174, time: 0.05105\n",
      "Epoch: 135, train_loss: 0.90631, time: 0.05191\n",
      "Epoch: 136, train_loss: 0.91242, time: 0.05126\n",
      "Epoch: 137, train_loss: 0.91150, time: 0.05372\n",
      "Epoch: 138, train_loss: 0.90716, time: 0.05189\n",
      "Epoch: 139, train_loss: 0.90936, time: 0.05141\n",
      "Epoch: 140, train_loss: 0.90624, time: 0.05161\n",
      "Epoch: 141, train_loss: 0.90396, time: 0.05047\n",
      "Epoch: 142, train_loss: 0.90792, time: 0.05249\n",
      "Epoch: 143, train_loss: 0.90965, time: 0.05258\n",
      "Epoch: 144, train_loss: 0.91360, time: 0.05074\n",
      "Epoch: 145, train_loss: 0.91245, time: 0.05133\n",
      "Epoch: 146, train_loss: 0.91187, time: 0.05080\n",
      "Epoch: 147, train_loss: 0.91255, time: 0.05133\n",
      "Epoch: 148, train_loss: 0.91072, time: 0.05183\n",
      "Epoch: 149, train_loss: 0.90587, time: 0.05124\n",
      "Epoch: 150, train_loss: 0.91021, time: 0.05172\n",
      "Epoch: 151, train_loss: 0.90834, time: 0.05123\n",
      "Epoch: 152, train_loss: 0.90663, time: 0.05209\n",
      "Epoch: 153, train_loss: 0.91083, time: 0.05040\n",
      "Epoch: 154, train_loss: 0.91203, time: 0.04969\n",
      "Epoch: 155, train_loss: 0.91608, time: 0.05072\n",
      "Epoch: 156, train_loss: 0.90358, time: 0.05177\n",
      "Epoch: 157, train_loss: 0.90969, time: 0.05161\n",
      "Epoch: 158, train_loss: 0.90724, time: 0.05103\n",
      "Epoch: 159, train_loss: 0.90596, time: 0.05104\n",
      "Epoch: 160, train_loss: 0.90838, time: 0.05141\n",
      "Epoch: 161, train_loss: 0.91391, time: 0.05269\n",
      "Epoch: 162, train_loss: 0.90559, time: 0.05075\n",
      "Epoch: 163, train_loss: 0.91011, time: 0.05037\n",
      "Epoch: 164, train_loss: 0.90599, time: 0.05100\n",
      "Epoch: 165, train_loss: 0.91111, time: 0.05315\n",
      "Epoch: 166, train_loss: 0.90757, time: 0.05328\n",
      "Epoch: 167, train_loss: 0.91083, time: 0.05162\n",
      "Epoch: 168, train_loss: 0.90618, time: 0.05237\n",
      "Epoch: 169, train_loss: 0.91249, time: 0.05108\n",
      "Epoch: 170, train_loss: 0.91502, time: 0.05081\n",
      "Epoch: 171, train_loss: 0.90736, time: 0.05397\n",
      "Epoch: 172, train_loss: 0.90957, time: 0.05007\n",
      "Epoch: 173, train_loss: 0.91109, time: 0.05031\n",
      "Epoch: 174, train_loss: 0.90675, time: 0.05111\n",
      "Epoch: 175, train_loss: 0.90803, time: 0.05105\n",
      "Epoch: 176, train_loss: 0.90602, time: 0.05062\n",
      "Epoch: 177, train_loss: 0.90900, time: 0.05230\n",
      "Epoch: 178, train_loss: 0.90620, time: 0.05059\n",
      "Epoch: 179, train_loss: 0.90489, time: 0.05092\n",
      "Epoch: 180, train_loss: 0.91384, time: 0.05174\n",
      "Epoch: 181, train_loss: 0.91543, time: 0.05126\n",
      "Epoch: 182, train_loss: 0.90302, time: 0.05097\n",
      "Epoch: 183, train_loss: 0.90663, time: 0.05127\n",
      "Epoch: 184, train_loss: 0.90446, time: 0.05130\n",
      "Epoch: 185, train_loss: 0.90884, time: 0.05034\n",
      "Epoch: 186, train_loss: 0.90560, time: 0.05107\n",
      "Epoch: 187, train_loss: 0.90873, time: 0.05115\n",
      "Epoch: 188, train_loss: 0.90867, time: 0.05345\n",
      "Epoch: 189, train_loss: 0.91260, time: 0.05123\n",
      "Epoch: 190, train_loss: 0.90924, time: 0.05357\n",
      "Epoch: 191, train_loss: 0.91255, time: 0.05009\n",
      "Epoch: 192, train_loss: 0.91265, time: 0.04907\n",
      "Epoch: 193, train_loss: 0.90901, time: 0.05123\n",
      "Epoch: 194, train_loss: 0.90992, time: 0.05063\n",
      "Epoch: 195, train_loss: 0.91003, time: 0.05151\n",
      "Epoch: 196, train_loss: 0.91078, time: 0.05105\n",
      "Epoch: 197, train_loss: 0.90838, time: 0.05144\n",
      "Epoch: 198, train_loss: 0.91851, time: 0.05130\n",
      "Epoch: 199, train_loss: 0.90801, time: 0.05160\n",
      "Epoch: 200, train_loss: 0.91325, time: 0.05127\n",
      "pairwise precision 0.06146 recall 0.13182 f1 0.08383\n",
      "average until now [0.430552648832027, 0.8302749365565956, 0.5670514784669799]\n",
      "74 names 316.32542419433594 avg time 4.274667894518053\n",
      "Loading mian_chen dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 185 nodes, 7091 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.79070, time: 0.11338\n",
      "Epoch: 2, train_loss: 0.79460, time: 0.01586\n",
      "Epoch: 3, train_loss: 0.79106, time: 0.01554\n",
      "Epoch: 4, train_loss: 0.79225, time: 0.01470\n",
      "Epoch: 5, train_loss: 0.79488, time: 0.01436\n",
      "Epoch: 6, train_loss: 0.79258, time: 0.01429\n",
      "Epoch: 7, train_loss: 0.78857, time: 0.01410\n",
      "Epoch: 8, train_loss: 0.79145, time: 0.01433\n",
      "Epoch: 9, train_loss: 0.79195, time: 0.01439\n",
      "Epoch: 10, train_loss: 0.78928, time: 0.01373\n",
      "Epoch: 11, train_loss: 0.79183, time: 0.01403\n",
      "Epoch: 12, train_loss: 0.79053, time: 0.01419\n",
      "Epoch: 13, train_loss: 0.79113, time: 0.01425\n",
      "Epoch: 14, train_loss: 0.79600, time: 0.01352\n",
      "Epoch: 15, train_loss: 0.78664, time: 0.01411\n",
      "Epoch: 16, train_loss: 0.79641, time: 0.01377\n",
      "Epoch: 17, train_loss: 0.79127, time: 0.01357\n",
      "Epoch: 18, train_loss: 0.78623, time: 0.01360\n",
      "Epoch: 19, train_loss: 0.79443, time: 0.01436\n",
      "Epoch: 20, train_loss: 0.79093, time: 0.01430\n",
      "Epoch: 21, train_loss: 0.79087, time: 0.01411\n",
      "Epoch: 22, train_loss: 0.78941, time: 0.01405\n",
      "Epoch: 23, train_loss: 0.79204, time: 0.01386\n",
      "Epoch: 24, train_loss: 0.79047, time: 0.01408\n",
      "Epoch: 25, train_loss: 0.79126, time: 0.01381\n",
      "Epoch: 26, train_loss: 0.79157, time: 0.01363\n",
      "Epoch: 27, train_loss: 0.79251, time: 0.01419\n",
      "Epoch: 28, train_loss: 0.79579, time: 0.01380\n",
      "Epoch: 29, train_loss: 0.79310, time: 0.01404\n",
      "Epoch: 30, train_loss: 0.79276, time: 0.01426\n",
      "Epoch: 31, train_loss: 0.79453, time: 0.01449\n",
      "Epoch: 32, train_loss: 0.78993, time: 0.01408\n",
      "Epoch: 33, train_loss: 0.79780, time: 0.01395\n",
      "Epoch: 34, train_loss: 0.79384, time: 0.01417\n",
      "Epoch: 35, train_loss: 0.79152, time: 0.01418\n",
      "Epoch: 36, train_loss: 0.79354, time: 0.01369\n",
      "Epoch: 37, train_loss: 0.79159, time: 0.01451\n",
      "Epoch: 38, train_loss: 0.79446, time: 0.01445\n",
      "Epoch: 39, train_loss: 0.79272, time: 0.01407\n",
      "Epoch: 40, train_loss: 0.79324, time: 0.01413\n",
      "Epoch: 41, train_loss: 0.79131, time: 0.01434\n",
      "Epoch: 42, train_loss: 0.79202, time: 0.01440\n",
      "Epoch: 43, train_loss: 0.79272, time: 0.01397\n",
      "Epoch: 44, train_loss: 0.79403, time: 0.01463\n",
      "Epoch: 45, train_loss: 0.79492, time: 0.01399\n",
      "Epoch: 46, train_loss: 0.79020, time: 0.01393\n",
      "Epoch: 47, train_loss: 0.78875, time: 0.01415\n",
      "Epoch: 48, train_loss: 0.79693, time: 0.01426\n",
      "Epoch: 49, train_loss: 0.79555, time: 0.01402\n",
      "Epoch: 50, train_loss: 0.79108, time: 0.01419\n",
      "Epoch: 51, train_loss: 0.79445, time: 0.01425\n",
      "Epoch: 52, train_loss: 0.79233, time: 0.01422\n",
      "Epoch: 53, train_loss: 0.79275, time: 0.01408\n",
      "Epoch: 54, train_loss: 0.79422, time: 0.01398\n",
      "Epoch: 55, train_loss: 0.79021, time: 0.01410\n",
      "Epoch: 56, train_loss: 0.79251, time: 0.01449\n",
      "Epoch: 57, train_loss: 0.78972, time: 0.01399\n",
      "Epoch: 58, train_loss: 0.78967, time: 0.01416\n",
      "Epoch: 59, train_loss: 0.79477, time: 0.01391\n",
      "Epoch: 60, train_loss: 0.79158, time: 0.01442\n",
      "Epoch: 61, train_loss: 0.79398, time: 0.01444\n",
      "Epoch: 62, train_loss: 0.78911, time: 0.01435\n",
      "Epoch: 63, train_loss: 0.79529, time: 0.01383\n",
      "Epoch: 64, train_loss: 0.79375, time: 0.01416\n",
      "Epoch: 65, train_loss: 0.79329, time: 0.01430\n",
      "Epoch: 66, train_loss: 0.78851, time: 0.01401\n",
      "Epoch: 67, train_loss: 0.79120, time: 0.01420\n",
      "Epoch: 68, train_loss: 0.79041, time: 0.01388\n",
      "Epoch: 69, train_loss: 0.79255, time: 0.01409\n",
      "Epoch: 70, train_loss: 0.79338, time: 0.01425\n",
      "Epoch: 71, train_loss: 0.78475, time: 0.01391\n",
      "Epoch: 72, train_loss: 0.79030, time: 0.01379\n",
      "Epoch: 73, train_loss: 0.79055, time: 0.01463\n",
      "Epoch: 74, train_loss: 0.78983, time: 0.01406\n",
      "Epoch: 75, train_loss: 0.79299, time: 0.01440\n",
      "Epoch: 76, train_loss: 0.78759, time: 0.01413\n",
      "Epoch: 77, train_loss: 0.78687, time: 0.01430\n",
      "Epoch: 78, train_loss: 0.79530, time: 0.01443\n",
      "Epoch: 79, train_loss: 0.78944, time: 0.01422\n",
      "Epoch: 80, train_loss: 0.79537, time: 0.01439\n",
      "Epoch: 81, train_loss: 0.78780, time: 0.01433\n",
      "Epoch: 82, train_loss: 0.79429, time: 0.01415\n",
      "Epoch: 83, train_loss: 0.79264, time: 0.01422\n",
      "Epoch: 84, train_loss: 0.78691, time: 0.01429\n",
      "Epoch: 85, train_loss: 0.79305, time: 0.01445\n",
      "Epoch: 86, train_loss: 0.78659, time: 0.01397\n",
      "Epoch: 87, train_loss: 0.78480, time: 0.01432\n",
      "Epoch: 88, train_loss: 0.79396, time: 0.01407\n",
      "Epoch: 89, train_loss: 0.78631, time: 0.01400\n",
      "Epoch: 90, train_loss: 0.78804, time: 0.01416\n",
      "Epoch: 91, train_loss: 0.79013, time: 0.01376\n",
      "Epoch: 92, train_loss: 0.78716, time: 0.01417\n",
      "Epoch: 93, train_loss: 0.78666, time: 0.01411\n",
      "Epoch: 94, train_loss: 0.78721, time: 0.01386\n",
      "Epoch: 95, train_loss: 0.78005, time: 0.01395\n",
      "Epoch: 96, train_loss: 0.79676, time: 0.01416\n",
      "Epoch: 97, train_loss: 0.78664, time: 0.01408\n",
      "Epoch: 98, train_loss: 0.78451, time: 0.01408\n",
      "Epoch: 99, train_loss: 0.78805, time: 0.01422\n",
      "Epoch: 100, train_loss: 0.78239, time: 0.01412\n",
      "Epoch: 101, train_loss: 0.78502, time: 0.01457\n",
      "Epoch: 102, train_loss: 0.78695, time: 0.01447\n",
      "Epoch: 103, train_loss: 0.78002, time: 0.01445\n",
      "Epoch: 104, train_loss: 0.78607, time: 0.01426\n",
      "Epoch: 105, train_loss: 0.79327, time: 0.01396\n",
      "Epoch: 106, train_loss: 0.78756, time: 0.01440\n",
      "Epoch: 107, train_loss: 0.77562, time: 0.01401\n",
      "Epoch: 108, train_loss: 0.78179, time: 0.01406\n",
      "Epoch: 109, train_loss: 0.78552, time: 0.01409\n",
      "Epoch: 110, train_loss: 0.77761, time: 0.01433\n",
      "Epoch: 111, train_loss: 0.77507, time: 0.01446\n",
      "Epoch: 112, train_loss: 0.79259, time: 0.01428\n",
      "Epoch: 113, train_loss: 0.78288, time: 0.01467\n",
      "Epoch: 114, train_loss: 0.79355, time: 0.01439\n",
      "Epoch: 115, train_loss: 0.78781, time: 0.01407\n",
      "Epoch: 116, train_loss: 0.78131, time: 0.01385\n",
      "Epoch: 117, train_loss: 0.78809, time: 0.01385\n",
      "Epoch: 118, train_loss: 0.78432, time: 0.01429\n",
      "Epoch: 119, train_loss: 0.79326, time: 0.01372\n",
      "Epoch: 120, train_loss: 0.78857, time: 0.01666\n",
      "Epoch: 121, train_loss: 0.78988, time: 0.01434\n",
      "Epoch: 122, train_loss: 0.78677, time: 0.01450\n",
      "Epoch: 123, train_loss: 0.78660, time: 0.01415\n",
      "Epoch: 124, train_loss: 0.77654, time: 0.01420\n",
      "Epoch: 125, train_loss: 0.78264, time: 0.01448\n",
      "Epoch: 126, train_loss: 0.78379, time: 0.01407\n",
      "Epoch: 127, train_loss: 0.77555, time: 0.01386\n",
      "Epoch: 128, train_loss: 0.78260, time: 0.01434\n",
      "Epoch: 129, train_loss: 0.78272, time: 0.01432\n",
      "Epoch: 130, train_loss: 0.78242, time: 0.01426\n",
      "Epoch: 131, train_loss: 0.77586, time: 0.01434\n",
      "Epoch: 132, train_loss: 0.78469, time: 0.01433\n",
      "Epoch: 133, train_loss: 0.78878, time: 0.01422\n",
      "Epoch: 134, train_loss: 0.77516, time: 0.01442\n",
      "Epoch: 135, train_loss: 0.78012, time: 0.01518\n",
      "Epoch: 136, train_loss: 0.77529, time: 0.01447\n",
      "Epoch: 137, train_loss: 0.77801, time: 0.01414\n",
      "Epoch: 138, train_loss: 0.77658, time: 0.01441\n",
      "Epoch: 139, train_loss: 0.77749, time: 0.01406\n",
      "Epoch: 140, train_loss: 0.77054, time: 0.01423\n",
      "Epoch: 141, train_loss: 0.77121, time: 0.01398\n",
      "Epoch: 142, train_loss: 0.77149, time: 0.01408\n",
      "Epoch: 143, train_loss: 0.77235, time: 0.01423\n",
      "Epoch: 144, train_loss: 0.76924, time: 0.01460\n",
      "Epoch: 145, train_loss: 0.77418, time: 0.01375\n",
      "Epoch: 146, train_loss: 0.76387, time: 0.01403\n",
      "Epoch: 147, train_loss: 0.78557, time: 0.01430\n",
      "Epoch: 148, train_loss: 0.77990, time: 0.01401\n",
      "Epoch: 149, train_loss: 0.79482, time: 0.01388\n",
      "Epoch: 150, train_loss: 0.77271, time: 0.01399\n",
      "Epoch: 151, train_loss: 0.77380, time: 0.01446\n",
      "Epoch: 152, train_loss: 0.78571, time: 0.01408\n",
      "Epoch: 153, train_loss: 0.77105, time: 0.01461\n",
      "Epoch: 154, train_loss: 0.78789, time: 0.01373\n",
      "Epoch: 155, train_loss: 0.78504, time: 0.01403\n",
      "Epoch: 156, train_loss: 0.76975, time: 0.01377\n",
      "Epoch: 157, train_loss: 0.78982, time: 0.01413\n",
      "Epoch: 158, train_loss: 0.76923, time: 0.01385\n",
      "Epoch: 159, train_loss: 0.77959, time: 0.01383\n",
      "Epoch: 160, train_loss: 0.76884, time: 0.01443\n",
      "Epoch: 161, train_loss: 0.78244, time: 0.01430\n",
      "Epoch: 162, train_loss: 0.77304, time: 0.01400\n",
      "Epoch: 163, train_loss: 0.79176, time: 0.01411\n",
      "Epoch: 164, train_loss: 0.77680, time: 0.01397\n",
      "Epoch: 165, train_loss: 0.77053, time: 0.01455\n",
      "Epoch: 166, train_loss: 0.77774, time: 0.01429\n",
      "Epoch: 167, train_loss: 0.77266, time: 0.01422\n",
      "Epoch: 168, train_loss: 0.76936, time: 0.01450\n",
      "Epoch: 169, train_loss: 0.76792, time: 0.01405\n",
      "Epoch: 170, train_loss: 0.77238, time: 0.01422\n",
      "Epoch: 171, train_loss: 0.77565, time: 0.01402\n",
      "Epoch: 172, train_loss: 0.78483, time: 0.01377\n",
      "Epoch: 173, train_loss: 0.77038, time: 0.01376\n",
      "Epoch: 174, train_loss: 0.77485, time: 0.01414\n",
      "Epoch: 175, train_loss: 0.78302, time: 0.01432\n",
      "Epoch: 176, train_loss: 0.77114, time: 0.01427\n",
      "Epoch: 177, train_loss: 0.77908, time: 0.01437\n",
      "Epoch: 178, train_loss: 0.76321, time: 0.01434\n",
      "Epoch: 179, train_loss: 0.78662, time: 0.01430\n",
      "Epoch: 180, train_loss: 0.77939, time: 0.01408\n",
      "Epoch: 181, train_loss: 0.77812, time: 0.01397\n",
      "Epoch: 182, train_loss: 0.77080, time: 0.01455\n",
      "Epoch: 183, train_loss: 0.78184, time: 0.01470\n",
      "Epoch: 184, train_loss: 0.79809, time: 0.01443\n",
      "Epoch: 185, train_loss: 0.77875, time: 0.01375\n",
      "Epoch: 186, train_loss: 0.76525, time: 0.01409\n",
      "Epoch: 187, train_loss: 0.76357, time: 0.01434\n",
      "Epoch: 188, train_loss: 0.78203, time: 0.01413\n",
      "Epoch: 189, train_loss: 0.78515, time: 0.01379\n",
      "Epoch: 190, train_loss: 0.76694, time: 0.01370\n",
      "Epoch: 191, train_loss: 0.76769, time: 0.01410\n",
      "Epoch: 192, train_loss: 0.76221, time: 0.01425\n",
      "Epoch: 193, train_loss: 0.76273, time: 0.01411\n",
      "Epoch: 194, train_loss: 0.77044, time: 0.01408\n",
      "Epoch: 195, train_loss: 0.77924, time: 0.01446\n",
      "Epoch: 196, train_loss: 0.76405, time: 0.01428\n",
      "Epoch: 197, train_loss: 0.76338, time: 0.01390\n",
      "Epoch: 198, train_loss: 0.77832, time: 0.01427\n",
      "Epoch: 199, train_loss: 0.75650, time: 0.01470\n",
      "Epoch: 200, train_loss: 0.76289, time: 0.01398\n",
      "pairwise precision 1.00000 recall 1.00000 f1 1.00000\n",
      "average until now [0.4381452801809333, 0.8325379374025077, 0.5741361226729584]\n",
      "75 names 319.3184504508972 avg time 4.2575793393452965\n",
      "Loading fuchu_he dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 274 nodes, 11073 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.84934, time: 0.11740\n",
      "Epoch: 2, train_loss: 0.85000, time: 0.01924\n",
      "Epoch: 3, train_loss: 0.85325, time: 0.01821\n",
      "Epoch: 4, train_loss: 0.85217, time: 0.01827\n",
      "Epoch: 5, train_loss: 0.85040, time: 0.01789\n",
      "Epoch: 6, train_loss: 0.84983, time: 0.01856\n",
      "Epoch: 7, train_loss: 0.85052, time: 0.01823\n",
      "Epoch: 8, train_loss: 0.85107, time: 0.01782\n",
      "Epoch: 9, train_loss: 0.85522, time: 0.01778\n",
      "Epoch: 10, train_loss: 0.85145, time: 0.01806\n",
      "Epoch: 11, train_loss: 0.84873, time: 0.01795\n",
      "Epoch: 12, train_loss: 0.85083, time: 0.01777\n",
      "Epoch: 13, train_loss: 0.85045, time: 0.01799\n",
      "Epoch: 14, train_loss: 0.85116, time: 0.01817\n",
      "Epoch: 15, train_loss: 0.85304, time: 0.01775\n",
      "Epoch: 16, train_loss: 0.85147, time: 0.01841\n",
      "Epoch: 17, train_loss: 0.85324, time: 0.01774\n",
      "Epoch: 18, train_loss: 0.85153, time: 0.01770\n",
      "Epoch: 19, train_loss: 0.85085, time: 0.01813\n",
      "Epoch: 20, train_loss: 0.85057, time: 0.01786\n",
      "Epoch: 21, train_loss: 0.84981, time: 0.01814\n",
      "Epoch: 22, train_loss: 0.85036, time: 0.01820\n",
      "Epoch: 23, train_loss: 0.85083, time: 0.01766\n",
      "Epoch: 24, train_loss: 0.85199, time: 0.01845\n",
      "Epoch: 25, train_loss: 0.85178, time: 0.01768\n",
      "Epoch: 26, train_loss: 0.85266, time: 0.01800\n",
      "Epoch: 27, train_loss: 0.85066, time: 0.01768\n",
      "Epoch: 28, train_loss: 0.84789, time: 0.01778\n",
      "Epoch: 29, train_loss: 0.85000, time: 0.01805\n",
      "Epoch: 30, train_loss: 0.85143, time: 0.01807\n",
      "Epoch: 31, train_loss: 0.85260, time: 0.01735\n",
      "Epoch: 32, train_loss: 0.84799, time: 0.01783\n",
      "Epoch: 33, train_loss: 0.85077, time: 0.01725\n",
      "Epoch: 34, train_loss: 0.84900, time: 0.01726\n",
      "Epoch: 35, train_loss: 0.85088, time: 0.01713\n",
      "Epoch: 36, train_loss: 0.85351, time: 0.01710\n",
      "Epoch: 37, train_loss: 0.85035, time: 0.01758\n",
      "Epoch: 38, train_loss: 0.85069, time: 0.01813\n",
      "Epoch: 39, train_loss: 0.85125, time: 0.01720\n",
      "Epoch: 40, train_loss: 0.85174, time: 0.01782\n",
      "Epoch: 41, train_loss: 0.85082, time: 0.01855\n",
      "Epoch: 42, train_loss: 0.85232, time: 0.01768\n",
      "Epoch: 43, train_loss: 0.84964, time: 0.01823\n",
      "Epoch: 44, train_loss: 0.84980, time: 0.01701\n",
      "Epoch: 45, train_loss: 0.84920, time: 0.01729\n",
      "Epoch: 46, train_loss: 0.85012, time: 0.01761\n",
      "Epoch: 47, train_loss: 0.84977, time: 0.01743\n",
      "Epoch: 48, train_loss: 0.85272, time: 0.01734\n",
      "Epoch: 49, train_loss: 0.85050, time: 0.01802\n",
      "Epoch: 50, train_loss: 0.85011, time: 0.01848\n",
      "Epoch: 51, train_loss: 0.84978, time: 0.01822\n",
      "Epoch: 52, train_loss: 0.85016, time: 0.01835\n",
      "Epoch: 53, train_loss: 0.85080, time: 0.01830\n",
      "Epoch: 54, train_loss: 0.85195, time: 0.01866\n",
      "Epoch: 55, train_loss: 0.85171, time: 0.01820\n",
      "Epoch: 56, train_loss: 0.85102, time: 0.01815\n",
      "Epoch: 57, train_loss: 0.85107, time: 0.01794\n",
      "Epoch: 58, train_loss: 0.84998, time: 0.01787\n",
      "Epoch: 59, train_loss: 0.85274, time: 0.01820\n",
      "Epoch: 60, train_loss: 0.85405, time: 0.01817\n",
      "Epoch: 61, train_loss: 0.85140, time: 0.01802\n",
      "Epoch: 62, train_loss: 0.84898, time: 0.01798\n",
      "Epoch: 63, train_loss: 0.85203, time: 0.01793\n",
      "Epoch: 64, train_loss: 0.85041, time: 0.01802\n",
      "Epoch: 65, train_loss: 0.85039, time: 0.01840\n",
      "Epoch: 66, train_loss: 0.85132, time: 0.01798\n",
      "Epoch: 67, train_loss: 0.85330, time: 0.01754\n",
      "Epoch: 68, train_loss: 0.85136, time: 0.02041\n",
      "Epoch: 69, train_loss: 0.85026, time: 0.01870\n",
      "Epoch: 70, train_loss: 0.85279, time: 0.01888\n",
      "Epoch: 71, train_loss: 0.85458, time: 0.01841\n",
      "Epoch: 72, train_loss: 0.85075, time: 0.01805\n",
      "Epoch: 73, train_loss: 0.85044, time: 0.01838\n",
      "Epoch: 74, train_loss: 0.85179, time: 0.01807\n",
      "Epoch: 75, train_loss: 0.84813, time: 0.01825\n",
      "Epoch: 76, train_loss: 0.85045, time: 0.01842\n",
      "Epoch: 77, train_loss: 0.85334, time: 0.01833\n",
      "Epoch: 78, train_loss: 0.85261, time: 0.01726\n",
      "Epoch: 79, train_loss: 0.85145, time: 0.01758\n",
      "Epoch: 80, train_loss: 0.85167, time: 0.01751\n",
      "Epoch: 81, train_loss: 0.85209, time: 0.01775\n",
      "Epoch: 82, train_loss: 0.84975, time: 0.01759\n",
      "Epoch: 83, train_loss: 0.85209, time: 0.01839\n",
      "Epoch: 84, train_loss: 0.84855, time: 0.01807\n",
      "Epoch: 85, train_loss: 0.85083, time: 0.01789\n",
      "Epoch: 86, train_loss: 0.84923, time: 0.01882\n",
      "Epoch: 87, train_loss: 0.85124, time: 0.01781\n",
      "Epoch: 88, train_loss: 0.85181, time: 0.01840\n",
      "Epoch: 89, train_loss: 0.85497, time: 0.01780\n",
      "Epoch: 90, train_loss: 0.85144, time: 0.01730\n",
      "Epoch: 91, train_loss: 0.85203, time: 0.01810\n",
      "Epoch: 92, train_loss: 0.85174, time: 0.01800\n",
      "Epoch: 93, train_loss: 0.84970, time: 0.01794\n",
      "Epoch: 94, train_loss: 0.85145, time: 0.01811\n",
      "Epoch: 95, train_loss: 0.85004, time: 0.01791\n",
      "Epoch: 96, train_loss: 0.85140, time: 0.01822\n",
      "Epoch: 97, train_loss: 0.85163, time: 0.01816\n",
      "Epoch: 98, train_loss: 0.85175, time: 0.01877\n",
      "Epoch: 99, train_loss: 0.85251, time: 0.01889\n",
      "Epoch: 100, train_loss: 0.84902, time: 0.01799\n",
      "Epoch: 101, train_loss: 0.84992, time: 0.01785\n",
      "Epoch: 102, train_loss: 0.85180, time: 0.01868\n",
      "Epoch: 103, train_loss: 0.85016, time: 0.01828\n",
      "Epoch: 104, train_loss: 0.85180, time: 0.01727\n",
      "Epoch: 105, train_loss: 0.85017, time: 0.01796\n",
      "Epoch: 106, train_loss: 0.84993, time: 0.01822\n",
      "Epoch: 107, train_loss: 0.84978, time: 0.01807\n",
      "Epoch: 108, train_loss: 0.84971, time: 0.01837\n",
      "Epoch: 109, train_loss: 0.84960, time: 0.01837\n",
      "Epoch: 110, train_loss: 0.84825, time: 0.01854\n",
      "Epoch: 111, train_loss: 0.85125, time: 0.01773\n",
      "Epoch: 112, train_loss: 0.85044, time: 0.01792\n",
      "Epoch: 113, train_loss: 0.84916, time: 0.01786\n",
      "Epoch: 114, train_loss: 0.85104, time: 0.01748\n",
      "Epoch: 115, train_loss: 0.85281, time: 0.01842\n",
      "Epoch: 116, train_loss: 0.85171, time: 0.01873\n",
      "Epoch: 117, train_loss: 0.85115, time: 0.01809\n",
      "Epoch: 118, train_loss: 0.84982, time: 0.01775\n",
      "Epoch: 119, train_loss: 0.85235, time: 0.01745\n",
      "Epoch: 120, train_loss: 0.85277, time: 0.01792\n",
      "Epoch: 121, train_loss: 0.85108, time: 0.01780\n",
      "Epoch: 122, train_loss: 0.84962, time: 0.01762\n",
      "Epoch: 123, train_loss: 0.85222, time: 0.01816\n",
      "Epoch: 124, train_loss: 0.85337, time: 0.01786\n",
      "Epoch: 125, train_loss: 0.85159, time: 0.01736\n",
      "Epoch: 126, train_loss: 0.84961, time: 0.01773\n",
      "Epoch: 127, train_loss: 0.85194, time: 0.01876\n",
      "Epoch: 128, train_loss: 0.85055, time: 0.01846\n",
      "Epoch: 129, train_loss: 0.85084, time: 0.01806\n",
      "Epoch: 130, train_loss: 0.85302, time: 0.01777\n",
      "Epoch: 131, train_loss: 0.85145, time: 0.01779\n",
      "Epoch: 132, train_loss: 0.84906, time: 0.01805\n",
      "Epoch: 133, train_loss: 0.85125, time: 0.01811\n",
      "Epoch: 134, train_loss: 0.85114, time: 0.01818\n",
      "Epoch: 135, train_loss: 0.84843, time: 0.01754\n",
      "Epoch: 136, train_loss: 0.85272, time: 0.01789\n",
      "Epoch: 137, train_loss: 0.85063, time: 0.01769\n",
      "Epoch: 138, train_loss: 0.85060, time: 0.01741\n",
      "Epoch: 139, train_loss: 0.85288, time: 0.01846\n",
      "Epoch: 140, train_loss: 0.85043, time: 0.01792\n",
      "Epoch: 141, train_loss: 0.84799, time: 0.01849\n",
      "Epoch: 142, train_loss: 0.85081, time: 0.01888\n",
      "Epoch: 143, train_loss: 0.85250, time: 0.01877\n",
      "Epoch: 144, train_loss: 0.84948, time: 0.01901\n",
      "Epoch: 145, train_loss: 0.85188, time: 0.01919\n",
      "Epoch: 146, train_loss: 0.85212, time: 0.01845\n",
      "Epoch: 147, train_loss: 0.85342, time: 0.01851\n",
      "Epoch: 148, train_loss: 0.85039, time: 0.01781\n",
      "Epoch: 149, train_loss: 0.84856, time: 0.01742\n",
      "Epoch: 150, train_loss: 0.85218, time: 0.01770\n",
      "Epoch: 151, train_loss: 0.84908, time: 0.01825\n",
      "Epoch: 152, train_loss: 0.85069, time: 0.01748\n",
      "Epoch: 153, train_loss: 0.85222, time: 0.01736\n",
      "Epoch: 154, train_loss: 0.85116, time: 0.01769\n",
      "Epoch: 155, train_loss: 0.85194, time: 0.01917\n",
      "Epoch: 156, train_loss: 0.85246, time: 0.01861\n",
      "Epoch: 157, train_loss: 0.84879, time: 0.01814\n",
      "Epoch: 158, train_loss: 0.85160, time: 0.01904\n",
      "Epoch: 159, train_loss: 0.84992, time: 0.01753\n",
      "Epoch: 160, train_loss: 0.84969, time: 0.01861\n",
      "Epoch: 161, train_loss: 0.85013, time: 0.01894\n",
      "Epoch: 162, train_loss: 0.85335, time: 0.01814\n",
      "Epoch: 163, train_loss: 0.85201, time: 0.01822\n",
      "Epoch: 164, train_loss: 0.85281, time: 0.01853\n",
      "Epoch: 165, train_loss: 0.85149, time: 0.01772\n",
      "Epoch: 166, train_loss: 0.85001, time: 0.01897\n",
      "Epoch: 167, train_loss: 0.85221, time: 0.01850\n",
      "Epoch: 168, train_loss: 0.85134, time: 0.01828\n",
      "Epoch: 169, train_loss: 0.84945, time: 0.01807\n",
      "Epoch: 170, train_loss: 0.85283, time: 0.01827\n",
      "Epoch: 171, train_loss: 0.85065, time: 0.01757\n",
      "Epoch: 172, train_loss: 0.85278, time: 0.01769\n",
      "Epoch: 173, train_loss: 0.85337, time: 0.01874\n",
      "Epoch: 174, train_loss: 0.85100, time: 0.01900\n",
      "Epoch: 175, train_loss: 0.85170, time: 0.01878\n",
      "Epoch: 176, train_loss: 0.85263, time: 0.01814\n",
      "Epoch: 177, train_loss: 0.85123, time: 0.01779\n",
      "Epoch: 178, train_loss: 0.84995, time: 0.01808\n",
      "Epoch: 179, train_loss: 0.84972, time: 0.01906\n",
      "Epoch: 180, train_loss: 0.85380, time: 0.01927\n",
      "Epoch: 181, train_loss: 0.84990, time: 0.01823\n",
      "Epoch: 182, train_loss: 0.85177, time: 0.01844\n",
      "Epoch: 183, train_loss: 0.84840, time: 0.01865\n",
      "Epoch: 184, train_loss: 0.85157, time: 0.01886\n",
      "Epoch: 185, train_loss: 0.84999, time: 0.01789\n",
      "Epoch: 186, train_loss: 0.85059, time: 0.01915\n",
      "Epoch: 187, train_loss: 0.85275, time: 0.01853\n",
      "Epoch: 188, train_loss: 0.84941, time: 0.01815\n",
      "Epoch: 189, train_loss: 0.85099, time: 0.01780\n",
      "Epoch: 190, train_loss: 0.85033, time: 0.01876\n",
      "Epoch: 191, train_loss: 0.85044, time: 0.01828\n",
      "Epoch: 192, train_loss: 0.85291, time: 0.01794\n",
      "Epoch: 193, train_loss: 0.84893, time: 0.01820\n",
      "Epoch: 194, train_loss: 0.85044, time: 0.01824\n",
      "Epoch: 195, train_loss: 0.85195, time: 0.01820\n",
      "Epoch: 196, train_loss: 0.85134, time: 0.01889\n",
      "Epoch: 197, train_loss: 0.85281, time: 0.01785\n",
      "Epoch: 198, train_loss: 0.85189, time: 0.01791\n",
      "Epoch: 199, train_loss: 0.85092, time: 0.01802\n",
      "Epoch: 200, train_loss: 0.84970, time: 0.01850\n",
      "pairwise precision 1.00000 recall 1.00000 f1 1.00000\n",
      "average until now [0.4455381054417105, 0.8347413855945799, 0.5809811030723687]\n",
      "76 names 323.1196789741516 avg time 4.2515747233441\n",
      "Loading shuang_song dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 154 nodes, 2125 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.91172, time: 0.10901\n",
      "Epoch: 2, train_loss: 0.91010, time: 0.01195\n",
      "Epoch: 3, train_loss: 0.91251, time: 0.01067\n",
      "Epoch: 4, train_loss: 0.91269, time: 0.01046\n",
      "Epoch: 5, train_loss: 0.91367, time: 0.01041\n",
      "Epoch: 6, train_loss: 0.91128, time: 0.01078\n",
      "Epoch: 7, train_loss: 0.90753, time: 0.01038\n",
      "Epoch: 8, train_loss: 0.91346, time: 0.01017\n",
      "Epoch: 9, train_loss: 0.90890, time: 0.01047\n",
      "Epoch: 10, train_loss: 0.91418, time: 0.01019\n",
      "Epoch: 11, train_loss: 0.91245, time: 0.01048\n",
      "Epoch: 12, train_loss: 0.90975, time: 0.01015\n",
      "Epoch: 13, train_loss: 0.91196, time: 0.01050\n",
      "Epoch: 14, train_loss: 0.91106, time: 0.01035\n",
      "Epoch: 15, train_loss: 0.91414, time: 0.01030\n",
      "Epoch: 16, train_loss: 0.91139, time: 0.01038\n",
      "Epoch: 17, train_loss: 0.91496, time: 0.01017\n",
      "Epoch: 18, train_loss: 0.91479, time: 0.01042\n",
      "Epoch: 19, train_loss: 0.91255, time: 0.01031\n",
      "Epoch: 20, train_loss: 0.91268, time: 0.01055\n",
      "Epoch: 21, train_loss: 0.90636, time: 0.01025\n",
      "Epoch: 22, train_loss: 0.91376, time: 0.01049\n",
      "Epoch: 23, train_loss: 0.91565, time: 0.01022\n",
      "Epoch: 24, train_loss: 0.90996, time: 0.01046\n",
      "Epoch: 25, train_loss: 0.91704, time: 0.01002\n",
      "Epoch: 26, train_loss: 0.91254, time: 0.01068\n",
      "Epoch: 27, train_loss: 0.90967, time: 0.01009\n",
      "Epoch: 28, train_loss: 0.91670, time: 0.01056\n",
      "Epoch: 29, train_loss: 0.90907, time: 0.01024\n",
      "Epoch: 30, train_loss: 0.91092, time: 0.01063\n",
      "Epoch: 31, train_loss: 0.91182, time: 0.01035\n",
      "Epoch: 32, train_loss: 0.90701, time: 0.01052\n",
      "Epoch: 33, train_loss: 0.90931, time: 0.01009\n",
      "Epoch: 34, train_loss: 0.90654, time: 0.01048\n",
      "Epoch: 35, train_loss: 0.90899, time: 0.01009\n",
      "Epoch: 36, train_loss: 0.90870, time: 0.01050\n",
      "Epoch: 37, train_loss: 0.91312, time: 0.01010\n",
      "Epoch: 38, train_loss: 0.90752, time: 0.01050\n",
      "Epoch: 39, train_loss: 0.91110, time: 0.01020\n",
      "Epoch: 40, train_loss: 0.91184, time: 0.01059\n",
      "Epoch: 41, train_loss: 0.91060, time: 0.01018\n",
      "Epoch: 42, train_loss: 0.91094, time: 0.01050\n",
      "Epoch: 43, train_loss: 0.91075, time: 0.01015\n",
      "Epoch: 44, train_loss: 0.90833, time: 0.01051\n",
      "Epoch: 45, train_loss: 0.90784, time: 0.01021\n",
      "Epoch: 46, train_loss: 0.90977, time: 0.01066\n",
      "Epoch: 47, train_loss: 0.91188, time: 0.01015\n",
      "Epoch: 48, train_loss: 0.91252, time: 0.01045\n",
      "Epoch: 49, train_loss: 0.91260, time: 0.01010\n",
      "Epoch: 50, train_loss: 0.91088, time: 0.01049\n",
      "Epoch: 51, train_loss: 0.91040, time: 0.01060\n",
      "Epoch: 52, train_loss: 0.91423, time: 0.01057\n",
      "Epoch: 53, train_loss: 0.91148, time: 0.01054\n",
      "Epoch: 54, train_loss: 0.91075, time: 0.01056\n",
      "Epoch: 55, train_loss: 0.91154, time: 0.01038\n",
      "Epoch: 56, train_loss: 0.90807, time: 0.01050\n",
      "Epoch: 57, train_loss: 0.91164, time: 0.01028\n",
      "Epoch: 58, train_loss: 0.90893, time: 0.01037\n",
      "Epoch: 59, train_loss: 0.91388, time: 0.01033\n",
      "Epoch: 60, train_loss: 0.90760, time: 0.01037\n",
      "Epoch: 61, train_loss: 0.91172, time: 0.01001\n",
      "Epoch: 62, train_loss: 0.90838, time: 0.01058\n",
      "Epoch: 63, train_loss: 0.91430, time: 0.01021\n",
      "Epoch: 64, train_loss: 0.91137, time: 0.01050\n",
      "Epoch: 65, train_loss: 0.90826, time: 0.01014\n",
      "Epoch: 66, train_loss: 0.91213, time: 0.01072\n",
      "Epoch: 67, train_loss: 0.90998, time: 0.01043\n",
      "Epoch: 68, train_loss: 0.91150, time: 0.01030\n",
      "Epoch: 69, train_loss: 0.91325, time: 0.01059\n",
      "Epoch: 70, train_loss: 0.91106, time: 0.01027\n",
      "Epoch: 71, train_loss: 0.90961, time: 0.01078\n",
      "Epoch: 72, train_loss: 0.90745, time: 0.01039\n",
      "Epoch: 73, train_loss: 0.90975, time: 0.01049\n",
      "Epoch: 74, train_loss: 0.90496, time: 0.01056\n",
      "Epoch: 75, train_loss: 0.90953, time: 0.01028\n",
      "Epoch: 76, train_loss: 0.91586, time: 0.01031\n",
      "Epoch: 77, train_loss: 0.90428, time: 0.01058\n",
      "Epoch: 78, train_loss: 0.91167, time: 0.01008\n",
      "Epoch: 79, train_loss: 0.91722, time: 0.01057\n",
      "Epoch: 80, train_loss: 0.90864, time: 0.01024\n",
      "Epoch: 81, train_loss: 0.90909, time: 0.01056\n",
      "Epoch: 82, train_loss: 0.91380, time: 0.01008\n",
      "Epoch: 83, train_loss: 0.90618, time: 0.01047\n",
      "Epoch: 84, train_loss: 0.90746, time: 0.01019\n",
      "Epoch: 85, train_loss: 0.91144, time: 0.01067\n",
      "Epoch: 86, train_loss: 0.91900, time: 0.01039\n",
      "Epoch: 87, train_loss: 0.90495, time: 0.01048\n",
      "Epoch: 88, train_loss: 0.89876, time: 0.01024\n",
      "Epoch: 89, train_loss: 0.90550, time: 0.01029\n",
      "Epoch: 90, train_loss: 0.91790, time: 0.01030\n",
      "Epoch: 91, train_loss: 0.90871, time: 0.01062\n",
      "Epoch: 92, train_loss: 0.91846, time: 0.01004\n",
      "Epoch: 93, train_loss: 0.90925, time: 0.01047\n",
      "Epoch: 94, train_loss: 0.91385, time: 0.00979\n",
      "Epoch: 95, train_loss: 0.90934, time: 0.01066\n",
      "Epoch: 96, train_loss: 0.90774, time: 0.01043\n",
      "Epoch: 97, train_loss: 0.90467, time: 0.01044\n",
      "Epoch: 98, train_loss: 0.91304, time: 0.01015\n",
      "Epoch: 99, train_loss: 0.90795, time: 0.01036\n",
      "Epoch: 100, train_loss: 0.91222, time: 0.01058\n",
      "Epoch: 101, train_loss: 0.91102, time: 0.01043\n",
      "Epoch: 102, train_loss: 0.90520, time: 0.01037\n",
      "Epoch: 103, train_loss: 0.90286, time: 0.01023\n",
      "Epoch: 104, train_loss: 0.90031, time: 0.01023\n",
      "Epoch: 105, train_loss: 0.90400, time: 0.01019\n",
      "Epoch: 106, train_loss: 0.90086, time: 0.01081\n",
      "Epoch: 107, train_loss: 0.89668, time: 0.01048\n",
      "Epoch: 108, train_loss: 0.91525, time: 0.01044\n",
      "Epoch: 109, train_loss: 0.91714, time: 0.01065\n",
      "Epoch: 110, train_loss: 0.90601, time: 0.01024\n",
      "Epoch: 111, train_loss: 0.89628, time: 0.01071\n",
      "Epoch: 112, train_loss: 0.90901, time: 0.01047\n",
      "Epoch: 113, train_loss: 0.90544, time: 0.01029\n",
      "Epoch: 114, train_loss: 0.90243, time: 0.01068\n",
      "Epoch: 115, train_loss: 0.90900, time: 0.01011\n",
      "Epoch: 116, train_loss: 0.90987, time: 0.01066\n",
      "Epoch: 117, train_loss: 0.89732, time: 0.01031\n",
      "Epoch: 118, train_loss: 0.88426, time: 0.01051\n",
      "Epoch: 119, train_loss: 0.90613, time: 0.01008\n",
      "Epoch: 120, train_loss: 0.89323, time: 0.01060\n",
      "Epoch: 121, train_loss: 0.90385, time: 0.01014\n",
      "Epoch: 122, train_loss: 0.90539, time: 0.01048\n",
      "Epoch: 123, train_loss: 0.89929, time: 0.01023\n",
      "Epoch: 124, train_loss: 0.90414, time: 0.01056\n",
      "Epoch: 125, train_loss: 0.90684, time: 0.01012\n",
      "Epoch: 126, train_loss: 0.89700, time: 0.01083\n",
      "Epoch: 127, train_loss: 0.89472, time: 0.01021\n",
      "Epoch: 128, train_loss: 0.89039, time: 0.01046\n",
      "Epoch: 129, train_loss: 0.89088, time: 0.01017\n",
      "Epoch: 130, train_loss: 0.90129, time: 0.01051\n",
      "Epoch: 131, train_loss: 0.90203, time: 0.01006\n",
      "Epoch: 132, train_loss: 0.89474, time: 0.01027\n",
      "Epoch: 133, train_loss: 0.90235, time: 0.01051\n",
      "Epoch: 134, train_loss: 0.89155, time: 0.01050\n",
      "Epoch: 135, train_loss: 0.90218, time: 0.01037\n",
      "Epoch: 136, train_loss: 0.90164, time: 0.01041\n",
      "Epoch: 137, train_loss: 0.90115, time: 0.00991\n",
      "Epoch: 138, train_loss: 0.88410, time: 0.01041\n",
      "Epoch: 139, train_loss: 0.89676, time: 0.01062\n",
      "Epoch: 140, train_loss: 0.88969, time: 0.01025\n",
      "Epoch: 141, train_loss: 0.88442, time: 0.01080\n",
      "Epoch: 142, train_loss: 0.89232, time: 0.01012\n",
      "Epoch: 143, train_loss: 0.90388, time: 0.01056\n",
      "Epoch: 144, train_loss: 0.89474, time: 0.01030\n",
      "Epoch: 145, train_loss: 0.87799, time: 0.01064\n",
      "Epoch: 146, train_loss: 0.88001, time: 0.01035\n",
      "Epoch: 147, train_loss: 0.89201, time: 0.01083\n",
      "Epoch: 148, train_loss: 0.88550, time: 0.01034\n",
      "Epoch: 149, train_loss: 0.88652, time: 0.01053\n",
      "Epoch: 150, train_loss: 0.88493, time: 0.01011\n",
      "Epoch: 151, train_loss: 0.88394, time: 0.01065\n",
      "Epoch: 152, train_loss: 0.89694, time: 0.01010\n",
      "Epoch: 153, train_loss: 0.88889, time: 0.01069\n",
      "Epoch: 154, train_loss: 0.88033, time: 0.01045\n",
      "Epoch: 155, train_loss: 0.89247, time: 0.01058\n",
      "Epoch: 156, train_loss: 0.89823, time: 0.01023\n",
      "Epoch: 157, train_loss: 0.88140, time: 0.01052\n",
      "Epoch: 158, train_loss: 0.87879, time: 0.01050\n",
      "Epoch: 159, train_loss: 0.88716, time: 0.01020\n",
      "Epoch: 160, train_loss: 0.87525, time: 0.01037\n",
      "Epoch: 161, train_loss: 0.88350, time: 0.01041\n",
      "Epoch: 162, train_loss: 0.91068, time: 0.01043\n",
      "Epoch: 163, train_loss: 0.89058, time: 0.01032\n",
      "Epoch: 164, train_loss: 0.88714, time: 0.01030\n",
      "Epoch: 165, train_loss: 0.88948, time: 0.01025\n",
      "Epoch: 166, train_loss: 0.89256, time: 0.01069\n",
      "Epoch: 167, train_loss: 0.87699, time: 0.01028\n",
      "Epoch: 168, train_loss: 0.89121, time: 0.01041\n",
      "Epoch: 169, train_loss: 0.89139, time: 0.01041\n",
      "Epoch: 170, train_loss: 0.88815, time: 0.01034\n",
      "Epoch: 171, train_loss: 0.89280, time: 0.01063\n",
      "Epoch: 172, train_loss: 0.90671, time: 0.01002\n",
      "Epoch: 173, train_loss: 0.88282, time: 0.01049\n",
      "Epoch: 174, train_loss: 0.90439, time: 0.01017\n",
      "Epoch: 175, train_loss: 0.89501, time: 0.01050\n",
      "Epoch: 176, train_loss: 0.89095, time: 0.01002\n",
      "Epoch: 177, train_loss: 0.88718, time: 0.01075\n",
      "Epoch: 178, train_loss: 0.90153, time: 0.01030\n",
      "Epoch: 179, train_loss: 0.90063, time: 0.01053\n",
      "Epoch: 180, train_loss: 0.90602, time: 0.01011\n",
      "Epoch: 181, train_loss: 0.87406, time: 0.01042\n",
      "Epoch: 182, train_loss: 0.89076, time: 0.01029\n",
      "Epoch: 183, train_loss: 0.88361, time: 0.01039\n",
      "Epoch: 184, train_loss: 0.87536, time: 0.01018\n",
      "Epoch: 185, train_loss: 0.89445, time: 0.01058\n",
      "Epoch: 186, train_loss: 0.90266, time: 0.01051\n",
      "Epoch: 187, train_loss: 0.89470, time: 0.01058\n",
      "Epoch: 188, train_loss: 0.87723, time: 0.00997\n",
      "Epoch: 189, train_loss: 0.91329, time: 0.01070\n",
      "Epoch: 190, train_loss: 0.90007, time: 0.01046\n",
      "Epoch: 191, train_loss: 0.87990, time: 0.01030\n",
      "Epoch: 192, train_loss: 0.89381, time: 0.01050\n",
      "Epoch: 193, train_loss: 0.89562, time: 0.01023\n",
      "Epoch: 194, train_loss: 0.88998, time: 0.01037\n",
      "Epoch: 195, train_loss: 0.89555, time: 0.01025\n",
      "Epoch: 196, train_loss: 0.88791, time: 0.01027\n",
      "Epoch: 197, train_loss: 0.89272, time: 0.01036\n",
      "Epoch: 198, train_loss: 0.88745, time: 0.01049\n",
      "Epoch: 199, train_loss: 0.89002, time: 0.01042\n",
      "Epoch: 200, train_loss: 0.89217, time: 0.01043\n",
      "pairwise precision 0.33185 recall 0.52429 f1 0.40644\n",
      "average until now [0.44406162387230025, 0.8307095248362436, 0.5787489322121602]\n",
      "77 names 325.3397982120514 avg time 4.225192184572096\n",
      "Loading hongtao_liu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 328 nodes, 1645 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99261, time: 0.11291\n",
      "Epoch: 2, train_loss: 0.98599, time: 0.01526\n",
      "Epoch: 3, train_loss: 0.98543, time: 0.01374\n",
      "Epoch: 4, train_loss: 0.98532, time: 0.01366\n",
      "Epoch: 5, train_loss: 0.98277, time: 0.01374\n",
      "Epoch: 6, train_loss: 0.98376, time: 0.01402\n",
      "Epoch: 7, train_loss: 0.98299, time: 0.01398\n",
      "Epoch: 8, train_loss: 0.98547, time: 0.01335\n",
      "Epoch: 9, train_loss: 0.98549, time: 0.01358\n",
      "Epoch: 10, train_loss: 0.98439, time: 0.01334\n",
      "Epoch: 11, train_loss: 0.98213, time: 0.01391\n",
      "Epoch: 12, train_loss: 0.98348, time: 0.01366\n",
      "Epoch: 13, train_loss: 0.98409, time: 0.01364\n",
      "Epoch: 14, train_loss: 0.98359, time: 0.01331\n",
      "Epoch: 15, train_loss: 0.98609, time: 0.01354\n",
      "Epoch: 16, train_loss: 0.98199, time: 0.01351\n",
      "Epoch: 17, train_loss: 0.98393, time: 0.01368\n",
      "Epoch: 18, train_loss: 0.98290, time: 0.01362\n",
      "Epoch: 19, train_loss: 0.98247, time: 0.01358\n",
      "Epoch: 20, train_loss: 0.98300, time: 0.01359\n",
      "Epoch: 21, train_loss: 0.98409, time: 0.01363\n",
      "Epoch: 22, train_loss: 0.98318, time: 0.01393\n",
      "Epoch: 23, train_loss: 0.98209, time: 0.01339\n",
      "Epoch: 24, train_loss: 0.98593, time: 0.01404\n",
      "Epoch: 25, train_loss: 0.98263, time: 0.01369\n",
      "Epoch: 26, train_loss: 0.98092, time: 0.01313\n",
      "Epoch: 27, train_loss: 0.98435, time: 0.01386\n",
      "Epoch: 28, train_loss: 0.98327, time: 0.01363\n",
      "Epoch: 29, train_loss: 0.98467, time: 0.01376\n",
      "Epoch: 30, train_loss: 0.98451, time: 0.01332\n",
      "Epoch: 31, train_loss: 0.98104, time: 0.01351\n",
      "Epoch: 32, train_loss: 0.98222, time: 0.01378\n",
      "Epoch: 33, train_loss: 0.98520, time: 0.01375\n",
      "Epoch: 34, train_loss: 0.98268, time: 0.01370\n",
      "Epoch: 35, train_loss: 0.98073, time: 0.01359\n",
      "Epoch: 36, train_loss: 0.98251, time: 0.01377\n",
      "Epoch: 37, train_loss: 0.98389, time: 0.01322\n",
      "Epoch: 38, train_loss: 0.98235, time: 0.01364\n",
      "Epoch: 39, train_loss: 0.98594, time: 0.01343\n",
      "Epoch: 40, train_loss: 0.98216, time: 0.01383\n",
      "Epoch: 41, train_loss: 0.98278, time: 0.01316\n",
      "Epoch: 42, train_loss: 0.98212, time: 0.01360\n",
      "Epoch: 43, train_loss: 0.98454, time: 0.01326\n",
      "Epoch: 44, train_loss: 0.98350, time: 0.01353\n",
      "Epoch: 45, train_loss: 0.98102, time: 0.01347\n",
      "Epoch: 46, train_loss: 0.98450, time: 0.01358\n",
      "Epoch: 47, train_loss: 0.98365, time: 0.01351\n",
      "Epoch: 48, train_loss: 0.98233, time: 0.01365\n",
      "Epoch: 49, train_loss: 0.98235, time: 0.01368\n",
      "Epoch: 50, train_loss: 0.98275, time: 0.01301\n",
      "Epoch: 51, train_loss: 0.98281, time: 0.01371\n",
      "Epoch: 52, train_loss: 0.98224, time: 0.01319\n",
      "Epoch: 53, train_loss: 0.98125, time: 0.01376\n",
      "Epoch: 54, train_loss: 0.98055, time: 0.01384\n",
      "Epoch: 55, train_loss: 0.98413, time: 0.01399\n",
      "Epoch: 56, train_loss: 0.98227, time: 0.01396\n",
      "Epoch: 57, train_loss: 0.98302, time: 0.01362\n",
      "Epoch: 58, train_loss: 0.98365, time: 0.01353\n",
      "Epoch: 59, train_loss: 0.98446, time: 0.01391\n",
      "Epoch: 60, train_loss: 0.98486, time: 0.01393\n",
      "Epoch: 61, train_loss: 0.98227, time: 0.01349\n",
      "Epoch: 62, train_loss: 0.98320, time: 0.01372\n",
      "Epoch: 63, train_loss: 0.98351, time: 0.01349\n",
      "Epoch: 64, train_loss: 0.98151, time: 0.01372\n",
      "Epoch: 65, train_loss: 0.98364, time: 0.01389\n",
      "Epoch: 66, train_loss: 0.98259, time: 0.01323\n",
      "Epoch: 67, train_loss: 0.98467, time: 0.01357\n",
      "Epoch: 68, train_loss: 0.98416, time: 0.01344\n",
      "Epoch: 69, train_loss: 0.98481, time: 0.01330\n",
      "Epoch: 70, train_loss: 0.98269, time: 0.01341\n",
      "Epoch: 71, train_loss: 0.98160, time: 0.01336\n",
      "Epoch: 72, train_loss: 0.98488, time: 0.01348\n",
      "Epoch: 73, train_loss: 0.98374, time: 0.01357\n",
      "Epoch: 74, train_loss: 0.98345, time: 0.01395\n",
      "Epoch: 75, train_loss: 0.98104, time: 0.01361\n",
      "Epoch: 76, train_loss: 0.98469, time: 0.01338\n",
      "Epoch: 77, train_loss: 0.98476, time: 0.01370\n",
      "Epoch: 78, train_loss: 0.98552, time: 0.01350\n",
      "Epoch: 79, train_loss: 0.98252, time: 0.01371\n",
      "Epoch: 80, train_loss: 0.98335, time: 0.01382\n",
      "Epoch: 81, train_loss: 0.98287, time: 0.01339\n",
      "Epoch: 82, train_loss: 0.98455, time: 0.01378\n",
      "Epoch: 83, train_loss: 0.98250, time: 0.01364\n",
      "Epoch: 84, train_loss: 0.98133, time: 0.01323\n",
      "Epoch: 85, train_loss: 0.98444, time: 0.01330\n",
      "Epoch: 86, train_loss: 0.98284, time: 0.01395\n",
      "Epoch: 87, train_loss: 0.98194, time: 0.01327\n",
      "Epoch: 88, train_loss: 0.98254, time: 0.01338\n",
      "Epoch: 89, train_loss: 0.98373, time: 0.01336\n",
      "Epoch: 90, train_loss: 0.98257, time: 0.01367\n",
      "Epoch: 91, train_loss: 0.98163, time: 0.01353\n",
      "Epoch: 92, train_loss: 0.98328, time: 0.01329\n",
      "Epoch: 93, train_loss: 0.98396, time: 0.01318\n",
      "Epoch: 94, train_loss: 0.98388, time: 0.01309\n",
      "Epoch: 95, train_loss: 0.98274, time: 0.01319\n",
      "Epoch: 96, train_loss: 0.98262, time: 0.01369\n",
      "Epoch: 97, train_loss: 0.98390, time: 0.01343\n",
      "Epoch: 98, train_loss: 0.98072, time: 0.01326\n",
      "Epoch: 99, train_loss: 0.98415, time: 0.01243\n",
      "Epoch: 100, train_loss: 0.98148, time: 0.01265\n",
      "Epoch: 101, train_loss: 0.98554, time: 0.01291\n",
      "Epoch: 102, train_loss: 0.98278, time: 0.01392\n",
      "Epoch: 103, train_loss: 0.98630, time: 0.01333\n",
      "Epoch: 104, train_loss: 0.98360, time: 0.01395\n",
      "Epoch: 105, train_loss: 0.98305, time: 0.01357\n",
      "Epoch: 106, train_loss: 0.98268, time: 0.01325\n",
      "Epoch: 107, train_loss: 0.98447, time: 0.01311\n",
      "Epoch: 108, train_loss: 0.98482, time: 0.01267\n",
      "Epoch: 109, train_loss: 0.98183, time: 0.01346\n",
      "Epoch: 110, train_loss: 0.98242, time: 0.01330\n",
      "Epoch: 111, train_loss: 0.98132, time: 0.01366\n",
      "Epoch: 112, train_loss: 0.98463, time: 0.01379\n",
      "Epoch: 113, train_loss: 0.98385, time: 0.01370\n",
      "Epoch: 114, train_loss: 0.98133, time: 0.01405\n",
      "Epoch: 115, train_loss: 0.98418, time: 0.01370\n",
      "Epoch: 116, train_loss: 0.98315, time: 0.01356\n",
      "Epoch: 117, train_loss: 0.98451, time: 0.01399\n",
      "Epoch: 118, train_loss: 0.98228, time: 0.01331\n",
      "Epoch: 119, train_loss: 0.98392, time: 0.01371\n",
      "Epoch: 120, train_loss: 0.98336, time: 0.01372\n",
      "Epoch: 121, train_loss: 0.98485, time: 0.01371\n",
      "Epoch: 122, train_loss: 0.98328, time: 0.01384\n",
      "Epoch: 123, train_loss: 0.98340, time: 0.01371\n",
      "Epoch: 124, train_loss: 0.98382, time: 0.01358\n",
      "Epoch: 125, train_loss: 0.98355, time: 0.01387\n",
      "Epoch: 126, train_loss: 0.98430, time: 0.01396\n",
      "Epoch: 127, train_loss: 0.98202, time: 0.01353\n",
      "Epoch: 128, train_loss: 0.98294, time: 0.01374\n",
      "Epoch: 129, train_loss: 0.98299, time: 0.01358\n",
      "Epoch: 130, train_loss: 0.98200, time: 0.01362\n",
      "Epoch: 131, train_loss: 0.98344, time: 0.01371\n",
      "Epoch: 132, train_loss: 0.98251, time: 0.01321\n",
      "Epoch: 133, train_loss: 0.98324, time: 0.01362\n",
      "Epoch: 134, train_loss: 0.98393, time: 0.01373\n",
      "Epoch: 135, train_loss: 0.98400, time: 0.01369\n",
      "Epoch: 136, train_loss: 0.98480, time: 0.01346\n",
      "Epoch: 137, train_loss: 0.98460, time: 0.01342\n",
      "Epoch: 138, train_loss: 0.98326, time: 0.01374\n",
      "Epoch: 139, train_loss: 0.98282, time: 0.01347\n",
      "Epoch: 140, train_loss: 0.98397, time: 0.01339\n",
      "Epoch: 141, train_loss: 0.98192, time: 0.01324\n",
      "Epoch: 142, train_loss: 0.98362, time: 0.01347\n",
      "Epoch: 143, train_loss: 0.98313, time: 0.01354\n",
      "Epoch: 144, train_loss: 0.98093, time: 0.01339\n",
      "Epoch: 145, train_loss: 0.98287, time: 0.01308\n",
      "Epoch: 146, train_loss: 0.98363, time: 0.01371\n",
      "Epoch: 147, train_loss: 0.98287, time: 0.01304\n",
      "Epoch: 148, train_loss: 0.98205, time: 0.01304\n",
      "Epoch: 149, train_loss: 0.98450, time: 0.01368\n",
      "Epoch: 150, train_loss: 0.98339, time: 0.01343\n",
      "Epoch: 151, train_loss: 0.98274, time: 0.01330\n",
      "Epoch: 152, train_loss: 0.98475, time: 0.01390\n",
      "Epoch: 153, train_loss: 0.98362, time: 0.01317\n",
      "Epoch: 154, train_loss: 0.98165, time: 0.01393\n",
      "Epoch: 155, train_loss: 0.98241, time: 0.01365\n",
      "Epoch: 156, train_loss: 0.98343, time: 0.01354\n",
      "Epoch: 157, train_loss: 0.98506, time: 0.01377\n",
      "Epoch: 158, train_loss: 0.98249, time: 0.01362\n",
      "Epoch: 159, train_loss: 0.98434, time: 0.01387\n",
      "Epoch: 160, train_loss: 0.98332, time: 0.01332\n",
      "Epoch: 161, train_loss: 0.98565, time: 0.01394\n",
      "Epoch: 162, train_loss: 0.98400, time: 0.01355\n",
      "Epoch: 163, train_loss: 0.98357, time: 0.01370\n",
      "Epoch: 164, train_loss: 0.98279, time: 0.01384\n",
      "Epoch: 165, train_loss: 0.98317, time: 0.01279\n",
      "Epoch: 166, train_loss: 0.98349, time: 0.01314\n",
      "Epoch: 167, train_loss: 0.98272, time: 0.01359\n",
      "Epoch: 168, train_loss: 0.98315, time: 0.01328\n",
      "Epoch: 169, train_loss: 0.98338, time: 0.01368\n",
      "Epoch: 170, train_loss: 0.98404, time: 0.01376\n",
      "Epoch: 171, train_loss: 0.98236, time: 0.01339\n",
      "Epoch: 172, train_loss: 0.98271, time: 0.01382\n",
      "Epoch: 173, train_loss: 0.98223, time: 0.01327\n",
      "Epoch: 174, train_loss: 0.98373, time: 0.01352\n",
      "Epoch: 175, train_loss: 0.98342, time: 0.01353\n",
      "Epoch: 176, train_loss: 0.98294, time: 0.01357\n",
      "Epoch: 177, train_loss: 0.98185, time: 0.01331\n",
      "Epoch: 178, train_loss: 0.98293, time: 0.01346\n",
      "Epoch: 179, train_loss: 0.98544, time: 0.01315\n",
      "Epoch: 180, train_loss: 0.98096, time: 0.01359\n",
      "Epoch: 181, train_loss: 0.98250, time: 0.01372\n",
      "Epoch: 182, train_loss: 0.98305, time: 0.01354\n",
      "Epoch: 183, train_loss: 0.98224, time: 0.01377\n",
      "Epoch: 184, train_loss: 0.98240, time: 0.01377\n",
      "Epoch: 185, train_loss: 0.98190, time: 0.01406\n",
      "Epoch: 186, train_loss: 0.98365, time: 0.01355\n",
      "Epoch: 187, train_loss: 0.98376, time: 0.01324\n",
      "Epoch: 188, train_loss: 0.98544, time: 0.01336\n",
      "Epoch: 189, train_loss: 0.98020, time: 0.01397\n",
      "Epoch: 190, train_loss: 0.98269, time: 0.01375\n",
      "Epoch: 191, train_loss: 0.98455, time: 0.01343\n",
      "Epoch: 192, train_loss: 0.98182, time: 0.01385\n",
      "Epoch: 193, train_loss: 0.98283, time: 0.01369\n",
      "Epoch: 194, train_loss: 0.98504, time: 0.01414\n",
      "Epoch: 195, train_loss: 0.98093, time: 0.01391\n",
      "Epoch: 196, train_loss: 0.98310, time: 0.01343\n",
      "Epoch: 197, train_loss: 0.98361, time: 0.01334\n",
      "Epoch: 198, train_loss: 0.98326, time: 0.01326\n",
      "Epoch: 199, train_loss: 0.98454, time: 0.01331\n",
      "Epoch: 200, train_loss: 0.98259, time: 0.01370\n",
      "pairwise precision 0.41166 recall 0.94066 f1 0.57269\n",
      "average until now [0.4436462441334986, 0.832119100814935, 0.5787373284751072]\n",
      "78 names 328.2243802547455 avg time 4.20800487506084\n",
      "Loading c_y_huang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 203 nodes, 1043 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97972, time: 0.11014\n",
      "Epoch: 2, train_loss: 0.98008, time: 0.01236\n",
      "Epoch: 3, train_loss: 0.98109, time: 0.01109\n",
      "Epoch: 4, train_loss: 0.97503, time: 0.01058\n",
      "Epoch: 5, train_loss: 0.97228, time: 0.01083\n",
      "Epoch: 6, train_loss: 0.97446, time: 0.01045\n",
      "Epoch: 7, train_loss: 0.97272, time: 0.01056\n",
      "Epoch: 8, train_loss: 0.97570, time: 0.01066\n",
      "Epoch: 9, train_loss: 0.97708, time: 0.01108\n",
      "Epoch: 10, train_loss: 0.97274, time: 0.01092\n",
      "Epoch: 11, train_loss: 0.97235, time: 0.01083\n",
      "Epoch: 12, train_loss: 0.97421, time: 0.01051\n",
      "Epoch: 13, train_loss: 0.97493, time: 0.01072\n",
      "Epoch: 14, train_loss: 0.97720, time: 0.01034\n",
      "Epoch: 15, train_loss: 0.97240, time: 0.01064\n",
      "Epoch: 16, train_loss: 0.97755, time: 0.01051\n",
      "Epoch: 17, train_loss: 0.97135, time: 0.01061\n",
      "Epoch: 18, train_loss: 0.97345, time: 0.01034\n",
      "Epoch: 19, train_loss: 0.97448, time: 0.01065\n",
      "Epoch: 20, train_loss: 0.97624, time: 0.01050\n",
      "Epoch: 21, train_loss: 0.97782, time: 0.01059\n",
      "Epoch: 22, train_loss: 0.97644, time: 0.01056\n",
      "Epoch: 23, train_loss: 0.97725, time: 0.01059\n",
      "Epoch: 24, train_loss: 0.97428, time: 0.01055\n",
      "Epoch: 25, train_loss: 0.97412, time: 0.01064\n",
      "Epoch: 26, train_loss: 0.97608, time: 0.01035\n",
      "Epoch: 27, train_loss: 0.97434, time: 0.01063\n",
      "Epoch: 28, train_loss: 0.97532, time: 0.01043\n",
      "Epoch: 29, train_loss: 0.97570, time: 0.01055\n",
      "Epoch: 30, train_loss: 0.97096, time: 0.01075\n",
      "Epoch: 31, train_loss: 0.97793, time: 0.01031\n",
      "Epoch: 32, train_loss: 0.97780, time: 0.01054\n",
      "Epoch: 33, train_loss: 0.97438, time: 0.01038\n",
      "Epoch: 34, train_loss: 0.97544, time: 0.01057\n",
      "Epoch: 35, train_loss: 0.97437, time: 0.01039\n",
      "Epoch: 36, train_loss: 0.97428, time: 0.01065\n",
      "Epoch: 37, train_loss: 0.97274, time: 0.01048\n",
      "Epoch: 38, train_loss: 0.97437, time: 0.01082\n",
      "Epoch: 39, train_loss: 0.97069, time: 0.01061\n",
      "Epoch: 40, train_loss: 0.97060, time: 0.01044\n",
      "Epoch: 41, train_loss: 0.97152, time: 0.01021\n",
      "Epoch: 42, train_loss: 0.97569, time: 0.01061\n",
      "Epoch: 43, train_loss: 0.97601, time: 0.01001\n",
      "Epoch: 44, train_loss: 0.97262, time: 0.01057\n",
      "Epoch: 45, train_loss: 0.97424, time: 0.01022\n",
      "Epoch: 46, train_loss: 0.97390, time: 0.01059\n",
      "Epoch: 47, train_loss: 0.97296, time: 0.01010\n",
      "Epoch: 48, train_loss: 0.97652, time: 0.01058\n",
      "Epoch: 49, train_loss: 0.97459, time: 0.01047\n",
      "Epoch: 50, train_loss: 0.97256, time: 0.01046\n",
      "Epoch: 51, train_loss: 0.97599, time: 0.01053\n",
      "Epoch: 52, train_loss: 0.97438, time: 0.01030\n",
      "Epoch: 53, train_loss: 0.97338, time: 0.01058\n",
      "Epoch: 54, train_loss: 0.97645, time: 0.01049\n",
      "Epoch: 55, train_loss: 0.97272, time: 0.01098\n",
      "Epoch: 56, train_loss: 0.97453, time: 0.01053\n",
      "Epoch: 57, train_loss: 0.97354, time: 0.01082\n",
      "Epoch: 58, train_loss: 0.97538, time: 0.01041\n",
      "Epoch: 59, train_loss: 0.97285, time: 0.01075\n",
      "Epoch: 60, train_loss: 0.97731, time: 0.01042\n",
      "Epoch: 61, train_loss: 0.97504, time: 0.01068\n",
      "Epoch: 62, train_loss: 0.97381, time: 0.01054\n",
      "Epoch: 63, train_loss: 0.97529, time: 0.01081\n",
      "Epoch: 64, train_loss: 0.97460, time: 0.01063\n",
      "Epoch: 65, train_loss: 0.97152, time: 0.01068\n",
      "Epoch: 66, train_loss: 0.97440, time: 0.01035\n",
      "Epoch: 67, train_loss: 0.97411, time: 0.01074\n",
      "Epoch: 68, train_loss: 0.97068, time: 0.01084\n",
      "Epoch: 69, train_loss: 0.97308, time: 0.01031\n",
      "Epoch: 70, train_loss: 0.97501, time: 0.01057\n",
      "Epoch: 71, train_loss: 0.97636, time: 0.01065\n",
      "Epoch: 72, train_loss: 0.97152, time: 0.01065\n",
      "Epoch: 73, train_loss: 0.97070, time: 0.01041\n",
      "Epoch: 74, train_loss: 0.97559, time: 0.01061\n",
      "Epoch: 75, train_loss: 0.97393, time: 0.01044\n",
      "Epoch: 76, train_loss: 0.97356, time: 0.01076\n",
      "Epoch: 77, train_loss: 0.97223, time: 0.01038\n",
      "Epoch: 78, train_loss: 0.97212, time: 0.01069\n",
      "Epoch: 79, train_loss: 0.97458, time: 0.01057\n",
      "Epoch: 80, train_loss: 0.97401, time: 0.01063\n",
      "Epoch: 81, train_loss: 0.97122, time: 0.01057\n",
      "Epoch: 82, train_loss: 0.97720, time: 0.01079\n",
      "Epoch: 83, train_loss: 0.97580, time: 0.01032\n",
      "Epoch: 84, train_loss: 0.97408, time: 0.01060\n",
      "Epoch: 85, train_loss: 0.97330, time: 0.01070\n",
      "Epoch: 86, train_loss: 0.97621, time: 0.01058\n",
      "Epoch: 87, train_loss: 0.97473, time: 0.01079\n",
      "Epoch: 88, train_loss: 0.97516, time: 0.01064\n",
      "Epoch: 89, train_loss: 0.97309, time: 0.01080\n",
      "Epoch: 90, train_loss: 0.97610, time: 0.01081\n",
      "Epoch: 91, train_loss: 0.97222, time: 0.01044\n",
      "Epoch: 92, train_loss: 0.97371, time: 0.01076\n",
      "Epoch: 93, train_loss: 0.97201, time: 0.01040\n",
      "Epoch: 94, train_loss: 0.97370, time: 0.01067\n",
      "Epoch: 95, train_loss: 0.97290, time: 0.01054\n",
      "Epoch: 96, train_loss: 0.97098, time: 0.01068\n",
      "Epoch: 97, train_loss: 0.97353, time: 0.01064\n",
      "Epoch: 98, train_loss: 0.97580, time: 0.01064\n",
      "Epoch: 99, train_loss: 0.97256, time: 0.01051\n",
      "Epoch: 100, train_loss: 0.97213, time: 0.01033\n",
      "Epoch: 101, train_loss: 0.97204, time: 0.01063\n",
      "Epoch: 102, train_loss: 0.97957, time: 0.01049\n",
      "Epoch: 103, train_loss: 0.97391, time: 0.01038\n",
      "Epoch: 104, train_loss: 0.97459, time: 0.01028\n",
      "Epoch: 105, train_loss: 0.97238, time: 0.01061\n",
      "Epoch: 106, train_loss: 0.97510, time: 0.01033\n",
      "Epoch: 107, train_loss: 0.97403, time: 0.01059\n",
      "Epoch: 108, train_loss: 0.97539, time: 0.01033\n",
      "Epoch: 109, train_loss: 0.97100, time: 0.01054\n",
      "Epoch: 110, train_loss: 0.97289, time: 0.01042\n",
      "Epoch: 111, train_loss: 0.97127, time: 0.01070\n",
      "Epoch: 112, train_loss: 0.97302, time: 0.01059\n",
      "Epoch: 113, train_loss: 0.97290, time: 0.01076\n",
      "Epoch: 114, train_loss: 0.97356, time: 0.01055\n",
      "Epoch: 115, train_loss: 0.97235, time: 0.01071\n",
      "Epoch: 116, train_loss: 0.97441, time: 0.01051\n",
      "Epoch: 117, train_loss: 0.97597, time: 0.01062\n",
      "Epoch: 118, train_loss: 0.97369, time: 0.01038\n",
      "Epoch: 119, train_loss: 0.97283, time: 0.01063\n",
      "Epoch: 120, train_loss: 0.97694, time: 0.01067\n",
      "Epoch: 121, train_loss: 0.97615, time: 0.01093\n",
      "Epoch: 122, train_loss: 0.97970, time: 0.01053\n",
      "Epoch: 123, train_loss: 0.97296, time: 0.01099\n",
      "Epoch: 124, train_loss: 0.97565, time: 0.01070\n",
      "Epoch: 125, train_loss: 0.97227, time: 0.01039\n",
      "Epoch: 126, train_loss: 0.97186, time: 0.01071\n",
      "Epoch: 127, train_loss: 0.97274, time: 0.01047\n",
      "Epoch: 128, train_loss: 0.97321, time: 0.01067\n",
      "Epoch: 129, train_loss: 0.98340, time: 0.01057\n",
      "Epoch: 130, train_loss: 0.97044, time: 0.01067\n",
      "Epoch: 131, train_loss: 0.97273, time: 0.01061\n",
      "Epoch: 132, train_loss: 0.97208, time: 0.01073\n",
      "Epoch: 133, train_loss: 0.97809, time: 0.01070\n",
      "Epoch: 134, train_loss: 0.97364, time: 0.01037\n",
      "Epoch: 135, train_loss: 0.97250, time: 0.01034\n",
      "Epoch: 136, train_loss: 0.97327, time: 0.01062\n",
      "Epoch: 137, train_loss: 0.97260, time: 0.01022\n",
      "Epoch: 138, train_loss: 0.97407, time: 0.01075\n",
      "Epoch: 139, train_loss: 0.97492, time: 0.01048\n",
      "Epoch: 140, train_loss: 0.97319, time: 0.01041\n",
      "Epoch: 141, train_loss: 0.97600, time: 0.01059\n",
      "Epoch: 142, train_loss: 0.97507, time: 0.01054\n",
      "Epoch: 143, train_loss: 0.97064, time: 0.01073\n",
      "Epoch: 144, train_loss: 0.96992, time: 0.01083\n",
      "Epoch: 145, train_loss: 0.97407, time: 0.01050\n",
      "Epoch: 146, train_loss: 0.97392, time: 0.01079\n",
      "Epoch: 147, train_loss: 0.97153, time: 0.01035\n",
      "Epoch: 148, train_loss: 0.97408, time: 0.01064\n",
      "Epoch: 149, train_loss: 0.97015, time: 0.01063\n",
      "Epoch: 150, train_loss: 0.97555, time: 0.01046\n",
      "Epoch: 151, train_loss: 0.97239, time: 0.01054\n",
      "Epoch: 152, train_loss: 0.97108, time: 0.01074\n",
      "Epoch: 153, train_loss: 0.97401, time: 0.01040\n",
      "Epoch: 154, train_loss: 0.97696, time: 0.01070\n",
      "Epoch: 155, train_loss: 0.97342, time: 0.01035\n",
      "Epoch: 156, train_loss: 0.97294, time: 0.01064\n",
      "Epoch: 157, train_loss: 0.97490, time: 0.01049\n",
      "Epoch: 158, train_loss: 0.97703, time: 0.01052\n",
      "Epoch: 159, train_loss: 0.97544, time: 0.01046\n",
      "Epoch: 160, train_loss: 0.97549, time: 0.01065\n",
      "Epoch: 161, train_loss: 0.96979, time: 0.01044\n",
      "Epoch: 162, train_loss: 0.97603, time: 0.01081\n",
      "Epoch: 163, train_loss: 0.97480, time: 0.01073\n",
      "Epoch: 164, train_loss: 0.97493, time: 0.01044\n",
      "Epoch: 165, train_loss: 0.97623, time: 0.01069\n",
      "Epoch: 166, train_loss: 0.97373, time: 0.01057\n",
      "Epoch: 167, train_loss: 0.97463, time: 0.01057\n",
      "Epoch: 168, train_loss: 0.97124, time: 0.01032\n",
      "Epoch: 169, train_loss: 0.97306, time: 0.01072\n",
      "Epoch: 170, train_loss: 0.97776, time: 0.01054\n",
      "Epoch: 171, train_loss: 0.97729, time: 0.01052\n",
      "Epoch: 172, train_loss: 0.97253, time: 0.01073\n",
      "Epoch: 173, train_loss: 0.97212, time: 0.01063\n",
      "Epoch: 174, train_loss: 0.97437, time: 0.01060\n",
      "Epoch: 175, train_loss: 0.97774, time: 0.01041\n",
      "Epoch: 176, train_loss: 0.97093, time: 0.01070\n",
      "Epoch: 177, train_loss: 0.97421, time: 0.01052\n",
      "Epoch: 178, train_loss: 0.97331, time: 0.01085\n",
      "Epoch: 179, train_loss: 0.97129, time: 0.01062\n",
      "Epoch: 180, train_loss: 0.97339, time: 0.01056\n",
      "Epoch: 181, train_loss: 0.97169, time: 0.01034\n",
      "Epoch: 182, train_loss: 0.97333, time: 0.01057\n",
      "Epoch: 183, train_loss: 0.97405, time: 0.01033\n",
      "Epoch: 184, train_loss: 0.96901, time: 0.01073\n",
      "Epoch: 185, train_loss: 0.97141, time: 0.01036\n",
      "Epoch: 186, train_loss: 0.97634, time: 0.01071\n",
      "Epoch: 187, train_loss: 0.97326, time: 0.01039\n",
      "Epoch: 188, train_loss: 0.97127, time: 0.01054\n",
      "Epoch: 189, train_loss: 0.97376, time: 0.01035\n",
      "Epoch: 190, train_loss: 0.97407, time: 0.01079\n",
      "Epoch: 191, train_loss: 0.97294, time: 0.01058\n",
      "Epoch: 192, train_loss: 0.97289, time: 0.01080\n",
      "Epoch: 193, train_loss: 0.97197, time: 0.01052\n",
      "Epoch: 194, train_loss: 0.97442, time: 0.01076\n",
      "Epoch: 195, train_loss: 0.97266, time: 0.01033\n",
      "Epoch: 196, train_loss: 0.97370, time: 0.01072\n",
      "Epoch: 197, train_loss: 0.97396, time: 0.01066\n",
      "Epoch: 198, train_loss: 0.97362, time: 0.01067\n",
      "Epoch: 199, train_loss: 0.97178, time: 0.01065\n",
      "Epoch: 200, train_loss: 0.97435, time: 0.01053\n",
      "pairwise precision 0.04569 recall 0.78298 f1 0.08633\n",
      "average until now [0.4386087715415944, 0.831497070721129, 0.5742858533422751]\n",
      "79 names 330.48991799354553 avg time 4.183416683462601\n",
      "Loading s_c_wu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 208 nodes, 1533 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.96724, time: 0.11042\n",
      "Epoch: 2, train_loss: 0.96816, time: 0.01252\n",
      "Epoch: 3, train_loss: 0.97100, time: 0.01153\n",
      "Epoch: 4, train_loss: 0.96339, time: 0.01120\n",
      "Epoch: 5, train_loss: 0.96429, time: 0.01133\n",
      "Epoch: 6, train_loss: 0.96855, time: 0.01104\n",
      "Epoch: 7, train_loss: 0.96654, time: 0.01103\n",
      "Epoch: 8, train_loss: 0.96600, time: 0.01086\n",
      "Epoch: 9, train_loss: 0.96367, time: 0.01119\n",
      "Epoch: 10, train_loss: 0.96288, time: 0.01109\n",
      "Epoch: 11, train_loss: 0.96300, time: 0.01107\n",
      "Epoch: 12, train_loss: 0.96405, time: 0.01104\n",
      "Epoch: 13, train_loss: 0.96480, time: 0.01105\n",
      "Epoch: 14, train_loss: 0.96401, time: 0.01113\n",
      "Epoch: 15, train_loss: 0.96288, time: 0.01124\n",
      "Epoch: 16, train_loss: 0.96427, time: 0.01044\n",
      "Epoch: 17, train_loss: 0.96652, time: 0.01089\n",
      "Epoch: 18, train_loss: 0.96570, time: 0.01054\n",
      "Epoch: 19, train_loss: 0.96486, time: 0.01101\n",
      "Epoch: 20, train_loss: 0.96657, time: 0.01061\n",
      "Epoch: 21, train_loss: 0.96176, time: 0.01086\n",
      "Epoch: 22, train_loss: 0.96119, time: 0.01084\n",
      "Epoch: 23, train_loss: 0.96918, time: 0.01114\n",
      "Epoch: 24, train_loss: 0.96541, time: 0.01151\n",
      "Epoch: 25, train_loss: 0.96825, time: 0.01113\n",
      "Epoch: 26, train_loss: 0.96632, time: 0.01133\n",
      "Epoch: 27, train_loss: 0.96213, time: 0.01079\n",
      "Epoch: 28, train_loss: 0.96444, time: 0.01103\n",
      "Epoch: 29, train_loss: 0.96167, time: 0.01078\n",
      "Epoch: 30, train_loss: 0.96690, time: 0.01110\n",
      "Epoch: 31, train_loss: 0.96253, time: 0.01066\n",
      "Epoch: 32, train_loss: 0.96418, time: 0.01113\n",
      "Epoch: 33, train_loss: 0.96394, time: 0.01094\n",
      "Epoch: 34, train_loss: 0.96322, time: 0.01088\n",
      "Epoch: 35, train_loss: 0.96378, time: 0.01127\n",
      "Epoch: 36, train_loss: 0.96410, time: 0.01082\n",
      "Epoch: 37, train_loss: 0.96430, time: 0.01109\n",
      "Epoch: 38, train_loss: 0.96346, time: 0.01090\n",
      "Epoch: 39, train_loss: 0.96623, time: 0.01110\n",
      "Epoch: 40, train_loss: 0.96484, time: 0.01080\n",
      "Epoch: 41, train_loss: 0.96415, time: 0.01105\n",
      "Epoch: 42, train_loss: 0.96292, time: 0.01084\n",
      "Epoch: 43, train_loss: 0.96585, time: 0.01122\n",
      "Epoch: 44, train_loss: 0.96329, time: 0.01089\n",
      "Epoch: 45, train_loss: 0.96339, time: 0.01111\n",
      "Epoch: 46, train_loss: 0.96463, time: 0.01125\n",
      "Epoch: 47, train_loss: 0.96416, time: 0.01087\n",
      "Epoch: 48, train_loss: 0.96064, time: 0.01215\n",
      "Epoch: 49, train_loss: 0.96242, time: 0.01089\n",
      "Epoch: 50, train_loss: 0.96390, time: 0.01121\n",
      "Epoch: 51, train_loss: 0.96537, time: 0.01073\n",
      "Epoch: 52, train_loss: 0.96534, time: 0.01123\n",
      "Epoch: 53, train_loss: 0.96003, time: 0.01086\n",
      "Epoch: 54, train_loss: 0.96158, time: 0.01113\n",
      "Epoch: 55, train_loss: 0.96529, time: 0.01106\n",
      "Epoch: 56, train_loss: 0.96445, time: 0.01127\n",
      "Epoch: 57, train_loss: 0.96245, time: 0.01075\n",
      "Epoch: 58, train_loss: 0.96292, time: 0.01116\n",
      "Epoch: 59, train_loss: 0.96643, time: 0.01098\n",
      "Epoch: 60, train_loss: 0.96368, time: 0.01082\n",
      "Epoch: 61, train_loss: 0.96625, time: 0.01100\n",
      "Epoch: 62, train_loss: 0.96721, time: 0.01090\n",
      "Epoch: 63, train_loss: 0.96200, time: 0.01114\n",
      "Epoch: 64, train_loss: 0.96323, time: 0.01115\n",
      "Epoch: 65, train_loss: 0.96402, time: 0.01084\n",
      "Epoch: 66, train_loss: 0.96447, time: 0.01112\n",
      "Epoch: 67, train_loss: 0.96797, time: 0.01112\n",
      "Epoch: 68, train_loss: 0.96327, time: 0.01123\n",
      "Epoch: 69, train_loss: 0.96736, time: 0.01105\n",
      "Epoch: 70, train_loss: 0.96254, time: 0.01094\n",
      "Epoch: 71, train_loss: 0.96343, time: 0.01100\n",
      "Epoch: 72, train_loss: 0.96212, time: 0.01083\n",
      "Epoch: 73, train_loss: 0.96402, time: 0.01098\n",
      "Epoch: 74, train_loss: 0.96316, time: 0.01082\n",
      "Epoch: 75, train_loss: 0.96413, time: 0.01103\n",
      "Epoch: 76, train_loss: 0.96278, time: 0.01076\n",
      "Epoch: 77, train_loss: 0.96487, time: 0.01098\n",
      "Epoch: 78, train_loss: 0.96340, time: 0.01073\n",
      "Epoch: 79, train_loss: 0.96582, time: 0.01121\n",
      "Epoch: 80, train_loss: 0.95898, time: 0.01056\n",
      "Epoch: 81, train_loss: 0.96476, time: 0.01147\n",
      "Epoch: 82, train_loss: 0.95933, time: 0.01091\n",
      "Epoch: 83, train_loss: 0.96359, time: 0.01110\n",
      "Epoch: 84, train_loss: 0.96730, time: 0.01078\n",
      "Epoch: 85, train_loss: 0.96281, time: 0.01120\n",
      "Epoch: 86, train_loss: 0.96469, time: 0.01086\n",
      "Epoch: 87, train_loss: 0.96707, time: 0.01100\n",
      "Epoch: 88, train_loss: 0.96348, time: 0.01111\n",
      "Epoch: 89, train_loss: 0.96641, time: 0.01111\n",
      "Epoch: 90, train_loss: 0.96323, time: 0.01090\n",
      "Epoch: 91, train_loss: 0.96334, time: 0.01131\n",
      "Epoch: 92, train_loss: 0.96098, time: 0.01085\n",
      "Epoch: 93, train_loss: 0.96311, time: 0.01089\n",
      "Epoch: 94, train_loss: 0.96320, time: 0.01086\n",
      "Epoch: 95, train_loss: 0.96364, time: 0.01121\n",
      "Epoch: 96, train_loss: 0.96511, time: 0.01088\n",
      "Epoch: 97, train_loss: 0.96419, time: 0.01107\n",
      "Epoch: 98, train_loss: 0.96447, time: 0.01108\n",
      "Epoch: 99, train_loss: 0.96213, time: 0.01121\n",
      "Epoch: 100, train_loss: 0.96342, time: 0.01114\n",
      "Epoch: 101, train_loss: 0.96177, time: 0.01103\n",
      "Epoch: 102, train_loss: 0.96380, time: 0.01093\n",
      "Epoch: 103, train_loss: 0.96263, time: 0.01112\n",
      "Epoch: 104, train_loss: 0.96316, time: 0.01084\n",
      "Epoch: 105, train_loss: 0.96402, time: 0.01126\n",
      "Epoch: 106, train_loss: 0.96259, time: 0.01093\n",
      "Epoch: 107, train_loss: 0.96395, time: 0.01084\n",
      "Epoch: 108, train_loss: 0.96848, time: 0.01082\n",
      "Epoch: 109, train_loss: 0.96308, time: 0.01108\n",
      "Epoch: 110, train_loss: 0.96263, time: 0.01067\n",
      "Epoch: 111, train_loss: 0.96330, time: 0.01099\n",
      "Epoch: 112, train_loss: 0.96603, time: 0.01103\n",
      "Epoch: 113, train_loss: 0.96552, time: 0.01086\n",
      "Epoch: 114, train_loss: 0.96083, time: 0.01086\n",
      "Epoch: 115, train_loss: 0.96519, time: 0.01104\n",
      "Epoch: 116, train_loss: 0.96019, time: 0.01092\n",
      "Epoch: 117, train_loss: 0.96356, time: 0.01118\n",
      "Epoch: 118, train_loss: 0.96009, time: 0.01089\n",
      "Epoch: 119, train_loss: 0.96439, time: 0.01111\n",
      "Epoch: 120, train_loss: 0.96619, time: 0.01089\n",
      "Epoch: 121, train_loss: 0.96288, time: 0.01122\n",
      "Epoch: 122, train_loss: 0.96607, time: 0.01087\n",
      "Epoch: 123, train_loss: 0.96160, time: 0.01103\n",
      "Epoch: 124, train_loss: 0.96515, time: 0.01094\n",
      "Epoch: 125, train_loss: 0.96294, time: 0.01110\n",
      "Epoch: 126, train_loss: 0.96256, time: 0.01090\n",
      "Epoch: 127, train_loss: 0.96351, time: 0.01087\n",
      "Epoch: 128, train_loss: 0.96192, time: 0.01081\n",
      "Epoch: 129, train_loss: 0.96369, time: 0.01103\n",
      "Epoch: 130, train_loss: 0.96271, time: 0.01079\n",
      "Epoch: 131, train_loss: 0.96597, time: 0.01084\n",
      "Epoch: 132, train_loss: 0.96144, time: 0.01122\n",
      "Epoch: 133, train_loss: 0.96608, time: 0.01102\n",
      "Epoch: 134, train_loss: 0.96538, time: 0.01103\n",
      "Epoch: 135, train_loss: 0.96323, time: 0.01112\n",
      "Epoch: 136, train_loss: 0.96238, time: 0.01067\n",
      "Epoch: 137, train_loss: 0.96625, time: 0.01097\n",
      "Epoch: 138, train_loss: 0.96495, time: 0.01134\n",
      "Epoch: 139, train_loss: 0.96426, time: 0.01125\n",
      "Epoch: 140, train_loss: 0.96398, time: 0.01090\n",
      "Epoch: 141, train_loss: 0.96316, time: 0.01100\n",
      "Epoch: 142, train_loss: 0.96197, time: 0.01092\n",
      "Epoch: 143, train_loss: 0.96340, time: 0.01121\n",
      "Epoch: 144, train_loss: 0.95958, time: 0.01078\n",
      "Epoch: 145, train_loss: 0.96246, time: 0.01133\n",
      "Epoch: 146, train_loss: 0.96171, time: 0.01083\n",
      "Epoch: 147, train_loss: 0.96544, time: 0.01121\n",
      "Epoch: 148, train_loss: 0.96212, time: 0.01095\n",
      "Epoch: 149, train_loss: 0.96356, time: 0.01117\n",
      "Epoch: 150, train_loss: 0.96718, time: 0.01083\n",
      "Epoch: 151, train_loss: 0.96206, time: 0.01113\n",
      "Epoch: 152, train_loss: 0.96096, time: 0.01099\n",
      "Epoch: 153, train_loss: 0.96573, time: 0.01123\n",
      "Epoch: 154, train_loss: 0.96259, time: 0.01077\n",
      "Epoch: 155, train_loss: 0.96275, time: 0.01119\n",
      "Epoch: 156, train_loss: 0.96333, time: 0.01115\n",
      "Epoch: 157, train_loss: 0.96079, time: 0.01118\n",
      "Epoch: 158, train_loss: 0.96534, time: 0.01116\n",
      "Epoch: 159, train_loss: 0.96495, time: 0.01106\n",
      "Epoch: 160, train_loss: 0.96201, time: 0.01131\n",
      "Epoch: 161, train_loss: 0.96241, time: 0.01085\n",
      "Epoch: 162, train_loss: 0.96219, time: 0.01121\n",
      "Epoch: 163, train_loss: 0.96322, time: 0.01095\n",
      "Epoch: 164, train_loss: 0.96212, time: 0.01124\n",
      "Epoch: 165, train_loss: 0.96360, time: 0.01080\n",
      "Epoch: 166, train_loss: 0.96307, time: 0.01118\n",
      "Epoch: 167, train_loss: 0.96148, time: 0.01076\n",
      "Epoch: 168, train_loss: 0.96221, time: 0.01114\n",
      "Epoch: 169, train_loss: 0.96430, time: 0.01080\n",
      "Epoch: 170, train_loss: 0.96479, time: 0.01117\n",
      "Epoch: 171, train_loss: 0.96512, time: 0.01070\n",
      "Epoch: 172, train_loss: 0.96400, time: 0.01114\n",
      "Epoch: 173, train_loss: 0.96049, time: 0.01086\n",
      "Epoch: 174, train_loss: 0.96144, time: 0.01120\n",
      "Epoch: 175, train_loss: 0.96398, time: 0.01111\n",
      "Epoch: 176, train_loss: 0.96434, time: 0.01119\n",
      "Epoch: 177, train_loss: 0.96104, time: 0.01081\n",
      "Epoch: 178, train_loss: 0.96705, time: 0.01107\n",
      "Epoch: 179, train_loss: 0.96810, time: 0.01095\n",
      "Epoch: 180, train_loss: 0.96496, time: 0.01135\n",
      "Epoch: 181, train_loss: 0.96312, time: 0.01071\n",
      "Epoch: 182, train_loss: 0.96291, time: 0.01104\n",
      "Epoch: 183, train_loss: 0.96230, time: 0.01083\n",
      "Epoch: 184, train_loss: 0.96098, time: 0.01114\n",
      "Epoch: 185, train_loss: 0.96409, time: 0.01083\n",
      "Epoch: 186, train_loss: 0.96286, time: 0.01120\n",
      "Epoch: 187, train_loss: 0.96189, time: 0.01087\n",
      "Epoch: 188, train_loss: 0.96042, time: 0.01120\n",
      "Epoch: 189, train_loss: 0.96226, time: 0.01104\n",
      "Epoch: 190, train_loss: 0.96305, time: 0.01127\n",
      "Epoch: 191, train_loss: 0.96479, time: 0.01119\n",
      "Epoch: 192, train_loss: 0.96597, time: 0.01098\n",
      "Epoch: 193, train_loss: 0.96601, time: 0.01112\n",
      "Epoch: 194, train_loss: 0.96727, time: 0.01100\n",
      "Epoch: 195, train_loss: 0.96163, time: 0.01157\n",
      "Epoch: 196, train_loss: 0.96098, time: 0.01112\n",
      "Epoch: 197, train_loss: 0.96426, time: 0.01093\n",
      "Epoch: 198, train_loss: 0.96134, time: 0.01122\n",
      "Epoch: 199, train_loss: 0.96454, time: 0.01058\n",
      "Epoch: 200, train_loss: 0.96575, time: 0.01095\n",
      "pairwise precision 0.14392 recall 0.93914 f1 0.24959\n",
      "average until now [0.4349251773776017, 0.8328425796532603, 0.5714362187781934]\n",
      "80 names 332.84475898742676 avg time 4.160559487342835\n",
      "Loading wei_jun_zhang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 200 nodes, 1951 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.95297, time: 0.11037\n",
      "Epoch: 2, train_loss: 0.95537, time: 0.01345\n",
      "Epoch: 3, train_loss: 0.95066, time: 0.01156\n",
      "Epoch: 4, train_loss: 0.95565, time: 0.01116\n",
      "Epoch: 5, train_loss: 0.95269, time: 0.01120\n",
      "Epoch: 6, train_loss: 0.95251, time: 0.01124\n",
      "Epoch: 7, train_loss: 0.95230, time: 0.01100\n",
      "Epoch: 8, train_loss: 0.94846, time: 0.01117\n",
      "Epoch: 9, train_loss: 0.95252, time: 0.01100\n",
      "Epoch: 10, train_loss: 0.95377, time: 0.01108\n",
      "Epoch: 11, train_loss: 0.94972, time: 0.01073\n",
      "Epoch: 12, train_loss: 0.94964, time: 0.01131\n",
      "Epoch: 13, train_loss: 0.94945, time: 0.01081\n",
      "Epoch: 14, train_loss: 0.95061, time: 0.01119\n",
      "Epoch: 15, train_loss: 0.95308, time: 0.01107\n",
      "Epoch: 16, train_loss: 0.95306, time: 0.01128\n",
      "Epoch: 17, train_loss: 0.95232, time: 0.01090\n",
      "Epoch: 18, train_loss: 0.95256, time: 0.01136\n",
      "Epoch: 19, train_loss: 0.95255, time: 0.01095\n",
      "Epoch: 20, train_loss: 0.94792, time: 0.01149\n",
      "Epoch: 21, train_loss: 0.95404, time: 0.01099\n",
      "Epoch: 22, train_loss: 0.95000, time: 0.01123\n",
      "Epoch: 23, train_loss: 0.95086, time: 0.01070\n",
      "Epoch: 24, train_loss: 0.95203, time: 0.01092\n",
      "Epoch: 25, train_loss: 0.95059, time: 0.01089\n",
      "Epoch: 26, train_loss: 0.94764, time: 0.01119\n",
      "Epoch: 27, train_loss: 0.95342, time: 0.01086\n",
      "Epoch: 28, train_loss: 0.95122, time: 0.01100\n",
      "Epoch: 29, train_loss: 0.95139, time: 0.01087\n",
      "Epoch: 30, train_loss: 0.95272, time: 0.01130\n",
      "Epoch: 31, train_loss: 0.95202, time: 0.01106\n",
      "Epoch: 32, train_loss: 0.95123, time: 0.01117\n",
      "Epoch: 33, train_loss: 0.94925, time: 0.01093\n",
      "Epoch: 34, train_loss: 0.95576, time: 0.01129\n",
      "Epoch: 35, train_loss: 0.94736, time: 0.01097\n",
      "Epoch: 36, train_loss: 0.95059, time: 0.01132\n",
      "Epoch: 37, train_loss: 0.94877, time: 0.01080\n",
      "Epoch: 38, train_loss: 0.94972, time: 0.01149\n",
      "Epoch: 39, train_loss: 0.94877, time: 0.01107\n",
      "Epoch: 40, train_loss: 0.95311, time: 0.01119\n",
      "Epoch: 41, train_loss: 0.95287, time: 0.01097\n",
      "Epoch: 42, train_loss: 0.94963, time: 0.01108\n",
      "Epoch: 43, train_loss: 0.94897, time: 0.01090\n",
      "Epoch: 44, train_loss: 0.95201, time: 0.01128\n",
      "Epoch: 45, train_loss: 0.94729, time: 0.01073\n",
      "Epoch: 46, train_loss: 0.94898, time: 0.01117\n",
      "Epoch: 47, train_loss: 0.95049, time: 0.01092\n",
      "Epoch: 48, train_loss: 0.95108, time: 0.01099\n",
      "Epoch: 49, train_loss: 0.94660, time: 0.01075\n",
      "Epoch: 50, train_loss: 0.95304, time: 0.01111\n",
      "Epoch: 51, train_loss: 0.95066, time: 0.01118\n",
      "Epoch: 52, train_loss: 0.95065, time: 0.01114\n",
      "Epoch: 53, train_loss: 0.94636, time: 0.01091\n",
      "Epoch: 54, train_loss: 0.95097, time: 0.01114\n",
      "Epoch: 55, train_loss: 0.95131, time: 0.01086\n",
      "Epoch: 56, train_loss: 0.95035, time: 0.01085\n",
      "Epoch: 57, train_loss: 0.94916, time: 0.01131\n",
      "Epoch: 58, train_loss: 0.94984, time: 0.01138\n",
      "Epoch: 59, train_loss: 0.94931, time: 0.01086\n",
      "Epoch: 60, train_loss: 0.95198, time: 0.01112\n",
      "Epoch: 61, train_loss: 0.95108, time: 0.01093\n",
      "Epoch: 62, train_loss: 0.94945, time: 0.01096\n",
      "Epoch: 63, train_loss: 0.94847, time: 0.01087\n",
      "Epoch: 64, train_loss: 0.94864, time: 0.01113\n",
      "Epoch: 65, train_loss: 0.94951, time: 0.01076\n",
      "Epoch: 66, train_loss: 0.94848, time: 0.01120\n",
      "Epoch: 67, train_loss: 0.94901, time: 0.01095\n",
      "Epoch: 68, train_loss: 0.95071, time: 0.01121\n",
      "Epoch: 69, train_loss: 0.94830, time: 0.01077\n",
      "Epoch: 70, train_loss: 0.94887, time: 0.01107\n",
      "Epoch: 71, train_loss: 0.95129, time: 0.01087\n",
      "Epoch: 72, train_loss: 0.94855, time: 0.01135\n",
      "Epoch: 73, train_loss: 0.95091, time: 0.01087\n",
      "Epoch: 74, train_loss: 0.94693, time: 0.01102\n",
      "Epoch: 75, train_loss: 0.94942, time: 0.01085\n",
      "Epoch: 76, train_loss: 0.95228, time: 0.01123\n",
      "Epoch: 77, train_loss: 0.95180, time: 0.01091\n",
      "Epoch: 78, train_loss: 0.95163, time: 0.01104\n",
      "Epoch: 79, train_loss: 0.95076, time: 0.01071\n",
      "Epoch: 80, train_loss: 0.94857, time: 0.01120\n",
      "Epoch: 81, train_loss: 0.94947, time: 0.01087\n",
      "Epoch: 82, train_loss: 0.95082, time: 0.01105\n",
      "Epoch: 83, train_loss: 0.95234, time: 0.01092\n",
      "Epoch: 84, train_loss: 0.95003, time: 0.01113\n",
      "Epoch: 85, train_loss: 0.95240, time: 0.01079\n",
      "Epoch: 86, train_loss: 0.95180, time: 0.01109\n",
      "Epoch: 87, train_loss: 0.95176, time: 0.01082\n",
      "Epoch: 88, train_loss: 0.94950, time: 0.01081\n",
      "Epoch: 89, train_loss: 0.95104, time: 0.01096\n",
      "Epoch: 90, train_loss: 0.95168, time: 0.01116\n",
      "Epoch: 91, train_loss: 0.95143, time: 0.01081\n",
      "Epoch: 92, train_loss: 0.95229, time: 0.01109\n",
      "Epoch: 93, train_loss: 0.94835, time: 0.01089\n",
      "Epoch: 94, train_loss: 0.95177, time: 0.01105\n",
      "Epoch: 95, train_loss: 0.95163, time: 0.01110\n",
      "Epoch: 96, train_loss: 0.94828, time: 0.01119\n",
      "Epoch: 97, train_loss: 0.95065, time: 0.01092\n",
      "Epoch: 98, train_loss: 0.94850, time: 0.01110\n",
      "Epoch: 99, train_loss: 0.95191, time: 0.01094\n",
      "Epoch: 100, train_loss: 0.95065, time: 0.01097\n",
      "Epoch: 101, train_loss: 0.95012, time: 0.01085\n",
      "Epoch: 102, train_loss: 0.94819, time: 0.01106\n",
      "Epoch: 103, train_loss: 0.94984, time: 0.01079\n",
      "Epoch: 104, train_loss: 0.94851, time: 0.01084\n",
      "Epoch: 105, train_loss: 0.95121, time: 0.01074\n",
      "Epoch: 106, train_loss: 0.95049, time: 0.01130\n",
      "Epoch: 107, train_loss: 0.94778, time: 0.01084\n",
      "Epoch: 108, train_loss: 0.94868, time: 0.01121\n",
      "Epoch: 109, train_loss: 0.95160, time: 0.01088\n",
      "Epoch: 110, train_loss: 0.94990, time: 0.01116\n",
      "Epoch: 111, train_loss: 0.95148, time: 0.01089\n",
      "Epoch: 112, train_loss: 0.95174, time: 0.01102\n",
      "Epoch: 113, train_loss: 0.94959, time: 0.01076\n",
      "Epoch: 114, train_loss: 0.94951, time: 0.01143\n",
      "Epoch: 115, train_loss: 0.94932, time: 0.01099\n",
      "Epoch: 116, train_loss: 0.94868, time: 0.01104\n",
      "Epoch: 117, train_loss: 0.95161, time: 0.01079\n",
      "Epoch: 118, train_loss: 0.95273, time: 0.01093\n",
      "Epoch: 119, train_loss: 0.95032, time: 0.01099\n",
      "Epoch: 120, train_loss: 0.94893, time: 0.01132\n",
      "Epoch: 121, train_loss: 0.94851, time: 0.01099\n",
      "Epoch: 122, train_loss: 0.95025, time: 0.01119\n",
      "Epoch: 123, train_loss: 0.94931, time: 0.01082\n",
      "Epoch: 124, train_loss: 0.94802, time: 0.01118\n",
      "Epoch: 125, train_loss: 0.94958, time: 0.01076\n",
      "Epoch: 126, train_loss: 0.95053, time: 0.01121\n",
      "Epoch: 127, train_loss: 0.94630, time: 0.01102\n",
      "Epoch: 128, train_loss: 0.94980, time: 0.01121\n",
      "Epoch: 129, train_loss: 0.95141, time: 0.01079\n",
      "Epoch: 130, train_loss: 0.95273, time: 0.01130\n",
      "Epoch: 131, train_loss: 0.95167, time: 0.01081\n",
      "Epoch: 132, train_loss: 0.94702, time: 0.01128\n",
      "Epoch: 133, train_loss: 0.94751, time: 0.01158\n",
      "Epoch: 134, train_loss: 0.95051, time: 0.01140\n",
      "Epoch: 135, train_loss: 0.94886, time: 0.01076\n",
      "Epoch: 136, train_loss: 0.95347, time: 0.01119\n",
      "Epoch: 137, train_loss: 0.95073, time: 0.01077\n",
      "Epoch: 138, train_loss: 0.95206, time: 0.01141\n",
      "Epoch: 139, train_loss: 0.95172, time: 0.01086\n",
      "Epoch: 140, train_loss: 0.95086, time: 0.01146\n",
      "Epoch: 141, train_loss: 0.95067, time: 0.01095\n",
      "Epoch: 142, train_loss: 0.94811, time: 0.01111\n",
      "Epoch: 143, train_loss: 0.95105, time: 0.01102\n",
      "Epoch: 144, train_loss: 0.95283, time: 0.01123\n",
      "Epoch: 145, train_loss: 0.95361, time: 0.01085\n",
      "Epoch: 146, train_loss: 0.94802, time: 0.01115\n",
      "Epoch: 147, train_loss: 0.95100, time: 0.01094\n",
      "Epoch: 148, train_loss: 0.95129, time: 0.01126\n",
      "Epoch: 149, train_loss: 0.94932, time: 0.01086\n",
      "Epoch: 150, train_loss: 0.95060, time: 0.01116\n",
      "Epoch: 151, train_loss: 0.94783, time: 0.01115\n",
      "Epoch: 152, train_loss: 0.94702, time: 0.01109\n",
      "Epoch: 153, train_loss: 0.94889, time: 0.01097\n",
      "Epoch: 154, train_loss: 0.95456, time: 0.01095\n",
      "Epoch: 155, train_loss: 0.94878, time: 0.01075\n",
      "Epoch: 156, train_loss: 0.95266, time: 0.01132\n",
      "Epoch: 157, train_loss: 0.95417, time: 0.01098\n",
      "Epoch: 158, train_loss: 0.95066, time: 0.01092\n",
      "Epoch: 159, train_loss: 0.94428, time: 0.01068\n",
      "Epoch: 160, train_loss: 0.94938, time: 0.01100\n",
      "Epoch: 161, train_loss: 0.95148, time: 0.01078\n",
      "Epoch: 162, train_loss: 0.95173, time: 0.01117\n",
      "Epoch: 163, train_loss: 0.94979, time: 0.01083\n",
      "Epoch: 164, train_loss: 0.95295, time: 0.01095\n",
      "Epoch: 165, train_loss: 0.94916, time: 0.01078\n",
      "Epoch: 166, train_loss: 0.95327, time: 0.01125\n",
      "Epoch: 167, train_loss: 0.95115, time: 0.01093\n",
      "Epoch: 168, train_loss: 0.95202, time: 0.01111\n",
      "Epoch: 169, train_loss: 0.95092, time: 0.01063\n",
      "Epoch: 170, train_loss: 0.95016, time: 0.01078\n",
      "Epoch: 171, train_loss: 0.94944, time: 0.01095\n",
      "Epoch: 172, train_loss: 0.95136, time: 0.01139\n",
      "Epoch: 173, train_loss: 0.94919, time: 0.01094\n",
      "Epoch: 174, train_loss: 0.94744, time: 0.01122\n",
      "Epoch: 175, train_loss: 0.94872, time: 0.01071\n",
      "Epoch: 176, train_loss: 0.94846, time: 0.01117\n",
      "Epoch: 177, train_loss: 0.95058, time: 0.01090\n",
      "Epoch: 178, train_loss: 0.94891, time: 0.01130\n",
      "Epoch: 179, train_loss: 0.95006, time: 0.01105\n",
      "Epoch: 180, train_loss: 0.95095, time: 0.01111\n",
      "Epoch: 181, train_loss: 0.94900, time: 0.01080\n",
      "Epoch: 182, train_loss: 0.95344, time: 0.01123\n",
      "Epoch: 183, train_loss: 0.94750, time: 0.01079\n",
      "Epoch: 184, train_loss: 0.94891, time: 0.01125\n",
      "Epoch: 185, train_loss: 0.95165, time: 0.01098\n",
      "Epoch: 186, train_loss: 0.95162, time: 0.01117\n",
      "Epoch: 187, train_loss: 0.95127, time: 0.01100\n",
      "Epoch: 188, train_loss: 0.94934, time: 0.01118\n",
      "Epoch: 189, train_loss: 0.95133, time: 0.01085\n",
      "Epoch: 190, train_loss: 0.94965, time: 0.01115\n",
      "Epoch: 191, train_loss: 0.95225, time: 0.01112\n",
      "Epoch: 192, train_loss: 0.94702, time: 0.01107\n",
      "Epoch: 193, train_loss: 0.95287, time: 0.01099\n",
      "Epoch: 194, train_loss: 0.94969, time: 0.01105\n",
      "Epoch: 195, train_loss: 0.94970, time: 0.01087\n",
      "Epoch: 196, train_loss: 0.95061, time: 0.01119\n",
      "Epoch: 197, train_loss: 0.94940, time: 0.01083\n",
      "Epoch: 198, train_loss: 0.95134, time: 0.01116\n",
      "Epoch: 199, train_loss: 0.95052, time: 0.01096\n",
      "Epoch: 200, train_loss: 0.94974, time: 0.01110\n",
      "pairwise precision 0.45383 recall 0.98137 f1 0.62064\n",
      "average until now [0.4351585408206457, 0.8346762843207829, 0.5720689128244326]\n",
      "81 names 335.20414638519287 avg time 4.138322794878924\n",
      "Loading jiamo_fu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 364 nodes, 25214 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.80865, time: 0.13255\n",
      "Epoch: 2, train_loss: 0.80828, time: 0.03391\n",
      "Epoch: 3, train_loss: 0.80756, time: 0.03423\n",
      "Epoch: 4, train_loss: 0.80865, time: 0.03394\n",
      "Epoch: 5, train_loss: 0.80795, time: 0.03505\n",
      "Epoch: 6, train_loss: 0.80812, time: 0.03383\n",
      "Epoch: 7, train_loss: 0.80966, time: 0.03332\n",
      "Epoch: 8, train_loss: 0.80754, time: 0.03272\n",
      "Epoch: 9, train_loss: 0.80834, time: 0.03471\n",
      "Epoch: 10, train_loss: 0.80870, time: 0.03452\n",
      "Epoch: 11, train_loss: 0.80908, time: 0.03416\n",
      "Epoch: 12, train_loss: 0.80751, time: 0.03326\n",
      "Epoch: 13, train_loss: 0.80731, time: 0.03358\n",
      "Epoch: 14, train_loss: 0.80911, time: 0.03320\n",
      "Epoch: 15, train_loss: 0.80637, time: 0.03357\n",
      "Epoch: 16, train_loss: 0.80953, time: 0.03411\n",
      "Epoch: 17, train_loss: 0.80749, time: 0.03364\n",
      "Epoch: 18, train_loss: 0.80824, time: 0.03269\n",
      "Epoch: 19, train_loss: 0.80809, time: 0.03386\n",
      "Epoch: 20, train_loss: 0.80823, time: 0.03446\n",
      "Epoch: 21, train_loss: 0.80555, time: 0.03466\n",
      "Epoch: 22, train_loss: 0.80302, time: 0.03358\n",
      "Epoch: 23, train_loss: 0.80525, time: 0.03379\n",
      "Epoch: 24, train_loss: 0.80358, time: 0.03261\n",
      "Epoch: 25, train_loss: 0.80550, time: 0.03453\n",
      "Epoch: 26, train_loss: 0.80284, time: 0.03298\n",
      "Epoch: 27, train_loss: 0.80588, time: 0.03365\n",
      "Epoch: 28, train_loss: 0.80047, time: 0.03288\n",
      "Epoch: 29, train_loss: 0.79904, time: 0.03384\n",
      "Epoch: 30, train_loss: 0.79849, time: 0.03197\n",
      "Epoch: 31, train_loss: 0.79632, time: 0.03368\n",
      "Epoch: 32, train_loss: 0.79833, time: 0.03252\n",
      "Epoch: 33, train_loss: 0.79585, time: 0.03331\n",
      "Epoch: 34, train_loss: 0.79672, time: 0.03290\n",
      "Epoch: 35, train_loss: 0.80260, time: 0.03350\n",
      "Epoch: 36, train_loss: 0.80071, time: 0.03333\n",
      "Epoch: 37, train_loss: 0.79088, time: 0.03218\n",
      "Epoch: 38, train_loss: 0.79992, time: 0.03190\n",
      "Epoch: 39, train_loss: 0.78790, time: 0.03294\n",
      "Epoch: 40, train_loss: 0.79273, time: 0.03326\n",
      "Epoch: 41, train_loss: 0.79697, time: 0.03314\n",
      "Epoch: 42, train_loss: 0.79665, time: 0.03263\n",
      "Epoch: 43, train_loss: 0.80083, time: 0.03323\n",
      "Epoch: 44, train_loss: 0.79641, time: 0.03280\n",
      "Epoch: 45, train_loss: 0.79941, time: 0.03150\n",
      "Epoch: 46, train_loss: 0.80202, time: 0.03165\n",
      "Epoch: 47, train_loss: 0.79557, time: 0.03294\n",
      "Epoch: 48, train_loss: 0.79901, time: 0.03464\n",
      "Epoch: 49, train_loss: 0.79396, time: 0.03214\n",
      "Epoch: 50, train_loss: 0.78786, time: 0.03361\n",
      "Epoch: 51, train_loss: 0.80928, time: 0.03160\n",
      "Epoch: 52, train_loss: 0.79514, time: 0.03315\n",
      "Epoch: 53, train_loss: 0.79467, time: 0.03196\n",
      "Epoch: 54, train_loss: 0.79691, time: 0.03345\n",
      "Epoch: 55, train_loss: 0.79509, time: 0.03380\n",
      "Epoch: 56, train_loss: 0.79314, time: 0.03252\n",
      "Epoch: 57, train_loss: 0.79191, time: 0.03259\n",
      "Epoch: 58, train_loss: 0.79510, time: 0.03153\n",
      "Epoch: 59, train_loss: 0.79941, time: 0.03323\n",
      "Epoch: 60, train_loss: 0.79276, time: 0.03247\n",
      "Epoch: 61, train_loss: 0.78987, time: 0.03260\n",
      "Epoch: 62, train_loss: 0.79844, time: 0.03418\n",
      "Epoch: 63, train_loss: 0.79997, time: 0.03290\n",
      "Epoch: 64, train_loss: 0.78740, time: 0.03350\n",
      "Epoch: 65, train_loss: 0.79413, time: 0.03302\n",
      "Epoch: 66, train_loss: 0.79462, time: 0.03304\n",
      "Epoch: 67, train_loss: 0.79755, time: 0.03313\n",
      "Epoch: 68, train_loss: 0.78440, time: 0.03409\n",
      "Epoch: 69, train_loss: 0.79951, time: 0.03306\n",
      "Epoch: 70, train_loss: 0.79124, time: 0.03238\n",
      "Epoch: 71, train_loss: 0.79843, time: 0.03314\n",
      "Epoch: 72, train_loss: 0.79888, time: 0.03400\n",
      "Epoch: 73, train_loss: 0.79271, time: 0.03260\n",
      "Epoch: 74, train_loss: 0.79542, time: 0.03394\n",
      "Epoch: 75, train_loss: 0.79689, time: 0.03479\n",
      "Epoch: 76, train_loss: 0.79259, time: 0.03292\n",
      "Epoch: 77, train_loss: 0.79356, time: 0.03325\n",
      "Epoch: 78, train_loss: 0.79391, time: 0.03325\n",
      "Epoch: 79, train_loss: 0.79365, time: 0.03279\n",
      "Epoch: 80, train_loss: 0.79327, time: 0.03228\n",
      "Epoch: 81, train_loss: 0.79719, time: 0.03205\n",
      "Epoch: 82, train_loss: 0.78965, time: 0.03255\n",
      "Epoch: 83, train_loss: 0.78619, time: 0.03272\n",
      "Epoch: 84, train_loss: 0.80077, time: 0.03268\n",
      "Epoch: 85, train_loss: 0.78684, time: 0.03317\n",
      "Epoch: 86, train_loss: 0.79458, time: 0.03316\n",
      "Epoch: 87, train_loss: 0.79428, time: 0.03487\n",
      "Epoch: 88, train_loss: 0.79216, time: 0.03474\n",
      "Epoch: 89, train_loss: 0.78797, time: 0.03378\n",
      "Epoch: 90, train_loss: 0.79643, time: 0.03201\n",
      "Epoch: 91, train_loss: 0.79478, time: 0.03444\n",
      "Epoch: 92, train_loss: 0.79274, time: 0.03394\n",
      "Epoch: 93, train_loss: 0.78798, time: 0.03331\n",
      "Epoch: 94, train_loss: 0.79212, time: 0.03234\n",
      "Epoch: 95, train_loss: 0.79887, time: 0.03263\n",
      "Epoch: 96, train_loss: 0.79812, time: 0.03339\n",
      "Epoch: 97, train_loss: 0.78833, time: 0.03376\n",
      "Epoch: 98, train_loss: 0.79081, time: 0.03331\n",
      "Epoch: 99, train_loss: 0.78817, time: 0.03298\n",
      "Epoch: 100, train_loss: 0.79454, time: 0.03298\n",
      "Epoch: 101, train_loss: 0.79379, time: 0.03243\n",
      "Epoch: 102, train_loss: 0.79164, time: 0.03432\n",
      "Epoch: 103, train_loss: 0.79823, time: 0.03401\n",
      "Epoch: 104, train_loss: 0.79341, time: 0.03436\n",
      "Epoch: 105, train_loss: 0.78860, time: 0.03267\n",
      "Epoch: 106, train_loss: 0.78603, time: 0.03210\n",
      "Epoch: 107, train_loss: 0.79170, time: 0.03247\n",
      "Epoch: 108, train_loss: 0.79751, time: 0.03403\n",
      "Epoch: 109, train_loss: 0.78511, time: 0.03339\n",
      "Epoch: 110, train_loss: 0.79172, time: 0.03217\n",
      "Epoch: 111, train_loss: 0.79645, time: 0.03323\n",
      "Epoch: 112, train_loss: 0.79084, time: 0.03171\n",
      "Epoch: 113, train_loss: 0.79280, time: 0.03242\n",
      "Epoch: 114, train_loss: 0.78874, time: 0.03414\n",
      "Epoch: 115, train_loss: 0.79090, time: 0.03269\n",
      "Epoch: 116, train_loss: 0.78767, time: 0.03132\n",
      "Epoch: 117, train_loss: 0.79675, time: 0.03427\n",
      "Epoch: 118, train_loss: 0.80141, time: 0.03474\n",
      "Epoch: 119, train_loss: 0.79028, time: 0.03352\n",
      "Epoch: 120, train_loss: 0.79130, time: 0.03312\n",
      "Epoch: 121, train_loss: 0.78212, time: 0.03382\n",
      "Epoch: 122, train_loss: 0.78703, time: 0.03274\n",
      "Epoch: 123, train_loss: 0.78609, time: 0.03326\n",
      "Epoch: 124, train_loss: 0.79045, time: 0.03320\n",
      "Epoch: 125, train_loss: 0.79449, time: 0.03538\n",
      "Epoch: 126, train_loss: 0.79255, time: 0.03333\n",
      "Epoch: 127, train_loss: 0.79257, time: 0.03328\n",
      "Epoch: 128, train_loss: 0.79187, time: 0.03480\n",
      "Epoch: 129, train_loss: 0.79081, time: 0.03383\n",
      "Epoch: 130, train_loss: 0.78668, time: 0.03263\n",
      "Epoch: 131, train_loss: 0.78962, time: 0.03335\n",
      "Epoch: 132, train_loss: 0.79337, time: 0.03246\n",
      "Epoch: 133, train_loss: 0.80295, time: 0.03206\n",
      "Epoch: 134, train_loss: 0.78436, time: 0.03253\n",
      "Epoch: 135, train_loss: 0.79950, time: 0.03351\n",
      "Epoch: 136, train_loss: 0.78611, time: 0.03296\n",
      "Epoch: 137, train_loss: 0.79171, time: 0.03210\n",
      "Epoch: 138, train_loss: 0.78792, time: 0.03122\n",
      "Epoch: 139, train_loss: 0.79595, time: 0.03175\n",
      "Epoch: 140, train_loss: 0.79288, time: 0.03248\n",
      "Epoch: 141, train_loss: 0.80045, time: 0.03311\n",
      "Epoch: 142, train_loss: 0.80510, time: 0.03195\n",
      "Epoch: 143, train_loss: 0.78570, time: 0.03233\n",
      "Epoch: 144, train_loss: 0.80528, time: 0.03268\n",
      "Epoch: 145, train_loss: 0.80405, time: 0.03325\n",
      "Epoch: 146, train_loss: 0.78574, time: 0.03333\n",
      "Epoch: 147, train_loss: 0.78413, time: 0.03394\n",
      "Epoch: 148, train_loss: 0.79118, time: 0.03317\n",
      "Epoch: 149, train_loss: 0.78971, time: 0.03429\n",
      "Epoch: 150, train_loss: 0.79116, time: 0.03472\n",
      "Epoch: 151, train_loss: 0.78750, time: 0.03484\n",
      "Epoch: 152, train_loss: 0.78392, time: 0.03341\n",
      "Epoch: 153, train_loss: 0.79341, time: 0.03380\n",
      "Epoch: 154, train_loss: 0.79590, time: 0.03465\n",
      "Epoch: 155, train_loss: 0.78317, time: 0.03275\n",
      "Epoch: 156, train_loss: 0.78989, time: 0.03357\n",
      "Epoch: 157, train_loss: 0.78472, time: 0.03379\n",
      "Epoch: 158, train_loss: 0.78462, time: 0.03220\n",
      "Epoch: 159, train_loss: 0.79638, time: 0.03364\n",
      "Epoch: 160, train_loss: 0.79351, time: 0.03362\n",
      "Epoch: 161, train_loss: 0.79028, time: 0.03400\n",
      "Epoch: 162, train_loss: 0.79066, time: 0.03540\n",
      "Epoch: 163, train_loss: 0.79018, time: 0.03478\n",
      "Epoch: 164, train_loss: 0.79304, time: 0.03234\n",
      "Epoch: 165, train_loss: 0.80649, time: 0.03350\n",
      "Epoch: 166, train_loss: 0.79380, time: 0.03398\n",
      "Epoch: 167, train_loss: 0.78599, time: 0.03177\n",
      "Epoch: 168, train_loss: 0.79146, time: 0.03287\n",
      "Epoch: 169, train_loss: 0.79281, time: 0.03359\n",
      "Epoch: 170, train_loss: 0.78965, time: 0.03194\n",
      "Epoch: 171, train_loss: 0.79791, time: 0.03264\n",
      "Epoch: 172, train_loss: 0.78706, time: 0.03232\n",
      "Epoch: 173, train_loss: 0.79077, time: 0.03285\n",
      "Epoch: 174, train_loss: 0.78756, time: 0.03184\n",
      "Epoch: 175, train_loss: 0.79370, time: 0.03249\n",
      "Epoch: 176, train_loss: 0.80031, time: 0.03343\n",
      "Epoch: 177, train_loss: 0.79032, time: 0.03465\n",
      "Epoch: 178, train_loss: 0.80091, time: 0.03339\n",
      "Epoch: 179, train_loss: 0.80723, time: 0.03424\n",
      "Epoch: 180, train_loss: 0.78753, time: 0.03366\n",
      "Epoch: 181, train_loss: 0.79994, time: 0.03361\n",
      "Epoch: 182, train_loss: 0.78776, time: 0.03366\n",
      "Epoch: 183, train_loss: 0.79282, time: 0.03332\n",
      "Epoch: 184, train_loss: 0.78216, time: 0.03260\n",
      "Epoch: 185, train_loss: 0.79811, time: 0.03222\n",
      "Epoch: 186, train_loss: 0.78886, time: 0.03372\n",
      "Epoch: 187, train_loss: 0.79719, time: 0.03402\n",
      "Epoch: 188, train_loss: 0.79184, time: 0.03226\n",
      "Epoch: 189, train_loss: 0.78973, time: 0.03246\n",
      "Epoch: 190, train_loss: 0.79538, time: 0.03307\n",
      "Epoch: 191, train_loss: 0.78874, time: 0.03268\n",
      "Epoch: 192, train_loss: 0.78550, time: 0.03320\n",
      "Epoch: 193, train_loss: 0.78764, time: 0.03287\n",
      "Epoch: 194, train_loss: 0.79625, time: 0.03251\n",
      "Epoch: 195, train_loss: 0.79539, time: 0.03348\n",
      "Epoch: 196, train_loss: 0.80579, time: 0.03281\n",
      "Epoch: 197, train_loss: 0.78834, time: 0.03361\n",
      "Epoch: 198, train_loss: 0.79532, time: 0.03313\n",
      "Epoch: 199, train_loss: 0.78765, time: 0.03358\n",
      "Epoch: 200, train_loss: 0.78590, time: 0.03293\n",
      "pairwise precision 0.97311 recall 0.49999 f1 0.66058\n",
      "average until now [0.4417189202112404, 0.830594771346646, 0.576727937405286]\n",
      "82 names 342.0749144554138 avg time 4.171645298236754\n",
      "Loading hong_fan dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 366 nodes, 3496 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.97935, time: 0.11388\n",
      "Epoch: 2, train_loss: 0.97468, time: 0.01715\n",
      "Epoch: 3, train_loss: 0.97620, time: 0.01595\n",
      "Epoch: 4, train_loss: 0.97268, time: 0.01630\n",
      "Epoch: 5, train_loss: 0.97226, time: 0.01577\n",
      "Epoch: 6, train_loss: 0.97274, time: 0.01539\n",
      "Epoch: 7, train_loss: 0.97249, time: 0.01549\n",
      "Epoch: 8, train_loss: 0.97244, time: 0.01515\n",
      "Epoch: 9, train_loss: 0.97378, time: 0.01550\n",
      "Epoch: 10, train_loss: 0.97277, time: 0.01587\n",
      "Epoch: 11, train_loss: 0.97313, time: 0.01580\n",
      "Epoch: 12, train_loss: 0.97251, time: 0.01548\n",
      "Epoch: 13, train_loss: 0.97168, time: 0.01579\n",
      "Epoch: 14, train_loss: 0.97282, time: 0.01575\n",
      "Epoch: 15, train_loss: 0.97122, time: 0.01596\n",
      "Epoch: 16, train_loss: 0.97433, time: 0.01611\n",
      "Epoch: 17, train_loss: 0.97270, time: 0.01560\n",
      "Epoch: 18, train_loss: 0.97263, time: 0.01565\n",
      "Epoch: 19, train_loss: 0.97367, time: 0.01595\n",
      "Epoch: 20, train_loss: 0.97138, time: 0.01603\n",
      "Epoch: 21, train_loss: 0.97389, time: 0.01602\n",
      "Epoch: 22, train_loss: 0.97354, time: 0.01584\n",
      "Epoch: 23, train_loss: 0.97628, time: 0.01576\n",
      "Epoch: 24, train_loss: 0.97233, time: 0.01572\n",
      "Epoch: 25, train_loss: 0.97166, time: 0.01584\n",
      "Epoch: 26, train_loss: 0.97214, time: 0.01502\n",
      "Epoch: 27, train_loss: 0.97031, time: 0.01552\n",
      "Epoch: 28, train_loss: 0.97373, time: 0.01581\n",
      "Epoch: 29, train_loss: 0.97313, time: 0.01474\n",
      "Epoch: 30, train_loss: 0.97275, time: 0.01603\n",
      "Epoch: 31, train_loss: 0.97494, time: 0.01511\n",
      "Epoch: 32, train_loss: 0.97028, time: 0.01616\n",
      "Epoch: 33, train_loss: 0.97181, time: 0.01566\n",
      "Epoch: 34, train_loss: 0.97234, time: 0.01623\n",
      "Epoch: 35, train_loss: 0.97086, time: 0.01524\n",
      "Epoch: 36, train_loss: 0.97222, time: 0.01512\n",
      "Epoch: 37, train_loss: 0.97198, time: 0.01522\n",
      "Epoch: 38, train_loss: 0.97226, time: 0.01496\n",
      "Epoch: 39, train_loss: 0.97274, time: 0.01482\n",
      "Epoch: 40, train_loss: 0.97264, time: 0.01465\n",
      "Epoch: 41, train_loss: 0.97311, time: 0.01492\n",
      "Epoch: 42, train_loss: 0.97262, time: 0.01515\n",
      "Epoch: 43, train_loss: 0.97321, time: 0.01508\n",
      "Epoch: 44, train_loss: 0.97110, time: 0.01478\n",
      "Epoch: 45, train_loss: 0.97251, time: 0.01508\n",
      "Epoch: 46, train_loss: 0.97207, time: 0.01564\n",
      "Epoch: 47, train_loss: 0.97336, time: 0.01585\n",
      "Epoch: 48, train_loss: 0.97223, time: 0.01565\n",
      "Epoch: 49, train_loss: 0.97348, time: 0.01464\n",
      "Epoch: 50, train_loss: 0.97465, time: 0.01572\n",
      "Epoch: 51, train_loss: 0.97301, time: 0.01570\n",
      "Epoch: 52, train_loss: 0.97209, time: 0.01546\n",
      "Epoch: 53, train_loss: 0.97203, time: 0.01557\n",
      "Epoch: 54, train_loss: 0.97153, time: 0.01540\n",
      "Epoch: 55, train_loss: 0.97166, time: 0.01498\n",
      "Epoch: 56, train_loss: 0.97192, time: 0.01606\n",
      "Epoch: 57, train_loss: 0.97330, time: 0.01538\n",
      "Epoch: 58, train_loss: 0.97128, time: 0.01524\n",
      "Epoch: 59, train_loss: 0.97416, time: 0.01610\n",
      "Epoch: 60, train_loss: 0.97082, time: 0.01561\n",
      "Epoch: 61, train_loss: 0.97081, time: 0.01585\n",
      "Epoch: 62, train_loss: 0.97116, time: 0.01560\n",
      "Epoch: 63, train_loss: 0.97161, time: 0.01541\n",
      "Epoch: 64, train_loss: 0.97246, time: 0.01537\n",
      "Epoch: 65, train_loss: 0.97129, time: 0.01507\n",
      "Epoch: 66, train_loss: 0.97327, time: 0.01516\n",
      "Epoch: 67, train_loss: 0.97197, time: 0.01709\n",
      "Epoch: 68, train_loss: 0.97160, time: 0.01571\n",
      "Epoch: 69, train_loss: 0.97131, time: 0.01541\n",
      "Epoch: 70, train_loss: 0.97340, time: 0.01525\n",
      "Epoch: 71, train_loss: 0.97363, time: 0.01509\n",
      "Epoch: 72, train_loss: 0.97099, time: 0.01576\n",
      "Epoch: 73, train_loss: 0.97338, time: 0.01558\n",
      "Epoch: 74, train_loss: 0.97297, time: 0.01557\n",
      "Epoch: 75, train_loss: 0.97173, time: 0.01599\n",
      "Epoch: 76, train_loss: 0.97199, time: 0.01536\n",
      "Epoch: 77, train_loss: 0.97213, time: 0.01556\n",
      "Epoch: 78, train_loss: 0.97216, time: 0.01565\n",
      "Epoch: 79, train_loss: 0.97369, time: 0.01571\n",
      "Epoch: 80, train_loss: 0.97025, time: 0.01536\n",
      "Epoch: 81, train_loss: 0.97097, time: 0.01764\n",
      "Epoch: 82, train_loss: 0.97016, time: 0.01666\n",
      "Epoch: 83, train_loss: 0.97215, time: 0.01622\n",
      "Epoch: 84, train_loss: 0.97264, time: 0.01570\n",
      "Epoch: 85, train_loss: 0.97200, time: 0.01579\n",
      "Epoch: 86, train_loss: 0.97237, time: 0.01556\n",
      "Epoch: 87, train_loss: 0.97328, time: 0.01534\n",
      "Epoch: 88, train_loss: 0.97174, time: 0.01552\n",
      "Epoch: 89, train_loss: 0.97238, time: 0.01524\n",
      "Epoch: 90, train_loss: 0.97168, time: 0.01549\n",
      "Epoch: 91, train_loss: 0.97178, time: 0.01522\n",
      "Epoch: 92, train_loss: 0.97252, time: 0.01528\n",
      "Epoch: 93, train_loss: 0.97257, time: 0.01561\n",
      "Epoch: 94, train_loss: 0.97290, time: 0.01517\n",
      "Epoch: 95, train_loss: 0.97351, time: 0.01643\n",
      "Epoch: 96, train_loss: 0.97277, time: 0.01522\n",
      "Epoch: 97, train_loss: 0.97178, time: 0.01560\n",
      "Epoch: 98, train_loss: 0.97165, time: 0.01516\n",
      "Epoch: 99, train_loss: 0.97376, time: 0.01551\n",
      "Epoch: 100, train_loss: 0.97232, time: 0.01572\n",
      "Epoch: 101, train_loss: 0.97108, time: 0.01589\n",
      "Epoch: 102, train_loss: 0.97269, time: 0.01528\n",
      "Epoch: 103, train_loss: 0.97025, time: 0.01525\n",
      "Epoch: 104, train_loss: 0.97090, time: 0.01545\n",
      "Epoch: 105, train_loss: 0.97113, time: 0.01490\n",
      "Epoch: 106, train_loss: 0.97290, time: 0.01437\n",
      "Epoch: 107, train_loss: 0.97109, time: 0.01512\n",
      "Epoch: 108, train_loss: 0.97335, time: 0.01560\n",
      "Epoch: 109, train_loss: 0.97398, time: 0.01582\n",
      "Epoch: 110, train_loss: 0.97099, time: 0.01672\n",
      "Epoch: 111, train_loss: 0.97132, time: 0.01589\n",
      "Epoch: 112, train_loss: 0.97173, time: 0.01604\n",
      "Epoch: 113, train_loss: 0.97243, time: 0.01530\n",
      "Epoch: 114, train_loss: 0.97162, time: 0.01548\n",
      "Epoch: 115, train_loss: 0.97311, time: 0.01504\n",
      "Epoch: 116, train_loss: 0.97020, time: 0.01583\n",
      "Epoch: 117, train_loss: 0.97281, time: 0.01565\n",
      "Epoch: 118, train_loss: 0.97137, time: 0.01531\n",
      "Epoch: 119, train_loss: 0.97216, time: 0.01530\n",
      "Epoch: 120, train_loss: 0.97447, time: 0.01537\n",
      "Epoch: 121, train_loss: 0.97248, time: 0.01564\n",
      "Epoch: 122, train_loss: 0.97326, time: 0.01547\n",
      "Epoch: 123, train_loss: 0.97151, time: 0.01520\n",
      "Epoch: 124, train_loss: 0.97086, time: 0.01565\n",
      "Epoch: 125, train_loss: 0.97118, time: 0.01551\n",
      "Epoch: 126, train_loss: 0.97190, time: 0.01542\n",
      "Epoch: 127, train_loss: 0.97285, time: 0.01484\n",
      "Epoch: 128, train_loss: 0.97252, time: 0.01559\n",
      "Epoch: 129, train_loss: 0.97318, time: 0.01544\n",
      "Epoch: 130, train_loss: 0.97417, time: 0.01517\n",
      "Epoch: 131, train_loss: 0.97211, time: 0.01587\n",
      "Epoch: 132, train_loss: 0.97057, time: 0.01514\n",
      "Epoch: 133, train_loss: 0.97331, time: 0.01607\n",
      "Epoch: 134, train_loss: 0.97286, time: 0.01562\n",
      "Epoch: 135, train_loss: 0.97176, time: 0.01545\n",
      "Epoch: 136, train_loss: 0.97392, time: 0.01539\n",
      "Epoch: 137, train_loss: 0.97119, time: 0.01576\n",
      "Epoch: 138, train_loss: 0.97200, time: 0.01630\n",
      "Epoch: 139, train_loss: 0.97088, time: 0.01581\n",
      "Epoch: 140, train_loss: 0.97248, time: 0.01548\n",
      "Epoch: 141, train_loss: 0.97314, time: 0.01580\n",
      "Epoch: 142, train_loss: 0.97261, time: 0.01556\n",
      "Epoch: 143, train_loss: 0.97248, time: 0.01587\n",
      "Epoch: 144, train_loss: 0.97287, time: 0.01536\n",
      "Epoch: 145, train_loss: 0.97290, time: 0.01541\n",
      "Epoch: 146, train_loss: 0.97287, time: 0.01509\n",
      "Epoch: 147, train_loss: 0.97218, time: 0.01474\n",
      "Epoch: 148, train_loss: 0.97297, time: 0.01599\n",
      "Epoch: 149, train_loss: 0.97215, time: 0.01599\n",
      "Epoch: 150, train_loss: 0.97046, time: 0.01545\n",
      "Epoch: 151, train_loss: 0.97393, time: 0.01553\n",
      "Epoch: 152, train_loss: 0.97243, time: 0.01564\n",
      "Epoch: 153, train_loss: 0.97339, time: 0.01593\n",
      "Epoch: 154, train_loss: 0.97168, time: 0.01595\n",
      "Epoch: 155, train_loss: 0.97253, time: 0.01549\n",
      "Epoch: 156, train_loss: 0.97372, time: 0.01559\n",
      "Epoch: 157, train_loss: 0.97162, time: 0.01617\n",
      "Epoch: 158, train_loss: 0.97242, time: 0.01553\n",
      "Epoch: 159, train_loss: 0.97187, time: 0.01561\n",
      "Epoch: 160, train_loss: 0.97407, time: 0.01513\n",
      "Epoch: 161, train_loss: 0.97084, time: 0.01513\n",
      "Epoch: 162, train_loss: 0.97120, time: 0.01552\n",
      "Epoch: 163, train_loss: 0.97214, time: 0.01552\n",
      "Epoch: 164, train_loss: 0.97065, time: 0.01541\n",
      "Epoch: 165, train_loss: 0.97309, time: 0.01556\n",
      "Epoch: 166, train_loss: 0.97147, time: 0.01574\n",
      "Epoch: 167, train_loss: 0.97183, time: 0.01567\n",
      "Epoch: 168, train_loss: 0.97250, time: 0.01496\n",
      "Epoch: 169, train_loss: 0.97180, time: 0.01560\n",
      "Epoch: 170, train_loss: 0.97320, time: 0.01567\n",
      "Epoch: 171, train_loss: 0.97081, time: 0.01550\n",
      "Epoch: 172, train_loss: 0.97323, time: 0.01494\n",
      "Epoch: 173, train_loss: 0.96986, time: 0.01505\n",
      "Epoch: 174, train_loss: 0.97212, time: 0.01572\n",
      "Epoch: 175, train_loss: 0.97592, time: 0.01563\n",
      "Epoch: 176, train_loss: 0.97114, time: 0.01603\n",
      "Epoch: 177, train_loss: 0.97190, time: 0.01569\n",
      "Epoch: 178, train_loss: 0.97161, time: 0.01601\n",
      "Epoch: 179, train_loss: 0.97208, time: 0.01485\n",
      "Epoch: 180, train_loss: 0.97245, time: 0.01440\n",
      "Epoch: 181, train_loss: 0.97139, time: 0.01476\n",
      "Epoch: 182, train_loss: 0.97155, time: 0.01460\n",
      "Epoch: 183, train_loss: 0.97230, time: 0.01552\n",
      "Epoch: 184, train_loss: 0.97170, time: 0.01521\n",
      "Epoch: 185, train_loss: 0.97140, time: 0.01498\n",
      "Epoch: 186, train_loss: 0.97336, time: 0.01501\n",
      "Epoch: 187, train_loss: 0.97260, time: 0.01537\n",
      "Epoch: 188, train_loss: 0.97050, time: 0.01549\n",
      "Epoch: 189, train_loss: 0.97237, time: 0.01556\n",
      "Epoch: 190, train_loss: 0.97208, time: 0.01562\n",
      "Epoch: 191, train_loss: 0.97131, time: 0.01523\n",
      "Epoch: 192, train_loss: 0.97214, time: 0.01553\n",
      "Epoch: 193, train_loss: 0.97162, time: 0.01512\n",
      "Epoch: 194, train_loss: 0.97100, time: 0.01543\n",
      "Epoch: 195, train_loss: 0.97195, time: 0.01561\n",
      "Epoch: 196, train_loss: 0.97160, time: 0.01546\n",
      "Epoch: 197, train_loss: 0.97188, time: 0.01539\n",
      "Epoch: 198, train_loss: 0.97363, time: 0.01530\n",
      "Epoch: 199, train_loss: 0.97241, time: 0.01534\n",
      "Epoch: 200, train_loss: 0.97481, time: 0.01565\n",
      "pairwise precision 0.11497 recall 0.91762 f1 0.20434\n",
      "average until now [0.43778217990712276, 0.8316432625668015, 0.573611632806146]\n",
      "83 names 345.3655354976654 avg time 4.161030548164644\n",
      "Loading h_xie dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 105 nodes, 308 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98345, time: 0.10695\n",
      "Epoch: 2, train_loss: 0.97980, time: 0.00993\n",
      "Epoch: 3, train_loss: 0.97064, time: 0.00912\n",
      "Epoch: 4, train_loss: 0.97906, time: 0.00877\n",
      "Epoch: 5, train_loss: 0.98156, time: 0.00875\n",
      "Epoch: 6, train_loss: 0.97601, time: 0.00862\n",
      "Epoch: 7, train_loss: 0.97707, time: 0.00862\n",
      "Epoch: 8, train_loss: 0.98043, time: 0.00857\n",
      "Epoch: 9, train_loss: 0.97590, time: 0.00854\n",
      "Epoch: 10, train_loss: 0.96988, time: 0.00853\n",
      "Epoch: 11, train_loss: 0.97499, time: 0.00883\n",
      "Epoch: 12, train_loss: 0.97539, time: 0.00867\n",
      "Epoch: 13, train_loss: 0.97673, time: 0.00862\n",
      "Epoch: 14, train_loss: 0.97253, time: 0.00849\n",
      "Epoch: 15, train_loss: 0.97553, time: 0.00846\n",
      "Epoch: 16, train_loss: 0.96903, time: 0.00846\n",
      "Epoch: 17, train_loss: 0.97222, time: 0.00849\n",
      "Epoch: 18, train_loss: 0.97624, time: 0.00850\n",
      "Epoch: 19, train_loss: 0.97802, time: 0.00845\n",
      "Epoch: 20, train_loss: 0.97312, time: 0.00853\n",
      "Epoch: 21, train_loss: 0.96939, time: 0.00859\n",
      "Epoch: 22, train_loss: 0.97350, time: 0.00853\n",
      "Epoch: 23, train_loss: 0.97362, time: 0.00856\n",
      "Epoch: 24, train_loss: 0.97826, time: 0.00861\n",
      "Epoch: 25, train_loss: 0.97157, time: 0.00855\n",
      "Epoch: 26, train_loss: 0.96908, time: 0.00852\n",
      "Epoch: 27, train_loss: 0.98342, time: 0.00852\n",
      "Epoch: 28, train_loss: 0.97390, time: 0.00854\n",
      "Epoch: 29, train_loss: 0.97796, time: 0.00852\n",
      "Epoch: 30, train_loss: 0.97075, time: 0.00847\n",
      "Epoch: 31, train_loss: 0.97508, time: 0.00845\n",
      "Epoch: 32, train_loss: 0.97405, time: 0.00853\n",
      "Epoch: 33, train_loss: 0.97501, time: 0.00849\n",
      "Epoch: 34, train_loss: 0.97357, time: 0.00859\n",
      "Epoch: 35, train_loss: 0.97196, time: 0.00907\n",
      "Epoch: 36, train_loss: 0.97653, time: 0.00872\n",
      "Epoch: 37, train_loss: 0.97734, time: 0.00853\n",
      "Epoch: 38, train_loss: 0.97360, time: 0.00857\n",
      "Epoch: 39, train_loss: 0.97445, time: 0.00859\n",
      "Epoch: 40, train_loss: 0.97824, time: 0.00855\n",
      "Epoch: 41, train_loss: 0.97351, time: 0.00857\n",
      "Epoch: 42, train_loss: 0.97571, time: 0.00858\n",
      "Epoch: 43, train_loss: 0.97680, time: 0.00859\n",
      "Epoch: 44, train_loss: 0.97412, time: 0.00852\n",
      "Epoch: 45, train_loss: 0.97467, time: 0.00856\n",
      "Epoch: 46, train_loss: 0.97707, time: 0.00858\n",
      "Epoch: 47, train_loss: 0.97083, time: 0.00858\n",
      "Epoch: 48, train_loss: 0.97331, time: 0.00860\n",
      "Epoch: 49, train_loss: 0.97605, time: 0.00858\n",
      "Epoch: 50, train_loss: 0.97371, time: 0.00862\n",
      "Epoch: 51, train_loss: 0.97531, time: 0.00856\n",
      "Epoch: 52, train_loss: 0.97179, time: 0.00852\n",
      "Epoch: 53, train_loss: 0.97757, time: 0.00854\n",
      "Epoch: 54, train_loss: 0.97727, time: 0.00856\n",
      "Epoch: 55, train_loss: 0.97413, time: 0.00857\n",
      "Epoch: 56, train_loss: 0.97441, time: 0.00851\n",
      "Epoch: 57, train_loss: 0.97881, time: 0.00851\n",
      "Epoch: 58, train_loss: 0.97467, time: 0.00863\n",
      "Epoch: 59, train_loss: 0.97497, time: 0.00907\n",
      "Epoch: 60, train_loss: 0.97603, time: 0.00872\n",
      "Epoch: 61, train_loss: 0.97189, time: 0.00855\n",
      "Epoch: 62, train_loss: 0.97507, time: 0.00849\n",
      "Epoch: 63, train_loss: 0.97531, time: 0.00856\n",
      "Epoch: 64, train_loss: 0.97585, time: 0.00850\n",
      "Epoch: 65, train_loss: 0.97688, time: 0.00840\n",
      "Epoch: 66, train_loss: 0.97686, time: 0.00854\n",
      "Epoch: 67, train_loss: 0.97334, time: 0.00852\n",
      "Epoch: 68, train_loss: 0.97827, time: 0.00850\n",
      "Epoch: 69, train_loss: 0.97742, time: 0.00861\n",
      "Epoch: 70, train_loss: 0.97605, time: 0.00861\n",
      "Epoch: 71, train_loss: 0.97191, time: 0.00849\n",
      "Epoch: 72, train_loss: 0.97992, time: 0.00855\n",
      "Epoch: 73, train_loss: 0.96987, time: 0.00858\n",
      "Epoch: 74, train_loss: 0.97467, time: 0.00846\n",
      "Epoch: 75, train_loss: 0.98039, time: 0.00854\n",
      "Epoch: 76, train_loss: 0.97612, time: 0.00852\n",
      "Epoch: 77, train_loss: 0.98049, time: 0.00852\n",
      "Epoch: 78, train_loss: 0.96974, time: 0.00856\n",
      "Epoch: 79, train_loss: 0.98327, time: 0.00851\n",
      "Epoch: 80, train_loss: 0.97865, time: 0.00847\n",
      "Epoch: 81, train_loss: 0.98228, time: 0.00841\n",
      "Epoch: 82, train_loss: 0.97392, time: 0.00848\n",
      "Epoch: 83, train_loss: 0.98631, time: 0.00890\n",
      "Epoch: 84, train_loss: 0.97712, time: 0.00869\n",
      "Epoch: 85, train_loss: 0.97929, time: 0.00848\n",
      "Epoch: 86, train_loss: 0.97601, time: 0.00843\n",
      "Epoch: 87, train_loss: 0.97811, time: 0.00850\n",
      "Epoch: 88, train_loss: 0.97727, time: 0.00854\n",
      "Epoch: 89, train_loss: 0.97660, time: 0.00850\n",
      "Epoch: 90, train_loss: 0.98077, time: 0.00845\n",
      "Epoch: 91, train_loss: 0.97439, time: 0.00850\n",
      "Epoch: 92, train_loss: 0.97575, time: 0.00853\n",
      "Epoch: 93, train_loss: 0.97995, time: 0.00851\n",
      "Epoch: 94, train_loss: 0.98223, time: 0.00849\n",
      "Epoch: 95, train_loss: 0.97515, time: 0.00860\n",
      "Epoch: 96, train_loss: 0.97692, time: 0.00853\n",
      "Epoch: 97, train_loss: 0.97480, time: 0.00859\n",
      "Epoch: 98, train_loss: 0.98348, time: 0.00858\n",
      "Epoch: 99, train_loss: 0.97756, time: 0.00854\n",
      "Epoch: 100, train_loss: 0.97955, time: 0.00864\n",
      "Epoch: 101, train_loss: 0.97054, time: 0.00855\n",
      "Epoch: 102, train_loss: 0.97421, time: 0.00855\n",
      "Epoch: 103, train_loss: 0.97710, time: 0.00859\n",
      "Epoch: 104, train_loss: 0.98243, time: 0.00854\n",
      "Epoch: 105, train_loss: 0.96853, time: 0.00850\n",
      "Epoch: 106, train_loss: 0.97234, time: 0.00862\n",
      "Epoch: 107, train_loss: 0.97426, time: 0.00907\n",
      "Epoch: 108, train_loss: 0.97613, time: 0.00874\n",
      "Epoch: 109, train_loss: 0.97499, time: 0.00856\n",
      "Epoch: 110, train_loss: 0.97790, time: 0.00860\n",
      "Epoch: 111, train_loss: 0.97450, time: 0.00851\n",
      "Epoch: 112, train_loss: 0.97681, time: 0.00841\n",
      "Epoch: 113, train_loss: 0.98077, time: 0.00856\n",
      "Epoch: 114, train_loss: 0.98256, time: 0.00854\n",
      "Epoch: 115, train_loss: 0.98375, time: 0.00849\n",
      "Epoch: 116, train_loss: 0.98407, time: 0.00857\n",
      "Epoch: 117, train_loss: 0.97489, time: 0.00851\n",
      "Epoch: 118, train_loss: 0.98057, time: 0.00850\n",
      "Epoch: 119, train_loss: 0.97706, time: 0.00857\n",
      "Epoch: 120, train_loss: 0.97623, time: 0.00867\n",
      "Epoch: 121, train_loss: 0.97168, time: 0.00848\n",
      "Epoch: 122, train_loss: 0.97678, time: 0.00842\n",
      "Epoch: 123, train_loss: 0.97572, time: 0.00849\n",
      "Epoch: 124, train_loss: 0.98664, time: 0.00849\n",
      "Epoch: 125, train_loss: 0.97811, time: 0.00849\n",
      "Epoch: 126, train_loss: 0.97768, time: 0.00856\n",
      "Epoch: 127, train_loss: 0.97715, time: 0.00858\n",
      "Epoch: 128, train_loss: 0.97115, time: 0.00850\n",
      "Epoch: 129, train_loss: 0.97304, time: 0.00849\n",
      "Epoch: 130, train_loss: 0.97784, time: 0.00848\n",
      "Epoch: 131, train_loss: 0.97669, time: 0.00902\n",
      "Epoch: 132, train_loss: 0.97669, time: 0.00869\n",
      "Epoch: 133, train_loss: 0.97549, time: 0.00853\n",
      "Epoch: 134, train_loss: 0.97050, time: 0.00854\n",
      "Epoch: 135, train_loss: 0.97234, time: 0.00849\n",
      "Epoch: 136, train_loss: 0.97868, time: 0.00847\n",
      "Epoch: 137, train_loss: 0.97481, time: 0.00851\n",
      "Epoch: 138, train_loss: 0.97613, time: 0.00854\n",
      "Epoch: 139, train_loss: 0.98057, time: 0.00856\n",
      "Epoch: 140, train_loss: 0.97031, time: 0.00853\n",
      "Epoch: 141, train_loss: 0.97359, time: 0.00848\n",
      "Epoch: 142, train_loss: 0.97237, time: 0.00848\n",
      "Epoch: 143, train_loss: 0.97535, time: 0.00849\n",
      "Epoch: 144, train_loss: 0.97108, time: 0.00852\n",
      "Epoch: 145, train_loss: 0.97547, time: 0.00850\n",
      "Epoch: 146, train_loss: 0.97774, time: 0.00860\n",
      "Epoch: 147, train_loss: 0.98128, time: 0.00847\n",
      "Epoch: 148, train_loss: 0.98297, time: 0.00854\n",
      "Epoch: 149, train_loss: 0.97441, time: 0.00853\n",
      "Epoch: 150, train_loss: 0.97089, time: 0.00855\n",
      "Epoch: 151, train_loss: 0.98097, time: 0.00853\n",
      "Epoch: 152, train_loss: 0.97343, time: 0.00859\n",
      "Epoch: 153, train_loss: 0.97877, time: 0.00856\n",
      "Epoch: 154, train_loss: 0.97505, time: 0.00847\n",
      "Epoch: 155, train_loss: 0.97636, time: 0.00900\n",
      "Epoch: 156, train_loss: 0.97655, time: 0.00876\n",
      "Epoch: 157, train_loss: 0.98575, time: 0.00850\n",
      "Epoch: 158, train_loss: 0.97726, time: 0.00856\n",
      "Epoch: 159, train_loss: 0.97232, time: 0.00857\n",
      "Epoch: 160, train_loss: 0.97603, time: 0.00854\n",
      "Epoch: 161, train_loss: 0.97986, time: 0.00854\n",
      "Epoch: 162, train_loss: 0.97910, time: 0.00848\n",
      "Epoch: 163, train_loss: 0.97519, time: 0.00848\n",
      "Epoch: 164, train_loss: 0.98020, time: 0.00853\n",
      "Epoch: 165, train_loss: 0.97165, time: 0.00847\n",
      "Epoch: 166, train_loss: 0.97652, time: 0.00851\n",
      "Epoch: 167, train_loss: 0.97562, time: 0.00856\n",
      "Epoch: 168, train_loss: 0.97805, time: 0.00852\n",
      "Epoch: 169, train_loss: 0.97995, time: 0.00852\n",
      "Epoch: 170, train_loss: 0.96825, time: 0.00853\n",
      "Epoch: 171, train_loss: 0.97391, time: 0.00850\n",
      "Epoch: 172, train_loss: 0.98202, time: 0.00846\n",
      "Epoch: 173, train_loss: 0.97095, time: 0.00852\n",
      "Epoch: 174, train_loss: 0.98087, time: 0.00843\n",
      "Epoch: 175, train_loss: 0.97923, time: 0.00843\n",
      "Epoch: 176, train_loss: 0.97303, time: 0.00841\n",
      "Epoch: 177, train_loss: 0.97933, time: 0.00851\n",
      "Epoch: 178, train_loss: 0.97691, time: 0.00847\n",
      "Epoch: 179, train_loss: 0.97567, time: 0.00904\n",
      "Epoch: 180, train_loss: 0.97700, time: 0.00864\n",
      "Epoch: 181, train_loss: 0.97135, time: 0.00853\n",
      "Epoch: 182, train_loss: 0.97653, time: 0.00854\n",
      "Epoch: 183, train_loss: 0.97244, time: 0.00854\n",
      "Epoch: 184, train_loss: 0.97656, time: 0.00858\n",
      "Epoch: 185, train_loss: 0.97253, time: 0.00851\n",
      "Epoch: 186, train_loss: 0.97005, time: 0.00858\n",
      "Epoch: 187, train_loss: 0.98406, time: 0.00863\n",
      "Epoch: 188, train_loss: 0.98001, time: 0.00850\n",
      "Epoch: 189, train_loss: 0.97126, time: 0.00850\n",
      "Epoch: 190, train_loss: 0.98514, time: 0.00856\n",
      "Epoch: 191, train_loss: 0.97554, time: 0.00855\n",
      "Epoch: 192, train_loss: 0.97038, time: 0.00855\n",
      "Epoch: 193, train_loss: 0.97784, time: 0.00844\n",
      "Epoch: 194, train_loss: 0.97698, time: 0.00841\n",
      "Epoch: 195, train_loss: 0.97222, time: 0.00851\n",
      "Epoch: 196, train_loss: 0.96826, time: 0.00857\n",
      "Epoch: 197, train_loss: 0.97730, time: 0.00856\n",
      "Epoch: 198, train_loss: 0.97556, time: 0.00849\n",
      "Epoch: 199, train_loss: 0.97977, time: 0.00849\n",
      "Epoch: 200, train_loss: 0.97577, time: 0.00851\n",
      "pairwise precision 0.08400 recall 0.61895 f1 0.14792\n",
      "average until now [0.43357048728918085, 0.8291111685888759, 0.5693883833799794]\n",
      "84 names 347.2072243690491 avg time 4.1334193377267745\n",
      "Loading dan_sun dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 204 nodes, 892 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.98712, time: 0.11020\n",
      "Epoch: 2, train_loss: 0.98240, time: 0.01242\n",
      "Epoch: 3, train_loss: 0.98350, time: 0.01105\n",
      "Epoch: 4, train_loss: 0.97741, time: 0.01070\n",
      "Epoch: 5, train_loss: 0.98051, time: 0.01073\n",
      "Epoch: 6, train_loss: 0.97849, time: 0.01074\n",
      "Epoch: 7, train_loss: 0.98444, time: 0.01067\n",
      "Epoch: 8, train_loss: 0.97611, time: 0.01067\n",
      "Epoch: 9, train_loss: 0.97771, time: 0.01079\n",
      "Epoch: 10, train_loss: 0.97812, time: 0.01064\n",
      "Epoch: 11, train_loss: 0.97709, time: 0.01077\n",
      "Epoch: 12, train_loss: 0.98181, time: 0.01067\n",
      "Epoch: 13, train_loss: 0.97595, time: 0.01053\n",
      "Epoch: 14, train_loss: 0.98019, time: 0.01082\n",
      "Epoch: 15, train_loss: 0.98126, time: 0.01052\n",
      "Epoch: 16, train_loss: 0.97863, time: 0.01078\n",
      "Epoch: 17, train_loss: 0.97999, time: 0.01041\n",
      "Epoch: 18, train_loss: 0.98004, time: 0.01065\n",
      "Epoch: 19, train_loss: 0.97701, time: 0.01063\n",
      "Epoch: 20, train_loss: 0.98035, time: 0.01056\n",
      "Epoch: 21, train_loss: 0.98001, time: 0.01058\n",
      "Epoch: 22, train_loss: 0.97571, time: 0.01068\n",
      "Epoch: 23, train_loss: 0.98259, time: 0.01075\n",
      "Epoch: 24, train_loss: 0.98307, time: 0.01060\n",
      "Epoch: 25, train_loss: 0.97854, time: 0.01055\n",
      "Epoch: 26, train_loss: 0.97817, time: 0.01061\n",
      "Epoch: 27, train_loss: 0.97869, time: 0.01063\n",
      "Epoch: 28, train_loss: 0.98137, time: 0.01104\n",
      "Epoch: 29, train_loss: 0.97721, time: 0.01075\n",
      "Epoch: 30, train_loss: 0.97715, time: 0.01052\n",
      "Epoch: 31, train_loss: 0.98064, time: 0.01058\n",
      "Epoch: 32, train_loss: 0.97959, time: 0.01061\n",
      "Epoch: 33, train_loss: 0.98305, time: 0.01066\n",
      "Epoch: 34, train_loss: 0.97698, time: 0.01067\n",
      "Epoch: 35, train_loss: 0.98322, time: 0.01054\n",
      "Epoch: 36, train_loss: 0.97666, time: 0.01063\n",
      "Epoch: 37, train_loss: 0.98169, time: 0.01062\n",
      "Epoch: 38, train_loss: 0.97665, time: 0.01064\n",
      "Epoch: 39, train_loss: 0.97560, time: 0.01057\n",
      "Epoch: 40, train_loss: 0.97860, time: 0.01110\n",
      "Epoch: 41, train_loss: 0.97720, time: 0.01079\n",
      "Epoch: 42, train_loss: 0.98152, time: 0.01065\n",
      "Epoch: 43, train_loss: 0.98040, time: 0.01063\n",
      "Epoch: 44, train_loss: 0.98083, time: 0.01053\n",
      "Epoch: 45, train_loss: 0.98071, time: 0.01064\n",
      "Epoch: 46, train_loss: 0.98184, time: 0.01039\n",
      "Epoch: 47, train_loss: 0.98383, time: 0.01096\n",
      "Epoch: 48, train_loss: 0.97970, time: 0.01070\n",
      "Epoch: 49, train_loss: 0.98095, time: 0.01053\n",
      "Epoch: 50, train_loss: 0.98317, time: 0.01077\n",
      "Epoch: 51, train_loss: 0.98006, time: 0.01041\n",
      "Epoch: 52, train_loss: 0.97911, time: 0.01032\n",
      "Epoch: 53, train_loss: 0.97773, time: 0.01046\n",
      "Epoch: 54, train_loss: 0.97888, time: 0.01033\n",
      "Epoch: 55, train_loss: 0.97777, time: 0.01042\n",
      "Epoch: 56, train_loss: 0.97695, time: 0.01036\n",
      "Epoch: 57, train_loss: 0.97889, time: 0.01045\n",
      "Epoch: 58, train_loss: 0.97779, time: 0.01043\n",
      "Epoch: 59, train_loss: 0.97862, time: 0.01027\n",
      "Epoch: 60, train_loss: 0.97869, time: 0.01057\n",
      "Epoch: 61, train_loss: 0.97929, time: 0.01057\n",
      "Epoch: 62, train_loss: 0.97902, time: 0.01045\n",
      "Epoch: 63, train_loss: 0.97994, time: 0.01030\n",
      "Epoch: 64, train_loss: 0.97888, time: 0.01048\n",
      "Epoch: 65, train_loss: 0.97933, time: 0.01054\n",
      "Epoch: 66, train_loss: 0.97873, time: 0.01042\n",
      "Epoch: 67, train_loss: 0.97628, time: 0.01079\n",
      "Epoch: 68, train_loss: 0.97722, time: 0.01097\n",
      "Epoch: 69, train_loss: 0.97777, time: 0.01030\n",
      "Epoch: 70, train_loss: 0.98101, time: 0.01090\n",
      "Epoch: 71, train_loss: 0.97833, time: 0.01046\n",
      "Epoch: 72, train_loss: 0.97989, time: 0.01043\n",
      "Epoch: 73, train_loss: 0.97753, time: 0.01054\n",
      "Epoch: 74, train_loss: 0.97863, time: 0.01073\n",
      "Epoch: 75, train_loss: 0.97609, time: 0.01057\n",
      "Epoch: 76, train_loss: 0.97931, time: 0.01051\n",
      "Epoch: 77, train_loss: 0.98219, time: 0.01058\n",
      "Epoch: 78, train_loss: 0.98147, time: 0.01056\n",
      "Epoch: 79, train_loss: 0.97800, time: 0.01068\n",
      "Epoch: 80, train_loss: 0.97826, time: 0.01060\n",
      "Epoch: 81, train_loss: 0.97699, time: 0.01063\n",
      "Epoch: 82, train_loss: 0.97977, time: 0.01034\n",
      "Epoch: 83, train_loss: 0.97883, time: 0.01064\n",
      "Epoch: 84, train_loss: 0.98207, time: 0.01058\n",
      "Epoch: 85, train_loss: 0.97899, time: 0.01008\n",
      "Epoch: 86, train_loss: 0.98302, time: 0.01077\n",
      "Epoch: 87, train_loss: 0.97765, time: 0.01058\n",
      "Epoch: 88, train_loss: 0.97693, time: 0.01061\n",
      "Epoch: 89, train_loss: 0.97884, time: 0.01055\n",
      "Epoch: 90, train_loss: 0.97531, time: 0.01050\n",
      "Epoch: 91, train_loss: 0.97842, time: 0.01054\n",
      "Epoch: 92, train_loss: 0.97940, time: 0.01063\n",
      "Epoch: 93, train_loss: 0.98010, time: 0.01042\n",
      "Epoch: 94, train_loss: 0.97404, time: 0.01056\n",
      "Epoch: 95, train_loss: 0.97983, time: 0.01048\n",
      "Epoch: 96, train_loss: 0.97613, time: 0.01044\n",
      "Epoch: 97, train_loss: 0.97757, time: 0.01053\n",
      "Epoch: 98, train_loss: 0.98439, time: 0.01064\n",
      "Epoch: 99, train_loss: 0.97927, time: 0.01067\n",
      "Epoch: 100, train_loss: 0.97804, time: 0.01058\n",
      "Epoch: 101, train_loss: 0.97620, time: 0.01042\n",
      "Epoch: 102, train_loss: 0.97847, time: 0.01025\n",
      "Epoch: 103, train_loss: 0.98040, time: 0.01043\n",
      "Epoch: 104, train_loss: 0.97779, time: 0.01051\n",
      "Epoch: 105, train_loss: 0.97829, time: 0.01043\n",
      "Epoch: 106, train_loss: 0.97802, time: 0.01070\n",
      "Epoch: 107, train_loss: 0.97726, time: 0.01068\n",
      "Epoch: 108, train_loss: 0.97768, time: 0.01052\n",
      "Epoch: 109, train_loss: 0.97491, time: 0.01056\n",
      "Epoch: 110, train_loss: 0.97569, time: 0.01032\n",
      "Epoch: 111, train_loss: 0.97876, time: 0.01034\n",
      "Epoch: 112, train_loss: 0.97951, time: 0.01037\n",
      "Epoch: 113, train_loss: 0.97860, time: 0.01048\n",
      "Epoch: 114, train_loss: 0.97565, time: 0.01060\n",
      "Epoch: 115, train_loss: 0.97738, time: 0.01059\n",
      "Epoch: 116, train_loss: 0.97615, time: 0.01051\n",
      "Epoch: 117, train_loss: 0.98060, time: 0.01045\n",
      "Epoch: 118, train_loss: 0.97757, time: 0.01035\n",
      "Epoch: 119, train_loss: 0.97620, time: 0.01046\n",
      "Epoch: 120, train_loss: 0.97759, time: 0.01058\n",
      "Epoch: 121, train_loss: 0.97972, time: 0.01057\n",
      "Epoch: 122, train_loss: 0.97383, time: 0.01036\n",
      "Epoch: 123, train_loss: 0.97829, time: 0.01044\n",
      "Epoch: 124, train_loss: 0.97527, time: 0.01050\n",
      "Epoch: 125, train_loss: 0.97559, time: 0.01048\n",
      "Epoch: 126, train_loss: 0.97812, time: 0.01090\n",
      "Epoch: 127, train_loss: 0.97509, time: 0.01068\n",
      "Epoch: 128, train_loss: 0.97830, time: 0.01053\n",
      "Epoch: 129, train_loss: 0.97877, time: 0.01050\n",
      "Epoch: 130, train_loss: 0.97849, time: 0.01072\n",
      "Epoch: 131, train_loss: 0.97727, time: 0.01070\n",
      "Epoch: 132, train_loss: 0.97954, time: 0.01050\n",
      "Epoch: 133, train_loss: 0.97587, time: 0.01045\n",
      "Epoch: 134, train_loss: 0.97706, time: 0.01049\n",
      "Epoch: 135, train_loss: 0.97715, time: 0.01049\n",
      "Epoch: 136, train_loss: 0.97693, time: 0.01056\n",
      "Epoch: 137, train_loss: 0.98096, time: 0.01059\n",
      "Epoch: 138, train_loss: 0.97449, time: 0.01063\n",
      "Epoch: 139, train_loss: 0.97703, time: 0.01073\n",
      "Epoch: 140, train_loss: 0.97860, time: 0.01064\n",
      "Epoch: 141, train_loss: 0.97518, time: 0.01035\n",
      "Epoch: 142, train_loss: 0.98192, time: 0.01053\n",
      "Epoch: 143, train_loss: 0.98007, time: 0.01042\n",
      "Epoch: 144, train_loss: 0.97694, time: 0.01067\n",
      "Epoch: 145, train_loss: 0.97673, time: 0.01083\n",
      "Epoch: 146, train_loss: 0.97565, time: 0.01091\n",
      "Epoch: 147, train_loss: 0.97838, time: 0.01051\n",
      "Epoch: 148, train_loss: 0.97742, time: 0.01060\n",
      "Epoch: 149, train_loss: 0.97739, time: 0.01042\n",
      "Epoch: 150, train_loss: 0.97813, time: 0.01071\n",
      "Epoch: 151, train_loss: 0.97649, time: 0.01049\n",
      "Epoch: 152, train_loss: 0.97890, time: 0.01060\n",
      "Epoch: 153, train_loss: 0.97821, time: 0.01053\n",
      "Epoch: 154, train_loss: 0.98180, time: 0.01053\n",
      "Epoch: 155, train_loss: 0.97902, time: 0.01052\n",
      "Epoch: 156, train_loss: 0.97620, time: 0.01060\n",
      "Epoch: 157, train_loss: 0.97808, time: 0.01067\n",
      "Epoch: 158, train_loss: 0.97732, time: 0.01053\n",
      "Epoch: 159, train_loss: 0.97541, time: 0.01070\n",
      "Epoch: 160, train_loss: 0.97283, time: 0.01027\n",
      "Epoch: 161, train_loss: 0.97931, time: 0.01054\n",
      "Epoch: 162, train_loss: 0.97980, time: 0.01028\n",
      "Epoch: 163, train_loss: 0.98028, time: 0.01052\n",
      "Epoch: 164, train_loss: 0.98309, time: 0.01040\n",
      "Epoch: 165, train_loss: 0.97980, time: 0.01086\n",
      "Epoch: 166, train_loss: 0.97847, time: 0.01063\n",
      "Epoch: 167, train_loss: 0.97736, time: 0.01050\n",
      "Epoch: 168, train_loss: 0.97844, time: 0.01052\n",
      "Epoch: 169, train_loss: 0.97808, time: 0.01057\n",
      "Epoch: 170, train_loss: 0.97519, time: 0.01067\n",
      "Epoch: 171, train_loss: 0.97827, time: 0.01062\n",
      "Epoch: 172, train_loss: 0.97742, time: 0.01049\n",
      "Epoch: 173, train_loss: 0.97660, time: 0.01058\n",
      "Epoch: 174, train_loss: 0.97482, time: 0.01052\n",
      "Epoch: 175, train_loss: 0.98048, time: 0.01059\n",
      "Epoch: 176, train_loss: 0.98002, time: 0.01065\n",
      "Epoch: 177, train_loss: 0.97737, time: 0.01046\n",
      "Epoch: 178, train_loss: 0.97711, time: 0.01057\n",
      "Epoch: 179, train_loss: 0.97372, time: 0.01039\n",
      "Epoch: 180, train_loss: 0.97721, time: 0.01050\n",
      "Epoch: 181, train_loss: 0.97780, time: 0.01062\n",
      "Epoch: 182, train_loss: 0.97649, time: 0.01038\n",
      "Epoch: 183, train_loss: 0.98090, time: 0.01049\n",
      "Epoch: 184, train_loss: 0.97788, time: 0.01047\n",
      "Epoch: 185, train_loss: 0.97617, time: 0.01087\n",
      "Epoch: 186, train_loss: 0.97559, time: 0.01058\n",
      "Epoch: 187, train_loss: 0.97802, time: 0.01034\n",
      "Epoch: 188, train_loss: 0.98019, time: 0.01050\n",
      "Epoch: 189, train_loss: 0.97900, time: 0.01047\n",
      "Epoch: 190, train_loss: 0.97923, time: 0.01048\n",
      "Epoch: 191, train_loss: 0.97791, time: 0.01044\n",
      "Epoch: 192, train_loss: 0.97557, time: 0.01071\n",
      "Epoch: 193, train_loss: 0.97958, time: 0.01042\n",
      "Epoch: 194, train_loss: 0.97634, time: 0.01038\n",
      "Epoch: 195, train_loss: 0.97844, time: 0.01052\n",
      "Epoch: 196, train_loss: 0.97635, time: 0.01049\n",
      "Epoch: 197, train_loss: 0.97532, time: 0.01045\n",
      "Epoch: 198, train_loss: 0.97682, time: 0.01049\n",
      "Epoch: 199, train_loss: 0.97615, time: 0.01042\n",
      "Epoch: 200, train_loss: 0.97729, time: 0.01048\n",
      "pairwise precision 0.10165 recall 0.86619 f1 0.18196\n",
      "average until now [0.42966559739561044, 0.8295473554332387, 0.5661122834536417]\n",
      "85 names 349.4705722332001 avg time 4.1114184968611776\n",
      "Loading shui_wang dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 305 nodes, 5360 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.94213, time: 0.11561\n",
      "Epoch: 2, train_loss: 0.94398, time: 0.01660\n",
      "Epoch: 3, train_loss: 0.94325, time: 0.01557\n",
      "Epoch: 4, train_loss: 0.93924, time: 0.01499\n",
      "Epoch: 5, train_loss: 0.94101, time: 0.01514\n",
      "Epoch: 6, train_loss: 0.94064, time: 0.01475\n",
      "Epoch: 7, train_loss: 0.94040, time: 0.01556\n",
      "Epoch: 8, train_loss: 0.94251, time: 0.01532\n",
      "Epoch: 9, train_loss: 0.94216, time: 0.01515\n",
      "Epoch: 10, train_loss: 0.94200, time: 0.01525\n",
      "Epoch: 11, train_loss: 0.93975, time: 0.01488\n",
      "Epoch: 12, train_loss: 0.94058, time: 0.01474\n",
      "Epoch: 13, train_loss: 0.93908, time: 0.01528\n",
      "Epoch: 14, train_loss: 0.94181, time: 0.01515\n",
      "Epoch: 15, train_loss: 0.94042, time: 0.01496\n",
      "Epoch: 16, train_loss: 0.94015, time: 0.01476\n",
      "Epoch: 17, train_loss: 0.94192, time: 0.01489\n",
      "Epoch: 18, train_loss: 0.93949, time: 0.01518\n",
      "Epoch: 19, train_loss: 0.93907, time: 0.01525\n",
      "Epoch: 20, train_loss: 0.94170, time: 0.01506\n",
      "Epoch: 21, train_loss: 0.94131, time: 0.01574\n",
      "Epoch: 22, train_loss: 0.94014, time: 0.01516\n",
      "Epoch: 23, train_loss: 0.93859, time: 0.01452\n",
      "Epoch: 24, train_loss: 0.94398, time: 0.01491\n",
      "Epoch: 25, train_loss: 0.94227, time: 0.01527\n",
      "Epoch: 26, train_loss: 0.94158, time: 0.01498\n",
      "Epoch: 27, train_loss: 0.94082, time: 0.01464\n",
      "Epoch: 28, train_loss: 0.94399, time: 0.01468\n",
      "Epoch: 29, train_loss: 0.94210, time: 0.01518\n",
      "Epoch: 30, train_loss: 0.94075, time: 0.01458\n",
      "Epoch: 31, train_loss: 0.94321, time: 0.01498\n",
      "Epoch: 32, train_loss: 0.94048, time: 0.01622\n",
      "Epoch: 33, train_loss: 0.94219, time: 0.01536\n",
      "Epoch: 34, train_loss: 0.94307, time: 0.01503\n",
      "Epoch: 35, train_loss: 0.94120, time: 0.01559\n",
      "Epoch: 36, train_loss: 0.94462, time: 0.01525\n",
      "Epoch: 37, train_loss: 0.93966, time: 0.01523\n",
      "Epoch: 38, train_loss: 0.94086, time: 0.01491\n",
      "Epoch: 39, train_loss: 0.94157, time: 0.01506\n",
      "Epoch: 40, train_loss: 0.93975, time: 0.01484\n",
      "Epoch: 41, train_loss: 0.94283, time: 0.01521\n",
      "Epoch: 42, train_loss: 0.93880, time: 0.01547\n",
      "Epoch: 43, train_loss: 0.94198, time: 0.01483\n",
      "Epoch: 44, train_loss: 0.94105, time: 0.01540\n",
      "Epoch: 45, train_loss: 0.94406, time: 0.01514\n",
      "Epoch: 46, train_loss: 0.94140, time: 0.01476\n",
      "Epoch: 47, train_loss: 0.94061, time: 0.01464\n",
      "Epoch: 48, train_loss: 0.94075, time: 0.01501\n",
      "Epoch: 49, train_loss: 0.94101, time: 0.01548\n",
      "Epoch: 50, train_loss: 0.94302, time: 0.01514\n",
      "Epoch: 51, train_loss: 0.94115, time: 0.01506\n",
      "Epoch: 52, train_loss: 0.94069, time: 0.01542\n",
      "Epoch: 53, train_loss: 0.94060, time: 0.01557\n",
      "Epoch: 54, train_loss: 0.94131, time: 0.01552\n",
      "Epoch: 55, train_loss: 0.93929, time: 0.01536\n",
      "Epoch: 56, train_loss: 0.93990, time: 0.01533\n",
      "Epoch: 57, train_loss: 0.94176, time: 0.01512\n",
      "Epoch: 58, train_loss: 0.94067, time: 0.01491\n",
      "Epoch: 59, train_loss: 0.94340, time: 0.01491\n",
      "Epoch: 60, train_loss: 0.93966, time: 0.01478\n",
      "Epoch: 61, train_loss: 0.94093, time: 0.01502\n",
      "Epoch: 62, train_loss: 0.93912, time: 0.01493\n",
      "Epoch: 63, train_loss: 0.94102, time: 0.01571\n",
      "Epoch: 64, train_loss: 0.94076, time: 0.01515\n",
      "Epoch: 65, train_loss: 0.94282, time: 0.01496\n",
      "Epoch: 66, train_loss: 0.94179, time: 0.01485\n",
      "Epoch: 67, train_loss: 0.94039, time: 0.01482\n",
      "Epoch: 68, train_loss: 0.94028, time: 0.01481\n",
      "Epoch: 69, train_loss: 0.94189, time: 0.01482\n",
      "Epoch: 70, train_loss: 0.93866, time: 0.01556\n",
      "Epoch: 71, train_loss: 0.94117, time: 0.01509\n",
      "Epoch: 72, train_loss: 0.94033, time: 0.01534\n",
      "Epoch: 73, train_loss: 0.94083, time: 0.01572\n",
      "Epoch: 74, train_loss: 0.94038, time: 0.01582\n",
      "Epoch: 75, train_loss: 0.94123, time: 0.01487\n",
      "Epoch: 76, train_loss: 0.94151, time: 0.01507\n",
      "Epoch: 77, train_loss: 0.94153, time: 0.01566\n",
      "Epoch: 78, train_loss: 0.94152, time: 0.01544\n",
      "Epoch: 79, train_loss: 0.94198, time: 0.01547\n",
      "Epoch: 80, train_loss: 0.93945, time: 0.01513\n",
      "Epoch: 81, train_loss: 0.93825, time: 0.01524\n",
      "Epoch: 82, train_loss: 0.94065, time: 0.01523\n",
      "Epoch: 83, train_loss: 0.93967, time: 0.01464\n",
      "Epoch: 84, train_loss: 0.94114, time: 0.01454\n",
      "Epoch: 85, train_loss: 0.93870, time: 0.01497\n",
      "Epoch: 86, train_loss: 0.94015, time: 0.01515\n",
      "Epoch: 87, train_loss: 0.93996, time: 0.01511\n",
      "Epoch: 88, train_loss: 0.93956, time: 0.01493\n",
      "Epoch: 89, train_loss: 0.94044, time: 0.01529\n",
      "Epoch: 90, train_loss: 0.94098, time: 0.01530\n",
      "Epoch: 91, train_loss: 0.94243, time: 0.01560\n",
      "Epoch: 92, train_loss: 0.94252, time: 0.01541\n",
      "Epoch: 93, train_loss: 0.94188, time: 0.01575\n",
      "Epoch: 94, train_loss: 0.93976, time: 0.01549\n",
      "Epoch: 95, train_loss: 0.94203, time: 0.01460\n",
      "Epoch: 96, train_loss: 0.94072, time: 0.01499\n",
      "Epoch: 97, train_loss: 0.93886, time: 0.01442\n",
      "Epoch: 98, train_loss: 0.94082, time: 0.01446\n",
      "Epoch: 99, train_loss: 0.94121, time: 0.01467\n",
      "Epoch: 100, train_loss: 0.94276, time: 0.01572\n",
      "Epoch: 101, train_loss: 0.93964, time: 0.01566\n",
      "Epoch: 102, train_loss: 0.94222, time: 0.01541\n",
      "Epoch: 103, train_loss: 0.94012, time: 0.01504\n",
      "Epoch: 104, train_loss: 0.94183, time: 0.01534\n",
      "Epoch: 105, train_loss: 0.94203, time: 0.01587\n",
      "Epoch: 106, train_loss: 0.94070, time: 0.01546\n",
      "Epoch: 107, train_loss: 0.94089, time: 0.01592\n",
      "Epoch: 108, train_loss: 0.94036, time: 0.01513\n",
      "Epoch: 109, train_loss: 0.94015, time: 0.01519\n",
      "Epoch: 110, train_loss: 0.94160, time: 0.01511\n",
      "Epoch: 111, train_loss: 0.94175, time: 0.01509\n",
      "Epoch: 112, train_loss: 0.94023, time: 0.01489\n",
      "Epoch: 113, train_loss: 0.94188, time: 0.01540\n",
      "Epoch: 114, train_loss: 0.93976, time: 0.01522\n",
      "Epoch: 115, train_loss: 0.94287, time: 0.01458\n",
      "Epoch: 116, train_loss: 0.94093, time: 0.01526\n",
      "Epoch: 117, train_loss: 0.94299, time: 0.01515\n",
      "Epoch: 118, train_loss: 0.93912, time: 0.01505\n",
      "Epoch: 119, train_loss: 0.93919, time: 0.01571\n",
      "Epoch: 120, train_loss: 0.94141, time: 0.01531\n",
      "Epoch: 121, train_loss: 0.94284, time: 0.01531\n",
      "Epoch: 122, train_loss: 0.94123, time: 0.01513\n",
      "Epoch: 123, train_loss: 0.94188, time: 0.01500\n",
      "Epoch: 124, train_loss: 0.94058, time: 0.01542\n",
      "Epoch: 125, train_loss: 0.93940, time: 0.01481\n",
      "Epoch: 126, train_loss: 0.94059, time: 0.01477\n",
      "Epoch: 127, train_loss: 0.94033, time: 0.01532\n",
      "Epoch: 128, train_loss: 0.94040, time: 0.01489\n",
      "Epoch: 129, train_loss: 0.94118, time: 0.01513\n",
      "Epoch: 130, train_loss: 0.94276, time: 0.01515\n",
      "Epoch: 131, train_loss: 0.93898, time: 0.01565\n",
      "Epoch: 132, train_loss: 0.94025, time: 0.01539\n",
      "Epoch: 133, train_loss: 0.93909, time: 0.01551\n",
      "Epoch: 134, train_loss: 0.94203, time: 0.01536\n",
      "Epoch: 135, train_loss: 0.94185, time: 0.01550\n",
      "Epoch: 136, train_loss: 0.94052, time: 0.01509\n",
      "Epoch: 137, train_loss: 0.94053, time: 0.01484\n",
      "Epoch: 138, train_loss: 0.93993, time: 0.01561\n",
      "Epoch: 139, train_loss: 0.94053, time: 0.01533\n",
      "Epoch: 140, train_loss: 0.94011, time: 0.01514\n",
      "Epoch: 141, train_loss: 0.94124, time: 0.01541\n",
      "Epoch: 142, train_loss: 0.94070, time: 0.01544\n",
      "Epoch: 143, train_loss: 0.94205, time: 0.01493\n",
      "Epoch: 144, train_loss: 0.93994, time: 0.01465\n",
      "Epoch: 145, train_loss: 0.94196, time: 0.01536\n",
      "Epoch: 146, train_loss: 0.94119, time: 0.01554\n",
      "Epoch: 147, train_loss: 0.94361, time: 0.01566\n",
      "Epoch: 148, train_loss: 0.94266, time: 0.01516\n",
      "Epoch: 149, train_loss: 0.94192, time: 0.01504\n",
      "Epoch: 150, train_loss: 0.94247, time: 0.01587\n",
      "Epoch: 151, train_loss: 0.94038, time: 0.01531\n",
      "Epoch: 152, train_loss: 0.94006, time: 0.01489\n",
      "Epoch: 153, train_loss: 0.94265, time: 0.01531\n",
      "Epoch: 154, train_loss: 0.94356, time: 0.01486\n",
      "Epoch: 155, train_loss: 0.94146, time: 0.01505\n",
      "Epoch: 156, train_loss: 0.93966, time: 0.01542\n",
      "Epoch: 157, train_loss: 0.93852, time: 0.01545\n",
      "Epoch: 158, train_loss: 0.93945, time: 0.01507\n",
      "Epoch: 159, train_loss: 0.94059, time: 0.01534\n",
      "Epoch: 160, train_loss: 0.94048, time: 0.01537\n",
      "Epoch: 161, train_loss: 0.93974, time: 0.01546\n",
      "Epoch: 162, train_loss: 0.93908, time: 0.01477\n",
      "Epoch: 163, train_loss: 0.94203, time: 0.01506\n",
      "Epoch: 164, train_loss: 0.94417, time: 0.01510\n",
      "Epoch: 165, train_loss: 0.94112, time: 0.01531\n",
      "Epoch: 166, train_loss: 0.94156, time: 0.01486\n",
      "Epoch: 167, train_loss: 0.94267, time: 0.01503\n",
      "Epoch: 168, train_loss: 0.94015, time: 0.01544\n",
      "Epoch: 169, train_loss: 0.94108, time: 0.01515\n",
      "Epoch: 170, train_loss: 0.94183, time: 0.01506\n",
      "Epoch: 171, train_loss: 0.94053, time: 0.01495\n",
      "Epoch: 172, train_loss: 0.94034, time: 0.01539\n",
      "Epoch: 173, train_loss: 0.94028, time: 0.01496\n",
      "Epoch: 174, train_loss: 0.94075, time: 0.01510\n",
      "Epoch: 175, train_loss: 0.94201, time: 0.01538\n",
      "Epoch: 176, train_loss: 0.93961, time: 0.01580\n",
      "Epoch: 177, train_loss: 0.93836, time: 0.01470\n",
      "Epoch: 178, train_loss: 0.94029, time: 0.01522\n",
      "Epoch: 179, train_loss: 0.94146, time: 0.01493\n",
      "Epoch: 180, train_loss: 0.93910, time: 0.01518\n",
      "Epoch: 181, train_loss: 0.94107, time: 0.01521\n",
      "Epoch: 182, train_loss: 0.93887, time: 0.01574\n",
      "Epoch: 183, train_loss: 0.94085, time: 0.01564\n",
      "Epoch: 184, train_loss: 0.94084, time: 0.01507\n",
      "Epoch: 185, train_loss: 0.94180, time: 0.01496\n",
      "Epoch: 186, train_loss: 0.94122, time: 0.01564\n",
      "Epoch: 187, train_loss: 0.94317, time: 0.01515\n",
      "Epoch: 188, train_loss: 0.94167, time: 0.01511\n",
      "Epoch: 189, train_loss: 0.94143, time: 0.01529\n",
      "Epoch: 190, train_loss: 0.94121, time: 0.01560\n",
      "Epoch: 191, train_loss: 0.94075, time: 0.01508\n",
      "Epoch: 192, train_loss: 0.94055, time: 0.01556\n",
      "Epoch: 193, train_loss: 0.94039, time: 0.01448\n",
      "Epoch: 194, train_loss: 0.93898, time: 0.01513\n",
      "Epoch: 195, train_loss: 0.94217, time: 0.01494\n",
      "Epoch: 196, train_loss: 0.94263, time: 0.01499\n",
      "Epoch: 197, train_loss: 0.94185, time: 0.01485\n",
      "Epoch: 198, train_loss: 0.94032, time: 0.01463\n",
      "Epoch: 199, train_loss: 0.94037, time: 0.01530\n",
      "Epoch: 200, train_loss: 0.94160, time: 0.01504\n",
      "pairwise precision 0.19861 recall 0.94338 f1 0.32813\n",
      "average until now [0.4269788776422626, 0.8308709424337323, 0.5640806029522513]\n",
      "86 names 352.68414974212646 avg time 4.100978485373563\n",
      "Loading feng_teng dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 228 nodes, 9691 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.81485, time: 0.11759\n",
      "Epoch: 2, train_loss: 0.81278, time: 0.01797\n",
      "Epoch: 3, train_loss: 0.81150, time: 0.01766\n",
      "Epoch: 4, train_loss: 0.81278, time: 0.01636\n",
      "Epoch: 5, train_loss: 0.81130, time: 0.01635\n",
      "Epoch: 6, train_loss: 0.81149, time: 0.01710\n",
      "Epoch: 7, train_loss: 0.81167, time: 0.01681\n",
      "Epoch: 8, train_loss: 0.81294, time: 0.01800\n",
      "Epoch: 9, train_loss: 0.81565, time: 0.01751\n",
      "Epoch: 10, train_loss: 0.81316, time: 0.01670\n",
      "Epoch: 11, train_loss: 0.81273, time: 0.01704\n",
      "Epoch: 12, train_loss: 0.81173, time: 0.01690\n",
      "Epoch: 13, train_loss: 0.81186, time: 0.01678\n",
      "Epoch: 14, train_loss: 0.81309, time: 0.01669\n",
      "Epoch: 15, train_loss: 0.81341, time: 0.01573\n",
      "Epoch: 16, train_loss: 0.81359, time: 0.01617\n",
      "Epoch: 17, train_loss: 0.81475, time: 0.01683\n",
      "Epoch: 18, train_loss: 0.80900, time: 0.01724\n",
      "Epoch: 19, train_loss: 0.81288, time: 0.01676\n",
      "Epoch: 20, train_loss: 0.81394, time: 0.01730\n",
      "Epoch: 21, train_loss: 0.81106, time: 0.01779\n",
      "Epoch: 22, train_loss: 0.81004, time: 0.01675\n",
      "Epoch: 23, train_loss: 0.81499, time: 0.01632\n",
      "Epoch: 24, train_loss: 0.81197, time: 0.01665\n",
      "Epoch: 25, train_loss: 0.81438, time: 0.01760\n",
      "Epoch: 26, train_loss: 0.81334, time: 0.01668\n",
      "Epoch: 27, train_loss: 0.81185, time: 0.01637\n",
      "Epoch: 28, train_loss: 0.81007, time: 0.01592\n",
      "Epoch: 29, train_loss: 0.81388, time: 0.01603\n",
      "Epoch: 30, train_loss: 0.81028, time: 0.01676\n",
      "Epoch: 31, train_loss: 0.80995, time: 0.01597\n",
      "Epoch: 32, train_loss: 0.81047, time: 0.01675\n",
      "Epoch: 33, train_loss: 0.80883, time: 0.01741\n",
      "Epoch: 34, train_loss: 0.81144, time: 0.01795\n",
      "Epoch: 35, train_loss: 0.81199, time: 0.01669\n",
      "Epoch: 36, train_loss: 0.81228, time: 0.01723\n",
      "Epoch: 37, train_loss: 0.81202, time: 0.01645\n",
      "Epoch: 38, train_loss: 0.81353, time: 0.01699\n",
      "Epoch: 39, train_loss: 0.81090, time: 0.01727\n",
      "Epoch: 40, train_loss: 0.81006, time: 0.01690\n",
      "Epoch: 41, train_loss: 0.81386, time: 0.01631\n",
      "Epoch: 42, train_loss: 0.81351, time: 0.01689\n",
      "Epoch: 43, train_loss: 0.81406, time: 0.01637\n",
      "Epoch: 44, train_loss: 0.81288, time: 0.01705\n",
      "Epoch: 45, train_loss: 0.81576, time: 0.01701\n",
      "Epoch: 46, train_loss: 0.81292, time: 0.01598\n",
      "Epoch: 47, train_loss: 0.81164, time: 0.01607\n",
      "Epoch: 48, train_loss: 0.81195, time: 0.01688\n",
      "Epoch: 49, train_loss: 0.80993, time: 0.01742\n",
      "Epoch: 50, train_loss: 0.81591, time: 0.01781\n",
      "Epoch: 51, train_loss: 0.81346, time: 0.01857\n",
      "Epoch: 52, train_loss: 0.81242, time: 0.01724\n",
      "Epoch: 53, train_loss: 0.81279, time: 0.01701\n",
      "Epoch: 54, train_loss: 0.81110, time: 0.01759\n",
      "Epoch: 55, train_loss: 0.81106, time: 0.01671\n",
      "Epoch: 56, train_loss: 0.81326, time: 0.01686\n",
      "Epoch: 57, train_loss: 0.81303, time: 0.01648\n",
      "Epoch: 58, train_loss: 0.80985, time: 0.01674\n",
      "Epoch: 59, train_loss: 0.81228, time: 0.01736\n",
      "Epoch: 60, train_loss: 0.81222, time: 0.01729\n",
      "Epoch: 61, train_loss: 0.81212, time: 0.01627\n",
      "Epoch: 62, train_loss: 0.81212, time: 0.01671\n",
      "Epoch: 63, train_loss: 0.81199, time: 0.01680\n",
      "Epoch: 64, train_loss: 0.81344, time: 0.01731\n",
      "Epoch: 65, train_loss: 0.81020, time: 0.01679\n",
      "Epoch: 66, train_loss: 0.81232, time: 0.01673\n",
      "Epoch: 67, train_loss: 0.81186, time: 0.01711\n",
      "Epoch: 68, train_loss: 0.81507, time: 0.01692\n",
      "Epoch: 69, train_loss: 0.81233, time: 0.01700\n",
      "Epoch: 70, train_loss: 0.81374, time: 0.01674\n",
      "Epoch: 71, train_loss: 0.81275, time: 0.01678\n",
      "Epoch: 72, train_loss: 0.81387, time: 0.01650\n",
      "Epoch: 73, train_loss: 0.81248, time: 0.01688\n",
      "Epoch: 74, train_loss: 0.81476, time: 0.01633\n",
      "Epoch: 75, train_loss: 0.81352, time: 0.01618\n",
      "Epoch: 76, train_loss: 0.81196, time: 0.01647\n",
      "Epoch: 77, train_loss: 0.80900, time: 0.01654\n",
      "Epoch: 78, train_loss: 0.81245, time: 0.01669\n",
      "Epoch: 79, train_loss: 0.81066, time: 0.01670\n",
      "Epoch: 80, train_loss: 0.81413, time: 0.01633\n",
      "Epoch: 81, train_loss: 0.81113, time: 0.01629\n",
      "Epoch: 82, train_loss: 0.81374, time: 0.01660\n",
      "Epoch: 83, train_loss: 0.81529, time: 0.01569\n",
      "Epoch: 84, train_loss: 0.81314, time: 0.01578\n",
      "Epoch: 85, train_loss: 0.81044, time: 0.01626\n",
      "Epoch: 86, train_loss: 0.81023, time: 0.01629\n",
      "Epoch: 87, train_loss: 0.81068, time: 0.01629\n",
      "Epoch: 88, train_loss: 0.81465, time: 0.01642\n",
      "Epoch: 89, train_loss: 0.81289, time: 0.01639\n",
      "Epoch: 90, train_loss: 0.81246, time: 0.01621\n",
      "Epoch: 91, train_loss: 0.81256, time: 0.01643\n",
      "Epoch: 92, train_loss: 0.81032, time: 0.01892\n",
      "Epoch: 93, train_loss: 0.81318, time: 0.01780\n",
      "Epoch: 94, train_loss: 0.80894, time: 0.01757\n",
      "Epoch: 95, train_loss: 0.81250, time: 0.01682\n",
      "Epoch: 96, train_loss: 0.81321, time: 0.01671\n",
      "Epoch: 97, train_loss: 0.81288, time: 0.01708\n",
      "Epoch: 98, train_loss: 0.81485, time: 0.01792\n",
      "Epoch: 99, train_loss: 0.81069, time: 0.01817\n",
      "Epoch: 100, train_loss: 0.81374, time: 0.01760\n",
      "Epoch: 101, train_loss: 0.81173, time: 0.01767\n",
      "Epoch: 102, train_loss: 0.81092, time: 0.01756\n",
      "Epoch: 103, train_loss: 0.81139, time: 0.01788\n",
      "Epoch: 104, train_loss: 0.81168, time: 0.01886\n",
      "Epoch: 105, train_loss: 0.81263, time: 0.01798\n",
      "Epoch: 106, train_loss: 0.81214, time: 0.01731\n",
      "Epoch: 107, train_loss: 0.80847, time: 0.01667\n",
      "Epoch: 108, train_loss: 0.81345, time: 0.01642\n",
      "Epoch: 109, train_loss: 0.81181, time: 0.01836\n",
      "Epoch: 110, train_loss: 0.81172, time: 0.01684\n",
      "Epoch: 111, train_loss: 0.81373, time: 0.01731\n",
      "Epoch: 112, train_loss: 0.81144, time: 0.01758\n",
      "Epoch: 113, train_loss: 0.81337, time: 0.01699\n",
      "Epoch: 114, train_loss: 0.81219, time: 0.01654\n",
      "Epoch: 115, train_loss: 0.81426, time: 0.01620\n",
      "Epoch: 116, train_loss: 0.81272, time: 0.01618\n",
      "Epoch: 117, train_loss: 0.81177, time: 0.01701\n",
      "Epoch: 118, train_loss: 0.81367, time: 0.01595\n",
      "Epoch: 119, train_loss: 0.81294, time: 0.01612\n",
      "Epoch: 120, train_loss: 0.81078, time: 0.01594\n",
      "Epoch: 121, train_loss: 0.81050, time: 0.01551\n",
      "Epoch: 122, train_loss: 0.81138, time: 0.01589\n",
      "Epoch: 123, train_loss: 0.81279, time: 0.01580\n",
      "Epoch: 124, train_loss: 0.81137, time: 0.01616\n",
      "Epoch: 125, train_loss: 0.81132, time: 0.01599\n",
      "Epoch: 126, train_loss: 0.80930, time: 0.01580\n",
      "Epoch: 127, train_loss: 0.81090, time: 0.01622\n",
      "Epoch: 128, train_loss: 0.81331, time: 0.01634\n",
      "Epoch: 129, train_loss: 0.81399, time: 0.01705\n",
      "Epoch: 130, train_loss: 0.81057, time: 0.01676\n",
      "Epoch: 131, train_loss: 0.81298, time: 0.01668\n",
      "Epoch: 132, train_loss: 0.81243, time: 0.01650\n",
      "Epoch: 133, train_loss: 0.81143, time: 0.01602\n",
      "Epoch: 134, train_loss: 0.81169, time: 0.01631\n",
      "Epoch: 135, train_loss: 0.81156, time: 0.01560\n",
      "Epoch: 136, train_loss: 0.81381, time: 0.01541\n",
      "Epoch: 137, train_loss: 0.81059, time: 0.01648\n",
      "Epoch: 138, train_loss: 0.81241, time: 0.01675\n",
      "Epoch: 139, train_loss: 0.81041, time: 0.01684\n",
      "Epoch: 140, train_loss: 0.81153, time: 0.01675\n",
      "Epoch: 141, train_loss: 0.81386, time: 0.01726\n",
      "Epoch: 142, train_loss: 0.81087, time: 0.01813\n",
      "Epoch: 143, train_loss: 0.81083, time: 0.01806\n",
      "Epoch: 144, train_loss: 0.80974, time: 0.01772\n",
      "Epoch: 145, train_loss: 0.81659, time: 0.01838\n",
      "Epoch: 146, train_loss: 0.81226, time: 0.01783\n",
      "Epoch: 147, train_loss: 0.81393, time: 0.01738\n",
      "Epoch: 148, train_loss: 0.81281, time: 0.01644\n",
      "Epoch: 149, train_loss: 0.81241, time: 0.01724\n",
      "Epoch: 150, train_loss: 0.81387, time: 0.01646\n",
      "Epoch: 151, train_loss: 0.81131, time: 0.01611\n",
      "Epoch: 152, train_loss: 0.81263, time: 0.01734\n",
      "Epoch: 153, train_loss: 0.81244, time: 0.01914\n",
      "Epoch: 154, train_loss: 0.81370, time: 0.01876\n",
      "Epoch: 155, train_loss: 0.81409, time: 0.01866\n",
      "Epoch: 156, train_loss: 0.81363, time: 0.01796\n",
      "Epoch: 157, train_loss: 0.81495, time: 0.01761\n",
      "Epoch: 158, train_loss: 0.81290, time: 0.01807\n",
      "Epoch: 159, train_loss: 0.81381, time: 0.01767\n",
      "Epoch: 160, train_loss: 0.81326, time: 0.01690\n",
      "Epoch: 161, train_loss: 0.81137, time: 0.01669\n",
      "Epoch: 162, train_loss: 0.81431, time: 0.01694\n",
      "Epoch: 163, train_loss: 0.81369, time: 0.01783\n",
      "Epoch: 164, train_loss: 0.81168, time: 0.01872\n",
      "Epoch: 165, train_loss: 0.81178, time: 0.01816\n",
      "Epoch: 166, train_loss: 0.81128, time: 0.01857\n",
      "Epoch: 167, train_loss: 0.81288, time: 0.01760\n",
      "Epoch: 168, train_loss: 0.81470, time: 0.01827\n",
      "Epoch: 169, train_loss: 0.81003, time: 0.01786\n",
      "Epoch: 170, train_loss: 0.81312, time: 0.01754\n",
      "Epoch: 171, train_loss: 0.81245, time: 0.01771\n",
      "Epoch: 172, train_loss: 0.81297, time: 0.01805\n",
      "Epoch: 173, train_loss: 0.81062, time: 0.01722\n",
      "Epoch: 174, train_loss: 0.81340, time: 0.01651\n",
      "Epoch: 175, train_loss: 0.81277, time: 0.01644\n",
      "Epoch: 176, train_loss: 0.81240, time: 0.01737\n",
      "Epoch: 177, train_loss: 0.81098, time: 0.01724\n",
      "Epoch: 178, train_loss: 0.81139, time: 0.01722\n",
      "Epoch: 179, train_loss: 0.81181, time: 0.01693\n",
      "Epoch: 180, train_loss: 0.81462, time: 0.01766\n",
      "Epoch: 181, train_loss: 0.81302, time: 0.01716\n",
      "Epoch: 182, train_loss: 0.81328, time: 0.01714\n",
      "Epoch: 183, train_loss: 0.81293, time: 0.01775\n",
      "Epoch: 184, train_loss: 0.81312, time: 0.01728\n",
      "Epoch: 185, train_loss: 0.81515, time: 0.01767\n",
      "Epoch: 186, train_loss: 0.81139, time: 0.01733\n",
      "Epoch: 187, train_loss: 0.81183, time: 0.01718\n",
      "Epoch: 188, train_loss: 0.81331, time: 0.01663\n",
      "Epoch: 189, train_loss: 0.81171, time: 0.01696\n",
      "Epoch: 190, train_loss: 0.81194, time: 0.01735\n",
      "Epoch: 191, train_loss: 0.81103, time: 0.01736\n",
      "Epoch: 192, train_loss: 0.81180, time: 0.01737\n",
      "Epoch: 193, train_loss: 0.81108, time: 0.01747\n",
      "Epoch: 194, train_loss: 0.81243, time: 0.01772\n",
      "Epoch: 195, train_loss: 0.80698, time: 0.01735\n",
      "Epoch: 196, train_loss: 0.81195, time: 0.01763\n",
      "Epoch: 197, train_loss: 0.81199, time: 0.01767\n",
      "Epoch: 198, train_loss: 0.81038, time: 0.01657\n",
      "Epoch: 199, train_loss: 0.81135, time: 0.01723\n",
      "Epoch: 200, train_loss: 0.81281, time: 0.01667\n",
      "pairwise precision 0.93244 recall 0.51223 f1 0.66122\n",
      "average until now [0.4327887815857335, 0.8272083706801086, 0.5682655746012822]\n",
      "87 names 356.26554465293884 avg time 4.095006260378607\n",
      "Loading j_h_lin dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 189 nodes, 1803 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.94964, time: 0.10961\n",
      "Epoch: 2, train_loss: 0.95087, time: 0.01262\n",
      "Epoch: 3, train_loss: 0.94521, time: 0.01093\n",
      "Epoch: 4, train_loss: 0.95170, time: 0.01064\n",
      "Epoch: 5, train_loss: 0.95308, time: 0.01055\n",
      "Epoch: 6, train_loss: 0.95167, time: 0.01085\n",
      "Epoch: 7, train_loss: 0.95066, time: 0.01092\n",
      "Epoch: 8, train_loss: 0.95314, time: 0.01070\n",
      "Epoch: 9, train_loss: 0.94865, time: 0.01099\n",
      "Epoch: 10, train_loss: 0.94919, time: 0.01088\n",
      "Epoch: 11, train_loss: 0.94585, time: 0.01058\n",
      "Epoch: 12, train_loss: 0.94733, time: 0.01070\n",
      "Epoch: 13, train_loss: 0.94861, time: 0.01056\n",
      "Epoch: 14, train_loss: 0.95129, time: 0.01054\n",
      "Epoch: 15, train_loss: 0.94996, time: 0.01049\n",
      "Epoch: 16, train_loss: 0.94942, time: 0.01064\n",
      "Epoch: 17, train_loss: 0.95171, time: 0.01083\n",
      "Epoch: 18, train_loss: 0.94892, time: 0.01092\n",
      "Epoch: 19, train_loss: 0.95104, time: 0.01037\n",
      "Epoch: 20, train_loss: 0.95184, time: 0.01080\n",
      "Epoch: 21, train_loss: 0.94923, time: 0.01051\n",
      "Epoch: 22, train_loss: 0.94690, time: 0.01076\n",
      "Epoch: 23, train_loss: 0.95035, time: 0.01046\n",
      "Epoch: 24, train_loss: 0.95025, time: 0.01081\n",
      "Epoch: 25, train_loss: 0.94962, time: 0.01062\n",
      "Epoch: 26, train_loss: 0.95331, time: 0.01047\n",
      "Epoch: 27, train_loss: 0.95136, time: 0.01086\n",
      "Epoch: 28, train_loss: 0.94856, time: 0.01086\n",
      "Epoch: 29, train_loss: 0.94942, time: 0.01096\n",
      "Epoch: 30, train_loss: 0.95078, time: 0.01040\n",
      "Epoch: 31, train_loss: 0.95184, time: 0.01085\n",
      "Epoch: 32, train_loss: 0.94902, time: 0.01020\n",
      "Epoch: 33, train_loss: 0.94835, time: 0.01041\n",
      "Epoch: 34, train_loss: 0.94931, time: 0.01086\n",
      "Epoch: 35, train_loss: 0.94799, time: 0.01064\n",
      "Epoch: 36, train_loss: 0.94595, time: 0.01077\n",
      "Epoch: 37, train_loss: 0.94724, time: 0.01051\n",
      "Epoch: 38, train_loss: 0.95007, time: 0.01090\n",
      "Epoch: 39, train_loss: 0.94907, time: 0.01056\n",
      "Epoch: 40, train_loss: 0.94967, time: 0.01086\n",
      "Epoch: 41, train_loss: 0.94929, time: 0.01036\n",
      "Epoch: 42, train_loss: 0.94887, time: 0.01094\n",
      "Epoch: 43, train_loss: 0.95183, time: 0.01054\n",
      "Epoch: 44, train_loss: 0.94852, time: 0.01081\n",
      "Epoch: 45, train_loss: 0.94589, time: 0.01044\n",
      "Epoch: 46, train_loss: 0.95119, time: 0.01093\n",
      "Epoch: 47, train_loss: 0.94732, time: 0.01115\n",
      "Epoch: 48, train_loss: 0.94707, time: 0.01080\n",
      "Epoch: 49, train_loss: 0.94969, time: 0.01079\n",
      "Epoch: 50, train_loss: 0.94464, time: 0.01045\n",
      "Epoch: 51, train_loss: 0.94925, time: 0.01068\n",
      "Epoch: 52, train_loss: 0.94803, time: 0.01072\n",
      "Epoch: 53, train_loss: 0.94878, time: 0.01059\n",
      "Epoch: 54, train_loss: 0.94929, time: 0.01067\n",
      "Epoch: 55, train_loss: 0.94501, time: 0.01048\n",
      "Epoch: 56, train_loss: 0.94898, time: 0.01048\n",
      "Epoch: 57, train_loss: 0.94624, time: 0.01045\n",
      "Epoch: 58, train_loss: 0.94955, time: 0.01118\n",
      "Epoch: 59, train_loss: 0.95052, time: 0.01060\n",
      "Epoch: 60, train_loss: 0.94771, time: 0.01078\n",
      "Epoch: 61, train_loss: 0.94547, time: 0.01076\n",
      "Epoch: 62, train_loss: 0.94585, time: 0.01039\n",
      "Epoch: 63, train_loss: 0.94622, time: 0.01078\n",
      "Epoch: 64, train_loss: 0.95079, time: 0.01057\n",
      "Epoch: 65, train_loss: 0.94972, time: 0.01109\n",
      "Epoch: 66, train_loss: 0.95317, time: 0.01084\n",
      "Epoch: 67, train_loss: 0.94865, time: 0.01086\n",
      "Epoch: 68, train_loss: 0.95038, time: 0.01040\n",
      "Epoch: 69, train_loss: 0.95417, time: 0.01052\n",
      "Epoch: 70, train_loss: 0.94717, time: 0.01021\n",
      "Epoch: 71, train_loss: 0.95027, time: 0.01087\n",
      "Epoch: 72, train_loss: 0.94608, time: 0.01046\n",
      "Epoch: 73, train_loss: 0.94772, time: 0.01079\n",
      "Epoch: 74, train_loss: 0.94582, time: 0.01057\n",
      "Epoch: 75, train_loss: 0.95112, time: 0.01082\n",
      "Epoch: 76, train_loss: 0.94854, time: 0.01060\n",
      "Epoch: 77, train_loss: 0.94763, time: 0.01102\n",
      "Epoch: 78, train_loss: 0.94922, time: 0.01053\n",
      "Epoch: 79, train_loss: 0.95234, time: 0.01062\n",
      "Epoch: 80, train_loss: 0.95181, time: 0.01049\n",
      "Epoch: 81, train_loss: 0.94510, time: 0.01055\n",
      "Epoch: 82, train_loss: 0.94774, time: 0.01085\n",
      "Epoch: 83, train_loss: 0.95228, time: 0.01069\n",
      "Epoch: 84, train_loss: 0.95037, time: 0.01080\n",
      "Epoch: 85, train_loss: 0.94919, time: 0.01110\n",
      "Epoch: 86, train_loss: 0.94889, time: 0.01091\n",
      "Epoch: 87, train_loss: 0.94298, time: 0.01081\n",
      "Epoch: 88, train_loss: 0.94858, time: 0.01104\n",
      "Epoch: 89, train_loss: 0.94753, time: 0.01067\n",
      "Epoch: 90, train_loss: 0.94555, time: 0.01089\n",
      "Epoch: 91, train_loss: 0.94385, time: 0.01067\n",
      "Epoch: 92, train_loss: 0.94901, time: 0.01140\n",
      "Epoch: 93, train_loss: 0.94701, time: 0.01091\n",
      "Epoch: 94, train_loss: 0.94765, time: 0.01043\n",
      "Epoch: 95, train_loss: 0.94998, time: 0.01110\n",
      "Epoch: 96, train_loss: 0.94757, time: 0.01057\n",
      "Epoch: 97, train_loss: 0.95329, time: 0.01101\n",
      "Epoch: 98, train_loss: 0.94740, time: 0.01053\n",
      "Epoch: 99, train_loss: 0.94848, time: 0.01086\n",
      "Epoch: 100, train_loss: 0.95392, time: 0.01068\n",
      "Epoch: 101, train_loss: 0.94850, time: 0.01075\n",
      "Epoch: 102, train_loss: 0.94662, time: 0.01040\n",
      "Epoch: 103, train_loss: 0.94874, time: 0.01081\n",
      "Epoch: 104, train_loss: 0.94861, time: 0.01133\n",
      "Epoch: 105, train_loss: 0.95357, time: 0.01085\n",
      "Epoch: 106, train_loss: 0.95032, time: 0.01090\n",
      "Epoch: 107, train_loss: 0.94716, time: 0.01086\n",
      "Epoch: 108, train_loss: 0.94786, time: 0.01115\n",
      "Epoch: 109, train_loss: 0.95116, time: 0.01073\n",
      "Epoch: 110, train_loss: 0.94784, time: 0.01053\n",
      "Epoch: 111, train_loss: 0.94666, time: 0.01078\n",
      "Epoch: 112, train_loss: 0.95191, time: 0.01057\n",
      "Epoch: 113, train_loss: 0.95154, time: 0.01073\n",
      "Epoch: 114, train_loss: 0.95391, time: 0.01084\n",
      "Epoch: 115, train_loss: 0.95280, time: 0.01047\n",
      "Epoch: 116, train_loss: 0.94986, time: 0.01082\n",
      "Epoch: 117, train_loss: 0.95308, time: 0.01047\n",
      "Epoch: 118, train_loss: 0.95206, time: 0.01086\n",
      "Epoch: 119, train_loss: 0.95222, time: 0.01070\n",
      "Epoch: 120, train_loss: 0.95307, time: 0.01076\n",
      "Epoch: 121, train_loss: 0.94729, time: 0.01035\n",
      "Epoch: 122, train_loss: 0.94711, time: 0.01065\n",
      "Epoch: 123, train_loss: 0.94977, time: 0.01094\n",
      "Epoch: 124, train_loss: 0.94958, time: 0.01094\n",
      "Epoch: 125, train_loss: 0.95109, time: 0.01054\n",
      "Epoch: 126, train_loss: 0.95258, time: 0.01083\n",
      "Epoch: 127, train_loss: 0.94873, time: 0.01048\n",
      "Epoch: 128, train_loss: 0.95156, time: 0.01073\n",
      "Epoch: 129, train_loss: 0.95680, time: 0.01095\n",
      "Epoch: 130, train_loss: 0.95078, time: 0.01058\n",
      "Epoch: 131, train_loss: 0.94712, time: 0.01107\n",
      "Epoch: 132, train_loss: 0.94895, time: 0.01070\n",
      "Epoch: 133, train_loss: 0.94866, time: 0.01088\n",
      "Epoch: 134, train_loss: 0.94980, time: 0.01087\n",
      "Epoch: 135, train_loss: 0.94572, time: 0.01108\n",
      "Epoch: 136, train_loss: 0.94933, time: 0.01073\n",
      "Epoch: 137, train_loss: 0.94783, time: 0.01074\n",
      "Epoch: 138, train_loss: 0.94943, time: 0.01080\n",
      "Epoch: 139, train_loss: 0.95049, time: 0.01066\n",
      "Epoch: 140, train_loss: 0.94796, time: 0.01076\n",
      "Epoch: 141, train_loss: 0.95001, time: 0.01080\n",
      "Epoch: 142, train_loss: 0.94884, time: 0.01094\n",
      "Epoch: 143, train_loss: 0.94643, time: 0.01100\n",
      "Epoch: 144, train_loss: 0.94912, time: 0.01056\n",
      "Epoch: 145, train_loss: 0.94857, time: 0.01092\n",
      "Epoch: 146, train_loss: 0.95103, time: 0.01084\n",
      "Epoch: 147, train_loss: 0.95065, time: 0.01093\n",
      "Epoch: 148, train_loss: 0.95036, time: 0.01082\n",
      "Epoch: 149, train_loss: 0.94922, time: 0.01072\n",
      "Epoch: 150, train_loss: 0.94817, time: 0.01064\n",
      "Epoch: 151, train_loss: 0.94898, time: 0.01057\n",
      "Epoch: 152, train_loss: 0.95265, time: 0.01064\n",
      "Epoch: 153, train_loss: 0.94459, time: 0.01041\n",
      "Epoch: 154, train_loss: 0.95075, time: 0.01106\n",
      "Epoch: 155, train_loss: 0.94989, time: 0.01109\n",
      "Epoch: 156, train_loss: 0.95132, time: 0.01041\n",
      "Epoch: 157, train_loss: 0.94959, time: 0.01037\n",
      "Epoch: 158, train_loss: 0.94943, time: 0.01075\n",
      "Epoch: 159, train_loss: 0.94978, time: 0.01061\n",
      "Epoch: 160, train_loss: 0.94966, time: 0.01055\n",
      "Epoch: 161, train_loss: 0.94814, time: 0.01066\n",
      "Epoch: 162, train_loss: 0.94927, time: 0.01058\n",
      "Epoch: 163, train_loss: 0.94641, time: 0.01066\n",
      "Epoch: 164, train_loss: 0.94926, time: 0.01080\n",
      "Epoch: 165, train_loss: 0.95022, time: 0.01092\n",
      "Epoch: 166, train_loss: 0.94744, time: 0.01086\n",
      "Epoch: 167, train_loss: 0.95227, time: 0.01072\n",
      "Epoch: 168, train_loss: 0.94585, time: 0.01061\n",
      "Epoch: 169, train_loss: 0.95497, time: 0.01105\n",
      "Epoch: 170, train_loss: 0.94792, time: 0.01079\n",
      "Epoch: 171, train_loss: 0.94982, time: 0.01052\n",
      "Epoch: 172, train_loss: 0.95212, time: 0.01070\n",
      "Epoch: 173, train_loss: 0.94803, time: 0.01049\n",
      "Epoch: 174, train_loss: 0.94782, time: 0.01074\n",
      "Epoch: 175, train_loss: 0.94817, time: 0.01081\n",
      "Epoch: 176, train_loss: 0.94929, time: 0.01055\n",
      "Epoch: 177, train_loss: 0.94615, time: 0.01080\n",
      "Epoch: 178, train_loss: 0.95310, time: 0.01088\n",
      "Epoch: 179, train_loss: 0.94819, time: 0.01087\n",
      "Epoch: 180, train_loss: 0.95031, time: 0.01100\n",
      "Epoch: 181, train_loss: 0.94898, time: 0.01103\n",
      "Epoch: 182, train_loss: 0.94774, time: 0.01101\n",
      "Epoch: 183, train_loss: 0.94504, time: 0.01065\n",
      "Epoch: 184, train_loss: 0.94525, time: 0.01099\n",
      "Epoch: 185, train_loss: 0.94901, time: 0.01047\n",
      "Epoch: 186, train_loss: 0.95380, time: 0.01073\n",
      "Epoch: 187, train_loss: 0.94499, time: 0.01058\n",
      "Epoch: 188, train_loss: 0.95466, time: 0.01093\n",
      "Epoch: 189, train_loss: 0.94772, time: 0.01055\n",
      "Epoch: 190, train_loss: 0.94863, time: 0.01065\n",
      "Epoch: 191, train_loss: 0.94677, time: 0.01051\n",
      "Epoch: 192, train_loss: 0.94543, time: 0.01078\n",
      "Epoch: 193, train_loss: 0.95274, time: 0.01042\n",
      "Epoch: 194, train_loss: 0.94699, time: 0.01071\n",
      "Epoch: 195, train_loss: 0.94687, time: 0.01089\n",
      "Epoch: 196, train_loss: 0.94704, time: 0.01064\n",
      "Epoch: 197, train_loss: 0.94643, time: 0.01092\n",
      "Epoch: 198, train_loss: 0.95022, time: 0.01042\n",
      "Epoch: 199, train_loss: 0.95378, time: 0.01109\n",
      "Epoch: 200, train_loss: 0.95035, time: 0.01067\n",
      "pairwise precision 0.14604 recall 0.95328 f1 0.25327\n",
      "average until now [0.4295302404987753, 0.8286409919208214, 0.565783663424588]\n",
      "88 names 358.56165885925293 avg time 4.074564305218783\n",
      "Loading lili_ma dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 133 nodes, 261 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.99825, time: 0.10765\n",
      "Epoch: 2, train_loss: 0.99041, time: 0.01007\n",
      "Epoch: 3, train_loss: 0.99070, time: 0.00922\n",
      "Epoch: 4, train_loss: 0.98433, time: 0.00907\n",
      "Epoch: 5, train_loss: 0.98634, time: 0.00899\n",
      "Epoch: 6, train_loss: 0.98708, time: 0.00886\n",
      "Epoch: 7, train_loss: 0.98736, time: 0.00938\n",
      "Epoch: 8, train_loss: 0.99051, time: 0.00894\n",
      "Epoch: 9, train_loss: 0.98114, time: 0.00883\n",
      "Epoch: 10, train_loss: 0.98887, time: 0.00875\n",
      "Epoch: 11, train_loss: 0.98867, time: 0.00882\n",
      "Epoch: 12, train_loss: 0.98550, time: 0.00874\n",
      "Epoch: 13, train_loss: 0.98853, time: 0.00881\n",
      "Epoch: 14, train_loss: 0.98956, time: 0.00872\n",
      "Epoch: 15, train_loss: 0.99456, time: 0.00864\n",
      "Epoch: 16, train_loss: 0.98569, time: 0.00868\n",
      "Epoch: 17, train_loss: 0.99341, time: 0.00870\n",
      "Epoch: 18, train_loss: 0.98964, time: 0.00868\n",
      "Epoch: 19, train_loss: 0.99089, time: 0.00870\n",
      "Epoch: 20, train_loss: 0.98358, time: 0.00867\n",
      "Epoch: 21, train_loss: 0.98628, time: 0.00866\n",
      "Epoch: 22, train_loss: 0.98936, time: 0.00869\n",
      "Epoch: 23, train_loss: 0.98498, time: 0.00869\n",
      "Epoch: 24, train_loss: 0.98319, time: 0.00872\n",
      "Epoch: 25, train_loss: 0.98600, time: 0.00865\n",
      "Epoch: 26, train_loss: 0.98699, time: 0.00877\n",
      "Epoch: 27, train_loss: 0.98079, time: 0.00862\n",
      "Epoch: 28, train_loss: 0.98667, time: 0.00860\n",
      "Epoch: 29, train_loss: 0.98801, time: 0.00866\n",
      "Epoch: 30, train_loss: 0.98941, time: 0.00868\n",
      "Epoch: 31, train_loss: 0.98905, time: 0.00920\n",
      "Epoch: 32, train_loss: 0.98637, time: 0.00889\n",
      "Epoch: 33, train_loss: 0.98853, time: 0.00870\n",
      "Epoch: 34, train_loss: 0.99222, time: 0.00868\n",
      "Epoch: 35, train_loss: 0.99075, time: 0.00872\n",
      "Epoch: 36, train_loss: 0.98893, time: 0.00864\n",
      "Epoch: 37, train_loss: 0.98668, time: 0.00873\n",
      "Epoch: 38, train_loss: 0.99019, time: 0.00866\n",
      "Epoch: 39, train_loss: 0.98350, time: 0.00857\n",
      "Epoch: 40, train_loss: 0.98856, time: 0.00867\n",
      "Epoch: 41, train_loss: 0.98680, time: 0.00854\n",
      "Epoch: 42, train_loss: 0.98814, time: 0.00863\n",
      "Epoch: 43, train_loss: 0.98917, time: 0.00868\n",
      "Epoch: 44, train_loss: 0.98415, time: 0.00883\n",
      "Epoch: 45, train_loss: 0.98907, time: 0.00869\n",
      "Epoch: 46, train_loss: 0.98648, time: 0.00869\n",
      "Epoch: 47, train_loss: 0.98924, time: 0.00863\n",
      "Epoch: 48, train_loss: 0.98958, time: 0.00864\n",
      "Epoch: 49, train_loss: 0.98784, time: 0.00863\n",
      "Epoch: 50, train_loss: 0.98732, time: 0.00864\n",
      "Epoch: 51, train_loss: 0.98562, time: 0.00863\n",
      "Epoch: 52, train_loss: 0.98637, time: 0.00858\n",
      "Epoch: 53, train_loss: 0.99022, time: 0.00860\n",
      "Epoch: 54, train_loss: 0.98769, time: 0.00858\n",
      "Epoch: 55, train_loss: 0.98531, time: 0.00903\n",
      "Epoch: 56, train_loss: 0.98349, time: 0.00869\n",
      "Epoch: 57, train_loss: 0.98266, time: 0.00864\n",
      "Epoch: 58, train_loss: 0.98854, time: 0.00865\n",
      "Epoch: 59, train_loss: 0.98639, time: 0.00869\n",
      "Epoch: 60, train_loss: 0.98334, time: 0.00871\n",
      "Epoch: 61, train_loss: 0.99260, time: 0.00865\n",
      "Epoch: 62, train_loss: 0.99155, time: 0.00861\n",
      "Epoch: 63, train_loss: 0.98664, time: 0.00864\n",
      "Epoch: 64, train_loss: 0.99515, time: 0.00865\n",
      "Epoch: 65, train_loss: 0.98633, time: 0.00862\n",
      "Epoch: 66, train_loss: 0.98541, time: 0.00871\n",
      "Epoch: 67, train_loss: 0.98340, time: 0.00875\n",
      "Epoch: 68, train_loss: 0.99771, time: 0.00866\n",
      "Epoch: 69, train_loss: 0.99515, time: 0.00870\n",
      "Epoch: 70, train_loss: 0.98921, time: 0.00861\n",
      "Epoch: 71, train_loss: 0.98658, time: 0.00865\n",
      "Epoch: 72, train_loss: 0.98411, time: 0.00865\n",
      "Epoch: 73, train_loss: 0.99322, time: 0.00872\n",
      "Epoch: 74, train_loss: 0.99161, time: 0.00875\n",
      "Epoch: 75, train_loss: 0.98580, time: 0.00866\n",
      "Epoch: 76, train_loss: 0.98759, time: 0.00878\n",
      "Epoch: 77, train_loss: 0.99007, time: 0.00861\n",
      "Epoch: 78, train_loss: 0.98575, time: 0.00868\n",
      "Epoch: 79, train_loss: 0.98969, time: 0.00907\n",
      "Epoch: 80, train_loss: 0.98402, time: 0.00889\n",
      "Epoch: 81, train_loss: 0.98728, time: 0.00869\n",
      "Epoch: 82, train_loss: 0.98634, time: 0.00865\n",
      "Epoch: 83, train_loss: 0.98731, time: 0.00867\n",
      "Epoch: 84, train_loss: 0.98771, time: 0.00862\n",
      "Epoch: 85, train_loss: 0.99363, time: 0.00865\n",
      "Epoch: 86, train_loss: 0.98443, time: 0.00867\n",
      "Epoch: 87, train_loss: 0.98679, time: 0.00866\n",
      "Epoch: 88, train_loss: 0.99121, time: 0.00865\n",
      "Epoch: 89, train_loss: 0.98682, time: 0.00861\n",
      "Epoch: 90, train_loss: 0.98622, time: 0.00859\n",
      "Epoch: 91, train_loss: 0.98643, time: 0.00866\n",
      "Epoch: 92, train_loss: 0.98914, time: 0.00869\n",
      "Epoch: 93, train_loss: 0.98797, time: 0.00867\n",
      "Epoch: 94, train_loss: 0.98816, time: 0.00875\n",
      "Epoch: 95, train_loss: 0.98675, time: 0.00874\n",
      "Epoch: 96, train_loss: 0.98989, time: 0.00875\n",
      "Epoch: 97, train_loss: 0.98291, time: 0.00867\n",
      "Epoch: 98, train_loss: 0.98907, time: 0.00874\n",
      "Epoch: 99, train_loss: 0.99353, time: 0.00870\n",
      "Epoch: 100, train_loss: 0.99090, time: 0.00862\n",
      "Epoch: 101, train_loss: 0.98624, time: 0.00869\n",
      "Epoch: 102, train_loss: 0.98807, time: 0.00866\n",
      "Epoch: 103, train_loss: 0.98813, time: 0.00908\n",
      "Epoch: 104, train_loss: 0.98827, time: 0.00887\n",
      "Epoch: 105, train_loss: 0.98389, time: 0.00860\n",
      "Epoch: 106, train_loss: 0.98853, time: 0.00875\n",
      "Epoch: 107, train_loss: 0.98528, time: 0.00874\n",
      "Epoch: 108, train_loss: 0.98820, time: 0.00884\n",
      "Epoch: 109, train_loss: 0.99174, time: 0.00872\n",
      "Epoch: 110, train_loss: 0.98646, time: 0.00866\n",
      "Epoch: 111, train_loss: 0.98346, time: 0.00864\n",
      "Epoch: 112, train_loss: 0.98947, time: 0.00865\n",
      "Epoch: 113, train_loss: 0.98761, time: 0.00858\n",
      "Epoch: 114, train_loss: 0.98745, time: 0.00873\n",
      "Epoch: 115, train_loss: 0.98535, time: 0.00874\n",
      "Epoch: 116, train_loss: 0.98673, time: 0.00879\n",
      "Epoch: 117, train_loss: 0.99113, time: 0.00875\n",
      "Epoch: 118, train_loss: 0.99586, time: 0.00875\n",
      "Epoch: 119, train_loss: 0.98446, time: 0.00877\n",
      "Epoch: 120, train_loss: 0.98610, time: 0.00873\n",
      "Epoch: 121, train_loss: 0.98392, time: 0.00871\n",
      "Epoch: 122, train_loss: 0.99062, time: 0.00867\n",
      "Epoch: 123, train_loss: 0.98682, time: 0.00873\n",
      "Epoch: 124, train_loss: 0.98586, time: 0.00868\n",
      "Epoch: 125, train_loss: 0.98483, time: 0.00867\n",
      "Epoch: 126, train_loss: 0.98920, time: 0.00883\n",
      "Epoch: 127, train_loss: 0.98642, time: 0.00898\n",
      "Epoch: 128, train_loss: 0.98527, time: 0.00870\n",
      "Epoch: 129, train_loss: 0.98987, time: 0.00861\n",
      "Epoch: 130, train_loss: 0.98248, time: 0.00872\n",
      "Epoch: 131, train_loss: 0.98926, time: 0.00865\n",
      "Epoch: 132, train_loss: 0.98748, time: 0.00874\n",
      "Epoch: 133, train_loss: 0.98486, time: 0.00869\n",
      "Epoch: 134, train_loss: 0.98637, time: 0.00871\n",
      "Epoch: 135, train_loss: 0.98705, time: 0.00867\n",
      "Epoch: 136, train_loss: 0.98440, time: 0.00865\n",
      "Epoch: 137, train_loss: 0.98932, time: 0.00874\n",
      "Epoch: 138, train_loss: 0.98586, time: 0.00875\n",
      "Epoch: 139, train_loss: 0.98903, time: 0.00873\n",
      "Epoch: 140, train_loss: 0.98885, time: 0.00869\n",
      "Epoch: 141, train_loss: 0.99349, time: 0.00874\n",
      "Epoch: 142, train_loss: 0.98591, time: 0.00872\n",
      "Epoch: 143, train_loss: 0.98314, time: 0.00860\n",
      "Epoch: 144, train_loss: 0.98562, time: 0.00874\n",
      "Epoch: 145, train_loss: 0.98671, time: 0.00870\n",
      "Epoch: 146, train_loss: 0.98393, time: 0.00870\n",
      "Epoch: 147, train_loss: 0.98403, time: 0.00864\n",
      "Epoch: 148, train_loss: 0.98622, time: 0.00862\n",
      "Epoch: 149, train_loss: 0.98647, time: 0.00866\n",
      "Epoch: 150, train_loss: 0.98858, time: 0.00911\n",
      "Epoch: 151, train_loss: 0.98730, time: 0.00895\n",
      "Epoch: 152, train_loss: 0.98711, time: 0.00881\n",
      "Epoch: 153, train_loss: 0.97821, time: 0.00876\n",
      "Epoch: 154, train_loss: 0.99048, time: 0.00870\n",
      "Epoch: 155, train_loss: 0.98636, time: 0.00866\n",
      "Epoch: 156, train_loss: 0.98672, time: 0.00865\n",
      "Epoch: 157, train_loss: 0.98568, time: 0.00873\n",
      "Epoch: 158, train_loss: 0.98650, time: 0.00865\n",
      "Epoch: 159, train_loss: 0.98659, time: 0.00878\n",
      "Epoch: 160, train_loss: 0.99014, time: 0.00879\n",
      "Epoch: 161, train_loss: 0.98710, time: 0.00876\n",
      "Epoch: 162, train_loss: 0.98757, time: 0.00872\n",
      "Epoch: 163, train_loss: 0.98902, time: 0.00871\n",
      "Epoch: 164, train_loss: 0.98691, time: 0.00867\n",
      "Epoch: 165, train_loss: 0.98716, time: 0.00863\n",
      "Epoch: 166, train_loss: 0.98491, time: 0.00869\n",
      "Epoch: 167, train_loss: 0.98884, time: 0.00871\n",
      "Epoch: 168, train_loss: 0.98465, time: 0.00861\n",
      "Epoch: 169, train_loss: 0.98366, time: 0.00868\n",
      "Epoch: 170, train_loss: 0.98296, time: 0.00860\n",
      "Epoch: 171, train_loss: 0.98487, time: 0.00870\n",
      "Epoch: 172, train_loss: 0.98539, time: 0.00881\n",
      "Epoch: 173, train_loss: 0.98260, time: 0.00904\n",
      "Epoch: 174, train_loss: 0.98831, time: 0.00891\n",
      "Epoch: 175, train_loss: 0.98148, time: 0.00870\n",
      "Epoch: 176, train_loss: 0.99072, time: 0.00878\n",
      "Epoch: 177, train_loss: 0.98821, time: 0.00880\n",
      "Epoch: 178, train_loss: 0.99021, time: 0.00874\n",
      "Epoch: 179, train_loss: 0.98562, time: 0.00878\n",
      "Epoch: 180, train_loss: 0.98200, time: 0.00875\n",
      "Epoch: 181, train_loss: 0.98733, time: 0.00867\n",
      "Epoch: 182, train_loss: 0.98745, time: 0.00872\n",
      "Epoch: 183, train_loss: 0.99537, time: 0.00865\n",
      "Epoch: 184, train_loss: 0.98333, time: 0.00872\n",
      "Epoch: 185, train_loss: 0.99269, time: 0.01142\n",
      "Epoch: 186, train_loss: 0.98851, time: 0.00975\n",
      "Epoch: 187, train_loss: 0.98912, time: 0.00862\n",
      "Epoch: 188, train_loss: 0.98487, time: 0.00856\n",
      "Epoch: 189, train_loss: 0.98440, time: 0.00863\n",
      "Epoch: 190, train_loss: 0.98236, time: 0.00852\n",
      "Epoch: 191, train_loss: 0.99063, time: 0.00851\n",
      "Epoch: 192, train_loss: 0.98431, time: 0.00852\n",
      "Epoch: 193, train_loss: 0.98633, time: 0.00864\n",
      "Epoch: 194, train_loss: 0.99477, time: 0.00851\n",
      "Epoch: 195, train_loss: 0.98524, time: 0.00849\n",
      "Epoch: 196, train_loss: 0.99066, time: 0.00874\n",
      "Epoch: 197, train_loss: 0.98712, time: 0.00857\n",
      "Epoch: 198, train_loss: 0.98506, time: 0.00863\n",
      "Epoch: 199, train_loss: 0.98600, time: 0.00844\n",
      "Epoch: 200, train_loss: 0.98985, time: 0.00853\n",
      "pairwise precision 0.69100 recall 0.81648 f1 0.74852\n",
      "average until now [0.4324680756269911, 0.8285043851569662, 0.5682942462908307]\n",
      "89 names 360.44284176826477 avg time 4.0499195704299416\n",
      "Loading yongqing_li dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 150 nodes, 1425 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.94161, time: 0.10918\n",
      "Epoch: 2, train_loss: 0.93676, time: 0.01165\n",
      "Epoch: 3, train_loss: 0.93557, time: 0.01019\n",
      "Epoch: 4, train_loss: 0.93513, time: 0.01015\n",
      "Epoch: 5, train_loss: 0.93882, time: 0.01009\n",
      "Epoch: 6, train_loss: 0.93692, time: 0.00967\n",
      "Epoch: 7, train_loss: 0.93658, time: 0.01034\n",
      "Epoch: 8, train_loss: 0.93862, time: 0.01006\n",
      "Epoch: 9, train_loss: 0.93429, time: 0.00989\n",
      "Epoch: 10, train_loss: 0.93731, time: 0.00954\n",
      "Epoch: 11, train_loss: 0.93735, time: 0.01004\n",
      "Epoch: 12, train_loss: 0.93786, time: 0.00966\n",
      "Epoch: 13, train_loss: 0.93456, time: 0.01010\n",
      "Epoch: 14, train_loss: 0.93787, time: 0.00982\n",
      "Epoch: 15, train_loss: 0.94058, time: 0.00986\n",
      "Epoch: 16, train_loss: 0.93699, time: 0.00963\n",
      "Epoch: 17, train_loss: 0.94006, time: 0.01003\n",
      "Epoch: 18, train_loss: 0.93517, time: 0.00949\n",
      "Epoch: 19, train_loss: 0.93396, time: 0.00990\n",
      "Epoch: 20, train_loss: 0.93710, time: 0.00953\n",
      "Epoch: 21, train_loss: 0.93499, time: 0.01005\n",
      "Epoch: 22, train_loss: 0.93615, time: 0.00958\n",
      "Epoch: 23, train_loss: 0.93789, time: 0.01015\n",
      "Epoch: 24, train_loss: 0.93401, time: 0.00989\n",
      "Epoch: 25, train_loss: 0.93631, time: 0.01033\n",
      "Epoch: 26, train_loss: 0.94118, time: 0.00957\n",
      "Epoch: 27, train_loss: 0.93770, time: 0.01008\n",
      "Epoch: 28, train_loss: 0.93711, time: 0.00979\n",
      "Epoch: 29, train_loss: 0.93632, time: 0.01005\n",
      "Epoch: 30, train_loss: 0.93419, time: 0.00950\n",
      "Epoch: 31, train_loss: 0.93934, time: 0.01002\n",
      "Epoch: 32, train_loss: 0.93683, time: 0.00962\n",
      "Epoch: 33, train_loss: 0.93695, time: 0.00959\n",
      "Epoch: 34, train_loss: 0.93418, time: 0.00986\n",
      "Epoch: 35, train_loss: 0.93966, time: 0.00978\n",
      "Epoch: 36, train_loss: 0.93584, time: 0.01005\n",
      "Epoch: 37, train_loss: 0.93526, time: 0.00963\n",
      "Epoch: 38, train_loss: 0.93648, time: 0.01008\n",
      "Epoch: 39, train_loss: 0.93633, time: 0.00968\n",
      "Epoch: 40, train_loss: 0.93507, time: 0.01017\n",
      "Epoch: 41, train_loss: 0.93723, time: 0.00944\n",
      "Epoch: 42, train_loss: 0.93416, time: 0.00980\n",
      "Epoch: 43, train_loss: 0.94182, time: 0.01009\n",
      "Epoch: 44, train_loss: 0.93454, time: 0.00946\n",
      "Epoch: 45, train_loss: 0.93884, time: 0.01017\n",
      "Epoch: 46, train_loss: 0.94178, time: 0.00981\n",
      "Epoch: 47, train_loss: 0.93404, time: 0.01015\n",
      "Epoch: 48, train_loss: 0.93817, time: 0.00964\n",
      "Epoch: 49, train_loss: 0.93895, time: 0.01016\n",
      "Epoch: 50, train_loss: 0.93716, time: 0.00965\n",
      "Epoch: 51, train_loss: 0.93551, time: 0.00999\n",
      "Epoch: 52, train_loss: 0.93599, time: 0.01001\n",
      "Epoch: 53, train_loss: 0.94046, time: 0.01010\n",
      "Epoch: 54, train_loss: 0.94091, time: 0.00990\n",
      "Epoch: 55, train_loss: 0.93907, time: 0.00956\n",
      "Epoch: 56, train_loss: 0.94214, time: 0.00990\n",
      "Epoch: 57, train_loss: 0.93710, time: 0.00948\n",
      "Epoch: 58, train_loss: 0.93839, time: 0.01004\n",
      "Epoch: 59, train_loss: 0.94076, time: 0.00942\n",
      "Epoch: 60, train_loss: 0.93866, time: 0.01005\n",
      "Epoch: 61, train_loss: 0.94092, time: 0.00949\n",
      "Epoch: 62, train_loss: 0.93984, time: 0.00980\n",
      "Epoch: 63, train_loss: 0.93843, time: 0.00957\n",
      "Epoch: 64, train_loss: 0.94317, time: 0.01019\n",
      "Epoch: 65, train_loss: 0.93921, time: 0.00949\n",
      "Epoch: 66, train_loss: 0.94496, time: 0.01038\n",
      "Epoch: 67, train_loss: 0.93709, time: 0.00972\n",
      "Epoch: 68, train_loss: 0.93779, time: 0.01008\n",
      "Epoch: 69, train_loss: 0.93530, time: 0.00947\n",
      "Epoch: 70, train_loss: 0.93727, time: 0.01011\n",
      "Epoch: 71, train_loss: 0.93739, time: 0.00989\n",
      "Epoch: 72, train_loss: 0.93469, time: 0.00964\n",
      "Epoch: 73, train_loss: 0.94049, time: 0.01012\n",
      "Epoch: 74, train_loss: 0.93995, time: 0.00954\n",
      "Epoch: 75, train_loss: 0.93979, time: 0.01012\n",
      "Epoch: 76, train_loss: 0.94016, time: 0.00969\n",
      "Epoch: 77, train_loss: 0.93958, time: 0.00996\n",
      "Epoch: 78, train_loss: 0.93649, time: 0.00960\n",
      "Epoch: 79, train_loss: 0.93704, time: 0.00999\n",
      "Epoch: 80, train_loss: 0.93628, time: 0.00966\n",
      "Epoch: 81, train_loss: 0.93899, time: 0.01023\n",
      "Epoch: 82, train_loss: 0.93718, time: 0.00964\n",
      "Epoch: 83, train_loss: 0.93714, time: 0.01003\n",
      "Epoch: 84, train_loss: 0.94130, time: 0.00988\n",
      "Epoch: 85, train_loss: 0.93839, time: 0.00994\n",
      "Epoch: 86, train_loss: 0.93710, time: 0.00961\n",
      "Epoch: 87, train_loss: 0.93739, time: 0.01036\n",
      "Epoch: 88, train_loss: 0.94073, time: 0.00961\n",
      "Epoch: 89, train_loss: 0.93542, time: 0.00989\n",
      "Epoch: 90, train_loss: 0.94279, time: 0.00976\n",
      "Epoch: 91, train_loss: 0.93768, time: 0.01007\n",
      "Epoch: 92, train_loss: 0.93799, time: 0.00973\n",
      "Epoch: 93, train_loss: 0.93793, time: 0.00999\n",
      "Epoch: 94, train_loss: 0.93309, time: 0.00964\n",
      "Epoch: 95, train_loss: 0.93290, time: 0.01000\n",
      "Epoch: 96, train_loss: 0.93865, time: 0.00969\n",
      "Epoch: 97, train_loss: 0.93777, time: 0.00992\n",
      "Epoch: 98, train_loss: 0.93643, time: 0.00965\n",
      "Epoch: 99, train_loss: 0.93164, time: 0.00996\n",
      "Epoch: 100, train_loss: 0.93990, time: 0.00980\n",
      "Epoch: 101, train_loss: 0.93784, time: 0.00989\n",
      "Epoch: 102, train_loss: 0.93464, time: 0.00962\n",
      "Epoch: 103, train_loss: 0.93954, time: 0.00997\n",
      "Epoch: 104, train_loss: 0.93625, time: 0.00977\n",
      "Epoch: 105, train_loss: 0.93488, time: 0.00998\n",
      "Epoch: 106, train_loss: 0.93484, time: 0.00972\n",
      "Epoch: 107, train_loss: 0.93446, time: 0.01003\n",
      "Epoch: 108, train_loss: 0.93399, time: 0.00971\n",
      "Epoch: 109, train_loss: 0.93997, time: 0.01002\n",
      "Epoch: 110, train_loss: 0.93444, time: 0.00963\n",
      "Epoch: 111, train_loss: 0.93642, time: 0.01010\n",
      "Epoch: 112, train_loss: 0.93665, time: 0.00953\n",
      "Epoch: 113, train_loss: 0.93673, time: 0.00990\n",
      "Epoch: 114, train_loss: 0.93974, time: 0.00989\n",
      "Epoch: 115, train_loss: 0.93596, time: 0.01001\n",
      "Epoch: 116, train_loss: 0.93769, time: 0.00974\n",
      "Epoch: 117, train_loss: 0.93876, time: 0.00982\n",
      "Epoch: 118, train_loss: 0.93457, time: 0.01009\n",
      "Epoch: 119, train_loss: 0.93602, time: 0.00981\n",
      "Epoch: 120, train_loss: 0.93897, time: 0.00969\n",
      "Epoch: 121, train_loss: 0.93983, time: 0.01010\n",
      "Epoch: 122, train_loss: 0.93448, time: 0.01003\n",
      "Epoch: 123, train_loss: 0.93560, time: 0.00992\n",
      "Epoch: 124, train_loss: 0.93882, time: 0.00964\n",
      "Epoch: 125, train_loss: 0.93854, time: 0.01000\n",
      "Epoch: 126, train_loss: 0.93727, time: 0.00954\n",
      "Epoch: 127, train_loss: 0.93655, time: 0.01009\n",
      "Epoch: 128, train_loss: 0.93497, time: 0.00950\n",
      "Epoch: 129, train_loss: 0.93613, time: 0.01016\n",
      "Epoch: 130, train_loss: 0.94048, time: 0.00966\n",
      "Epoch: 131, train_loss: 0.93784, time: 0.01003\n",
      "Epoch: 132, train_loss: 0.93331, time: 0.00975\n",
      "Epoch: 133, train_loss: 0.93693, time: 0.01012\n",
      "Epoch: 134, train_loss: 0.93741, time: 0.00999\n",
      "Epoch: 135, train_loss: 0.93426, time: 0.00999\n",
      "Epoch: 136, train_loss: 0.93473, time: 0.01008\n",
      "Epoch: 137, train_loss: 0.93686, time: 0.00970\n",
      "Epoch: 138, train_loss: 0.93792, time: 0.01014\n",
      "Epoch: 139, train_loss: 0.93654, time: 0.00969\n",
      "Epoch: 140, train_loss: 0.93900, time: 0.01011\n",
      "Epoch: 141, train_loss: 0.93924, time: 0.00960\n",
      "Epoch: 142, train_loss: 0.93609, time: 0.01016\n",
      "Epoch: 143, train_loss: 0.93800, time: 0.00961\n",
      "Epoch: 144, train_loss: 0.93657, time: 0.01019\n",
      "Epoch: 145, train_loss: 0.93559, time: 0.00950\n",
      "Epoch: 146, train_loss: 0.93921, time: 0.00998\n",
      "Epoch: 147, train_loss: 0.93134, time: 0.00985\n",
      "Epoch: 148, train_loss: 0.93453, time: 0.00984\n",
      "Epoch: 149, train_loss: 0.93553, time: 0.00966\n",
      "Epoch: 150, train_loss: 0.93421, time: 0.01028\n",
      "Epoch: 151, train_loss: 0.93451, time: 0.00977\n",
      "Epoch: 152, train_loss: 0.93588, time: 0.00994\n",
      "Epoch: 153, train_loss: 0.93870, time: 0.00979\n",
      "Epoch: 154, train_loss: 0.93731, time: 0.01000\n",
      "Epoch: 155, train_loss: 0.93502, time: 0.00991\n",
      "Epoch: 156, train_loss: 0.93739, time: 0.01006\n",
      "Epoch: 157, train_loss: 0.93904, time: 0.00970\n",
      "Epoch: 158, train_loss: 0.94082, time: 0.00973\n",
      "Epoch: 159, train_loss: 0.93843, time: 0.00998\n",
      "Epoch: 160, train_loss: 0.93737, time: 0.00971\n",
      "Epoch: 161, train_loss: 0.93771, time: 0.01008\n",
      "Epoch: 162, train_loss: 0.94115, time: 0.00957\n",
      "Epoch: 163, train_loss: 0.93911, time: 0.01007\n",
      "Epoch: 164, train_loss: 0.93910, time: 0.00954\n",
      "Epoch: 165, train_loss: 0.93609, time: 0.01020\n",
      "Epoch: 166, train_loss: 0.93709, time: 0.00952\n",
      "Epoch: 167, train_loss: 0.93624, time: 0.00997\n",
      "Epoch: 168, train_loss: 0.93928, time: 0.00970\n",
      "Epoch: 169, train_loss: 0.93506, time: 0.01015\n",
      "Epoch: 170, train_loss: 0.93947, time: 0.00955\n",
      "Epoch: 171, train_loss: 0.93266, time: 0.01019\n",
      "Epoch: 172, train_loss: 0.94061, time: 0.00961\n",
      "Epoch: 173, train_loss: 0.93776, time: 0.00992\n",
      "Epoch: 174, train_loss: 0.94048, time: 0.00958\n",
      "Epoch: 175, train_loss: 0.93718, time: 0.00997\n",
      "Epoch: 176, train_loss: 0.93348, time: 0.00953\n",
      "Epoch: 177, train_loss: 0.93764, time: 0.00994\n",
      "Epoch: 178, train_loss: 0.93646, time: 0.01000\n",
      "Epoch: 179, train_loss: 0.93752, time: 0.00954\n",
      "Epoch: 180, train_loss: 0.93794, time: 0.00996\n",
      "Epoch: 181, train_loss: 0.93600, time: 0.00960\n",
      "Epoch: 182, train_loss: 0.93937, time: 0.00989\n",
      "Epoch: 183, train_loss: 0.93553, time: 0.00980\n",
      "Epoch: 184, train_loss: 0.93656, time: 0.01002\n",
      "Epoch: 185, train_loss: 0.93645, time: 0.00953\n",
      "Epoch: 186, train_loss: 0.93765, time: 0.01011\n",
      "Epoch: 187, train_loss: 0.94026, time: 0.00974\n",
      "Epoch: 188, train_loss: 0.94217, time: 0.01007\n",
      "Epoch: 189, train_loss: 0.93802, time: 0.00988\n",
      "Epoch: 190, train_loss: 0.93681, time: 0.00989\n",
      "Epoch: 191, train_loss: 0.93304, time: 0.00997\n",
      "Epoch: 192, train_loss: 0.93739, time: 0.01000\n",
      "Epoch: 193, train_loss: 0.93320, time: 0.01014\n",
      "Epoch: 194, train_loss: 0.93412, time: 0.00959\n",
      "Epoch: 195, train_loss: 0.93532, time: 0.00995\n",
      "Epoch: 196, train_loss: 0.93520, time: 0.00951\n",
      "Epoch: 197, train_loss: 0.93686, time: 0.01000\n",
      "Epoch: 198, train_loss: 0.92680, time: 0.00976\n",
      "Epoch: 199, train_loss: 0.93956, time: 0.00993\n",
      "Epoch: 200, train_loss: 0.93858, time: 0.00970\n",
      "pairwise precision 0.40239 recall 0.93273 f1 0.56223\n",
      "average until now [0.43213392629797254, 0.8296624082703791, 0.568277564556975]\n",
      "90 names 362.55568838119507 avg time 4.028396537568834\n",
      "Loading yin_wu dataset... path= /home/netdb/project/Name_Disambiguation_BERT/data/local/graph-10\n",
      "Dataset has 179 nodes, 1913 edges, 192 features.\n",
      "Epoch: 1, train_loss: 0.94851, time: 0.11037\n",
      "Epoch: 2, train_loss: 0.94010, time: 0.01227\n",
      "Epoch: 3, train_loss: 0.94014, time: 0.01051\n",
      "Epoch: 4, train_loss: 0.94451, time: 0.01048\n",
      "Epoch: 5, train_loss: 0.94360, time: 0.01084\n",
      "Epoch: 6, train_loss: 0.93994, time: 0.01075\n",
      "Epoch: 7, train_loss: 0.94232, time: 0.01075\n",
      "Epoch: 8, train_loss: 0.94104, time: 0.01070\n",
      "Epoch: 9, train_loss: 0.94364, time: 0.01062\n",
      "Epoch: 10, train_loss: 0.94461, time: 0.01035\n",
      "Epoch: 11, train_loss: 0.93713, time: 0.01056\n",
      "Epoch: 12, train_loss: 0.94158, time: 0.01055\n",
      "Epoch: 13, train_loss: 0.94400, time: 0.01057\n",
      "Epoch: 14, train_loss: 0.93943, time: 0.01061\n",
      "Epoch: 15, train_loss: 0.94118, time: 0.01066\n",
      "Epoch: 16, train_loss: 0.94080, time: 0.01061\n",
      "Epoch: 17, train_loss: 0.94182, time: 0.01022\n",
      "Epoch: 18, train_loss: 0.93967, time: 0.01050\n",
      "Epoch: 19, train_loss: 0.94164, time: 0.01044\n",
      "Epoch: 20, train_loss: 0.93845, time: 0.01076\n",
      "Epoch: 21, train_loss: 0.93854, time: 0.01074\n",
      "Epoch: 22, train_loss: 0.93725, time: 0.01065\n",
      "Epoch: 23, train_loss: 0.94320, time: 0.01096\n",
      "Epoch: 24, train_loss: 0.94357, time: 0.01062\n",
      "Epoch: 25, train_loss: 0.94390, time: 0.01062\n",
      "Epoch: 26, train_loss: 0.93528, time: 0.01072\n",
      "Epoch: 27, train_loss: 0.94123, time: 0.01037\n",
      "Epoch: 28, train_loss: 0.94161, time: 0.01038\n",
      "Epoch: 29, train_loss: 0.93723, time: 0.01067\n",
      "Epoch: 30, train_loss: 0.93968, time: 0.01031\n",
      "Epoch: 31, train_loss: 0.94051, time: 0.01067\n",
      "Epoch: 32, train_loss: 0.93940, time: 0.01054\n",
      "Epoch: 33, train_loss: 0.94508, time: 0.01065\n",
      "Epoch: 34, train_loss: 0.94190, time: 0.01064\n",
      "Epoch: 35, train_loss: 0.94543, time: 0.01063\n",
      "Epoch: 36, train_loss: 0.93730, time: 0.01071\n",
      "Epoch: 37, train_loss: 0.93847, time: 0.01039\n",
      "Epoch: 38, train_loss: 0.94561, time: 0.01037\n",
      "Epoch: 39, train_loss: 0.94042, time: 0.01048\n",
      "Epoch: 40, train_loss: 0.94410, time: 0.01068\n",
      "Epoch: 41, train_loss: 0.94082, time: 0.01040\n",
      "Epoch: 42, train_loss: 0.94210, time: 0.01048\n",
      "Epoch: 43, train_loss: 0.93678, time: 0.01048\n",
      "Epoch: 44, train_loss: 0.94052, time: 0.01033\n",
      "Epoch: 45, train_loss: 0.94221, time: 0.01060\n",
      "Epoch: 46, train_loss: 0.93815, time: 0.01057\n",
      "Epoch: 47, train_loss: 0.94205, time: 0.01012\n",
      "Epoch: 48, train_loss: 0.93985, time: 0.01043\n",
      "Epoch: 49, train_loss: 0.93948, time: 0.01027\n",
      "Epoch: 50, train_loss: 0.94405, time: 0.01015\n",
      "Epoch: 51, train_loss: 0.94034, time: 0.01037\n",
      "Epoch: 52, train_loss: 0.94256, time: 0.01057\n",
      "Epoch: 53, train_loss: 0.93679, time: 0.01026\n",
      "Epoch: 54, train_loss: 0.94214, time: 0.01021\n",
      "Epoch: 55, train_loss: 0.94559, time: 0.01043\n",
      "Epoch: 56, train_loss: 0.93798, time: 0.01057\n",
      "Epoch: 57, train_loss: 0.93703, time: 0.01064\n",
      "Epoch: 58, train_loss: 0.94023, time: 0.01034\n",
      "Epoch: 59, train_loss: 0.94142, time: 0.01056\n",
      "Epoch: 60, train_loss: 0.94385, time: 0.01062\n",
      "Epoch: 61, train_loss: 0.94194, time: 0.01074\n",
      "Epoch: 62, train_loss: 0.94004, time: 0.01042\n",
      "Epoch: 63, train_loss: 0.94048, time: 0.01067\n",
      "Epoch: 64, train_loss: 0.93772, time: 0.01031\n",
      "Epoch: 65, train_loss: 0.93759, time: 0.01073\n",
      "Epoch: 66, train_loss: 0.93742, time: 0.01037\n",
      "Epoch: 67, train_loss: 0.94014, time: 0.01045\n",
      "Epoch: 68, train_loss: 0.94034, time: 0.01066\n",
      "Epoch: 69, train_loss: 0.94032, time: 0.01027\n",
      "Epoch: 70, train_loss: 0.93860, time: 0.01031\n",
      "Epoch: 71, train_loss: 0.94012, time: 0.01072\n",
      "Epoch: 72, train_loss: 0.93946, time: 0.01022\n",
      "Epoch: 73, train_loss: 0.93861, time: 0.01060\n",
      "Epoch: 74, train_loss: 0.93754, time: 0.01049\n",
      "Epoch: 75, train_loss: 0.94455, time: 0.01051\n",
      "Epoch: 76, train_loss: 0.94592, time: 0.01057\n",
      "Epoch: 77, train_loss: 0.93800, time: 0.01023\n",
      "Epoch: 78, train_loss: 0.94034, time: 0.01059\n",
      "Epoch: 79, train_loss: 0.93836, time: 0.01042\n",
      "Epoch: 80, train_loss: 0.93680, time: 0.01041\n",
      "Epoch: 81, train_loss: 0.93770, time: 0.01054\n",
      "Epoch: 82, train_loss: 0.94141, time: 0.01056\n",
      "Epoch: 83, train_loss: 0.94092, time: 0.01053\n",
      "Epoch: 84, train_loss: 0.94086, time: 0.01017\n",
      "Epoch: 85, train_loss: 0.93969, time: 0.01062\n",
      "Epoch: 86, train_loss: 0.93858, time: 0.01046\n",
      "Epoch: 87, train_loss: 0.94466, time: 0.01053\n",
      "Epoch: 88, train_loss: 0.93724, time: 0.01042\n",
      "Epoch: 89, train_loss: 0.94019, time: 0.01029\n",
      "Epoch: 90, train_loss: 0.93877, time: 0.01028\n",
      "Epoch: 91, train_loss: 0.94277, time: 0.01059\n",
      "Epoch: 92, train_loss: 0.93934, time: 0.01053\n",
      "Epoch: 93, train_loss: 0.94209, time: 0.01048\n",
      "Epoch: 94, train_loss: 0.94037, time: 0.01065\n",
      "Epoch: 95, train_loss: 0.94001, time: 0.01028\n",
      "Epoch: 96, train_loss: 0.94217, time: 0.01056\n",
      "Epoch: 97, train_loss: 0.94003, time: 0.01044\n",
      "Epoch: 98, train_loss: 0.93937, time: 0.01046\n"
     ]
    }
   ],
   "source": [
    "names = load_test_names()\n",
    "wf = codecs.open(join(settings.OUT_DIR, 'local_clustering_results.csv'), 'w', encoding='utf-8')\n",
    "wf.write('name,n_pubs,n_clusters,precision,recall,f1\\n')\n",
    "metrics = np.zeros(3)\n",
    "cnt = 0\n",
    "for name in names:\n",
    "    cur_metric, num_nodes, n_clusters = gae_for_na(name)\n",
    "    wf.write('{0},{1},{2},{3:.5f},{4:.5f},{5:.5f}\\n'.format(\n",
    "        name, num_nodes, n_clusters, cur_metric[0], cur_metric[1], cur_metric[2]))\n",
    "    wf.flush()\n",
    "    for i, m in enumerate(cur_metric):\n",
    "        metrics[i] += m\n",
    "    cnt += 1\n",
    "    macro_prec = metrics[0] / cnt\n",
    "    macro_rec = metrics[1] / cnt\n",
    "    macro_f1 = eval_utils.cal_f1(macro_prec, macro_rec)\n",
    "    print('average until now', [macro_prec, macro_rec, macro_f1])\n",
    "    time_acc = time.time()-start_time\n",
    "    print(cnt, 'names', time_acc, 'avg time', time_acc/cnt)\n",
    "macro_prec = metrics[0] / cnt\n",
    "macro_rec = metrics[1] / cnt\n",
    "macro_f1 = eval_utils.cal_f1(macro_prec, macro_rec)\n",
    "wf.write('average,,,{0:.5f},{1:.5f},{2:.5f}\\n'.format(\n",
    "    macro_prec, macro_rec, macro_f1))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eaa572-0544-4ec7-abe5-c456094ebbad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
